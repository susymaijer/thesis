Starting at Mon May  2 10:57:57 CEST 2022
Running on hosts: res-hpc-lkeb05
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 8.
Account: div2-lkeb
Job ID: 10050024
Job name: NIHPancreasTrain
Node running script: res-hpc-lkeb05
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Mon May  2 10:58:02 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     Off  | 00000000:3B:00.0 Off |                  Off |
| 63%   62C    P0    80W / 260W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
nnUNet_raw_data_base = /exports/lkeb-hpc/smaijer/data/nnUNet_raw_data_base
nnUNet_preprocessed = /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed
RESULTS_FOLDER = /exports/lkeb-hpc/smaijer/results
Installing nnU-net..
Obtaining file:///home/smaijer/code/nnUNet
Requirement already satisfied: torch>1.10.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (1.11.0)
Requirement already satisfied: tqdm in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (4.64.0)
Requirement already satisfied: dicom2nifti in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (2.3.2)
Requirement already satisfied: scikit-image>=0.14 in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (0.19.2)
Requirement already satisfied: medpy in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (0.4.0)
Requirement already satisfied: scipy in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (1.8.0)
Requirement already satisfied: batchgenerators>=0.23 in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (0.23)
Requirement already satisfied: numpy in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (1.21.2)
Requirement already satisfied: sklearn in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (0.0)
Requirement already satisfied: SimpleITK in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (2.1.1)
Requirement already satisfied: pandas in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (1.4.2)
Requirement already satisfied: requests in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (2.27.1)
Requirement already satisfied: nibabel in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (3.2.2)
Requirement already satisfied: tifffile in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (2022.4.8)
Requirement already satisfied: matplotlib in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (3.5.1)
Requirement already satisfied: future in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)
Requirement already satisfied: scikit-learn in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.0.2)
Requirement already satisfied: unittest2 in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: pillow>=7.1.2 in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (9.0.1)
Requirement already satisfied: threadpoolctl in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.1.0)
Requirement already satisfied: packaging>=20.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (21.3)
Requirement already satisfied: networkx>=2.2 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.8)
Requirement already satisfied: imageio>=2.4.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.16.2)
Requirement already satisfied: PyWavelets>=1.1.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.3.0)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.conda/envs/nn/lib/python3.9/site-packages (from packaging>=20.0->scikit-image>=0.14->nnunet==1.7.0) (3.0.8)
Requirement already satisfied: typing_extensions in ./.conda/envs/nn/lib/python3.9/site-packages (from torch>1.10.0->nnunet==1.7.0) (4.1.1)
Requirement already satisfied: pydicom>=1.3.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from dicom2nifti->nnunet==1.7.0) (2.3.0)
Requirement already satisfied: kiwisolver>=1.0.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from matplotlib->nnunet==1.7.0) (1.4.2)
Requirement already satisfied: python-dateutil>=2.7 in ./.conda/envs/nn/lib/python3.9/site-packages (from matplotlib->nnunet==1.7.0) (2.8.2)
Requirement already satisfied: fonttools>=4.22.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from matplotlib->nnunet==1.7.0) (4.32.0)
Requirement already satisfied: cycler>=0.10 in ./.conda/envs/nn/lib/python3.9/site-packages (from matplotlib->nnunet==1.7.0) (0.11.0)
Requirement already satisfied: six>=1.5 in ./.conda/envs/nn/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->nnunet==1.7.0) (1.16.0)
Requirement already satisfied: setuptools in ./.conda/envs/nn/lib/python3.9/site-packages (from nibabel->nnunet==1.7.0) (58.0.4)
Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from pandas->nnunet==1.7.0) (2022.1)
Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/nn/lib/python3.9/site-packages (from requests->nnunet==1.7.0) (2021.10.8)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from requests->nnunet==1.7.0) (1.26.8)
Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/nn/lib/python3.9/site-packages (from requests->nnunet==1.7.0) (3.3)
Requirement already satisfied: charset-normalizer~=2.0.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from requests->nnunet==1.7.0) (2.0.4)
Requirement already satisfied: joblib>=0.11 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: traceback2 in ./.conda/envs/nn/lib/python3.9/site-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)
Requirement already satisfied: linecache2 in ./.conda/envs/nn/lib/python3.9/site-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)
Installing collected packages: argparse, nnunet
  Attempting uninstall: nnunet
    Found existing installation: nnunet 1.7.0
    Uninstalling nnunet-1.7.0:
      Successfully uninstalled nnunet-1.7.0
  Running setup.py develop for nnunet
Successfully installed argparse-1.4.0 nnunet-1.7.0


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

###############################################
I am running the following nnUNet: 3d_lowres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 0 from these plans
I am using sample dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task500_NIH_Pancreas/nnUNetData_plans_v2.1
###############################################
loading dataset
loading all case properties
2022-05-02 10:58:16.834287: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task500_NIH_Pancreas/splits_final.pkl
2022-05-02 10:58:16.843225: The split file contains 5 splits.
2022-05-02 10:58:16.845475: Desired fold for training: 2
2022-05-02 10:58:16.847435: This split has 64 training and 16 validation cases.
unpacking dataset
done
2022-05-02 10:58:20.423565: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_lowres/Task500_NIH_Pancreas/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_latest.model train= True
2022-05-02 10:58:20.668771: lr: 0.001259
using pin_memory on device 0
using pin_memory on device 0
2022-05-02 10:58:27.919647: Unable to plot network architecture:
2022-05-02 10:58:27.924915: No module named 'hiddenlayer'
2022-05-02 10:58:27.951292: 
printing the network instead:

2022-05-02 10:58:27.973308: Generic_UNet(
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose3d(320, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(320, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (4): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-05-02 10:58:28.002148: 

2022-05-02 10:58:28.019387: 
epoch:  450
2022-05-02 11:00:24.709971: train loss : -0.9000
2022-05-02 11:00:31.500496: validation loss: -0.8422
2022-05-02 11:00:31.529768: Average global foreground Dice: [0.8626]
2022-05-02 11:00:31.550037: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:00:32.549364: lr: 0.001236
2022-05-02 11:00:32.571284: This epoch took 124.528968 s

2022-05-02 11:00:32.588453: 
epoch:  451
2022-05-02 11:02:08.839463: train loss : -0.8994
2022-05-02 11:02:15.344348: validation loss: -0.8483
2022-05-02 11:02:15.347880: Average global foreground Dice: [0.8661]
2022-05-02 11:02:15.350201: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:02:15.826876: lr: 0.001214
2022-05-02 11:02:15.829351: This epoch took 103.218343 s

2022-05-02 11:02:15.831572: 
epoch:  452
2022-05-02 11:03:50.361897: train loss : -0.8999
2022-05-02 11:03:57.417605: validation loss: -0.8435
2022-05-02 11:03:57.447758: Average global foreground Dice: [0.8619]
2022-05-02 11:03:57.469322: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:03:58.085232: lr: 0.001191
2022-05-02 11:03:58.106428: This epoch took 102.272851 s

2022-05-02 11:03:58.126295: 
epoch:  453
2022-05-02 11:05:38.078843: train loss : -0.8990
2022-05-02 11:05:44.738085: validation loss: -0.8415
2022-05-02 11:05:44.770883: Average global foreground Dice: [0.8652]
2022-05-02 11:05:44.780799: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:05:45.826011: lr: 0.001168
2022-05-02 11:05:45.847347: This epoch took 107.701018 s

2022-05-02 11:05:45.870308: 
epoch:  454
2022-05-02 11:07:22.118253: train loss : -0.9003
2022-05-02 11:07:29.266548: validation loss: -0.8456
2022-05-02 11:07:29.311640: Average global foreground Dice: [0.8666]
2022-05-02 11:07:29.334313: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:07:30.169825: lr: 0.001145
2022-05-02 11:07:30.196505: This epoch took 104.303111 s

2022-05-02 11:07:30.219293: 
epoch:  455
2022-05-02 11:09:06.436748: train loss : -0.9009
2022-05-02 11:09:13.285351: validation loss: -0.8394
2022-05-02 11:09:13.317034: Average global foreground Dice: [0.8616]
2022-05-02 11:09:13.321870: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:09:14.247418: lr: 0.001122
2022-05-02 11:09:14.267324: This epoch took 104.011020 s

2022-05-02 11:09:14.289320: 
epoch:  456
2022-05-02 11:10:54.803174: train loss : -0.8986
2022-05-02 11:11:02.378733: validation loss: -0.8531
2022-05-02 11:11:02.410683: Average global foreground Dice: [0.8705]
2022-05-02 11:11:02.437314: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:11:03.222363: lr: 0.001099
2022-05-02 11:11:03.248330: This epoch took 108.938740 s

2022-05-02 11:11:03.279290: 
epoch:  457
2022-05-02 11:12:45.420367: train loss : -0.8987
2022-05-02 11:12:52.556766: validation loss: -0.8490
2022-05-02 11:12:52.598873: Average global foreground Dice: [0.867]
2022-05-02 11:12:52.631355: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:12:53.551754: lr: 0.001076
2022-05-02 11:12:53.567430: This epoch took 110.265116 s

2022-05-02 11:12:53.606308: 
epoch:  458
2022-05-02 11:14:29.707603: train loss : -0.8979
2022-05-02 11:14:36.613666: validation loss: -0.8381
2022-05-02 11:14:36.631812: Average global foreground Dice: [0.8571]
2022-05-02 11:14:36.658352: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:14:37.451942: lr: 0.001053
2022-05-02 11:14:37.483362: This epoch took 103.854992 s

2022-05-02 11:14:37.507302: 
epoch:  459
2022-05-02 11:16:13.451684: train loss : -0.8989
2022-05-02 11:16:20.773366: validation loss: -0.8437
2022-05-02 11:16:20.799687: Average global foreground Dice: [0.8642]
2022-05-02 11:16:20.819321: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:16:21.473691: lr: 0.00103
2022-05-02 11:16:21.504602: This epoch took 103.977294 s

2022-05-02 11:16:21.542306: 
epoch:  460
2022-05-02 11:18:01.803761: train loss : -0.8924
2022-05-02 11:18:08.759327: validation loss: -0.8399
2022-05-02 11:18:08.792815: Average global foreground Dice: [0.8597]
2022-05-02 11:18:08.814300: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:18:09.636717: lr: 0.001007
2022-05-02 11:18:09.660752: This epoch took 108.085042 s

2022-05-02 11:18:09.693461: 
epoch:  461
2022-05-02 11:19:45.887275: train loss : -0.9000
2022-05-02 11:19:52.991886: validation loss: -0.8405
2022-05-02 11:19:53.019806: Average global foreground Dice: [0.8614]
2022-05-02 11:19:53.041307: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:19:53.896715: lr: 0.000983
2022-05-02 11:19:53.916382: This epoch took 104.202832 s

2022-05-02 11:19:53.938296: 
epoch:  462
2022-05-02 11:21:35.849675: train loss : -0.8927
2022-05-02 11:21:42.590804: validation loss: -0.8499
2022-05-02 11:21:42.624713: Average global foreground Dice: [0.8674]
2022-05-02 11:21:42.666302: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:21:43.638680: lr: 0.00096
2022-05-02 11:21:43.669348: This epoch took 109.709000 s

2022-05-02 11:21:43.702298: 
epoch:  463
2022-05-02 11:23:24.010987: train loss : -0.8876
2022-05-02 11:23:31.534656: validation loss: -0.8468
2022-05-02 11:23:31.564635: Average global foreground Dice: [0.8684]
2022-05-02 11:23:31.587300: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:23:32.555954: lr: 0.000937
2022-05-02 11:23:32.589330: This epoch took 108.865023 s

2022-05-02 11:23:32.616328: 
epoch:  464
2022-05-02 11:25:08.777111: train loss : -0.8981
2022-05-02 11:25:16.279851: validation loss: -0.8435
2022-05-02 11:25:16.303636: Average global foreground Dice: [0.8642]
2022-05-02 11:25:16.341380: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:25:17.369906: lr: 0.000913
2022-05-02 11:25:17.391408: This epoch took 104.749109 s

2022-05-02 11:25:17.411514: 
epoch:  465
2022-05-02 11:26:54.475889: train loss : -0.8981
2022-05-02 11:27:01.495610: validation loss: -0.8539
2022-05-02 11:27:01.538708: Average global foreground Dice: [0.8725]
2022-05-02 11:27:01.560313: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:27:02.504946: lr: 0.00089
2022-05-02 11:27:02.540343: This epoch took 105.083478 s

2022-05-02 11:27:02.558301: 
epoch:  466
2022-05-02 11:28:39.279005: train loss : -0.8991
2022-05-02 11:28:46.099256: validation loss: -0.8340
2022-05-02 11:28:46.130829: Average global foreground Dice: [0.8534]
2022-05-02 11:28:46.152333: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:28:46.931952: lr: 0.000866
2022-05-02 11:28:46.972313: This epoch took 104.347603 s

2022-05-02 11:28:46.991065: 
epoch:  467
2022-05-02 11:30:23.516642: train loss : -0.8939
2022-05-02 11:30:30.722976: validation loss: -0.8432
2022-05-02 11:30:30.750662: Average global foreground Dice: [0.8633]
2022-05-02 11:30:30.768291: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:30:31.574662: lr: 0.000842
2022-05-02 11:30:31.595332: This epoch took 104.576038 s

2022-05-02 11:30:31.618308: 
epoch:  468
2022-05-02 11:32:12.155318: train loss : -0.8973
2022-05-02 11:32:19.839763: validation loss: -0.8494
2022-05-02 11:32:19.860445: Average global foreground Dice: [0.8681]
2022-05-02 11:32:19.880311: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:32:20.929035: lr: 0.000819
2022-05-02 11:32:20.960345: This epoch took 109.310030 s

2022-05-02 11:32:20.993313: 
epoch:  469
2022-05-02 11:33:58.114403: train loss : -0.8938
2022-05-02 11:34:04.749451: validation loss: -0.8286
2022-05-02 11:34:04.768692: Average global foreground Dice: [0.8509]
2022-05-02 11:34:04.787097: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:34:05.671831: lr: 0.000795
2022-05-02 11:34:05.694364: This epoch took 104.677895 s

2022-05-02 11:34:05.716292: 
epoch:  470
2022-05-02 11:35:41.878268: train loss : -0.8997
2022-05-02 11:35:48.890111: validation loss: -0.8455
2022-05-02 11:35:48.918765: Average global foreground Dice: [0.8634]
2022-05-02 11:35:48.930349: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:35:49.637878: lr: 0.000771
2022-05-02 11:35:49.669356: This epoch took 103.923928 s

2022-05-02 11:35:49.702294: 
epoch:  471
2022-05-02 11:37:28.638322: train loss : -0.8973
2022-05-02 11:37:35.703766: validation loss: -0.8497
2022-05-02 11:37:35.734710: Average global foreground Dice: [0.8671]
2022-05-02 11:37:35.750814: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:37:36.780322: lr: 0.000747
2022-05-02 11:37:36.811373: This epoch took 107.086075 s

2022-05-02 11:37:36.834306: 
epoch:  472
2022-05-02 11:39:17.119356: train loss : -0.8975
2022-05-02 11:39:24.305872: validation loss: -0.8418
2022-05-02 11:39:24.334918: Average global foreground Dice: [0.8611]
2022-05-02 11:39:24.352314: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:39:25.343624: lr: 0.000723
2022-05-02 11:39:25.377343: This epoch took 108.510028 s

2022-05-02 11:39:25.404294: 
epoch:  473
2022-05-02 11:41:01.791054: train loss : -0.8988
2022-05-02 11:41:09.105259: validation loss: -0.8542
2022-05-02 11:41:09.133394: Average global foreground Dice: [0.8715]
2022-05-02 11:41:09.147313: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:41:09.689905: lr: 0.000699
2022-05-02 11:41:09.727346: This epoch took 104.307030 s

2022-05-02 11:41:09.758292: 
epoch:  474
2022-05-02 11:42:46.669956: train loss : -0.8973
2022-05-02 11:42:53.360217: validation loss: -0.8454
2022-05-02 11:42:53.363029: Average global foreground Dice: [0.8628]
2022-05-02 11:42:53.365511: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:42:53.812247: lr: 0.000675
2022-05-02 11:42:53.814428: This epoch took 104.034132 s

2022-05-02 11:42:53.816291: 
epoch:  475
2022-05-02 11:44:29.240986: train loss : -0.9002
2022-05-02 11:44:35.995633: validation loss: -0.8513
2022-05-02 11:44:35.998509: Average global foreground Dice: [0.8679]
2022-05-02 11:44:36.000397: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:44:36.448066: lr: 0.00065
2022-05-02 11:44:36.450383: This epoch took 102.632054 s

2022-05-02 11:44:36.452403: 
epoch:  476
2022-05-02 11:46:11.409010: train loss : -0.9017
2022-05-02 11:46:18.065942: validation loss: -0.8455
2022-05-02 11:46:18.069285: Average global foreground Dice: [0.8649]
2022-05-02 11:46:18.071405: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:46:18.510642: lr: 0.000626
2022-05-02 11:46:18.512987: This epoch took 102.058564 s

2022-05-02 11:46:18.515135: 
epoch:  477
2022-05-02 11:47:54.154185: train loss : -0.9028
2022-05-02 11:48:01.223221: validation loss: -0.8463
2022-05-02 11:48:01.253698: Average global foreground Dice: [0.8663]
2022-05-02 11:48:01.276253: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:48:01.940143: lr: 0.000601
2022-05-02 11:48:01.961336: This epoch took 103.444076 s

2022-05-02 11:48:01.979306: 
epoch:  478
2022-05-02 11:49:41.279830: train loss : -0.8982
2022-05-02 11:49:48.501483: validation loss: -0.8489
2022-05-02 11:49:48.522759: Average global foreground Dice: [0.8665]
2022-05-02 11:49:48.560332: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:49:49.386798: lr: 0.000577
2022-05-02 11:49:49.425331: This epoch took 107.424032 s

2022-05-02 11:49:49.458308: 
epoch:  479
2022-05-02 11:51:26.425289: train loss : -0.8970
2022-05-02 11:51:33.376669: validation loss: -0.8410
2022-05-02 11:51:33.404490: Average global foreground Dice: [0.8594]
2022-05-02 11:51:33.448013: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:51:34.703335: lr: 0.000552
2022-05-02 11:51:34.734339: This epoch took 105.242048 s

2022-05-02 11:51:34.772300: 
epoch:  480
2022-05-02 11:53:14.990140: train loss : -0.8978
2022-05-02 11:53:22.145234: validation loss: -0.8534
2022-05-02 11:53:22.174847: Average global foreground Dice: [0.8698]
2022-05-02 11:53:22.207312: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:53:22.863734: lr: 0.000527
2022-05-02 11:53:22.880434: This epoch took 108.079138 s

2022-05-02 11:53:22.913382: 
epoch:  481
2022-05-02 11:55:00.980356: train loss : -0.8971
2022-05-02 11:55:08.224433: validation loss: -0.8431
2022-05-02 11:55:08.240663: Average global foreground Dice: [0.8637]
2022-05-02 11:55:08.258260: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:55:09.200182: lr: 0.000502
2022-05-02 11:55:09.226337: This epoch took 106.285329 s

2022-05-02 11:55:09.258459: 
epoch:  482
2022-05-02 11:56:47.766121: train loss : -0.8998
2022-05-02 11:56:54.490010: validation loss: -0.8566
2022-05-02 11:56:54.516345: Average global foreground Dice: [0.8749]
2022-05-02 11:56:54.542578: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:56:55.366537: lr: 0.000477
2022-05-02 11:56:55.435851: saving checkpoint...
2022-05-02 11:56:57.056338: done, saving took 1.67 seconds
2022-05-02 11:56:57.119302: This epoch took 107.838847 s

2022-05-02 11:56:57.151291: 
epoch:  483
2022-05-02 11:58:37.598750: train loss : -0.8997
2022-05-02 11:58:44.584918: validation loss: -0.8378
2022-05-02 11:58:44.612785: Average global foreground Dice: [0.8578]
2022-05-02 11:58:44.647313: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 11:58:45.334858: lr: 0.000451
2022-05-02 11:58:45.357332: This epoch took 108.174036 s

2022-05-02 11:58:45.380292: 
epoch:  484
2022-05-02 12:00:24.509677: train loss : -0.8995
2022-05-02 12:00:31.687186: validation loss: -0.8553
2022-05-02 12:00:31.709954: Average global foreground Dice: [0.8718]
2022-05-02 12:00:31.729321: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:00:32.564980: lr: 0.000426
2022-05-02 12:00:32.586329: This epoch took 107.185959 s

2022-05-02 12:00:32.608330: 
epoch:  485
2022-05-02 12:02:12.348142: train loss : -0.9013
2022-05-02 12:02:19.394045: validation loss: -0.8474
2022-05-02 12:02:19.428706: Average global foreground Dice: [0.8661]
2022-05-02 12:02:19.450325: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:02:20.474100: lr: 0.0004
2022-05-02 12:02:20.500389: This epoch took 107.871967 s

2022-05-02 12:02:20.521322: 
epoch:  486
2022-05-02 12:04:00.841500: train loss : -0.8969
2022-05-02 12:04:07.681488: validation loss: -0.8533
2022-05-02 12:04:07.726696: Average global foreground Dice: [0.8715]
2022-05-02 12:04:07.757341: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:04:08.666471: lr: 0.000375
2022-05-02 12:04:08.779756: saving checkpoint...
2022-05-02 12:04:10.176520: done, saving took 1.48 seconds
2022-05-02 12:04:10.229012: This epoch took 109.652676 s

2022-05-02 12:04:10.234912: 
epoch:  487
2022-05-02 12:05:47.229220: train loss : -0.8932
2022-05-02 12:05:54.824892: validation loss: -0.8445
2022-05-02 12:05:54.855866: Average global foreground Dice: [0.8644]
2022-05-02 12:05:54.876301: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:05:55.933448: lr: 0.000348
2022-05-02 12:05:55.961581: This epoch took 105.705364 s

2022-05-02 12:05:55.981667: 
epoch:  488
2022-05-02 12:07:31.988569: train loss : -0.8941
2022-05-02 12:07:39.138280: validation loss: -0.8453
2022-05-02 12:07:39.167746: Average global foreground Dice: [0.8655]
2022-05-02 12:07:39.187497: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:07:40.124118: lr: 0.000322
2022-05-02 12:07:40.145363: This epoch took 104.145058 s

2022-05-02 12:07:40.170794: 
epoch:  489
2022-05-02 12:09:21.744087: train loss : -0.9002
2022-05-02 12:09:29.694796: validation loss: -0.8396
2022-05-02 12:09:29.723438: Average global foreground Dice: [0.8635]
2022-05-02 12:09:29.765308: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:09:30.691559: lr: 0.000296
2022-05-02 12:09:30.721645: This epoch took 110.517350 s

2022-05-02 12:09:30.758647: 
epoch:  490
2022-05-02 12:11:11.095710: train loss : -0.9005
2022-05-02 12:11:17.984386: validation loss: -0.8492
2022-05-02 12:11:17.994422: Average global foreground Dice: [0.8683]
2022-05-02 12:11:18.022251: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:11:18.828354: lr: 0.000269
2022-05-02 12:11:18.831511: This epoch took 108.034873 s

2022-05-02 12:11:18.850510: 
epoch:  491
2022-05-02 12:12:54.738273: train loss : -0.8981
2022-05-02 12:13:01.866390: validation loss: -0.8476
2022-05-02 12:13:01.894177: Average global foreground Dice: [0.8672]
2022-05-02 12:13:01.912689: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:13:02.825173: lr: 0.000242
2022-05-02 12:13:02.860381: This epoch took 104.003407 s

2022-05-02 12:13:02.897296: 
epoch:  492
2022-05-02 12:14:39.086742: train loss : -0.8973
2022-05-02 12:14:46.426413: validation loss: -0.8466
2022-05-02 12:14:46.455702: Average global foreground Dice: [0.8636]
2022-05-02 12:14:46.488295: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:14:47.040715: lr: 0.000215
2022-05-02 12:14:47.063335: This epoch took 104.131031 s

2022-05-02 12:14:47.085293: 
epoch:  493
2022-05-02 12:16:23.019880: train loss : -0.9006
2022-05-02 12:16:29.915649: validation loss: -0.8496
2022-05-02 12:16:29.944968: Average global foreground Dice: [0.8664]
2022-05-02 12:16:29.964643: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:16:30.921222: lr: 0.000187
2022-05-02 12:16:30.939337: This epoch took 103.822040 s

2022-05-02 12:16:30.961343: 
epoch:  494
2022-05-02 12:18:08.556767: train loss : -0.8964
2022-05-02 12:18:16.182959: validation loss: -0.8455
2022-05-02 12:18:16.218777: Average global foreground Dice: [0.864]
2022-05-02 12:18:16.236837: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:18:17.617876: lr: 0.000158
2022-05-02 12:18:17.640414: This epoch took 106.657118 s

2022-05-02 12:18:17.663328: 
epoch:  495
2022-05-02 12:19:54.154362: train loss : -0.9005
2022-05-02 12:20:01.090761: validation loss: -0.8436
2022-05-02 12:20:01.136702: Average global foreground Dice: [0.8629]
2022-05-02 12:20:01.146357: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:20:02.254884: lr: 0.00013
2022-05-02 12:20:02.287334: This epoch took 104.606030 s

2022-05-02 12:20:02.322311: 
epoch:  496
2022-05-02 12:21:43.743323: train loss : -0.9002
2022-05-02 12:21:51.369370: validation loss: -0.8476
2022-05-02 12:21:51.393722: Average global foreground Dice: [0.8649]
2022-05-02 12:21:51.418890: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:21:52.527924: lr: 0.0001
2022-05-02 12:21:52.565336: This epoch took 110.205042 s

2022-05-02 12:21:52.588303: 
epoch:  497
2022-05-02 12:23:35.866308: train loss : -0.8969
2022-05-02 12:23:42.900213: validation loss: -0.8545
2022-05-02 12:23:42.923736: Average global foreground Dice: [0.8724]
2022-05-02 12:23:42.960317: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:23:43.900734: lr: 6.9e-05
2022-05-02 12:23:43.933327: This epoch took 111.331435 s

2022-05-02 12:23:43.956297: 
epoch:  498
2022-05-02 12:25:26.904029: train loss : -0.8984
2022-05-02 12:25:33.695632: validation loss: -0.8478
2022-05-02 12:25:33.719831: Average global foreground Dice: [0.8681]
2022-05-02 12:25:33.739438: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:25:34.520667: lr: 3.7e-05
2022-05-02 12:25:34.629440: saving checkpoint...
2022-05-02 12:25:36.023362: done, saving took 1.47 seconds
2022-05-02 12:25:36.082321: This epoch took 112.103000 s

2022-05-02 12:25:36.105305: 
epoch:  499
2022-05-02 12:27:15.221908: train loss : -0.8996
2022-05-02 12:27:22.853535: validation loss: -0.8359
2022-05-02 12:27:22.885725: Average global foreground Dice: [0.8579]
2022-05-02 12:27:22.896874: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-05-02 12:27:23.601126: lr: 0.0
2022-05-02 12:27:23.633314: saving scheduled checkpoint file...
2022-05-02 12:27:23.728400: saving checkpoint...
2022-05-02 12:27:25.016345: done, saving took 1.36 seconds
2022-05-02 12:27:25.144516: done
2022-05-02 12:27:25.190130: This epoch took 109.053391 s

2022-05-02 12:27:25.293234: saving checkpoint...
2022-05-02 12:27:26.395763: done, saving took 1.17 seconds
panc_0009 (2, 109, 284, 284)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 109, 284, 284)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 29], [0, 92], [0, 62, 124]]
number of tiles: 12
computing Gaussian
done
prediction done
panc_0013 (2, 103, 246, 246)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 103, 246, 246)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 23], [0, 54], [0, 43, 86]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0017 (2, 172, 259, 259)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 172, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 61, 92], [0, 67], [0, 50, 99]]
number of tiles: 24
using precomputed Gaussian
prediction done
panc_0020 (2, 121, 284, 284)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 121, 284, 284)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 20, 41], [0, 92], [0, 62, 124]]
number of tiles: 18
using precomputed Gaussian
prediction done
panc_0026 (2, 172, 285, 285)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 172, 285, 285)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 61, 92], [0, 93], [0, 62, 125]]
number of tiles: 24
using precomputed Gaussian
prediction done
panc_0034 (2, 114, 259, 259)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 114, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34], [0, 67], [0, 50, 99]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0042 (2, 172, 226, 226)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 172, 226, 226)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 61, 92], [0, 34], [0, 66]]
number of tiles: 16
using precomputed Gaussian
prediction done
panc_0044 (2, 130, 314, 314)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 130, 314, 314)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 25, 50], [0, 61, 122], [0, 77, 154]]
number of tiles: 27
using precomputed Gaussian
prediction done
panc_0046 (2, 110, 259, 259)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 110, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30], [0, 67], [0, 50, 99]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0048 (2, 115, 311, 311)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 115, 311, 311)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35], [0, 60, 119], [0, 76, 151]]
number of tiles: 18
using precomputed Gaussian
prediction done
panc_0052 (2, 116, 259, 259)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 116, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36], [0, 67], [0, 50, 99]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0055 (2, 107, 226, 226)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 107, 226, 226)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 27], [0, 34], [0, 66]]
number of tiles: 8
using precomputed Gaussian
prediction done
panc_0064 (2, 118, 298, 298)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 118, 298, 298)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38], [0, 53, 106], [0, 69, 138]]
number of tiles: 18
using precomputed Gaussian
prediction done
panc_0072 (2, 113, 259, 259)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 113, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33], [0, 67], [0, 50, 99]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0075 (2, 125, 298, 298)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 125, 298, 298)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 22, 45], [0, 53, 106], [0, 69, 138]]
number of tiles: 27
using precomputed Gaussian
prediction done
panc_0078 (2, 172, 259, 259)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 172, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 61, 92], [0, 67], [0, 50, 99]]
number of tiles: 24
using precomputed Gaussian
prediction done
2022-05-02 12:30:03.957858: finished prediction
2022-05-02 12:30:03.960994: evaluation of raw predictions
2022-05-02 12:30:15.925857: determining postprocessing
Foreground vs background
before: 0.8609305063573129
after:  0.8609430929278202
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
done
predicting segmentations for the next stage of the cascade
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
no separate z, order 1
panc_0009
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 109, 284, 284)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 29], [0, 92], [0, 62, 124]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0013
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 103, 246, 246)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 23], [0, 54], [0, 43, 86]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0017
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 172, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 61, 92], [0, 67], [0, 50, 99]]
number of tiles: 24
using precomputed Gaussian
prediction done
panc_0020
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 121, 284, 284)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 20, 41], [0, 92], [0, 62, 124]]
number of tiles: 18
using precomputed Gaussian
prediction done
panc_0026
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 172, 285, 285)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 61, 92], [0, 93], [0, 62, 125]]
number of tiles: 24
using precomputed Gaussian
prediction done
panc_0034
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 114, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34], [0, 67], [0, 50, 99]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0042
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 172, 226, 226)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 61, 92], [0, 34], [0, 66]]
number of tiles: 16
using precomputed Gaussian
prediction done
panc_0044
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 130, 314, 314)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 25, 50], [0, 61, 122], [0, 77, 154]]
number of tiles: 27
using precomputed Gaussian
prediction done
panc_0046
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 110, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30], [0, 67], [0, 50, 99]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0048
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 115, 311, 311)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35], [0, 60, 119], [0, 76, 151]]
number of tiles: 18
using precomputed Gaussian
prediction done
panc_0052
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 116, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36], [0, 67], [0, 50, 99]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0055
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 107, 226, 226)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 27], [0, 34], [0, 66]]
number of tiles: 8
using precomputed Gaussian
prediction done
panc_0064
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 118, 298, 298)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38], [0, 53, 106], [0, 69, 138]]
number of tiles: 18
using precomputed Gaussian
prediction done
panc_0072
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 113, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33], [0, 67], [0, 50, 99]]
number of tiles: 12
using precomputed Gaussian
prediction done
panc_0075
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 125, 298, 298)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 22, 45], [0, 53, 106], [0, 69, 138]]
number of tiles: 27
using precomputed Gaussian
prediction done
panc_0078
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 172, 259, 259)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 61, 92], [0, 67], [0, 50, 99]]
number of tiles: 24
using precomputed Gaussian
prediction done
Program finished with exit code 0 at: Mon May  2 10:57:57 CEST 2022
