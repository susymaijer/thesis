Starting at Tue Apr 26 07:42:45 CEST 2022
Running on hosts: res-hpc-lkeb07
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 8.
Account: div2-lkeb
Job ID: 10036872
Job name: NIHPancreasTrain
Node running script: res-hpc-lkeb07
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Tue Apr 26 07:42:47 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     Off  | 00000000:3B:00.0 Off |                  Off |
| 31%   37C    P0    57W / 260W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
nnUNet_raw_data_base = /exports/lkeb-hpc/smaijer/data/nnUNet_raw_data_base
nnUNet_preprocessed = /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed
RESULTS_FOLDER = /exports/lkeb-hpc/smaijer/results
Installing nnU-net..
Obtaining file:///home/smaijer/code/nnUNet
Requirement already satisfied: torch>1.10.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (1.11.0)
Requirement already satisfied: tqdm in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (4.64.0)
Requirement already satisfied: dicom2nifti in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (2.3.2)
Requirement already satisfied: scikit-image>=0.14 in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (0.19.2)
Requirement already satisfied: medpy in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (0.4.0)
Requirement already satisfied: scipy in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (1.8.0)
Requirement already satisfied: batchgenerators>=0.23 in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (0.23)
Requirement already satisfied: numpy in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (1.21.2)
Requirement already satisfied: sklearn in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (0.0)
Requirement already satisfied: SimpleITK in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (2.1.1)
Requirement already satisfied: pandas in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (1.4.2)
Requirement already satisfied: requests in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (2.27.1)
Requirement already satisfied: nibabel in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (3.2.2)
Requirement already satisfied: tifffile in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (2022.4.8)
Requirement already satisfied: matplotlib in ./.conda/envs/nn/lib/python3.9/site-packages (from nnunet==1.7.0) (3.5.1)
Requirement already satisfied: scikit-learn in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.0.2)
Requirement already satisfied: threadpoolctl in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.1.0)
Requirement already satisfied: unittest2 in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: future in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)
Requirement already satisfied: pillow>=7.1.2 in ./.conda/envs/nn/lib/python3.9/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (9.0.1)
Requirement already satisfied: PyWavelets>=1.1.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.3.0)
Requirement already satisfied: networkx>=2.2 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.8)
Requirement already satisfied: imageio>=2.4.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.16.2)
Requirement already satisfied: packaging>=20.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (21.3)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./.conda/envs/nn/lib/python3.9/site-packages (from packaging>=20.0->scikit-image>=0.14->nnunet==1.7.0) (3.0.8)
Requirement already satisfied: typing_extensions in ./.conda/envs/nn/lib/python3.9/site-packages (from torch>1.10.0->nnunet==1.7.0) (4.1.1)
Requirement already satisfied: pydicom>=1.3.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from dicom2nifti->nnunet==1.7.0) (2.3.0)
Requirement already satisfied: kiwisolver>=1.0.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from matplotlib->nnunet==1.7.0) (1.4.2)
Requirement already satisfied: cycler>=0.10 in ./.conda/envs/nn/lib/python3.9/site-packages (from matplotlib->nnunet==1.7.0) (0.11.0)
Requirement already satisfied: python-dateutil>=2.7 in ./.conda/envs/nn/lib/python3.9/site-packages (from matplotlib->nnunet==1.7.0) (2.8.2)
Requirement already satisfied: fonttools>=4.22.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from matplotlib->nnunet==1.7.0) (4.32.0)
Requirement already satisfied: six>=1.5 in ./.conda/envs/nn/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->nnunet==1.7.0) (1.16.0)
Requirement already satisfied: setuptools in ./.conda/envs/nn/lib/python3.9/site-packages (from nibabel->nnunet==1.7.0) (58.0.4)
Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from pandas->nnunet==1.7.0) (2022.1)
Requirement already satisfied: charset-normalizer~=2.0.0 in ./.conda/envs/nn/lib/python3.9/site-packages (from requests->nnunet==1.7.0) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/nn/lib/python3.9/site-packages (from requests->nnunet==1.7.0) (3.3)
Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/nn/lib/python3.9/site-packages (from requests->nnunet==1.7.0) (2021.10.8)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.conda/envs/nn/lib/python3.9/site-packages (from requests->nnunet==1.7.0) (1.26.8)
Requirement already satisfied: joblib>=0.11 in ./.conda/envs/nn/lib/python3.9/site-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: traceback2 in ./.conda/envs/nn/lib/python3.9/site-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: linecache2 in ./.conda/envs/nn/lib/python3.9/site-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)
Installing collected packages: argparse, nnunet
  Attempting uninstall: nnunet
    Found existing installation: nnunet 1.7.0
    Uninstalling nnunet-1.7.0:
      Successfully uninstalled nnunet-1.7.0
  Running setup.py develop for nnunet
Successfully installed argparse-1.4.0 nnunet-1.7.0


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

###############################################
I am running the following nnUNet: 3d_lowres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 0 from these plans
I am using sample dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task500_NIH_Pancreas/nnUNetData_plans_v2.1
###############################################
loading dataset
loading all case properties
2022-04-26 07:43:17.630292: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task500_NIH_Pancreas/splits_final.pkl
2022-04-26 07:43:17.641659: The split file contains 5 splits.
2022-04-26 07:43:17.644135: Desired fold for training: 2
2022-04-26 07:43:17.646289: This split has 64 training and 16 validation cases.
unpacking dataset
done
2022-04-26 07:43:21.033002: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_lowres/Task500_NIH_Pancreas/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_latest.model train= True
2022-04-26 07:43:31.981631: lr: 0.004384
using pin_memory on device 0
using pin_memory on device 0
2022-04-26 07:43:40.183785: Unable to plot network architecture:
2022-04-26 07:43:40.200967: No module named 'hiddenlayer'
2022-04-26 07:43:40.248703: 
printing the network instead:

2022-04-26 07:43:40.251167: Generic_UNet(
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose3d(320, 320, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (4): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(320, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (4): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-04-26 07:43:40.269983: 

2022-04-26 07:43:40.295225: 
epoch:  300
2022-04-26 07:45:29.271969: train loss : -0.8442
2022-04-26 07:45:35.981125: validation loss: -0.8202
2022-04-26 07:45:35.994529: Average global foreground Dice: [0.8458]
2022-04-26 07:45:36.013567: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:45:36.796082: lr: 0.004364
2022-04-26 07:45:36.837175: This epoch took 116.520081 s

2022-04-26 07:45:36.881068: 
epoch:  301
2022-04-26 07:47:10.402863: train loss : -0.8566
2022-04-26 07:47:16.849995: validation loss: -0.8344
2022-04-26 07:47:16.853447: Average global foreground Dice: [0.8576]
2022-04-26 07:47:16.856903: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:47:17.360751: lr: 0.004344
2022-04-26 07:47:17.427993: This epoch took 100.523919 s

2022-04-26 07:47:17.457063: 
epoch:  302
2022-04-26 07:48:52.435600: train loss : -0.8603
2022-04-26 07:48:58.845928: validation loss: -0.8292
2022-04-26 07:48:58.857671: Average global foreground Dice: [0.8561]
2022-04-26 07:48:58.887069: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:48:59.389512: lr: 0.004325
2022-04-26 07:48:59.416026: This epoch took 101.929014 s

2022-04-26 07:48:59.431664: 
epoch:  303
2022-04-26 07:50:32.264985: train loss : -0.8774
2022-04-26 07:50:38.498552: validation loss: -0.8375
2022-04-26 07:50:38.501670: Average global foreground Dice: [0.8611]
2022-04-26 07:50:38.504392: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:50:39.057141: lr: 0.004305
2022-04-26 07:50:39.059644: This epoch took 99.619336 s

2022-04-26 07:50:39.061717: 
epoch:  304
2022-04-26 07:52:11.609665: train loss : -0.8597
2022-04-26 07:52:18.203745: validation loss: -0.8485
2022-04-26 07:52:18.207845: Average global foreground Dice: [0.8709]
2022-04-26 07:52:18.210401: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:52:18.614606: lr: 0.004285
2022-04-26 07:52:18.617036: This epoch took 99.552999 s

2022-04-26 07:52:18.619111: 
epoch:  305
2022-04-26 07:53:52.430948: train loss : -0.8677
2022-04-26 07:53:58.867437: validation loss: -0.8456
2022-04-26 07:53:58.885389: Average global foreground Dice: [0.8669]
2022-04-26 07:53:58.887689: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:53:59.318423: lr: 0.004265
2022-04-26 07:53:59.323196: This epoch took 100.701838 s

2022-04-26 07:53:59.325327: 
epoch:  306
2022-04-26 07:55:40.469598: train loss : -0.8757
2022-04-26 07:55:47.406606: validation loss: -0.8462
2022-04-26 07:55:47.437438: Average global foreground Dice: [0.8679]
2022-04-26 07:55:47.455106: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:55:47.997135: lr: 0.004245
2022-04-26 07:55:48.023337: This epoch took 108.695934 s

2022-04-26 07:55:48.064069: 
epoch:  307
2022-04-26 07:57:21.191418: train loss : -0.8773
2022-04-26 07:57:27.167807: validation loss: -0.8466
2022-04-26 07:57:27.171388: Average global foreground Dice: [0.8689]
2022-04-26 07:57:27.173475: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:57:27.576731: lr: 0.004226
2022-04-26 07:57:27.579483: This epoch took 99.495212 s

2022-04-26 07:57:27.582505: 
epoch:  308
2022-04-26 07:59:01.100993: train loss : -0.8821
2022-04-26 07:59:09.489079: validation loss: -0.8506
2022-04-26 07:59:09.496278: Average global foreground Dice: [0.8715]
2022-04-26 07:59:09.514065: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 07:59:10.116045: lr: 0.004206
2022-04-26 07:59:10.118520: This epoch took 102.533382 s

2022-04-26 07:59:10.120495: 
epoch:  309
2022-04-26 08:00:43.000691: train loss : -0.8816
2022-04-26 08:00:49.198360: validation loss: -0.8406
2022-04-26 08:00:49.201712: Average global foreground Dice: [0.8609]
2022-04-26 08:00:49.203982: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:00:49.655041: lr: 0.004186
2022-04-26 08:00:49.657665: This epoch took 99.535086 s

2022-04-26 08:00:49.659936: 
epoch:  310
2022-04-26 08:02:22.785102: train loss : -0.8830
2022-04-26 08:02:29.110071: validation loss: -0.8396
2022-04-26 08:02:29.116407: Average global foreground Dice: [0.8629]
2022-04-26 08:02:29.140073: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:02:29.628534: lr: 0.004166
2022-04-26 08:02:29.630881: This epoch took 99.968895 s

2022-04-26 08:02:29.632877: 
epoch:  311
2022-04-26 08:04:03.273804: train loss : -0.8813
2022-04-26 08:04:09.729476: validation loss: -0.8334
2022-04-26 08:04:09.741485: Average global foreground Dice: [0.8598]
2022-04-26 08:04:09.754062: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:04:10.213871: lr: 0.004146
2022-04-26 08:04:10.216526: This epoch took 100.581655 s

2022-04-26 08:04:10.218634: 
epoch:  312
2022-04-26 08:05:51.581096: train loss : -0.8809
2022-04-26 08:05:58.367865: validation loss: -0.8372
2022-04-26 08:05:58.392874: Average global foreground Dice: [0.8615]
2022-04-26 08:05:58.416075: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:05:59.096684: lr: 0.004127
2022-04-26 08:05:59.108283: This epoch took 108.886885 s

2022-04-26 08:05:59.115731: 
epoch:  313
2022-04-26 08:07:39.889251: train loss : -0.8788
2022-04-26 08:07:46.483176: validation loss: -0.8470
2022-04-26 08:07:46.513475: Average global foreground Dice: [0.8666]
2022-04-26 08:07:46.536063: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:07:47.056711: lr: 0.004107
2022-04-26 08:07:47.089135: This epoch took 107.963651 s

2022-04-26 08:07:47.103193: 
epoch:  314
2022-04-26 08:09:23.348497: train loss : -0.8849
2022-04-26 08:09:29.899803: validation loss: -0.8344
2022-04-26 08:09:29.934727: Average global foreground Dice: [0.8548]
2022-04-26 08:09:29.967959: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:09:30.901832: lr: 0.004087
2022-04-26 08:09:30.954152: This epoch took 103.847684 s

2022-04-26 08:09:30.988070: 
epoch:  315
2022-04-26 08:11:12.763046: train loss : -0.8835
2022-04-26 08:11:19.577566: validation loss: -0.8427
2022-04-26 08:11:19.595308: Average global foreground Dice: [0.8668]
2022-04-26 08:11:19.597436: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:11:20.154591: lr: 0.004067
2022-04-26 08:11:20.198103: This epoch took 109.187030 s

2022-04-26 08:11:20.220059: 
epoch:  316
2022-04-26 08:12:54.245844: train loss : -0.8854
2022-04-26 08:13:01.174600: validation loss: -0.8402
2022-04-26 08:13:01.205166: Average global foreground Dice: [0.8626]
2022-04-26 08:13:01.228368: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:13:01.683614: lr: 0.004047
2022-04-26 08:13:01.694160: This epoch took 101.452106 s

2022-04-26 08:13:01.714260: 
epoch:  317
2022-04-26 08:14:34.271790: train loss : -0.8826
2022-04-26 08:14:40.787295: validation loss: -0.8477
2022-04-26 08:14:40.791455: Average global foreground Dice: [0.8675]
2022-04-26 08:14:40.795656: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:14:41.228987: lr: 0.004027
2022-04-26 08:14:41.237146: This epoch took 99.502816 s

2022-04-26 08:14:41.239765: 
epoch:  318
2022-04-26 08:16:13.960369: train loss : -0.8740
2022-04-26 08:16:20.027230: validation loss: -0.8395
2022-04-26 08:16:20.030799: Average global foreground Dice: [0.8636]
2022-04-26 08:16:20.033363: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:16:20.440364: lr: 0.004007
2022-04-26 08:16:20.443270: This epoch took 99.201240 s

2022-04-26 08:16:20.445945: 
epoch:  319
2022-04-26 08:17:53.582083: train loss : -0.8809
2022-04-26 08:18:00.811154: validation loss: -0.8302
2022-04-26 08:18:00.843390: Average global foreground Dice: [0.8543]
2022-04-26 08:18:00.866114: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:18:01.421920: lr: 0.003987
2022-04-26 08:18:01.424485: This epoch took 100.975672 s

2022-04-26 08:18:01.426708: 
epoch:  320
2022-04-26 08:19:35.543523: train loss : -0.8799
2022-04-26 08:19:41.783031: validation loss: -0.8513
2022-04-26 08:19:41.803602: Average global foreground Dice: [0.871]
2022-04-26 08:19:41.809429: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:19:42.221351: lr: 0.003967
2022-04-26 08:19:42.223896: This epoch took 100.794920 s

2022-04-26 08:19:42.226242: 
epoch:  321
2022-04-26 08:21:14.445713: train loss : -0.8863
2022-04-26 08:21:21.305862: validation loss: -0.8435
2022-04-26 08:21:21.308946: Average global foreground Dice: [0.8638]
2022-04-26 08:21:21.311598: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:21:21.728242: lr: 0.003947
2022-04-26 08:21:21.730423: This epoch took 99.502139 s

2022-04-26 08:21:21.732469: 
epoch:  322
2022-04-26 08:23:02.370075: train loss : -0.8816
2022-04-26 08:23:09.657470: validation loss: -0.8482
2022-04-26 08:23:09.683675: Average global foreground Dice: [0.8685]
2022-04-26 08:23:09.727059: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:23:10.193103: lr: 0.003927
2022-04-26 08:23:10.195473: This epoch took 108.461014 s

2022-04-26 08:23:10.197588: 
epoch:  323
2022-04-26 08:24:43.805424: train loss : -0.8837
2022-04-26 08:24:50.446580: validation loss: -0.8429
2022-04-26 08:24:50.487441: Average global foreground Dice: [0.8629]
2022-04-26 08:24:50.555071: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:24:51.258762: lr: 0.003907
2022-04-26 08:24:51.283102: This epoch took 101.083336 s

2022-04-26 08:24:51.306062: 
epoch:  324
2022-04-26 08:26:28.523822: train loss : -0.8802
2022-04-26 08:26:35.175947: validation loss: -0.8454
2022-04-26 08:26:35.203717: Average global foreground Dice: [0.8638]
2022-04-26 08:26:35.219089: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:26:35.849740: lr: 0.003887
2022-04-26 08:26:35.885181: This epoch took 104.556100 s

2022-04-26 08:26:35.914107: 
epoch:  325
2022-04-26 08:28:09.216552: train loss : -0.8836
2022-04-26 08:28:15.573694: validation loss: -0.8434
2022-04-26 08:28:15.579645: Average global foreground Dice: [0.8647]
2022-04-26 08:28:15.582115: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:28:16.003349: lr: 0.003867
2022-04-26 08:28:16.005697: This epoch took 100.067645 s

2022-04-26 08:28:16.007906: 
epoch:  326
2022-04-26 08:29:48.904695: train loss : -0.8808
2022-04-26 08:29:55.093524: validation loss: -0.8388
2022-04-26 08:29:55.096719: Average global foreground Dice: [0.86]
2022-04-26 08:29:55.098911: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:29:55.504913: lr: 0.003847
2022-04-26 08:29:55.507417: This epoch took 99.497396 s

2022-04-26 08:29:55.509611: 
epoch:  327
2022-04-26 08:31:35.848154: train loss : -0.8840
2022-04-26 08:31:43.096509: validation loss: -0.8280
2022-04-26 08:31:43.120528: Average global foreground Dice: [0.8499]
2022-04-26 08:31:43.142088: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:31:43.922951: lr: 0.003827
2022-04-26 08:31:43.925458: This epoch took 108.413623 s

2022-04-26 08:31:43.927765: 
epoch:  328
2022-04-26 08:33:18.682232: train loss : -0.8857
2022-04-26 08:33:25.711628: validation loss: -0.8503
2022-04-26 08:33:25.746451: Average global foreground Dice: [0.8712]
2022-04-26 08:33:25.790091: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:33:26.478474: lr: 0.003807
2022-04-26 08:33:26.511092: This epoch took 102.581017 s

2022-04-26 08:33:26.544058: 
epoch:  329
2022-04-26 08:35:00.507658: train loss : -0.8839
2022-04-26 08:35:07.989296: validation loss: -0.8305
2022-04-26 08:35:08.020082: Average global foreground Dice: [0.8523]
2022-04-26 08:35:08.047058: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:35:08.746559: lr: 0.003787
2022-04-26 08:35:08.784109: This epoch took 102.217042 s

2022-04-26 08:35:08.800512: 
epoch:  330
2022-04-26 08:36:41.742167: train loss : -0.8853
2022-04-26 08:36:48.206432: validation loss: -0.8365
2022-04-26 08:36:48.252748: Average global foreground Dice: [0.8586]
2022-04-26 08:36:48.275959: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:36:48.878618: lr: 0.003767
2022-04-26 08:36:48.881135: This epoch took 100.078161 s

2022-04-26 08:36:48.883168: 
epoch:  331
2022-04-26 08:38:20.805640: train loss : -0.8890
2022-04-26 08:38:26.823572: validation loss: -0.8457
2022-04-26 08:38:26.826594: Average global foreground Dice: [0.8654]
2022-04-26 08:38:26.828768: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:38:27.234164: lr: 0.003747
2022-04-26 08:38:27.236437: This epoch took 98.350990 s

2022-04-26 08:38:27.238575: 
epoch:  332
2022-04-26 08:40:00.796144: train loss : -0.8870
2022-04-26 08:40:06.986714: validation loss: -0.8416
2022-04-26 08:40:06.995200: Average global foreground Dice: [0.8608]
2022-04-26 08:40:07.003769: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:40:07.442731: lr: 0.003727
2022-04-26 08:40:07.445126: This epoch took 100.204326 s

2022-04-26 08:40:07.454876: 
epoch:  333
2022-04-26 08:41:40.666579: train loss : -0.8825
2022-04-26 08:41:47.144984: validation loss: -0.8303
2022-04-26 08:41:47.169347: Average global foreground Dice: [0.8598]
2022-04-26 08:41:47.198413: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:41:47.754984: lr: 0.003707
2022-04-26 08:41:47.789116: This epoch took 100.332163 s

2022-04-26 08:41:47.811149: 
epoch:  334
2022-04-26 08:43:21.324627: train loss : -0.8793
2022-04-26 08:43:27.480971: validation loss: -0.8356
2022-04-26 08:43:27.493291: Average global foreground Dice: [0.8573]
2022-04-26 08:43:27.495841: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:43:27.909933: lr: 0.003687
2022-04-26 08:43:27.912356: This epoch took 100.093159 s

2022-04-26 08:43:27.914430: 
epoch:  335
2022-04-26 08:45:01.509997: train loss : -0.8904
2022-04-26 08:45:08.288734: validation loss: -0.8391
2022-04-26 08:45:08.296229: Average global foreground Dice: [0.8597]
2022-04-26 08:45:08.316916: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:45:09.063519: lr: 0.003667
2022-04-26 08:45:09.066381: This epoch took 101.149540 s

2022-04-26 08:45:09.068999: 
epoch:  336
2022-04-26 08:46:42.435614: train loss : -0.8785
2022-04-26 08:46:48.782931: validation loss: -0.8417
2022-04-26 08:46:48.816541: Average global foreground Dice: [0.8653]
2022-04-26 08:46:48.835094: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:46:49.614590: lr: 0.003647
2022-04-26 08:46:49.640236: This epoch took 100.566159 s

2022-04-26 08:46:49.663100: 
epoch:  337
2022-04-26 08:48:23.249889: train loss : -0.8724
2022-04-26 08:48:30.164189: validation loss: -0.8292
2022-04-26 08:48:30.199380: Average global foreground Dice: [0.8536]
2022-04-26 08:48:30.231071: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-04-26 08:48:30.925428: lr: 0.003627
2022-04-26 08:48:30.936739: This epoch took 101.251666 s

2022-04-26 08:48:30.946755: 
epoch:  338
