Starting at Thu Jun 30 00:30:11 CEST 2022
Running on hosts: res-hpc-lkeb06
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 6.
Account: div2-lkeb
Job ID: 10588193
Job name: PancreasTrain
Node running script: res-hpc-lkeb06
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Thu Jun 30 07:45:28 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     On   | 00000000:AF:00.0 Off |                  Off |
| 34%   22C    P8    17W / 260W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-8nz076d0/hiddenlayer_94d36b75fab54d3a84847d77b20ac19e
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Obtaining file:///home/smaijer/code/nnUNet
Requirement already satisfied: torch>1.10.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.12.0)
Requirement already satisfied: tqdm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.64.0)
Requirement already satisfied: dicom2nifti in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.4.2)
Requirement already satisfied: scikit-image>=0.14 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.19.3)
Requirement already satisfied: medpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.0)
Requirement already satisfied: scipy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.8.1)
Requirement already satisfied: batchgenerators>=0.23 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.24)
Requirement already satisfied: numpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.23.0)
Requirement already satisfied: sklearn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.0)
Requirement already satisfied: SimpleITK in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.1.1.2)
Requirement already satisfied: pandas in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.4.3)
Requirement already satisfied: requests in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.28.0)
Requirement already satisfied: nibabel in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.0.1)
Requirement already satisfied: tifffile in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2022.5.4)
Requirement already satisfied: matplotlib in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (3.5.2)
Requirement already satisfied: monai in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.9.0)
Requirement already satisfied: einops in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.1)
Requirement already satisfied: ipython in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (8.4.0)
Requirement already satisfied: graphviz in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.20)
Requirement already satisfied: unittest2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: scikit-learn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.1)
Requirement already satisfied: pillow>=7.1.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (9.1.1)
Requirement already satisfied: future in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)
Requirement already satisfied: threadpoolctl in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.1.0)
Requirement already satisfied: networkx>=2.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.8.4)
Requirement already satisfied: PyWavelets>=1.1.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.3.0)
Requirement already satisfied: imageio>=2.4.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.19.3)
Requirement already satisfied: packaging>=20.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (21.3)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from packaging>=20.0->scikit-image>=0.14->nnunet==1.7.0) (3.0.9)
Requirement already satisfied: typing-extensions in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from torch>1.10.0->nnunet==1.7.0) (4.2.0)
Requirement already satisfied: python-gdcm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (3.0.14)
Requirement already satisfied: pydicom>=2.2.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (2.3.0)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (3.0.30)
Requirement already satisfied: setuptools>=18.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (58.1.0)
Requirement already satisfied: matplotlib-inline in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.1.3)
Requirement already satisfied: stack-data in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.3.0)
Requirement already satisfied: traitlets>=5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.3.0)
Requirement already satisfied: backcall in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.2.0)
Requirement already satisfied: jedi>=0.16 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.18.1)
Requirement already satisfied: pygments>=2.4.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (2.12.0)
Requirement already satisfied: pexpect>4.3 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (4.8.0)
Requirement already satisfied: decorator in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.1.1)
Requirement already satisfied: pickleshare in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.7.5)
Requirement already satisfied: parso<0.9.0,>=0.8.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from jedi>=0.16->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: ptyprocess>=0.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pexpect>4.3->ipython->nnunet==1.7.0) (0.7.0)
Requirement already satisfied: wcwidth in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nnunet==1.7.0) (0.2.5)
Requirement already satisfied: cycler>=0.10 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (0.11.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (1.4.3)
Requirement already satisfied: fonttools>=4.22.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (4.33.3)
Requirement already satisfied: python-dateutil>=2.7 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (2.8.2)
Requirement already satisfied: six>=1.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunet==1.7.0) (1.16.0)
Requirement already satisfied: pytz>=2020.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pandas->nnunet==1.7.0) (2022.1)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (1.26.9)
Requirement already satisfied: idna<4,>=2.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (3.3)
Requirement already satisfied: certifi>=2017.4.17 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2022.6.15)
Requirement already satisfied: charset-normalizer~=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2.0.12)
Requirement already satisfied: joblib>=1.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: asttokens in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (2.0.5)
Requirement already satisfied: pure-eval in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.2.2)
Requirement already satisfied: executing in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: traceback2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: linecache2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)
Installing collected packages: argparse, nnunet
  Attempting uninstall: nnunet
    Found existing installation: nnunet 1.7.0
    Uninstalling nnunet-1.7.0:
      Successfully uninstalled nnunet-1.7.0
  Running setup.py develop for nnunet
Successfully installed argparse-1.4.0 nnunet-1.7.0


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='1', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-30 07:46:08.536776: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-30 07:46:08.546960: The split file contains 5 splits.
2022-06-30 07:46:08.549144: Desired fold for training: 1
2022-06-30 07:46:08.551049: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-06-30 07:46:12.014577: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_1/model_latest.model train= True
SuusB run_training - zet learning rate als  
2022-06-30 07:46:28.536794: Suus1 maybe_update_lr lr: 0.002349
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-06-30 07:46:38.295867: Unable to plot network architecture:
2022-06-30 07:46:38.333027: local variable 'g' referenced before assignment
2022-06-30 07:46:38.366028: 
printing the network instead:

2022-06-30 07:46:38.395394: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-06-30 07:46:38.420645: 

2022-06-30 07:46:38.448416: 
epoch:  400
2022-06-30 07:51:30.867497: train loss : 0.1370
2022-06-30 07:51:47.486730: validation loss: 0.1527
2022-06-30 07:51:47.489674: Average global foreground Dice: [0.6817, 0.6779, 0.682, 0.4177, 0.4004, 0.8832, 0.4384, 0.7169, 0.5177, 0.4017, 0.256, 0.207, 0.2025, 0.587, 0.2541]
2022-06-30 07:51:47.491600: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:51:47.969677: Suus1 maybe_update_lr lr: 0.002328
2022-06-30 07:51:47.971883: saving best epoch checkpoint...
2022-06-30 07:51:48.140706: saving checkpoint...
2022-06-30 07:51:50.890946: done, saving took 2.92 seconds
2022-06-30 07:51:50.906858: This epoch took 312.438555 s

2022-06-30 07:51:50.908986: 
epoch:  401
2022-06-30 07:56:28.049242: train loss : 0.1484
2022-06-30 07:56:44.670604: validation loss: 0.1116
2022-06-30 07:56:44.674064: Average global foreground Dice: [0.6906, 0.6306, 0.648, 0.4399, 0.4254, 0.8587, 0.3915, 0.6938, 0.5688, 0.4045, 0.3353, 0.1737, 0.2067, 0.5255, 0.4419]
2022-06-30 07:56:44.676071: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:56:45.106537: Suus1 maybe_update_lr lr: 0.002307
2022-06-30 07:56:45.108823: saving best epoch checkpoint...
2022-06-30 07:56:45.271914: saving checkpoint...
2022-06-30 07:56:48.148747: done, saving took 3.04 seconds
2022-06-30 07:56:48.162944: This epoch took 297.251896 s

2022-06-30 07:56:48.165126: 
epoch:  402
2022-06-30 08:01:25.279493: train loss : 0.1440
2022-06-30 08:01:41.891492: validation loss: 0.1298
2022-06-30 08:01:41.895273: Average global foreground Dice: [0.7626, 0.5044, 0.6968, 0.4309, 0.4567, 0.8872, 0.4848, 0.6969, 0.5154, 0.402, 0.3546, 0.1221, 0.2331, 0.48, 0.4955]
2022-06-30 08:01:41.897325: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:01:42.456957: Suus1 maybe_update_lr lr: 0.002286
2022-06-30 08:01:42.459230: saving best epoch checkpoint...
2022-06-30 08:01:42.623864: saving checkpoint...
2022-06-30 08:01:45.504096: done, saving took 3.04 seconds
2022-06-30 08:01:45.525809: This epoch took 297.358708 s

2022-06-30 08:01:45.527991: 
epoch:  403
2022-06-30 08:06:24.213481: train loss : 0.1408
2022-06-30 08:06:40.849785: validation loss: 0.2002
2022-06-30 08:06:40.853077: Average global foreground Dice: [0.5241, 0.5602, 0.446, 0.3614, 0.31, 0.8473, 0.393, 0.6587, 0.5612, 0.4524, 0.3371, 0.176, 0.277, 0.7383, 0.581]
2022-06-30 08:06:40.855000: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:06:41.282958: Suus1 maybe_update_lr lr: 0.002264
2022-06-30 08:06:41.285481: This epoch took 295.755390 s

2022-06-30 08:06:41.287465: 
epoch:  404
2022-06-30 08:11:18.885911: train loss : 0.1262
2022-06-30 08:11:35.491510: validation loss: 0.1524
2022-06-30 08:11:35.495142: Average global foreground Dice: [0.7029, 0.5701, 0.603, 0.3497, 0.4208, 0.8813, 0.461, 0.6905, 0.5234, 0.4301, 0.319, 0.1586, 0.1812, 0.6861, 0.5817]
2022-06-30 08:11:35.497327: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:11:35.921955: Suus1 maybe_update_lr lr: 0.002243
2022-06-30 08:11:35.924044: saving best epoch checkpoint...
2022-06-30 08:11:36.089399: saving checkpoint...
2022-06-30 08:11:38.969590: done, saving took 3.04 seconds
2022-06-30 08:11:38.984079: This epoch took 297.694695 s

2022-06-30 08:11:38.986238: 
epoch:  405
2022-06-30 08:16:16.123349: train loss : 0.1146
2022-06-30 08:16:32.722386: validation loss: 0.2121
2022-06-30 08:16:32.725835: Average global foreground Dice: [0.7494, 0.6158, 0.6133, 0.3663, 0.4556, 0.8087, 0.2852, 0.6725, 0.5688, 0.4251, 0.3035, 0.1144, 0.2099, 0.35, 0.5689]
2022-06-30 08:16:32.728025: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:16:33.155216: Suus1 maybe_update_lr lr: 0.002222
2022-06-30 08:16:33.157703: This epoch took 294.169436 s

2022-06-30 08:16:33.159746: 
epoch:  406
2022-06-30 08:21:10.826633: train loss : 0.1528
2022-06-30 08:21:27.511261: validation loss: 0.1649
2022-06-30 08:21:27.531094: Average global foreground Dice: [0.7325, 0.5959, 0.5842, 0.3622, 0.363, 0.8468, 0.4733, 0.6417, 0.5008, 0.4393, 0.3458, 0.1709, 0.2614, 0.6029, 0.4382]
2022-06-30 08:21:27.546541: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:21:28.056265: Suus1 maybe_update_lr lr: 0.002201
2022-06-30 08:21:28.078297: This epoch took 294.916625 s

2022-06-30 08:21:28.098057: 
epoch:  407
2022-06-30 08:26:05.485322: train loss : 0.1189
2022-06-30 08:26:22.099109: validation loss: 0.1137
2022-06-30 08:26:22.102200: Average global foreground Dice: [0.7108, 0.7213, 0.6116, 0.3769, 0.3403, 0.8739, 0.3954, 0.674, 0.5472, 0.4709, 0.3333, 0.1568, 0.2897, 0.6202, 0.5042]
2022-06-30 08:26:22.104312: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:26:22.539655: Suus1 maybe_update_lr lr: 0.002179
2022-06-30 08:26:22.542101: saving best epoch checkpoint...
2022-06-30 08:26:22.894763: saving checkpoint...
2022-06-30 08:26:25.751147: done, saving took 3.21 seconds
2022-06-30 08:26:25.768483: This epoch took 297.654475 s

2022-06-30 08:26:25.770403: 
epoch:  408
2022-06-30 08:31:02.974044: train loss : 0.1284
2022-06-30 08:31:19.618015: validation loss: 0.1182
2022-06-30 08:31:19.621869: Average global foreground Dice: [0.6793, 0.5845, 0.6009, 0.4511, 0.416, 0.8545, 0.4407, 0.6756, 0.5712, 0.4038, 0.3658, 0.1843, 0.2731, 0.2461, 0.3834]
2022-06-30 08:31:19.624075: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:31:20.071309: Suus1 maybe_update_lr lr: 0.002158
2022-06-30 08:31:20.073623: This epoch took 294.301339 s

2022-06-30 08:31:20.075677: 
epoch:  409
2022-06-30 08:35:57.888765: train loss : 0.1411
2022-06-30 08:36:14.542955: validation loss: 0.1384
2022-06-30 08:36:14.546628: Average global foreground Dice: [0.6563, 0.5979, 0.5714, 0.3907, 0.397, 0.8162, 0.3638, 0.6809, 0.5392, 0.3701, 0.3842, 0.148, 0.1798, 0.5778, 0.4999]
2022-06-30 08:36:14.548625: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:36:14.990963: Suus1 maybe_update_lr lr: 0.002137
2022-06-30 08:36:14.993252: This epoch took 294.915582 s

2022-06-30 08:36:14.995194: 
epoch:  410
2022-06-30 08:40:53.139774: train loss : 0.1125
2022-06-30 08:41:09.946945: validation loss: 0.1209
2022-06-30 08:41:09.971751: Average global foreground Dice: [0.6551, 0.6186, 0.558, 0.5607, 0.3764, 0.8663, 0.4494, 0.6891, 0.6003, 0.4442, 0.3341, 0.205, 0.2874, 0.5934, 0.5102]
2022-06-30 08:41:09.973932: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:41:10.476492: Suus1 maybe_update_lr lr: 0.002115
2022-06-30 08:41:10.478753: saving best epoch checkpoint...
2022-06-30 08:41:10.961778: saving checkpoint...
2022-06-30 08:41:14.037535: done, saving took 3.56 seconds
2022-06-30 08:41:14.050736: This epoch took 299.053676 s

2022-06-30 08:41:14.052602: 
epoch:  411
2022-06-30 08:45:52.754976: train loss : 0.1413
2022-06-30 08:46:09.573066: validation loss: 0.1504
2022-06-30 08:46:09.576700: Average global foreground Dice: [0.624, 0.6942, 0.4268, 0.4378, 0.3336, 0.9024, 0.4131, 0.6201, 0.5535, 0.4383, 0.3095, 0.1612, 0.2694, 0.7074, 0.6188]
2022-06-30 08:46:09.578760: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:46:10.058062: Suus1 maybe_update_lr lr: 0.002094
2022-06-30 08:46:10.060314: saving best epoch checkpoint...
2022-06-30 08:46:10.498268: saving checkpoint...
2022-06-30 08:46:13.652500: done, saving took 3.59 seconds
2022-06-30 08:46:13.667134: This epoch took 299.612721 s

2022-06-30 08:46:13.669172: 
epoch:  412
2022-06-30 08:50:52.675239: train loss : 0.1344
2022-06-30 08:51:09.340578: validation loss: 0.1732
2022-06-30 08:51:09.343875: Average global foreground Dice: [0.6798, 0.4915, 0.6504, 0.3013, 0.3877, 0.7939, 0.4789, 0.7029, 0.5416, 0.3564, 0.299, 0.1342, 0.1964, 0.5705, 0.4965]
2022-06-30 08:51:09.347796: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:51:09.917382: Suus1 maybe_update_lr lr: 0.002072
2022-06-30 08:51:09.933330: This epoch took 296.262102 s

2022-06-30 08:51:09.942590: 
epoch:  413
2022-06-30 08:55:47.456998: train loss : 0.1214
2022-06-30 08:56:04.046362: validation loss: 0.1701
2022-06-30 08:56:04.050528: Average global foreground Dice: [0.6756, 0.5337, 0.5783, 0.367, 0.381, 0.8248, 0.5024, 0.658, 0.5064, 0.3549, 0.3003, 0.1719, 0.2621, 0.7086, 0.4387]
2022-06-30 08:56:04.052691: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:56:04.473108: Suus1 maybe_update_lr lr: 0.002051
2022-06-30 08:56:04.475841: This epoch took 294.529321 s

2022-06-30 08:56:04.477744: 
epoch:  414
2022-06-30 09:00:40.537521: train loss : 0.1294
2022-06-30 09:00:57.098455: validation loss: 0.1447
2022-06-30 09:00:57.101528: Average global foreground Dice: [0.6566, 0.6688, 0.6371, 0.5117, 0.3568, 0.8718, 0.4704, 0.6267, 0.5356, 0.4065, 0.3199, 0.1992, 0.2705, 0.6806, 0.5137]
2022-06-30 09:00:57.103491: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:00:57.520515: Suus1 maybe_update_lr lr: 0.00203
2022-06-30 09:00:57.522860: This epoch took 293.043010 s

2022-06-30 09:00:57.524813: 
epoch:  415
2022-06-30 09:05:33.632252: train loss : 0.1265
2022-06-30 09:05:50.172735: validation loss: 0.0752
2022-06-30 09:05:50.175905: Average global foreground Dice: [0.7129, 0.7155, 0.6566, 0.4538, 0.3919, 0.8661, 0.4811, 0.738, 0.5919, 0.4527, 0.352, 0.1623, 0.28, 0.4235, 0.4927]
2022-06-30 09:05:50.177904: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:05:50.598504: Suus1 maybe_update_lr lr: 0.002008
2022-06-30 09:05:50.600698: saving best epoch checkpoint...
2022-06-30 09:05:50.815151: saving checkpoint...
2022-06-30 09:05:53.683466: done, saving took 3.08 seconds
2022-06-30 09:05:53.709824: This epoch took 296.183135 s

2022-06-30 09:05:53.711797: 
epoch:  416
2022-06-30 09:10:29.787320: train loss : 0.1169
2022-06-30 09:10:46.335047: validation loss: 0.1084
2022-06-30 09:10:46.337950: Average global foreground Dice: [0.6699, 0.6669, 0.6218, 0.3905, 0.4512, 0.8611, 0.4306, 0.6615, 0.4921, 0.4059, 0.3428, 0.212, 0.269, 0.6096, 0.5397]
2022-06-30 09:10:46.339844: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:10:46.755394: Suus1 maybe_update_lr lr: 0.001987
2022-06-30 09:10:46.757672: saving best epoch checkpoint...
2022-06-30 09:10:46.909267: saving checkpoint...
2022-06-30 09:10:49.788964: done, saving took 3.03 seconds
2022-06-30 09:10:49.803147: This epoch took 296.089269 s

2022-06-30 09:10:49.805285: 
epoch:  417
2022-06-30 09:15:26.051053: train loss : 0.1196
2022-06-30 09:15:42.627428: validation loss: 0.1490
2022-06-30 09:15:42.630751: Average global foreground Dice: [0.7177, 0.6238, 0.5947, 0.3642, 0.3796, 0.8366, 0.4715, 0.6703, 0.5891, 0.411, 0.379, 0.2382, 0.3138, 0.3985, 0.429]
2022-06-30 09:15:42.632731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:15:43.049433: Suus1 maybe_update_lr lr: 0.001965
2022-06-30 09:15:43.051458: This epoch took 293.243716 s

2022-06-30 09:15:43.053449: 
epoch:  418
2022-06-30 09:20:19.505420: train loss : 0.1172
2022-06-30 09:20:36.058889: validation loss: 0.1328
2022-06-30 09:20:36.062265: Average global foreground Dice: [0.6676, 0.5764, 0.6126, 0.4228, 0.4737, 0.8421, 0.4709, 0.6365, 0.5304, 0.4299, 0.317, 0.1728, 0.2702, 0.7741, 0.5512]
2022-06-30 09:20:36.064228: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:20:36.490820: Suus1 maybe_update_lr lr: 0.001943
2022-06-30 09:20:36.492890: saving best epoch checkpoint...
2022-06-30 09:20:36.650877: saving checkpoint...
2022-06-30 09:20:39.576010: done, saving took 3.08 seconds
2022-06-30 09:20:39.590055: This epoch took 296.534791 s

2022-06-30 09:20:39.592120: 
epoch:  419
2022-06-30 09:25:15.959487: train loss : 0.1180
2022-06-30 09:25:32.511389: validation loss: 0.1483
2022-06-30 09:25:32.514646: Average global foreground Dice: [0.5753, 0.4614, 0.6138, 0.4493, 0.3496, 0.8386, 0.4264, 0.6917, 0.5331, 0.3764, 0.2657, 0.1626, 0.1395, 0.5253, 0.5258]
2022-06-30 09:25:32.516546: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:25:32.942296: Suus1 maybe_update_lr lr: 0.001922
2022-06-30 09:25:32.944336: This epoch took 293.350191 s

2022-06-30 09:25:32.946141: 
epoch:  420
2022-06-30 09:30:09.337981: train loss : 0.0989
2022-06-30 09:30:25.900249: validation loss: 0.1494
2022-06-30 09:30:25.903093: Average global foreground Dice: [0.6981, 0.6029, 0.6719, 0.4838, 0.4405, 0.8485, 0.4375, 0.7095, 0.5809, 0.383, 0.2834, 0.1017, 0.2717, 0.3685, 0.3587]
2022-06-30 09:30:25.904951: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:30:26.332804: Suus1 maybe_update_lr lr: 0.0019
2022-06-30 09:30:26.334988: This epoch took 293.387023 s

2022-06-30 09:30:26.336796: 
epoch:  421
2022-06-30 09:35:02.794886: train loss : 0.1581
2022-06-30 09:35:19.373728: validation loss: 0.1367
2022-06-30 09:35:19.377242: Average global foreground Dice: [0.6615, 0.5812, 0.6017, 0.3948, 0.3841, 0.8411, 0.4011, 0.6751, 0.5368, 0.4457, 0.3051, 0.1837, 0.3111, 0.5909, 0.5362]
2022-06-30 09:35:19.379143: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:35:19.806620: Suus1 maybe_update_lr lr: 0.001879
2022-06-30 09:35:19.808781: This epoch took 293.470193 s

2022-06-30 09:35:19.810716: 
epoch:  422
2022-06-30 09:39:56.157099: train loss : 0.1069
2022-06-30 09:40:12.715755: validation loss: 0.1750
2022-06-30 09:40:12.719551: Average global foreground Dice: [0.667, 0.6404, 0.5659, 0.3598, 0.4406, 0.8609, 0.4432, 0.6546, 0.5096, 0.4001, 0.3183, 0.2067, 0.3607, 0.3398, 0.3875]
2022-06-30 09:40:12.721636: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:40:13.153414: Suus1 maybe_update_lr lr: 0.001857
2022-06-30 09:40:13.155546: This epoch took 293.342801 s

2022-06-30 09:40:13.157445: 
epoch:  423
2022-06-30 09:44:49.523641: train loss : 0.0877
2022-06-30 09:45:06.076282: validation loss: 0.1451
2022-06-30 09:45:06.080816: Average global foreground Dice: [0.7186, 0.663, 0.5753, 0.377, 0.4965, 0.8138, 0.4691, 0.7272, 0.5865, 0.4323, 0.338, 0.2234, 0.2939, 0.4089, 0.5601]
2022-06-30 09:45:06.082991: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:45:06.658807: Suus1 maybe_update_lr lr: 0.001835
2022-06-30 09:45:06.661057: This epoch took 293.501686 s

2022-06-30 09:45:06.662924: 
epoch:  424
2022-06-30 09:49:43.000794: train loss : 0.1017
2022-06-30 09:49:59.558064: validation loss: 0.1113
2022-06-30 09:49:59.561654: Average global foreground Dice: [0.7131, 0.6771, 0.6276, 0.4504, 0.429, 0.8508, 0.4253, 0.7285, 0.6268, 0.4755, 0.3352, 0.2256, 0.3536, 0.6203, 0.4746]
2022-06-30 09:49:59.563651: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:49:59.981995: Suus1 maybe_update_lr lr: 0.001813
2022-06-30 09:49:59.984308: This epoch took 293.319354 s

2022-06-30 09:49:59.986328: 
epoch:  425
2022-06-30 09:54:36.413588: train loss : 0.1164
2022-06-30 09:54:52.967663: validation loss: 0.1072
2022-06-30 09:54:52.971175: Average global foreground Dice: [0.6794, 0.6115, 0.5734, 0.4948, 0.3631, 0.864, 0.4202, 0.6829, 0.5804, 0.4464, 0.3283, 0.2238, 0.2505, 0.6977, 0.6271]
2022-06-30 09:54:52.973381: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:54:53.392780: Suus1 maybe_update_lr lr: 0.001792
2022-06-30 09:54:53.395177: saving best epoch checkpoint...
2022-06-30 09:54:53.524502: saving checkpoint...
2022-06-30 09:54:56.410708: done, saving took 3.01 seconds
2022-06-30 09:54:56.431301: This epoch took 296.442944 s

2022-06-30 09:54:56.433380: 
epoch:  426
2022-06-30 09:59:33.021180: train loss : 0.1067
2022-06-30 09:59:49.584382: validation loss: 0.0828
2022-06-30 09:59:49.588168: Average global foreground Dice: [0.6877, 0.6175, 0.6129, 0.4403, 0.4247, 0.8741, 0.5517, 0.7177, 0.5514, 0.4664, 0.326, 0.1872, 0.284, 0.7331, 0.4243]
2022-06-30 09:59:49.590349: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:59:50.022690: Suus1 maybe_update_lr lr: 0.00177
2022-06-30 09:59:50.024949: saving best epoch checkpoint...
2022-06-30 09:59:50.218434: saving checkpoint...
2022-06-30 09:59:53.221558: done, saving took 3.19 seconds
2022-06-30 09:59:53.245227: This epoch took 296.809879 s

2022-06-30 09:59:53.247396: 
epoch:  427
2022-06-30 10:04:30.091393: train loss : 0.1191
2022-06-30 10:04:46.691514: validation loss: 0.2118
2022-06-30 10:04:46.694900: Average global foreground Dice: [0.5897, 0.6082, 0.4738, 0.3788, 0.3579, 0.8322, 0.5102, 0.6375, 0.5429, 0.4, 0.2712, 0.2075, 0.2407, 0.6373, 0.6072]
2022-06-30 10:04:46.697150: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:04:47.113061: Suus1 maybe_update_lr lr: 0.001748
2022-06-30 10:04:47.115174: This epoch took 293.865711 s

2022-06-30 10:04:47.117003: 
epoch:  428
2022-06-30 10:09:24.483267: train loss : 0.1358
2022-06-30 10:09:41.090959: validation loss: 0.1572
2022-06-30 10:09:41.094113: Average global foreground Dice: [0.6488, 0.5963, 0.662, 0.3082, 0.3492, 0.8329, 0.3929, 0.6554, 0.5289, 0.3919, 0.2335, 0.232, 0.2581, 0.6044, 0.4816]
2022-06-30 10:09:41.096245: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:09:41.521679: Suus1 maybe_update_lr lr: 0.001726
2022-06-30 10:09:41.524149: This epoch took 294.405210 s

2022-06-30 10:09:41.526264: 
epoch:  429
2022-06-30 10:14:18.861443: train loss : 0.1236
2022-06-30 10:14:35.477099: validation loss: 0.1084
2022-06-30 10:14:35.480008: Average global foreground Dice: [0.6483, 0.7528, 0.6071, 0.4644, 0.4145, 0.8755, 0.5492, 0.7142, 0.5587, 0.4874, 0.3435, 0.2037, 0.3448, 0.6639, 0.582]
2022-06-30 10:14:35.481878: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:14:35.905375: Suus1 maybe_update_lr lr: 0.001704
2022-06-30 10:14:35.907508: saving best epoch checkpoint...
2022-06-30 10:14:36.090847: saving checkpoint...
2022-06-30 10:14:38.972160: done, saving took 3.06 seconds
2022-06-30 10:14:38.991186: This epoch took 297.462752 s

2022-06-30 10:14:38.993114: 
epoch:  430
2022-06-30 10:19:16.073920: train loss : 0.1046
2022-06-30 10:19:32.687465: validation loss: 0.1878
2022-06-30 10:19:32.690614: Average global foreground Dice: [0.724, 0.6422, 0.6406, 0.3449, 0.4482, 0.8465, 0.4448, 0.6677, 0.5536, 0.4172, 0.3159, 0.1724, 0.3353, 0.3065, 0.1099]
2022-06-30 10:19:32.692510: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:19:33.116035: Suus1 maybe_update_lr lr: 0.001682
2022-06-30 10:19:33.118294: This epoch took 294.123150 s

2022-06-30 10:19:33.120107: 
epoch:  431
2022-06-30 10:24:10.297005: train loss : 0.1182
2022-06-30 10:24:26.909967: validation loss: 0.1979
2022-06-30 10:24:26.913222: Average global foreground Dice: [0.737, 0.6467, 0.6373, 0.4443, 0.4811, 0.8578, 0.3836, 0.72, 0.5005, 0.4662, 0.303, 0.2471, 0.2619, 0.6307, 0.2778]
2022-06-30 10:24:26.915227: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:24:27.339814: Suus1 maybe_update_lr lr: 0.00166
2022-06-30 10:24:27.341959: This epoch took 294.219855 s

2022-06-30 10:24:27.343820: 
epoch:  432
2022-06-30 10:29:04.487124: train loss : 0.0943
2022-06-30 10:29:21.081839: validation loss: 0.2289
2022-06-30 10:29:21.084789: Average global foreground Dice: [0.6547, 0.6141, 0.6352, 0.4522, 0.3533, 0.8409, 0.5212, 0.6684, 0.5489, 0.4877, 0.2857, 0.2219, 0.312, 0.7147, 0.5877]
2022-06-30 10:29:21.086820: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:29:21.507710: Suus1 maybe_update_lr lr: 0.001638
2022-06-30 10:29:21.509824: This epoch took 294.164227 s

2022-06-30 10:29:21.511837: 
epoch:  433
2022-06-30 10:33:58.815161: train loss : 0.1003
2022-06-30 10:34:15.415617: validation loss: 0.1826
2022-06-30 10:34:15.418793: Average global foreground Dice: [0.779, 0.6187, 0.6792, 0.4096, 0.506, 0.8659, 0.4526, 0.6757, 0.5239, 0.3488, 0.2959, 0.2589, 0.3025, 0.686, 0.4506]
2022-06-30 10:34:15.420775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:34:15.839723: Suus1 maybe_update_lr lr: 0.001616
2022-06-30 10:34:15.841939: saving best epoch checkpoint...
2022-06-30 10:34:15.970102: saving checkpoint...
2022-06-30 10:34:18.941688: done, saving took 3.10 seconds
2022-06-30 10:34:18.957758: This epoch took 297.443642 s

2022-06-30 10:34:18.959834: 
epoch:  434
2022-06-30 10:38:56.016858: train loss : 0.1358
2022-06-30 10:39:12.617807: validation loss: 0.1409
2022-06-30 10:39:12.621180: Average global foreground Dice: [0.657, 0.673, 0.654, 0.4418, 0.4513, 0.8291, 0.5763, 0.6817, 0.5534, 0.4373, 0.3092, 0.1962, 0.2215, 0.7504, 0.5372]
2022-06-30 10:39:12.623312: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:39:13.040820: Suus1 maybe_update_lr lr: 0.001594
2022-06-30 10:39:13.042816: saving best epoch checkpoint...
2022-06-30 10:39:13.242232: saving checkpoint...
2022-06-30 10:39:16.156695: done, saving took 3.11 seconds
2022-06-30 10:39:16.180355: This epoch took 297.218548 s

2022-06-30 10:39:16.182431: 
epoch:  435
2022-06-30 10:43:53.072779: train loss : 0.0963
2022-06-30 10:44:09.667877: validation loss: 0.1132
2022-06-30 10:44:09.671262: Average global foreground Dice: [0.7803, 0.7306, 0.7207, 0.4023, 0.4325, 0.8663, 0.5279, 0.6992, 0.535, 0.4659, 0.3428, 0.2354, 0.3144, 0.5203, 0.5795]
2022-06-30 10:44:09.673402: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:44:10.090262: Suus1 maybe_update_lr lr: 0.001572
2022-06-30 10:44:10.092869: saving best epoch checkpoint...
2022-06-30 10:44:10.309448: saving checkpoint...
2022-06-30 10:44:13.308711: done, saving took 3.21 seconds
2022-06-30 10:44:13.334731: This epoch took 297.150123 s

2022-06-30 10:44:13.336840: 
epoch:  436
2022-06-30 10:48:50.124081: train loss : 0.1297
2022-06-30 10:49:06.720745: validation loss: 0.1536
2022-06-30 10:49:06.725338: Average global foreground Dice: [0.6662, 0.6192, 0.4663, 0.4538, 0.381, 0.8468, 0.4881, 0.6269, 0.5466, 0.4225, 0.2659, 0.1301, 0.3079, 0.5181, 0.4835]
2022-06-30 10:49:06.727602: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:49:07.143339: Suus1 maybe_update_lr lr: 0.00155
2022-06-30 10:49:07.145607: This epoch took 293.806758 s

2022-06-30 10:49:07.147901: 
epoch:  437
2022-06-30 10:53:44.131633: train loss : 0.1156
2022-06-30 10:54:00.710337: validation loss: 0.1381
2022-06-30 10:54:00.713757: Average global foreground Dice: [0.7187, 0.622, 0.633, 0.3872, 0.4331, 0.8536, 0.4862, 0.6894, 0.5595, 0.4327, 0.3069, 0.2323, 0.2943, 0.6681, 0.4679]
2022-06-30 10:54:00.716203: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:54:01.135995: Suus1 maybe_update_lr lr: 0.001528
2022-06-30 10:54:01.138340: This epoch took 293.988343 s

2022-06-30 10:54:01.140664: 
epoch:  438
2022-06-30 10:58:38.203899: train loss : 0.1178
2022-06-30 10:58:54.799113: validation loss: 0.1120
2022-06-30 10:58:54.802735: Average global foreground Dice: [0.7325, 0.6065, 0.6874, 0.3259, 0.4512, 0.832, 0.4192, 0.6928, 0.5377, 0.4198, 0.2837, 0.1844, 0.2512, 0.571, 0.5837]
2022-06-30 10:58:54.805184: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:58:55.235453: Suus1 maybe_update_lr lr: 0.001506
2022-06-30 10:58:55.237966: This epoch took 294.094779 s

2022-06-30 10:58:55.240134: 
epoch:  439
2022-06-30 11:03:32.243482: train loss : 0.0997
2022-06-30 11:03:48.875706: validation loss: 0.1213
2022-06-30 11:03:48.884532: Average global foreground Dice: [0.6961, 0.6257, 0.6796, 0.3667, 0.425, 0.8461, 0.4964, 0.7119, 0.5495, 0.413, 0.3566, 0.2911, 0.2756, 0.7206, 0.6217]
2022-06-30 11:03:48.887971: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:03:49.313634: Suus1 maybe_update_lr lr: 0.001483
2022-06-30 11:03:49.316885: saving best epoch checkpoint...
2022-06-30 11:03:49.629617: saving checkpoint...
2022-06-30 11:03:53.045367: done, saving took 3.72 seconds
2022-06-30 11:03:53.078740: This epoch took 297.836534 s

2022-06-30 11:03:53.081388: 
epoch:  440
2022-06-30 11:08:30.097575: train loss : 0.1328
2022-06-30 11:08:46.740803: validation loss: 0.1404
2022-06-30 11:08:46.744033: Average global foreground Dice: [0.4819, 0.5274, 0.5981, 0.381, 0.4652, 0.8195, 0.519, 0.6844, 0.5389, 0.4295, 0.3618, 0.2048, 0.2168, 0.5532, 0.4881]
2022-06-30 11:08:46.746280: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:08:47.175920: Suus1 maybe_update_lr lr: 0.001461
2022-06-30 11:08:47.178329: This epoch took 294.094579 s

2022-06-30 11:08:47.180561: 
epoch:  441
2022-06-30 11:13:23.813409: train loss : 0.1293
2022-06-30 11:13:40.471245: validation loss: 0.1359
2022-06-30 11:13:40.474810: Average global foreground Dice: [0.7197, 0.7381, 0.6475, 0.4146, 0.4013, 0.8451, 0.3869, 0.7107, 0.5706, 0.4364, 0.3005, 0.1517, 0.2505, 0.4572, 0.4644]
2022-06-30 11:13:40.477277: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:13:40.907097: Suus1 maybe_update_lr lr: 0.001439
2022-06-30 11:13:40.909740: This epoch took 293.726896 s

2022-06-30 11:13:40.912155: 
epoch:  442
2022-06-30 11:18:17.448776: train loss : 0.1246
2022-06-30 11:18:34.099834: validation loss: 0.2147
2022-06-30 11:18:34.106003: Average global foreground Dice: [0.6357, 0.6559, 0.6317, 0.3729, 0.4011, 0.7494, 0.4403, 0.6745, 0.5457, 0.3999, 0.3124, 0.2182, 0.3043, 0.6002, 0.4013]
2022-06-30 11:18:34.108569: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:18:34.539977: Suus1 maybe_update_lr lr: 0.001416
2022-06-30 11:18:34.542382: This epoch took 293.625964 s

2022-06-30 11:18:34.544483: 
epoch:  443
2022-06-30 11:23:11.835028: train loss : 0.1418
2022-06-30 11:23:28.491048: validation loss: 0.1161
2022-06-30 11:23:28.494793: Average global foreground Dice: [0.7396, 0.4634, 0.7145, 0.3771, 0.4111, 0.8603, 0.4618, 0.6689, 0.5668, 0.4317, 0.3314, 0.2246, 0.2554, 0.7522, 0.4745]
2022-06-30 11:23:28.497281: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:23:28.915298: Suus1 maybe_update_lr lr: 0.001394
2022-06-30 11:23:28.918174: This epoch took 294.371261 s

2022-06-30 11:23:28.921041: 
epoch:  444
2022-06-30 11:28:06.307547: train loss : 0.1035
2022-06-30 11:28:22.951879: validation loss: 0.1413
2022-06-30 11:28:22.959690: Average global foreground Dice: [0.7751, 0.6956, 0.7029, 0.3504, 0.3196, 0.8487, 0.4728, 0.6882, 0.5411, 0.4076, 0.3387, 0.2408, 0.2199, 0.7787, 0.5762]
2022-06-30 11:28:22.962068: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:28:23.387034: Suus1 maybe_update_lr lr: 0.001372
2022-06-30 11:28:23.389990: This epoch took 294.466883 s

2022-06-30 11:28:23.392641: 
epoch:  445
2022-06-30 11:33:00.758159: train loss : 0.1083
2022-06-30 11:33:17.412836: validation loss: 0.0649
2022-06-30 11:33:17.416308: Average global foreground Dice: [0.7018, 0.6282, 0.6437, 0.4383, 0.3426, 0.8835, 0.5304, 0.6779, 0.5847, 0.4393, 0.3033, 0.2243, 0.3107, 0.6754, 0.6519]
2022-06-30 11:33:17.418911: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:33:17.843395: Suus1 maybe_update_lr lr: 0.001349
2022-06-30 11:33:17.846390: saving best epoch checkpoint...
2022-06-30 11:33:18.044588: saving checkpoint...
2022-06-30 11:33:22.458638: done, saving took 4.61 seconds
2022-06-30 11:33:22.474222: This epoch took 299.079395 s

2022-06-30 11:33:22.476774: 
epoch:  446
2022-06-30 11:37:59.708549: train loss : 0.1161
2022-06-30 11:38:16.408555: validation loss: 0.1767
2022-06-30 11:38:16.413167: Average global foreground Dice: [0.6791, 0.705, 0.6056, 0.4456, 0.3974, 0.8629, 0.4701, 0.7036, 0.5779, 0.4492, 0.3396, 0.2293, 0.2647, 0.7293, 0.6411]
2022-06-30 11:38:16.415655: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:38:16.845809: Suus1 maybe_update_lr lr: 0.001327
2022-06-30 11:38:16.884112: saving best epoch checkpoint...
2022-06-30 11:38:17.152916: saving checkpoint...
2022-06-30 11:38:20.901025: done, saving took 3.99 seconds
2022-06-30 11:38:20.931513: This epoch took 298.452296 s

2022-06-30 11:38:20.934195: 
epoch:  447
2022-06-30 11:42:58.066236: train loss : 0.0964
2022-06-30 11:43:14.700217: validation loss: 0.1039
2022-06-30 11:43:14.704188: Average global foreground Dice: [0.6864, 0.7095, 0.6495, 0.476, 0.4057, 0.8978, 0.5146, 0.6957, 0.5456, 0.4159, 0.374, 0.2199, 0.2942, 0.7132, 0.5722]
2022-06-30 11:43:14.706865: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:43:15.142253: Suus1 maybe_update_lr lr: 0.001304
2022-06-30 11:43:15.144556: saving best epoch checkpoint...
2022-06-30 11:43:15.466676: saving checkpoint...
2022-06-30 11:43:19.148647: done, saving took 4.00 seconds
2022-06-30 11:43:19.163351: This epoch took 298.226404 s

2022-06-30 11:43:19.167719: 
epoch:  448
2022-06-30 11:47:56.167572: train loss : 0.0678
2022-06-30 11:48:12.816282: validation loss: 0.1786
2022-06-30 11:48:12.831719: Average global foreground Dice: [0.6272, 0.6923, 0.6314, 0.512, 0.4646, 0.8591, 0.4052, 0.6868, 0.5414, 0.3513, 0.3125, 0.2197, 0.2484, 0.4891, 0.3665]
2022-06-30 11:48:12.842131: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:48:13.299919: Suus1 maybe_update_lr lr: 0.001282
2022-06-30 11:48:13.302630: This epoch took 294.131161 s

2022-06-30 11:48:13.305372: 
epoch:  449
2022-06-30 11:52:50.409623: train loss : 0.1321
2022-06-30 11:53:07.048879: validation loss: 0.1044
2022-06-30 11:53:07.063656: Average global foreground Dice: [0.6618, 0.5002, 0.6051, 0.448, 0.4332, 0.8803, 0.3676, 0.6961, 0.5533, 0.3725, 0.3727, 0.2278, 0.2434, 0.6858, 0.5761]
2022-06-30 11:53:07.073919: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:53:07.527363: Suus1 maybe_update_lr lr: 0.001259
2022-06-30 11:53:07.530679: saving scheduled checkpoint file...
2022-06-30 11:53:07.852752: saving checkpoint...
2022-06-30 11:53:12.297754: done, saving took 4.76 seconds
2022-06-30 11:53:12.318874: done
2022-06-30 11:53:12.333839: This epoch took 299.026178 s

2022-06-30 11:53:12.347788: 
epoch:  450
2022-06-30 11:57:49.224892: train loss : 0.1063
2022-06-30 11:58:05.902695: validation loss: 0.1362
2022-06-30 11:58:05.906255: Average global foreground Dice: [0.6391, 0.6789, 0.5961, 0.486, 0.4644, 0.8967, 0.5118, 0.6591, 0.579, 0.4139, 0.3857, 0.1699, 0.2962, 0.8098, 0.6241]
2022-06-30 11:58:05.908538: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:58:06.338712: Suus1 maybe_update_lr lr: 0.001236
2022-06-30 11:58:06.341313: saving best epoch checkpoint...
2022-06-30 11:58:06.656037: saving checkpoint...
2022-06-30 11:58:10.184778: done, saving took 3.84 seconds
2022-06-30 11:58:10.199462: This epoch took 297.846374 s

2022-06-30 11:58:10.202055: 
epoch:  451
2022-06-30 12:02:47.137072: train loss : 0.1190
2022-06-30 12:03:03.818221: validation loss: 0.1050
2022-06-30 12:03:03.821578: Average global foreground Dice: [0.7141, 0.7113, 0.7109, 0.3218, 0.3587, 0.8717, 0.4375, 0.7078, 0.5425, 0.4352, 0.3721, 0.2858, 0.2454, 0.6583, 0.4596]
2022-06-30 12:03:03.823997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:03:04.248576: Suus1 maybe_update_lr lr: 0.001214
2022-06-30 12:03:04.250885: saving best epoch checkpoint...
2022-06-30 12:03:04.596425: saving checkpoint...
2022-06-30 12:03:07.994901: done, saving took 3.74 seconds
2022-06-30 12:03:08.026884: This epoch took 297.822465 s

2022-06-30 12:03:08.029013: 
epoch:  452
2022-06-30 12:07:44.859890: train loss : 0.1216
2022-06-30 12:08:01.532334: validation loss: 0.1745
2022-06-30 12:08:01.535963: Average global foreground Dice: [0.5657, 0.6424, 0.4968, 0.5404, 0.3805, 0.8337, 0.3419, 0.6291, 0.5597, 0.4774, 0.2627, 0.1971, 0.3343, 0.4971, 0.5092]
2022-06-30 12:08:01.538237: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:08:02.115641: Suus1 maybe_update_lr lr: 0.001191
2022-06-30 12:08:02.118039: This epoch took 294.086909 s

2022-06-30 12:08:02.120620: 
epoch:  453
2022-06-30 12:12:39.284423: train loss : 0.1221
2022-06-30 12:12:55.887597: validation loss: 0.1087
2022-06-30 12:12:55.890944: Average global foreground Dice: [0.732, 0.6304, 0.6628, 0.3705, 0.3378, 0.8541, 0.4601, 0.6813, 0.5776, 0.4235, 0.3135, 0.2277, 0.3257, 0.3156, 0.4459]
2022-06-30 12:12:55.903864: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:12:56.337521: Suus1 maybe_update_lr lr: 0.001168
2022-06-30 12:12:56.339961: This epoch took 294.217240 s

2022-06-30 12:12:56.342317: 
epoch:  454
2022-06-30 12:17:33.425761: train loss : 0.1126
2022-06-30 12:17:50.062414: validation loss: 0.0829
2022-06-30 12:17:50.067249: Average global foreground Dice: [0.6341, 0.6039, 0.6173, 0.4015, 0.4557, 0.8617, 0.5353, 0.7169, 0.5845, 0.4382, 0.3812, 0.2102, 0.3032, 0.7017, 0.6349]
2022-06-30 12:17:50.069484: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:17:50.492447: Suus1 maybe_update_lr lr: 0.001145
2022-06-30 12:17:50.494729: This epoch took 294.150115 s

2022-06-30 12:17:50.497288: 
epoch:  455
2022-06-30 12:22:27.467176: train loss : 0.1114
2022-06-30 12:22:44.098362: validation loss: 0.1275
2022-06-30 12:22:44.108253: Average global foreground Dice: [0.7146, 0.6006, 0.6071, 0.3375, 0.3936, 0.8645, 0.5559, 0.6844, 0.5413, 0.4255, 0.3471, 0.2012, 0.2808, 0.7716, 0.43]
2022-06-30 12:22:44.118076: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:22:44.543707: Suus1 maybe_update_lr lr: 0.001122
2022-06-30 12:22:44.546645: This epoch took 294.047269 s

2022-06-30 12:22:44.551350: 
epoch:  456
2022-06-30 12:27:21.496347: train loss : 0.1074
2022-06-30 12:27:38.091491: validation loss: 0.1313
2022-06-30 12:27:38.098380: Average global foreground Dice: [0.7073, 0.5826, 0.6137, 0.2899, 0.3652, 0.8769, 0.5175, 0.6252, 0.5611, 0.3758, 0.2984, 0.2199, 0.2914, 0.7926, 0.417]
2022-06-30 12:27:38.101028: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:27:38.523070: Suus1 maybe_update_lr lr: 0.001099
2022-06-30 12:27:38.525415: This epoch took 293.970721 s

2022-06-30 12:27:38.527405: 
epoch:  457
2022-06-30 12:32:15.553627: train loss : 0.1030
2022-06-30 12:32:32.145279: validation loss: 0.1847
2022-06-30 12:32:32.149080: Average global foreground Dice: [0.6402, 0.5096, 0.6621, 0.4036, 0.371, 0.799, 0.4898, 0.6621, 0.5742, 0.416, 0.2832, 0.2739, 0.2781, 0.6158, 0.544]
2022-06-30 12:32:32.151953: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:32:32.573259: Suus1 maybe_update_lr lr: 0.001076
2022-06-30 12:32:32.578436: This epoch took 294.048870 s

2022-06-30 12:32:32.580800: 
epoch:  458
2022-06-30 12:37:09.399194: train loss : 0.0854
2022-06-30 12:37:25.971840: validation loss: 0.0769
2022-06-30 12:37:25.975368: Average global foreground Dice: [0.7331, 0.7451, 0.7377, 0.5449, 0.4661, 0.8638, 0.4651, 0.736, 0.6103, 0.4883, 0.3493, 0.2341, 0.2204, 0.7051, 0.5755]
2022-06-30 12:37:25.977494: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:37:26.407495: Suus1 maybe_update_lr lr: 0.001053
2022-06-30 12:37:26.409721: This epoch took 293.826163 s

2022-06-30 12:37:26.412041: 
epoch:  459
2022-06-30 12:42:03.253630: train loss : 0.1005
2022-06-30 12:42:19.866513: validation loss: 0.0872
2022-06-30 12:42:19.869812: Average global foreground Dice: [0.7928, 0.5876, 0.5921, 0.4206, 0.3774, 0.8821, 0.4922, 0.7, 0.5762, 0.444, 0.3977, 0.2556, 0.2397, 0.4952, 0.5614]
2022-06-30 12:42:19.872118: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:42:20.299913: Suus1 maybe_update_lr lr: 0.00103
2022-06-30 12:42:20.302369: This epoch took 293.888182 s

2022-06-30 12:42:20.305052: 
epoch:  460
2022-06-30 12:46:57.070690: train loss : 0.0670
2022-06-30 12:47:13.657775: validation loss: 0.0710
2022-06-30 12:47:13.661016: Average global foreground Dice: [0.8007, 0.778, 0.7459, 0.5054, 0.4099, 0.8843, 0.5325, 0.7074, 0.6384, 0.4961, 0.3503, 0.2507, 0.3418, 0.7407, 0.4629]
2022-06-30 12:47:13.663224: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:47:14.092018: Suus1 maybe_update_lr lr: 0.001007
2022-06-30 12:47:14.094593: saving best epoch checkpoint...
2022-06-30 12:47:14.331552: saving checkpoint...
2022-06-30 12:47:17.775211: done, saving took 3.68 seconds
2022-06-30 12:47:17.791495: This epoch took 297.484292 s

2022-06-30 12:47:17.793866: 
epoch:  461
2022-06-30 12:51:54.621498: train loss : 0.0898
2022-06-30 12:52:11.200187: validation loss: 0.1233
2022-06-30 12:52:11.203841: Average global foreground Dice: [0.6517, 0.6501, 0.6586, 0.2876, 0.5311, 0.8417, 0.513, 0.7011, 0.5851, 0.4375, 0.3396, 0.2837, 0.2596, 0.35, 0.5434]
2022-06-30 12:52:11.206208: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:52:11.636528: Suus1 maybe_update_lr lr: 0.000983
2022-06-30 12:52:11.639009: This epoch took 293.842041 s

2022-06-30 12:52:11.641080: 
epoch:  462
2022-06-30 12:56:48.851140: train loss : 0.1006
2022-06-30 12:57:05.436692: validation loss: 0.1960
2022-06-30 12:57:05.440145: Average global foreground Dice: [0.6879, 0.6301, 0.6536, 0.4199, 0.3838, 0.8386, 0.4432, 0.6803, 0.5513, 0.4249, 0.3554, 0.2017, 0.2928, 0.3682, 0.4536]
2022-06-30 12:57:05.442217: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:57:05.872358: Suus1 maybe_update_lr lr: 0.00096
2022-06-30 12:57:05.874944: This epoch took 294.231229 s

2022-06-30 12:57:05.877201: 
epoch:  463
2022-06-30 13:01:42.836538: train loss : 0.0652
2022-06-30 13:01:59.428493: validation loss: 0.0702
2022-06-30 13:01:59.431964: Average global foreground Dice: [0.6388, 0.7727, 0.7207, 0.4971, 0.4687, 0.8674, 0.5252, 0.735, 0.5993, 0.4356, 0.3518, 0.2945, 0.3379, 0.583, 0.6209]
2022-06-30 13:01:59.434304: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:01:59.853544: Suus1 maybe_update_lr lr: 0.000937
2022-06-30 13:01:59.855673: This epoch took 293.976331 s

2022-06-30 13:01:59.857782: 
epoch:  464
2022-06-30 13:06:36.909907: train loss : 0.0852
2022-06-30 13:06:53.518551: validation loss: 0.0509
2022-06-30 13:06:53.526444: Average global foreground Dice: [0.6433, 0.6661, 0.6291, 0.4597, 0.5182, 0.8779, 0.5394, 0.729, 0.5902, 0.4831, 0.3513, 0.2179, 0.3473, 0.7567, 0.5685]
2022-06-30 13:06:53.533215: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:06:53.968763: Suus1 maybe_update_lr lr: 0.000913
2022-06-30 13:06:53.971117: saving best epoch checkpoint...
2022-06-30 13:06:54.163439: saving checkpoint...
2022-06-30 13:06:57.716381: done, saving took 3.74 seconds
2022-06-30 13:06:57.740698: This epoch took 297.880851 s

2022-06-30 13:06:57.742914: 
epoch:  465
2022-06-30 13:11:34.682535: train loss : 0.0887
2022-06-30 13:11:51.281182: validation loss: 0.0772
2022-06-30 13:11:51.285363: Average global foreground Dice: [0.7125, 0.7204, 0.7172, 0.4696, 0.4239, 0.879, 0.4659, 0.7084, 0.5574, 0.4312, 0.2762, 0.2703, 0.2959, 0.5676, 0.6435]
2022-06-30 13:11:51.288022: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:11:51.711019: Suus1 maybe_update_lr lr: 0.00089
2022-06-30 13:11:51.713639: saving best epoch checkpoint...
2022-06-30 13:11:51.853698: saving checkpoint...
2022-06-30 13:11:55.291507: done, saving took 3.58 seconds
2022-06-30 13:11:55.309910: This epoch took 297.564853 s

2022-06-30 13:11:55.312281: 
epoch:  466
2022-06-30 13:16:32.091808: train loss : 0.1020
2022-06-30 13:16:48.767298: validation loss: 0.0678
2022-06-30 13:16:48.771321: Average global foreground Dice: [0.7542, 0.7013, 0.6937, 0.3672, 0.4425, 0.8437, 0.5842, 0.7199, 0.597, 0.4365, 0.3145, 0.2664, 0.2726, 0.7474, 0.6042]
2022-06-30 13:16:48.773412: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:16:49.197931: Suus1 maybe_update_lr lr: 0.000866
2022-06-30 13:16:49.202522: saving best epoch checkpoint...
2022-06-30 13:16:49.387941: saving checkpoint...
2022-06-30 13:16:52.903842: done, saving took 3.70 seconds
2022-06-30 13:16:52.918319: This epoch took 297.603670 s

2022-06-30 13:16:52.934357: 
epoch:  467
2022-06-30 13:21:30.136835: train loss : 0.0872
2022-06-30 13:21:46.730634: validation loss: 0.0546
2022-06-30 13:21:46.737584: Average global foreground Dice: [0.7521, 0.726, 0.7335, 0.4635, 0.5079, 0.8743, 0.5215, 0.776, 0.6447, 0.4874, 0.3717, 0.2728, 0.3119, 0.5306, 0.56]
2022-06-30 13:21:46.740261: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:21:47.181364: Suus1 maybe_update_lr lr: 0.000842
2022-06-30 13:21:47.184613: saving best epoch checkpoint...
2022-06-30 13:21:47.335015: saving checkpoint...
2022-06-30 13:21:50.653006: done, saving took 3.47 seconds
2022-06-30 13:21:50.668263: This epoch took 297.730093 s

2022-06-30 13:21:50.672328: 
epoch:  468
2022-06-30 13:26:27.927159: train loss : 0.0982
2022-06-30 13:26:44.509571: validation loss: 0.0580
2022-06-30 13:26:44.513884: Average global foreground Dice: [0.7448, 0.7005, 0.647, 0.3317, 0.4311, 0.8779, 0.4999, 0.7465, 0.5854, 0.4568, 0.3954, 0.2766, 0.2922, 0.7347, 0.5233]
2022-06-30 13:26:44.516176: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:26:44.958611: Suus1 maybe_update_lr lr: 0.000819
2022-06-30 13:26:44.960948: saving best epoch checkpoint...
2022-06-30 13:26:45.114913: saving checkpoint...
2022-06-30 13:26:48.575090: done, saving took 3.61 seconds
2022-06-30 13:26:48.595793: This epoch took 297.920952 s

2022-06-30 13:26:48.598502: 
epoch:  469
2022-06-30 13:31:25.617044: train loss : 0.0770
2022-06-30 13:31:42.207526: validation loss: 0.1383
2022-06-30 13:31:42.214504: Average global foreground Dice: [0.6992, 0.6685, 0.5753, 0.4051, 0.4424, 0.8725, 0.5701, 0.6672, 0.62, 0.4392, 0.3621, 0.1814, 0.2714, 0.7019, 0.4657]
2022-06-30 13:31:42.216765: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:31:42.657045: Suus1 maybe_update_lr lr: 0.000795
2022-06-30 13:31:42.659352: This epoch took 294.057357 s

2022-06-30 13:31:42.661765: 
epoch:  470
2022-06-30 13:36:19.809862: train loss : 0.0783
2022-06-30 13:36:36.387844: validation loss: 0.0696
2022-06-30 13:36:36.391630: Average global foreground Dice: [0.7507, 0.7604, 0.7396, 0.5, 0.3755, 0.8492, 0.4439, 0.767, 0.6252, 0.4753, 0.3634, 0.2353, 0.3008, 0.4047, 0.5512]
2022-06-30 13:36:36.393891: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:36:36.825874: Suus1 maybe_update_lr lr: 0.000771
2022-06-30 13:36:36.828434: This epoch took 294.164181 s

2022-06-30 13:36:36.830666: 
epoch:  471
2022-06-30 13:41:13.964566: train loss : 0.0835
2022-06-30 13:41:30.539029: validation loss: 0.0553
2022-06-30 13:41:30.547545: Average global foreground Dice: [0.7525, 0.6394, 0.6473, 0.5502, 0.411, 0.8725, 0.5309, 0.729, 0.6465, 0.5204, 0.4213, 0.2445, 0.3402, 0.6203, 0.497]
2022-06-30 13:41:30.561644: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:41:31.008529: Suus1 maybe_update_lr lr: 0.000747
2022-06-30 13:41:31.011317: saving best epoch checkpoint...
2022-06-30 13:41:31.289258: saving checkpoint...
2022-06-30 13:41:34.696487: done, saving took 3.68 seconds
2022-06-30 13:41:34.711284: This epoch took 297.878363 s

2022-06-30 13:41:34.713326: 
epoch:  472
2022-06-30 13:46:11.624883: train loss : 0.0737
2022-06-30 13:46:28.308941: validation loss: 0.0323
2022-06-30 13:46:28.312461: Average global foreground Dice: [0.6619, 0.6963, 0.6262, 0.4449, 0.4746, 0.8858, 0.4661, 0.7154, 0.5852, 0.4622, 0.4098, 0.2329, 0.3618, 0.6003, 0.591]
2022-06-30 13:46:28.314710: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:46:28.870415: Suus1 maybe_update_lr lr: 0.000723
2022-06-30 13:46:28.873028: saving best epoch checkpoint...
2022-06-30 13:46:28.998587: saving checkpoint...
2022-06-30 13:46:32.527235: done, saving took 3.65 seconds
2022-06-30 13:46:32.543939: This epoch took 297.828287 s

2022-06-30 13:46:32.546158: 
epoch:  473
2022-06-30 13:51:09.745011: train loss : 0.0799
2022-06-30 13:51:26.340659: validation loss: 0.0877
2022-06-30 13:51:26.345655: Average global foreground Dice: [0.721, 0.6941, 0.5638, 0.4494, 0.415, 0.8882, 0.5015, 0.709, 0.6049, 0.5272, 0.4078, 0.2063, 0.3422, 0.776, 0.6032]
2022-06-30 13:51:26.348691: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:51:26.779364: Suus1 maybe_update_lr lr: 0.000699
2022-06-30 13:51:26.781755: saving best epoch checkpoint...
2022-06-30 13:51:26.912892: saving checkpoint...
2022-06-30 13:51:30.375881: done, saving took 3.59 seconds
2022-06-30 13:51:30.394882: This epoch took 297.846290 s

2022-06-30 13:51:30.397501: 
epoch:  474
2022-06-30 13:56:07.659196: train loss : 0.0855
2022-06-30 13:56:24.248858: validation loss: 0.1653
2022-06-30 13:56:24.251941: Average global foreground Dice: [0.7632, 0.6212, 0.6271, 0.4041, 0.4232, 0.85, 0.4943, 0.6955, 0.5685, 0.432, 0.3769, 0.2485, 0.3324, 0.589, 0.5074]
2022-06-30 13:56:24.254122: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:56:24.678647: Suus1 maybe_update_lr lr: 0.000675
2022-06-30 13:56:24.681606: This epoch took 294.280588 s

2022-06-30 13:56:24.684227: 
epoch:  475
2022-06-30 14:01:01.920826: train loss : 0.1029
2022-06-30 14:01:18.519940: validation loss: 0.0586
2022-06-30 14:01:18.523641: Average global foreground Dice: [0.7557, 0.6901, 0.6922, 0.4357, 0.4252, 0.9058, 0.44, 0.6903, 0.5766, 0.4416, 0.3755, 0.2285, 0.3684, 0.4488, 0.5248]
2022-06-30 14:01:18.526407: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:01:18.947733: Suus1 maybe_update_lr lr: 0.00065
2022-06-30 14:01:18.951209: This epoch took 294.264435 s

2022-06-30 14:01:18.960581: 
epoch:  476
2022-06-30 14:05:56.208228: train loss : 0.0625
2022-06-30 14:06:12.795257: validation loss: 0.1069
2022-06-30 14:06:12.799680: Average global foreground Dice: [0.6218, 0.6161, 0.6171, 0.4702, 0.4414, 0.8662, 0.4546, 0.6913, 0.5494, 0.4615, 0.3864, 0.2585, 0.3053, 0.6597, 0.497]
2022-06-30 14:06:12.802414: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:06:13.235837: Suus1 maybe_update_lr lr: 0.000626
2022-06-30 14:06:13.238468: This epoch took 294.274701 s

2022-06-30 14:06:13.240747: 
epoch:  477
2022-06-30 14:10:50.346729: train loss : 0.0797
2022-06-30 14:11:06.937624: validation loss: 0.1081
2022-06-30 14:11:06.948025: Average global foreground Dice: [0.7672, 0.707, 0.734, 0.4847, 0.3541, 0.8579, 0.5582, 0.6803, 0.5866, 0.4393, 0.4105, 0.3083, 0.2707, 0.7224, 0.6021]
2022-06-30 14:11:06.952057: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:11:07.395012: Suus1 maybe_update_lr lr: 0.000601
2022-06-30 14:11:07.397628: This epoch took 294.154585 s

2022-06-30 14:11:07.414822: 
epoch:  478
2022-06-30 14:15:44.486774: train loss : 0.0906
2022-06-30 14:16:01.091557: validation loss: 0.1061
2022-06-30 14:16:01.094503: Average global foreground Dice: [0.7243, 0.6525, 0.6321, 0.491, 0.4075, 0.8747, 0.5932, 0.6768, 0.5325, 0.4531, 0.3323, 0.3363, 0.2855, 0.5878, 0.573]
2022-06-30 14:16:01.096725: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:16:01.532823: Suus1 maybe_update_lr lr: 0.000577
2022-06-30 14:16:01.535361: This epoch took 294.118175 s

2022-06-30 14:16:01.537580: 
epoch:  479
2022-06-30 14:20:38.643918: train loss : 0.0918
2022-06-30 14:20:55.238504: validation loss: 0.0587
2022-06-30 14:20:55.244648: Average global foreground Dice: [0.7464, 0.7078, 0.7152, 0.4216, 0.5628, 0.871, 0.5853, 0.755, 0.6003, 0.4391, 0.4102, 0.3203, 0.2784, 0.7992, 0.6154]
2022-06-30 14:20:55.248477: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:20:55.689019: Suus1 maybe_update_lr lr: 0.000552
2022-06-30 14:20:55.692396: saving best epoch checkpoint...
2022-06-30 14:20:55.909586: saving checkpoint...
2022-06-30 14:21:02.826954: done, saving took 7.13 seconds
2022-06-30 14:21:02.853846: This epoch took 301.314079 s

2022-06-30 14:21:02.861180: 
epoch:  480
2022-06-30 14:25:39.887059: train loss : 0.0676
2022-06-30 14:25:56.487235: validation loss: 0.0769
2022-06-30 14:25:56.491284: Average global foreground Dice: [0.7451, 0.6978, 0.6512, 0.5364, 0.4509, 0.8946, 0.5423, 0.7004, 0.5932, 0.4792, 0.3731, 0.2962, 0.3371, 0.684, 0.5298]
2022-06-30 14:25:56.494333: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:25:56.926154: Suus1 maybe_update_lr lr: 0.000527
2022-06-30 14:25:56.928570: saving best epoch checkpoint...
2022-06-30 14:25:57.235451: saving checkpoint...
2022-06-30 14:26:01.201814: done, saving took 4.27 seconds
2022-06-30 14:26:01.218015: This epoch took 298.353053 s

2022-06-30 14:26:01.224922: 
epoch:  481
2022-06-30 14:30:38.600785: train loss : 0.0699
2022-06-30 14:30:55.194894: validation loss: 0.0806
2022-06-30 14:30:55.199060: Average global foreground Dice: [0.7487, 0.6804, 0.6197, 0.4421, 0.4433, 0.8367, 0.4745, 0.7266, 0.5977, 0.4975, 0.315, 0.2323, 0.3211, 0.742, 0.6359]
2022-06-30 14:30:55.202537: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:30:55.647290: Suus1 maybe_update_lr lr: 0.000502
2022-06-30 14:30:55.649669: saving best epoch checkpoint...
2022-06-30 14:30:55.950951: saving checkpoint...
2022-06-30 14:31:01.603230: done, saving took 5.95 seconds
2022-06-30 14:31:01.623650: This epoch took 300.393251 s

2022-06-30 14:31:01.626172: 
epoch:  482
2022-06-30 14:35:38.809265: train loss : 0.0614
2022-06-30 14:35:55.408220: validation loss: 0.0765
2022-06-30 14:35:55.416134: Average global foreground Dice: [0.738, 0.6751, 0.6874, 0.3822, 0.3485, 0.8865, 0.5307, 0.6569, 0.6228, 0.4426, 0.3265, 0.2385, 0.2924, 0.7485, 0.6741]
2022-06-30 14:35:55.428270: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:35:55.995068: Suus1 maybe_update_lr lr: 0.000477
2022-06-30 14:35:55.998457: saving best epoch checkpoint...
2022-06-30 14:35:56.116737: saving checkpoint...
2022-06-30 14:36:00.107303: done, saving took 4.11 seconds
2022-06-30 14:36:00.129050: This epoch took 298.499618 s

2022-06-30 14:36:00.131561: 
epoch:  483
2022-06-30 14:40:37.176383: train loss : 0.0746
2022-06-30 14:40:53.763279: validation loss: 0.0700
2022-06-30 14:40:53.770357: Average global foreground Dice: [0.7058, 0.6598, 0.5545, 0.4684, 0.5298, 0.8945, 0.4662, 0.7561, 0.5827, 0.4764, 0.3646, 0.2754, 0.3354, 0.5484, 0.5583]
2022-06-30 14:40:53.774413: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:40:54.211112: Suus1 maybe_update_lr lr: 0.000451
2022-06-30 14:40:54.217366: This epoch took 294.081797 s

2022-06-30 14:40:54.220056: 
epoch:  484
2022-06-30 14:45:31.088388: train loss : 0.0793
2022-06-30 14:45:47.650968: validation loss: 0.0528
2022-06-30 14:45:47.656203: Average global foreground Dice: [0.8005, 0.764, 0.6767, 0.4656, 0.4997, 0.85, 0.5347, 0.7496, 0.6013, 0.4629, 0.3931, 0.3083, 0.334, 0.6747, 0.7618]
2022-06-30 14:45:47.660715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:45:48.103772: Suus1 maybe_update_lr lr: 0.000426
2022-06-30 14:45:48.110244: saving best epoch checkpoint...
2022-06-30 14:45:48.239852: saving checkpoint...
2022-06-30 14:45:54.338753: done, saving took 6.22 seconds
2022-06-30 14:45:54.359664: This epoch took 300.135468 s

2022-06-30 14:45:54.363087: 
epoch:  485
2022-06-30 14:50:31.130826: train loss : 0.0575
2022-06-30 14:50:47.726770: validation loss: 0.0703
2022-06-30 14:50:47.731101: Average global foreground Dice: [0.7941, 0.687, 0.7019, 0.4164, 0.4577, 0.9001, 0.4858, 0.73, 0.5989, 0.461, 0.3889, 0.3115, 0.3499, 0.6317, 0.5714]
2022-06-30 14:50:47.734105: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:50:48.172503: Suus1 maybe_update_lr lr: 0.0004
2022-06-30 14:50:48.177079: saving best epoch checkpoint...
2022-06-30 14:50:48.358934: saving checkpoint...
2022-06-30 14:50:53.639032: done, saving took 5.46 seconds
2022-06-30 14:50:53.672507: This epoch took 299.305653 s

2022-06-30 14:50:53.682440: 
epoch:  486
2022-06-30 14:55:30.532916: train loss : 0.0483
2022-06-30 14:55:47.157865: validation loss: 0.1296
2022-06-30 14:55:47.163708: Average global foreground Dice: [0.694, 0.5991, 0.6933, 0.3881, 0.5008, 0.8431, 0.5279, 0.7433, 0.5642, 0.4729, 0.2703, 0.304, 0.2616, 0.4415, 0.468]
2022-06-30 14:55:47.166745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:55:47.617333: Suus1 maybe_update_lr lr: 0.000375
2022-06-30 14:55:47.620772: This epoch took 293.932449 s

2022-06-30 14:55:47.623832: 
epoch:  487
2022-06-30 15:00:24.778113: train loss : 0.0629
2022-06-30 15:00:41.450172: validation loss: 0.0810
2022-06-30 15:00:41.453613: Average global foreground Dice: [0.73, 0.6274, 0.7022, 0.4797, 0.4318, 0.8988, 0.5794, 0.6867, 0.5934, 0.4366, 0.3531, 0.3217, 0.3001, 0.6438, 0.5067]
2022-06-30 15:00:41.456124: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:00:41.893681: Suus1 maybe_update_lr lr: 0.000348
2022-06-30 15:00:41.897975: This epoch took 294.271156 s

2022-06-30 15:00:41.901023: 
epoch:  488
2022-06-30 15:05:19.090685: train loss : 0.0665
2022-06-30 15:05:35.784763: validation loss: 0.0665
2022-06-30 15:05:35.799796: Average global foreground Dice: [0.7783, 0.7361, 0.7124, 0.5536, 0.4601, 0.9173, 0.4788, 0.7389, 0.6218, 0.4598, 0.4523, 0.2822, 0.3253, 0.4789, 0.5701]
2022-06-30 15:05:35.816923: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:05:36.265634: Suus1 maybe_update_lr lr: 0.000322
2022-06-30 15:05:36.269861: This epoch took 294.365766 s

2022-06-30 15:05:36.273468: 
epoch:  489
2022-06-30 15:10:13.179424: train loss : 0.0811
2022-06-30 15:10:29.801167: validation loss: 0.1214
2022-06-30 15:10:29.806550: Average global foreground Dice: [0.7346, 0.6722, 0.6512, 0.4274, 0.4168, 0.8667, 0.5929, 0.652, 0.5703, 0.4418, 0.3423, 0.227, 0.2881, 0.5781, 0.4834]
2022-06-30 15:10:29.810426: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:10:30.255667: Suus1 maybe_update_lr lr: 0.000296
2022-06-30 15:10:30.259440: This epoch took 293.983382 s

2022-06-30 15:10:30.265994: 
epoch:  490
2022-06-30 15:15:07.210787: train loss : 0.0437
2022-06-30 15:15:23.880240: validation loss: 0.0487
2022-06-30 15:15:23.885540: Average global foreground Dice: [0.8216, 0.7051, 0.7743, 0.3607, 0.5005, 0.8607, 0.5042, 0.766, 0.5984, 0.4771, 0.3323, 0.2839, 0.3687, 0.6327, 0.5679]
2022-06-30 15:15:23.888306: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:15:24.344972: Suus1 maybe_update_lr lr: 0.000269
2022-06-30 15:15:24.348117: This epoch took 294.078750 s

2022-06-30 15:15:24.350841: 
epoch:  491
2022-06-30 15:20:01.256243: train loss : 0.1123
2022-06-30 15:20:17.920799: validation loss: 0.0732
2022-06-30 15:20:17.925645: Average global foreground Dice: [0.7316, 0.6707, 0.5776, 0.4187, 0.3733, 0.8888, 0.5036, 0.6617, 0.5932, 0.429, 0.3706, 0.232, 0.3082, 0.6456, 0.6223]
2022-06-30 15:20:17.928708: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:20:18.401617: Suus1 maybe_update_lr lr: 0.000242
2022-06-30 15:20:18.404948: This epoch took 294.049340 s

2022-06-30 15:20:18.408187: 
epoch:  492
2022-06-30 15:24:55.280209: train loss : 0.0675
2022-06-30 15:25:11.942498: validation loss: 0.0571
2022-06-30 15:25:11.947721: Average global foreground Dice: [0.7557, 0.6557, 0.7178, 0.5267, 0.4903, 0.8853, 0.5409, 0.7022, 0.6051, 0.4589, 0.4072, 0.2605, 0.3049, 0.6982, 0.609]
2022-06-30 15:25:11.950298: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:25:12.395629: Suus1 maybe_update_lr lr: 0.000215
2022-06-30 15:25:12.398537: This epoch took 293.986817 s

2022-06-30 15:25:12.400903: 
epoch:  493
2022-06-30 15:29:49.448388: train loss : 0.0692
2022-06-30 15:30:06.059356: validation loss: 0.0603
2022-06-30 15:30:06.067131: Average global foreground Dice: [0.711, 0.6828, 0.5987, 0.4644, 0.5129, 0.885, 0.5385, 0.7548, 0.6363, 0.4152, 0.3753, 0.3199, 0.3128, 0.6892, 0.5546]
2022-06-30 15:30:06.070243: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:30:06.498991: Suus1 maybe_update_lr lr: 0.000187
2022-06-30 15:30:06.501375: This epoch took 294.097591 s

2022-06-30 15:30:06.503949: 
epoch:  494
2022-06-30 15:34:43.399707: train loss : 0.0491
2022-06-30 15:35:00.024548: validation loss: 0.0901
2022-06-30 15:35:00.029397: Average global foreground Dice: [0.7253, 0.7641, 0.6839, 0.4336, 0.4231, 0.8747, 0.5132, 0.6542, 0.5899, 0.4553, 0.3585, 0.2456, 0.3213, 0.6978, 0.6886]
2022-06-30 15:35:00.032959: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:35:00.468390: Suus1 maybe_update_lr lr: 0.000158
2022-06-30 15:35:00.471685: saving best epoch checkpoint...
2022-06-30 15:35:00.669341: saving checkpoint...
2022-06-30 15:35:05.940296: done, saving took 5.47 seconds
2022-06-30 15:35:05.971241: This epoch took 299.464649 s

2022-06-30 15:35:05.974809: 
epoch:  495
2022-06-30 15:39:42.847485: train loss : 0.0498
2022-06-30 15:39:59.449513: validation loss: 0.0762
2022-06-30 15:39:59.454966: Average global foreground Dice: [0.7643, 0.6683, 0.6941, 0.3597, 0.4383, 0.8577, 0.5241, 0.712, 0.6238, 0.4642, 0.3609, 0.3345, 0.2781, 0.657, 0.6579]
2022-06-30 15:39:59.457795: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:39:59.890801: Suus1 maybe_update_lr lr: 0.00013
2022-06-30 15:39:59.893672: saving best epoch checkpoint...
2022-06-30 15:40:00.094460: saving checkpoint...
2022-06-30 15:40:05.761316: done, saving took 5.87 seconds
2022-06-30 15:40:05.794622: This epoch took 299.814463 s

2022-06-30 15:40:05.797443: 
epoch:  496
2022-06-30 15:44:42.289640: train loss : 0.0544
2022-06-30 15:44:58.922325: validation loss: 0.0301
2022-06-30 15:44:58.930781: Average global foreground Dice: [0.7028, 0.742, 0.6495, 0.5594, 0.5368, 0.8895, 0.5482, 0.7309, 0.6559, 0.4863, 0.4154, 0.331, 0.3279, 0.753, 0.5807]
2022-06-30 15:44:58.935610: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:44:59.371965: Suus1 maybe_update_lr lr: 0.0001
2022-06-30 15:44:59.375748: saving best epoch checkpoint...
2022-06-30 15:44:59.548894: saving checkpoint...
2022-06-30 15:45:07.666212: done, saving took 8.29 seconds
2022-06-30 15:45:07.684607: This epoch took 301.884195 s

2022-06-30 15:45:07.687846: 
epoch:  497
2022-06-30 15:49:44.079168: train loss : 0.0733
2022-06-30 15:50:01.035791: validation loss: 0.0557
2022-06-30 15:50:01.040707: Average global foreground Dice: [0.7551, 0.7675, 0.7542, 0.462, 0.4421, 0.8943, 0.4269, 0.6829, 0.5992, 0.3853, 0.4153, 0.2031, 0.3165, 0.6668, 0.5258]
2022-06-30 15:50:01.044065: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:50:01.488716: Suus1 maybe_update_lr lr: 6.9e-05
2022-06-30 15:50:01.491627: This epoch took 293.799409 s

2022-06-30 15:50:01.494282: 
epoch:  498
2022-06-30 15:54:38.257435: train loss : 0.0681
2022-06-30 15:54:55.733856: validation loss: 0.0233
2022-06-30 15:54:55.739311: Average global foreground Dice: [0.7933, 0.7655, 0.7294, 0.5489, 0.4516, 0.9005, 0.561, 0.7025, 0.6079, 0.4658, 0.3719, 0.2714, 0.2997, 0.7012, 0.583]
2022-06-30 15:54:55.744544: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:54:56.191953: Suus1 maybe_update_lr lr: 3.7e-05
2022-06-30 15:54:56.194487: saving best epoch checkpoint...
2022-06-30 15:54:56.466975: saving checkpoint...
2022-06-30 15:55:01.188058: done, saving took 4.99 seconds
2022-06-30 15:55:01.205946: This epoch took 299.707521 s

2022-06-30 15:55:01.208313: 
epoch:  499
2022-06-30 15:59:37.486169: train loss : 0.0352
2022-06-30 15:59:54.150395: validation loss: 0.0738
2022-06-30 15:59:54.155958: Average global foreground Dice: [0.7449, 0.6754, 0.735, 0.4402, 0.5316, 0.8933, 0.5289, 0.7484, 0.6217, 0.5038, 0.3222, 0.3267, 0.3316, 0.518, 0.6637]
2022-06-30 15:59:54.158705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:59:54.608389: Suus1 maybe_update_lr lr: 0.0
2022-06-30 15:59:54.611076: saving scheduled checkpoint file...
2022-06-30 15:59:54.901518: saving checkpoint...
2022-06-30 15:59:59.794194: done, saving took 5.18 seconds
2022-06-30 15:59:59.823955: done
2022-06-30 15:59:59.826767: saving best epoch checkpoint...
2022-06-30 15:59:59.921754: saving checkpoint...
2022-06-30 16:00:04.070040: done, saving took 4.24 seconds
2022-06-30 16:00:04.087655: This epoch took 302.876743 s

2022-06-30 16:00:04.149168: saving checkpoint...
2022-06-30 16:00:08.405475: done, saving took 4.32 seconds
panc_0016 (2, 228, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 252
computing Gaussian
done
prediction done
suus panc_0016 transposed
suus panc_0016 not saving softmax
suus panc_0016 we moeten gekke ding doen met groot commentaar
suus panc_0016 voeg toe aan pred_gt tuples voor later
panc_0023 (2, 215, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 91, 121, 151], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0023 transposed
suus panc_0023 not saving softmax
suus panc_0023 we moeten gekke ding doen met groot commentaar
suus panc_0023 voeg toe aan pred_gt tuples voor later
panc_0027 (2, 310, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 92, 123, 154, 184, 215, 246], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 324
using precomputed Gaussian
prediction done
suus panc_0027 transposed
suus panc_0027 not saving softmax
suus panc_0027 we moeten gekke ding doen met groot commentaar
suus panc_0027 voeg toe aan pred_gt tuples voor later
panc_0033 (2, 265, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 265, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 57, 86, 115, 144, 172, 201], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 288
using precomputed Gaussian
prediction done
suus panc_0033 transposed
suus panc_0033 not saving softmax
suus panc_0033 we moeten gekke ding doen met groot commentaar
suus panc_0033 voeg toe aan pred_gt tuples voor later
panc_0043 (2, 212, 553, 553)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 212, 553, 553)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 118, 148], [0, 79, 157, 236, 314, 393], [0, 79, 157, 236, 314, 393]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0043 transposed
suus panc_0043 not saving softmax
suus panc_0043 we moeten gekke ding doen met groot commentaar
suus panc_0043 voeg toe aan pred_gt tuples voor later
panc_0045 (2, 228, 641, 641)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 641, 641)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 69, 137, 206, 275, 344, 412, 481], [0, 69, 137, 206, 275, 344, 412, 481]]
number of tiles: 448
using precomputed Gaussian
prediction done
suus panc_0045 transposed
suus panc_0045 not saving softmax
suus panc_0045 we moeten gekke ding doen met groot commentaar
suus panc_0045 voeg toe aan pred_gt tuples voor later
panc_0059 (2, 195, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 52, 79, 105, 131], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0059 transposed
suus panc_0059 not saving softmax
suus panc_0059 we moeten gekke ding doen met groot commentaar
suus panc_0059 voeg toe aan pred_gt tuples voor later
panc_0072 (2, 225, 579, 579)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 225, 579, 579)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 54, 80, 107, 134, 161], [0, 70, 140, 210, 279, 349, 419], [0, 70, 140, 210, 279, 349, 419]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0072 transposed
suus panc_0072 not saving softmax
suus panc_0072 we moeten gekke ding doen met groot commentaar
suus panc_0072 voeg toe aan pred_gt tuples voor later
panc_0083 (2, 250, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 250, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 93, 124, 155, 186], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0083 transposed
suus panc_0083 not saving softmax
suus panc_0083 we moeten gekke ding doen met groot commentaar
suus panc_0083 voeg toe aan pred_gt tuples voor later
panc_0086 (2, 200, 563, 563)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 200, 563, 563)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 54, 82, 109, 136], [0, 67, 134, 202, 269, 336, 403], [0, 67, 134, 202, 269, 336, 403]]
number of tiles: 294
using precomputed Gaussian
prediction done
suus panc_0086 transposed
suus panc_0086 not saving softmax
suus panc_0086 we moeten gekke ding doen met groot commentaar
suus panc_0086 voeg toe aan pred_gt tuples voor later
panc_0094 (2, 228, 541, 541)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 541, 541)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 76, 152, 229, 305, 381], [0, 76, 152, 229, 305, 381]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0094 transposed
suus panc_0094 not saving softmax
suus panc_0094 we moeten gekke ding doen met groot commentaar
suus panc_0094 voeg toe aan pred_gt tuples voor later
panc_0097 (2, 315, 701, 701)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 315, 701, 701)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 63, 94, 126, 157, 188, 220, 251], [0, 77, 155, 232, 309, 386, 464, 541], [0, 77, 155, 232, 309, 386, 464, 541]]
number of tiles: 576
using precomputed Gaussian
prediction done
suus panc_0097 transposed
suus panc_0097 not saving softmax
suus panc_0097 we moeten gekke ding doen met groot commentaar
suus panc_0097 voeg toe aan pred_gt tuples voor later
panc_0104 (2, 220, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 220, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 94, 125, 156], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0104 transposed
suus panc_0104 not saving softmax
suus panc_0104 we moeten gekke ding doen met groot commentaar
suus panc_0104 voeg toe aan pred_gt tuples voor later
panc_0115 (2, 220, 465, 465)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 220, 465, 465)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 94, 125, 156], [0, 76, 152, 229, 305], [0, 76, 152, 229, 305]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0115 transposed
suus panc_0115 not saving softmax
suus panc_0115 we moeten gekke ding doen met groot commentaar
suus panc_0115 voeg toe aan pred_gt tuples voor later
panc_0119 (2, 350, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 350, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 95, 127, 159, 191, 222, 254, 286], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0119 transposed
suus panc_0119 not saving softmax
suus panc_0119 we moeten gekke ding doen met groot commentaar
suus panc_0119 voeg toe aan pred_gt tuples voor later
panc_0133 (2, 170, 446, 446)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 170, 446, 446)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 53, 80, 106], [0, 72, 143, 214, 286], [0, 72, 143, 214, 286]]
number of tiles: 125
using precomputed Gaussian
prediction done
suus panc_0133 transposed
suus panc_0133 not saving softmax
suus panc_0133 we moeten gekke ding doen met groot commentaar
suus panc_0133 voeg toe aan pred_gt tuples voor later
panc_0143 (2, 180, 522, 522)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 180, 522, 522)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 87, 116], [0, 72, 145, 217, 290, 362], [0, 72, 145, 217, 290, 362]]
number of tiles: 180
using precomputed Gaussian
prediction done
suus panc_0143 transposed
suus panc_0143 not saving softmax
suus panc_0143 we moeten gekke ding doen met groot commentaar
suus panc_0143 voeg toe aan pred_gt tuples voor later
panc_0158 (2, 225, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 225, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 54, 80, 107, 134, 161], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0158 transposed
suus panc_0158 not saving softmax
suus panc_0158 we moeten gekke ding doen met groot commentaar
suus panc_0158 voeg toe aan pred_gt tuples voor later
panc_0171 (2, 242, 641, 641)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 242, 641, 641)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 119, 148, 178], [0, 69, 137, 206, 275, 344, 412, 481], [0, 69, 137, 206, 275, 344, 412, 481]]
number of tiles: 448
using precomputed Gaussian
prediction done
suus panc_0171 transposed
suus panc_0171 not saving softmax
suus panc_0171 we moeten gekke ding doen met groot commentaar
suus panc_0171 voeg toe aan pred_gt tuples voor later
panc_0172 (2, 238, 574, 574)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 238, 574, 574)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 87, 116, 145, 174], [0, 69, 138, 207, 276, 345, 414], [0, 69, 138, 207, 276, 345, 414]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0172 transposed
suus panc_0172 not saving softmax
suus panc_0172 we moeten gekke ding doen met groot commentaar
suus panc_0172 voeg toe aan pred_gt tuples voor later
panc_0235 (2, 197, 451, 451)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 197, 451, 451)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 53, 80, 106, 133], [0, 73, 146, 218, 291], [0, 73, 146, 218, 291]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0235 transposed
suus panc_0235 not saving softmax
suus panc_0235 we moeten gekke ding doen met groot commentaar
suus panc_0235 voeg toe aan pred_gt tuples voor later
panc_0249 (2, 298, 415, 415)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 298, 415, 415)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 88, 117, 146, 176, 205, 234], [0, 64, 128, 191, 255], [0, 64, 128, 191, 255]]
number of tiles: 225
using precomputed Gaussian
prediction done
suus panc_0249 transposed
suus panc_0249 not saving softmax
suus panc_0249 we moeten gekke ding doen met groot commentaar
suus panc_0249 voeg toe aan pred_gt tuples voor later
panc_0254 (2, 310, 392, 392)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 392, 392)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 92, 123, 154, 184, 215, 246], [0, 77, 155, 232], [0, 77, 155, 232]]
number of tiles: 144
using precomputed Gaussian
prediction done
suus panc_0254 transposed
suus panc_0254 not saving softmax
suus panc_0254 we moeten gekke ding doen met groot commentaar
suus panc_0254 voeg toe aan pred_gt tuples voor later
panc_0259 (2, 195, 428, 428)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 428, 428)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 52, 79, 105, 131], [0, 67, 134, 201, 268], [0, 67, 134, 201, 268]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0259 transposed
suus panc_0259 not saving softmax
suus panc_0259 we moeten gekke ding doen met groot commentaar
suus panc_0259 voeg toe aan pred_gt tuples voor later
panc_0268 (2, 191, 426, 426)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 191, 426, 426)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 95, 127], [0, 66, 133, 200, 266], [0, 66, 133, 200, 266]]
number of tiles: 125
using precomputed Gaussian
prediction done
suus panc_0268 transposed
suus panc_0268 not saving softmax
suus panc_0268 we moeten gekke ding doen met groot commentaar
suus panc_0268 voeg toe aan pred_gt tuples voor later
panc_0288 (2, 328, 490, 490)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 328, 490, 490)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 59, 88, 117, 147, 176, 205, 235, 264], [0, 66, 132, 198, 264, 330], [0, 66, 132, 198, 264, 330]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0288 transposed
suus panc_0288 not saving softmax
suus panc_0288 we moeten gekke ding doen met groot commentaar
suus panc_0288 voeg toe aan pred_gt tuples voor later
panc_0294 (2, 239, 445, 445)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 239, 445, 445)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 88, 117, 146, 175], [0, 71, 142, 214, 285], [0, 71, 142, 214, 285]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0294 transposed
suus panc_0294 not saving softmax
suus panc_0294 we moeten gekke ding doen met groot commentaar
suus panc_0294 voeg toe aan pred_gt tuples voor later
panc_0320 (2, 322, 410, 410)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 322, 410, 410)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 57, 86, 115, 143, 172, 201, 229, 258], [0, 62, 125, 188, 250], [0, 62, 125, 188, 250]]
number of tiles: 250
using precomputed Gaussian
prediction done
suus panc_0320 transposed
suus panc_0320 not saving softmax
suus panc_0320 we moeten gekke ding doen met groot commentaar
suus panc_0320 voeg toe aan pred_gt tuples voor later
panc_0321 (2, 192, 472, 472)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 192, 472, 472)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 96, 128], [0, 78, 156, 234, 312], [0, 78, 156, 234, 312]]
number of tiles: 125
using precomputed Gaussian
prediction done
suus panc_0321 transposed
suus panc_0321 not saving softmax
suus panc_0321 we moeten gekke ding doen met groot commentaar
suus panc_0321 voeg toe aan pred_gt tuples voor later
panc_0341 (2, 302, 400, 400)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 302, 400, 400)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 89, 119, 149, 178, 208, 238], [0, 80, 160, 240], [0, 80, 160, 240]]
number of tiles: 144
using precomputed Gaussian
prediction done
suus panc_0341 transposed
suus panc_0341 not saving softmax
suus panc_0341 we moeten gekke ding doen met groot commentaar
suus panc_0341 voeg toe aan pred_gt tuples voor later
panc_0350 (2, 237, 408, 408)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 408, 408)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 86, 115, 144, 173], [0, 62, 124, 186, 248], [0, 62, 124, 186, 248]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0350 transposed
suus panc_0350 not saving softmax
suus panc_0350 we moeten gekke ding doen met groot commentaar
suus panc_0350 voeg toe aan pred_gt tuples voor later
panc_0353 (2, 217, 427, 427)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 217, 427, 427)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 61, 92, 122, 153], [0, 67, 134, 200, 267], [0, 67, 134, 200, 267]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0353 transposed
suus panc_0353 not saving softmax
suus panc_0353 we moeten gekke ding doen met groot commentaar
suus panc_0353 voeg toe aan pred_gt tuples voor later
panc_0366 (2, 215, 527, 527)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 527, 527)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 91, 121, 151], [0, 73, 147, 220, 294, 367], [0, 73, 147, 220, 294, 367]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0366 transposed
suus panc_0366 not saving softmax
suus panc_0366 we moeten gekke ding doen met groot commentaar
suus panc_0366 voeg toe aan pred_gt tuples voor later
panc_0367 (2, 228, 641, 641)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 641, 641)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 69, 137, 206, 275, 344, 412, 481], [0, 69, 137, 206, 275, 344, 412, 481]]
number of tiles: 448
using precomputed Gaussian
prediction done
suus panc_0367 transposed
suus panc_0367 not saving softmax
suus panc_0367 we moeten gekke ding doen met groot commentaar
suus panc_0367 voeg toe aan pred_gt tuples voor later
panc_0383 (2, 242, 596, 596)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 242, 596, 596)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 119, 148, 178], [0, 73, 145, 218, 291, 363, 436], [0, 73, 145, 218, 291, 363, 436]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0383 transposed
suus panc_0383 not saving softmax
suus panc_0383 we moeten gekke ding doen met groot commentaar
suus panc_0383 voeg toe aan pred_gt tuples voor later
panc_0392 (2, 212, 528, 528)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 212, 528, 528)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 118, 148], [0, 74, 147, 221, 294, 368], [0, 74, 147, 221, 294, 368]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0392 transposed
suus panc_0392 not saving softmax
suus panc_0392 we moeten gekke ding doen met groot commentaar
suus panc_0392 voeg toe aan pred_gt tuples voor later
panc_0395 (2, 208, 455, 455)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 208, 455, 455)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 86, 115, 144], [0, 74, 148, 221, 295], [0, 74, 148, 221, 295]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0395 transposed
suus panc_0395 not saving softmax
suus panc_0395 we moeten gekke ding doen met groot commentaar
suus panc_0395 voeg toe aan pred_gt tuples voor later
panc_0403 (2, 218, 554, 554)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 554, 554)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 92, 123, 154], [0, 79, 158, 236, 315, 394], [0, 79, 158, 236, 315, 394]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0403 transposed
suus panc_0403 not saving softmax
suus panc_0403 we moeten gekke ding doen met groot commentaar
suus panc_0403 voeg toe aan pred_gt tuples voor later
panc_0404 (2, 210, 527, 527)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 527, 527)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 88, 117, 146], [0, 73, 147, 220, 294, 367], [0, 73, 147, 220, 294, 367]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0404 transposed
suus panc_0404 not saving softmax
suus panc_0404 we moeten gekke ding doen met groot commentaar
suus panc_0404 voeg toe aan pred_gt tuples voor later
panc_0410 (2, 268, 574, 574)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 268, 574, 574)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 87, 117, 146, 175, 204], [0, 69, 138, 207, 276, 345, 414], [0, 69, 138, 207, 276, 345, 414]]
number of tiles: 392
using precomputed Gaussian
prediction done
suus panc_0410 transposed
suus panc_0410 not saving softmax
suus panc_0410 we moeten gekke ding doen met groot commentaar
suus panc_0410 voeg toe aan pred_gt tuples voor later
panc_0507 (2, 90, 440, 486)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 90, 440, 486)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26], [0, 70, 140, 210, 280], [0, 65, 130, 196, 261, 326]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0507 transposed
suus panc_0507 not saving softmax
suus panc_0507 voeg toe aan pred_gt tuples voor later
panc_0510 (2, 224, 200, 467)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 200, 467)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 96, 128, 160], [0, 40], [0, 77, 154, 230, 307]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0510 transposed
suus panc_0510 not saving softmax
suus panc_0510 voeg toe aan pred_gt tuples voor later
panc_0514 (2, 224, 215, 467)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 215, 467)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 96, 128, 160], [0, 55], [0, 77, 154, 230, 307]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0514 transposed
suus panc_0514 not saving softmax
suus panc_0514 voeg toe aan pred_gt tuples voor later
panc_0522 (2, 108, 453, 562)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 453, 562)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 73, 146, 220, 293], [0, 67, 134, 201, 268, 335, 402]]
number of tiles: 105
using precomputed Gaussian
prediction done
suus panc_0522 transposed
suus panc_0522 not saving softmax
suus panc_0522 we moeten gekke ding doen met groot commentaar
suus panc_0522 voeg toe aan pred_gt tuples voor later
panc_0570 (2, 108, 393, 484)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 393, 484)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 78, 155, 233], [0, 65, 130, 194, 259, 324]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0570 transposed
suus panc_0570 not saving softmax
suus panc_0570 voeg toe aan pred_gt tuples voor later
panc_0587 (2, 108, 394, 486)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 394, 486)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 78, 156, 234], [0, 65, 130, 196, 261, 326]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0587 transposed
suus panc_0587 not saving softmax
suus panc_0587 voeg toe aan pred_gt tuples voor later
panc_0594 (2, 200, 237, 538)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 200, 237, 538)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 54, 82, 109, 136], [0, 77], [0, 76, 151, 227, 302, 378]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0594 transposed
suus panc_0594 not saving softmax
suus panc_0594 voeg toe aan pred_gt tuples voor later
panc_0599 (2, 108, 393, 486)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 393, 486)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 78, 155, 233], [0, 65, 130, 196, 261, 326]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0599 transposed
suus panc_0599 not saving softmax
suus panc_0599 voeg toe aan pred_gt tuples voor later
2022-06-30 20:30:57.313212: finished prediction
2022-06-30 20:30:57.319438: evaluation of raw predictions
2022-06-30 20:31:59.933679: determining postprocessing
Foreground vs background
before: 0.5931615349967055
after:  0.4954035805833578
1
before: 0.7949673997213305
after:  0.8251090555065476
Removing all but the largest region for class 1 improved results!
min_valid_object_sizes None
10
before: 0.4876756493692511
after:  0.4968302109704538
Removing all but the largest region for class 10 improved results!
min_valid_object_sizes None
11
before: 0.4581286565910347
after:  0.4647854846263144
Removing all but the largest region for class 11 improved results!
min_valid_object_sizes None
12
before: 0.3391562270486836
after:  0.3770128224965322
Removing all but the largest region for class 12 improved results!
min_valid_object_sizes None
13
before: 0.3624791574725064
after:  0.34339151398157025
14
before: 0.5855303168825898
after:  0.6063716675939265
Removing all but the largest region for class 14 improved results!
min_valid_object_sizes None
15
before: 0.5459004235442118
after:  0.5665305439119508
Removing all but the largest region for class 15 improved results!
min_valid_object_sizes None
2
before: 0.7756187470550321
after:  0.7886957093112311
Removing all but the largest region for class 2 improved results!
min_valid_object_sizes None
3
before: 0.7486466178298047
after:  0.7921288715797097
Removing all but the largest region for class 3 improved results!
min_valid_object_sizes None
4
before: 0.4692148900966239
after:  0.48605332127830375
Removing all but the largest region for class 4 improved results!
min_valid_object_sizes None
5
before: 0.4738927309367933
after:  0.44916332843297546
6
before: 0.8924775958654371
after:  0.9096836273329112
Removing all but the largest region for class 6 improved results!
min_valid_object_sizes None
7
before: 0.557607871236505
after:  0.5768459128697928
Removing all but the largest region for class 7 improved results!
min_valid_object_sizes None
8
before: 0.7448112194144096
after:  0.7583109143491805
Removing all but the largest region for class 8 improved results!
min_valid_object_sizes None
9
before: 0.6613155218863686
after:  0.6719581360894549
Removing all but the largest region for class 9 improved results!
min_valid_object_sizes None
done
for which classes:
[1, 10, 11, 12, 14, 15, 2, 3, 4, 6, 7, 8, 9]
min_object_sizes
None
done
Program finished with exit code 0 at: Thu Jun 30 00:30:11 CEST 2022
