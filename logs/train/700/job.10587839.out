Starting at Thu Jun 30 00:30:11 CEST 2022
Running on hosts: res-hpc-lkeb07
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 4.
Account: div2-lkeb
Job ID: 10587839
Job name: PancreasTrain
Node running script: res-hpc-lkeb07
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Thu Jun 30 00:30:13 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     Off  | 00000000:D8:00.0 Off |                  Off |
| 33%   33C    P0    51W / 260W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-mjtjoqnx/hiddenlayer_42298125070147acb69f7151b462af72
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Obtaining file:///home/smaijer/code/nnUNet
Requirement already satisfied: torch>1.10.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.12.0)
Requirement already satisfied: tqdm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.64.0)
Requirement already satisfied: dicom2nifti in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.4.2)
Requirement already satisfied: scikit-image>=0.14 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.19.3)
Requirement already satisfied: medpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.0)
Requirement already satisfied: scipy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.8.1)
Requirement already satisfied: batchgenerators>=0.23 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.24)
Requirement already satisfied: numpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.23.0)
Requirement already satisfied: sklearn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.0)
Requirement already satisfied: SimpleITK in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.1.1.2)
Requirement already satisfied: pandas in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.4.3)
Requirement already satisfied: requests in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.28.0)
Requirement already satisfied: nibabel in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.0.1)
Requirement already satisfied: tifffile in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2022.5.4)
Requirement already satisfied: matplotlib in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (3.5.2)
Requirement already satisfied: monai in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.9.0)
Requirement already satisfied: einops in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.1)
Requirement already satisfied: ipython in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (8.4.0)
Requirement already satisfied: graphviz in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.20)
Requirement already satisfied: scikit-learn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.1)
Requirement already satisfied: future in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)
Requirement already satisfied: unittest2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: threadpoolctl in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.1.0)
Requirement already satisfied: pillow>=7.1.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (9.1.1)
Requirement already satisfied: PyWavelets>=1.1.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.3.0)
Requirement already satisfied: packaging>=20.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (21.3)
Requirement already satisfied: imageio>=2.4.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.19.3)
Requirement already satisfied: networkx>=2.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.8.4)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from packaging>=20.0->scikit-image>=0.14->nnunet==1.7.0) (3.0.9)
Requirement already satisfied: typing-extensions in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from torch>1.10.0->nnunet==1.7.0) (4.2.0)
Requirement already satisfied: pydicom>=2.2.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (2.3.0)
Requirement already satisfied: python-gdcm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (3.0.14)
Requirement already satisfied: pickleshare in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.7.5)
Requirement already satisfied: backcall in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.2.0)
Requirement already satisfied: stack-data in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.3.0)
Requirement already satisfied: traitlets>=5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.3.0)
Requirement already satisfied: matplotlib-inline in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.1.3)
Requirement already satisfied: jedi>=0.16 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.18.1)
Requirement already satisfied: setuptools>=18.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (58.1.0)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (3.0.30)
Requirement already satisfied: pygments>=2.4.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (2.12.0)
Requirement already satisfied: decorator in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.1.1)
Requirement already satisfied: pexpect>4.3 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (4.8.0)
Requirement already satisfied: parso<0.9.0,>=0.8.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from jedi>=0.16->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: ptyprocess>=0.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pexpect>4.3->ipython->nnunet==1.7.0) (0.7.0)
Requirement already satisfied: wcwidth in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nnunet==1.7.0) (0.2.5)
Requirement already satisfied: python-dateutil>=2.7 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (2.8.2)
Requirement already satisfied: kiwisolver>=1.0.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (1.4.3)
Requirement already satisfied: cycler>=0.10 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (0.11.0)
Requirement already satisfied: fonttools>=4.22.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (4.33.3)
Requirement already satisfied: six>=1.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunet==1.7.0) (1.16.0)
Requirement already satisfied: pytz>=2020.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pandas->nnunet==1.7.0) (2022.1)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (1.26.9)
Requirement already satisfied: charset-normalizer~=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2.0.12)
Requirement already satisfied: idna<4,>=2.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (3.3)
Requirement already satisfied: certifi>=2017.4.17 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2022.6.15)
Requirement already satisfied: joblib>=1.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: executing in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: pure-eval in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.2.2)
Requirement already satisfied: asttokens in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (2.0.5)
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: traceback2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)
Requirement already satisfied: linecache2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)
Installing collected packages: argparse, nnunet
  Attempting uninstall: nnunet
    Found existing installation: nnunet 1.7.0
    Uninstalling nnunet-1.7.0:
      Successfully uninstalled nnunet-1.7.0
  Running setup.py develop for nnunet
Successfully installed argparse-1.4.0 nnunet-1.7.0


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='1', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-30 00:30:36.621679: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-30 00:30:36.631526: The split file contains 5 splits.
2022-06-30 00:30:36.634090: Desired fold for training: 1
2022-06-30 00:30:36.636217: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-06-30 00:30:39.945733: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_1/model_latest.model train= True
SuusB run_training - zet learning rate als  
2022-06-30 00:31:00.690519: Suus1 maybe_update_lr lr: 0.003384
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-06-30 00:31:15.737054: Unable to plot network architecture:
2022-06-30 00:31:15.769347: local variable 'g' referenced before assignment
2022-06-30 00:31:15.794348: 
printing the network instead:

2022-06-30 00:31:15.825349: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-06-30 00:31:15.858933: 

2022-06-30 00:31:15.887486: 
epoch:  350
2022-06-30 00:36:10.866823: train loss : 0.1592
2022-06-30 00:36:27.546297: validation loss: 0.1639
2022-06-30 00:36:27.549768: Average global foreground Dice: [0.6972, 0.6282, 0.6084, 0.3487, 0.453, 0.8019, 0.4409, 0.6747, 0.5377, 0.3419, 0.2214, 0.0615, 0.2413, 0.5306, 0.4885]
2022-06-30 00:36:27.552267: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:36:28.085515: Suus1 maybe_update_lr lr: 0.003364
2022-06-30 00:36:28.088136: saving best epoch checkpoint...
2022-06-30 00:36:28.471863: saving checkpoint...
2022-06-30 00:36:31.543029: done, saving took 3.45 seconds
2022-06-30 00:36:31.558713: This epoch took 315.638356 s

2022-06-30 00:36:31.560899: 
epoch:  351
2022-06-30 00:41:10.315892: train loss : 0.1590
2022-06-30 00:41:27.013316: validation loss: 0.2169
2022-06-30 00:41:27.017762: Average global foreground Dice: [0.6216, 0.6678, 0.6151, 0.4421, 0.409, 0.8313, 0.4426, 0.619, 0.4697, 0.3541, 0.284, 0.0758, 0.2343, 0.2856, 0.1272]
2022-06-30 00:41:27.020382: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:41:27.464928: Suus1 maybe_update_lr lr: 0.003343
2022-06-30 00:41:27.467587: This epoch took 295.904431 s

2022-06-30 00:41:27.470412: 
epoch:  352
2022-06-30 00:46:06.892843: train loss : 0.1868
2022-06-30 00:46:23.565710: validation loss: 0.1045
2022-06-30 00:46:23.570794: Average global foreground Dice: [0.6098, 0.6996, 0.5773, 0.4807, 0.3926, 0.8658, 0.3488, 0.623, 0.5534, 0.4115, 0.2726, 0.0654, 0.2279, 0.6766, 0.5708]
2022-06-30 00:46:23.573806: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:46:24.046154: Suus1 maybe_update_lr lr: 0.003323
2022-06-30 00:46:24.048696: This epoch took 296.575798 s

2022-06-30 00:46:24.050972: 
epoch:  353
2022-06-30 00:51:03.454831: train loss : 0.1966
2022-06-30 00:51:20.143726: validation loss: 0.1608
2022-06-30 00:51:20.148358: Average global foreground Dice: [0.5072, 0.676, 0.6489, 0.4462, 0.4063, 0.8295, 0.3523, 0.6249, 0.5231, 0.4021, 0.2517, 0.0981, 0.2405, 0.5642, 0.5891]
2022-06-30 00:51:20.150802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:51:20.779674: Suus1 maybe_update_lr lr: 0.003303
2022-06-30 00:51:20.782093: This epoch took 296.728883 s

2022-06-30 00:51:20.784331: 
epoch:  354
2022-06-30 00:55:59.755017: train loss : 0.1833
2022-06-30 00:56:16.446434: validation loss: 0.1682
2022-06-30 00:56:16.450260: Average global foreground Dice: [0.7115, 0.702, 0.6212, 0.3893, 0.295, 0.8448, 0.4401, 0.6376, 0.5181, 0.3878, 0.2907, 0.0599, 0.2585, 0.4404, 0.4235]
2022-06-30 00:56:16.452387: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:56:16.888254: Suus1 maybe_update_lr lr: 0.003282
2022-06-30 00:56:16.890751: This epoch took 296.104098 s

2022-06-30 00:56:16.892976: 
epoch:  355
2022-06-30 01:00:56.416677: train loss : 0.1802
2022-06-30 01:01:13.229954: validation loss: 0.1458
2022-06-30 01:01:13.234419: Average global foreground Dice: [0.6207, 0.5925, 0.6452, 0.3062, 0.384, 0.837, 0.4281, 0.5934, 0.4882, 0.3846, 0.2642, 0.1088, 0.2042, 0.6414, 0.3553]
2022-06-30 01:01:13.236630: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:01:13.704334: Suus1 maybe_update_lr lr: 0.003262
2022-06-30 01:01:13.706755: This epoch took 296.811407 s

2022-06-30 01:01:13.708952: 
epoch:  356
2022-06-30 01:05:52.560513: train loss : 0.1420
2022-06-30 01:06:09.217015: validation loss: 0.1472
2022-06-30 01:06:09.221349: Average global foreground Dice: [0.7112, 0.6054, 0.6234, 0.3599, 0.3924, 0.8636, 0.4273, 0.6683, 0.5033, 0.4128, 0.2896, 0.0926, 0.2438, 0.3487, 0.4467]
2022-06-30 01:06:09.224200: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:06:09.662702: Suus1 maybe_update_lr lr: 0.003241
2022-06-30 01:06:09.665403: This epoch took 295.954009 s

2022-06-30 01:06:09.667928: 
epoch:  357
2022-06-30 01:10:48.841457: train loss : 0.1617
2022-06-30 01:11:10.268284: validation loss: 0.2127
2022-06-30 01:11:10.272393: Average global foreground Dice: [0.6416, 0.6765, 0.5803, 0.2831, 0.3457, 0.794, 0.3727, 0.647, 0.5438, 0.3551, 0.2555, 0.0647, 0.181, 0.5309, 0.4798]
2022-06-30 01:11:10.275235: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:11:10.724571: Suus1 maybe_update_lr lr: 0.003221
2022-06-30 01:11:10.727628: This epoch took 301.057189 s

2022-06-30 01:11:10.730796: 
epoch:  358
2022-06-30 01:15:49.539885: train loss : 0.1434
2022-06-30 01:16:06.236938: validation loss: 0.1259
2022-06-30 01:16:06.240355: Average global foreground Dice: [0.6831, 0.6094, 0.5564, 0.3852, 0.4144, 0.8744, 0.3996, 0.6394, 0.4508, 0.3606, 0.342, 0.1011, 0.2883, 0.5637, 0.4531]
2022-06-30 01:16:06.242445: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:16:06.702224: Suus1 maybe_update_lr lr: 0.003201
2022-06-30 01:16:06.704291: This epoch took 295.970246 s

2022-06-30 01:16:06.706261: 
epoch:  359
2022-06-30 01:20:45.848820: train loss : 0.1471
2022-06-30 01:21:07.601957: validation loss: 0.1753
2022-06-30 01:21:07.606533: Average global foreground Dice: [0.4846, 0.6591, 0.618, 0.3655, 0.3592, 0.7753, 0.4281, 0.6741, 0.5203, 0.3633, 0.2721, 0.0916, 0.2078, 0.7189, 0.4341]
2022-06-30 01:21:07.609591: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:21:08.084631: Suus1 maybe_update_lr lr: 0.00318
2022-06-30 01:21:08.087976: This epoch took 301.379923 s

2022-06-30 01:21:08.090333: 
epoch:  360
2022-06-30 01:25:47.379139: train loss : 0.1442
2022-06-30 01:26:04.310021: validation loss: 0.1836
2022-06-30 01:26:04.314482: Average global foreground Dice: [0.6676, 0.5834, 0.5781, 0.3174, 0.3769, 0.8521, 0.4288, 0.6682, 0.4703, 0.4362, 0.2736, 0.0731, 0.2303, 0.6212, 0.5316]
2022-06-30 01:26:04.316972: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:26:04.763203: Suus1 maybe_update_lr lr: 0.00316
2022-06-30 01:26:04.765374: This epoch took 296.672784 s

2022-06-30 01:26:04.767611: 
epoch:  361
2022-06-30 01:30:44.029870: train loss : 0.1757
2022-06-30 01:31:00.795306: validation loss: 0.1933
2022-06-30 01:31:00.799653: Average global foreground Dice: [0.6402, 0.6129, 0.5869, 0.337, 0.3372, 0.8361, 0.4007, 0.6389, 0.5537, 0.3962, 0.3402, 0.062, 0.2864, 0.5565, 0.4989]
2022-06-30 01:31:00.802087: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:31:01.249497: Suus1 maybe_update_lr lr: 0.003139
2022-06-30 01:31:01.252182: This epoch took 296.482512 s

2022-06-30 01:31:01.254586: 
epoch:  362
2022-06-30 01:35:40.741912: train loss : 0.1690
2022-06-30 01:35:57.432662: validation loss: 0.1650
2022-06-30 01:35:57.436970: Average global foreground Dice: [0.5547, 0.574, 0.6115, 0.4611, 0.346, 0.8287, 0.43, 0.6683, 0.5124, 0.4359, 0.3113, 0.0841, 0.2782, 0.174, 0.1985]
2022-06-30 01:35:57.439426: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:35:57.881987: Suus1 maybe_update_lr lr: 0.003119
2022-06-30 01:35:57.884563: This epoch took 296.627292 s

2022-06-30 01:35:57.886963: 
epoch:  363
2022-06-30 01:40:37.500244: train loss : 0.1406
2022-06-30 01:40:54.209024: validation loss: 0.1921
2022-06-30 01:40:54.213034: Average global foreground Dice: [0.4915, 0.5981, 0.4944, 0.3754, 0.2817, 0.8549, 0.395, 0.6503, 0.5077, 0.4078, 0.2361, 0.0566, 0.2151, 0.5603, 0.5928]
2022-06-30 01:40:54.215529: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:40:54.661014: Suus1 maybe_update_lr lr: 0.003098
2022-06-30 01:40:54.663356: This epoch took 296.773993 s

2022-06-30 01:40:54.665486: 
epoch:  364
2022-06-30 01:45:34.147981: train loss : 0.1773
2022-06-30 01:45:50.843508: validation loss: 0.2171
