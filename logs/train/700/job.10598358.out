Starting at Fri Jul  1 00:13:25 CEST 2022
Running on hosts: res-hpc-lkeb05
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 8.
Account: div2-lkeb
Job ID: 10598358
Job name: 700hybrid
Node running script: res-hpc-lkeb05
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Fri Jul  1 00:16:53 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     Off  | 00000000:AF:00.0 Off |                  Off |
| 34%   34C    P0    61W / 260W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-5vsg5t6p/hiddenlayer_54746a4733d9479abae479aa2b25a351
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Obtaining file:///home/smaijer/code/nnUNet
Requirement already satisfied: torch>1.10.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.12.0)
Requirement already satisfied: tqdm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.64.0)
Requirement already satisfied: dicom2nifti in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.4.2)
Requirement already satisfied: scikit-image>=0.14 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.19.3)
Requirement already satisfied: medpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.0)
Requirement already satisfied: scipy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.8.1)
Requirement already satisfied: batchgenerators>=0.23 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.24)
Requirement already satisfied: numpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.23.0)
Requirement already satisfied: sklearn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.0)
Requirement already satisfied: SimpleITK in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.1.1.2)
Requirement already satisfied: pandas in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.4.3)
Requirement already satisfied: requests in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.28.0)
Requirement already satisfied: nibabel in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.0.1)
Requirement already satisfied: tifffile in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2022.5.4)
Requirement already satisfied: matplotlib in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (3.5.2)
Requirement already satisfied: monai in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.9.0)
Requirement already satisfied: einops in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.1)
Requirement already satisfied: ipython in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (8.4.0)
Requirement already satisfied: graphviz in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.20)
Requirement already satisfied: scikit-learn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.1)
Requirement already satisfied: threadpoolctl in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.1.0)
Requirement already satisfied: pillow>=7.1.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (9.1.1)
Requirement already satisfied: future in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)
Requirement already satisfied: unittest2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: packaging>=20.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (21.3)
Requirement already satisfied: PyWavelets>=1.1.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.3.0)
Requirement already satisfied: imageio>=2.4.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.19.3)
Requirement already satisfied: networkx>=2.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.8.4)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from packaging>=20.0->scikit-image>=0.14->nnunet==1.7.0) (3.0.9)
Requirement already satisfied: typing-extensions in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from torch>1.10.0->nnunet==1.7.0) (4.2.0)
Requirement already satisfied: pydicom>=2.2.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (2.3.0)
Requirement already satisfied: python-gdcm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (3.0.14)
Requirement already satisfied: decorator in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.1.1)
Requirement already satisfied: setuptools>=18.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (58.1.0)
Requirement already satisfied: pexpect>4.3 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (4.8.0)
Requirement already satisfied: matplotlib-inline in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.1.3)
Requirement already satisfied: pickleshare in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.7.5)
Requirement already satisfied: stack-data in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.3.0)
Requirement already satisfied: jedi>=0.16 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.18.1)
Requirement already satisfied: traitlets>=5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.3.0)
Requirement already satisfied: pygments>=2.4.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (2.12.0)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (3.0.30)
Requirement already satisfied: backcall in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.2.0)
Requirement already satisfied: parso<0.9.0,>=0.8.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from jedi>=0.16->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: ptyprocess>=0.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pexpect>4.3->ipython->nnunet==1.7.0) (0.7.0)
Requirement already satisfied: wcwidth in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nnunet==1.7.0) (0.2.5)
Requirement already satisfied: cycler>=0.10 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (0.11.0)
Requirement already satisfied: kiwisolver>=1.0.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (1.4.3)
Requirement already satisfied: fonttools>=4.22.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (4.33.3)
Requirement already satisfied: python-dateutil>=2.7 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (2.8.2)
Requirement already satisfied: six>=1.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunet==1.7.0) (1.16.0)
Requirement already satisfied: pytz>=2020.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pandas->nnunet==1.7.0) (2022.1)
Requirement already satisfied: certifi>=2017.4.17 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2022.6.15)
Requirement already satisfied: idna<4,>=2.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (3.3)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (1.26.9)
Requirement already satisfied: charset-normalizer~=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2.0.12)
Requirement already satisfied: joblib>=1.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: asttokens in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (2.0.5)
Requirement already satisfied: executing in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: pure-eval in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.2.2)
Requirement already satisfied: traceback2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: linecache2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)
Installing collected packages: argparse, nnunet
  Attempting uninstall: nnunet
    Found existing installation: nnunet 1.7.0
    Uninstalling nnunet-1.7.0:
      Successfully uninstalled nnunet-1.7.0
  Running setup.py develop for nnunet
Successfully installed argparse-1.4.0 nnunet-1.7.0


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='0', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-01 00:17:53.140451: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-07-01 00:17:53.154152: The split file contains 5 splits.
2022-07-01 00:17:53.156891: Desired fold for training: 0
2022-07-01 00:17:53.158890: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-01 00:17:56.309637: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-01 00:17:56.929005: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-01 00:18:17.743808: Unable to plot network architecture:
2022-07-01 00:18:17.747499: local variable 'g' referenced before assignment
2022-07-01 00:18:17.750492: 
printing the network instead:

2022-07-01 00:18:17.753157: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-01 00:18:17.759898: 

2022-07-01 00:18:18.101034: saving checkpoint...
2022-07-01 00:18:23.072023: done, saving took 5.31 seconds
panc_0015 (2, 258, 640, 640)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 258, 640, 640)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 55, 83, 111, 139, 166, 194], [0, 80, 160, 240, 320, 400, 480], [0, 80, 160, 240, 320, 400, 480]]
number of tiles: 392
computing Gaussian
done
prediction done
suus panc_0015 transposed
suus panc_0015 not saving softmax
suus panc_0015 we moeten gekke ding doen met groot commentaar
suus panc_0015 voeg toe aan pred_gt tuples voor later
panc_0025 (2, 202, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 202, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 55, 83, 110, 138], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0025 transposed
suus panc_0025 not saving softmax
suus panc_0025 we moeten gekke ding doen met groot commentaar
suus panc_0025 voeg toe aan pred_gt tuples voor later
panc_0035 (2, 232, 631, 631)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 232, 631, 631)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 56, 84, 112, 140, 168], [0, 78, 157, 236, 314, 392, 471], [0, 78, 157, 236, 314, 392, 471]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0035 transposed
suus panc_0035 not saving softmax
suus panc_0035 we moeten gekke ding doen met groot commentaar
suus panc_0035 voeg toe aan pred_gt tuples voor later
panc_0047 (2, 245, 579, 579)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 245, 579, 579)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 90, 121, 151, 181], [0, 70, 140, 210, 279, 349, 419], [0, 70, 140, 210, 279, 349, 419]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0047 transposed
suus panc_0047 not saving softmax
suus panc_0047 we moeten gekke ding doen met groot commentaar
suus panc_0047 voeg toe aan pred_gt tuples voor later
panc_0050 (2, 240, 558, 558)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 558, 558)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 59, 88, 117, 147, 176], [0, 80, 159, 239, 318, 398], [0, 80, 159, 239, 318, 398]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0050 transposed
suus panc_0050 not saving softmax
suus panc_0050 we moeten gekke ding doen met groot commentaar
suus panc_0050 voeg toe aan pred_gt tuples voor later
panc_0057 (2, 218, 641, 641)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 641, 641)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 92, 123, 154], [0, 69, 137, 206, 275, 344, 412, 481], [0, 69, 137, 206, 275, 344, 412, 481]]
number of tiles: 384
using precomputed Gaussian
prediction done
suus panc_0057 transposed
suus panc_0057 not saving softmax
suus panc_0057 we moeten gekke ding doen met groot commentaar
suus panc_0057 voeg toe aan pred_gt tuples voor later
panc_0069 (2, 245, 526, 526)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 245, 526, 526)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 90, 121, 151, 181], [0, 73, 146, 220, 293, 366], [0, 73, 146, 220, 293, 366]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0069 transposed
suus panc_0069 not saving softmax
suus panc_0069 we moeten gekke ding doen met groot commentaar
suus panc_0069 voeg toe aan pred_gt tuples voor later
panc_0079 (2, 202, 527, 527)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 202, 527, 527)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 55, 83, 110, 138], [0, 73, 147, 220, 294, 367], [0, 73, 147, 220, 294, 367]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0079 transposed
suus panc_0079 not saving softmax
suus panc_0079 we moeten gekke ding doen met groot commentaar
suus panc_0079 voeg toe aan pred_gt tuples voor later
panc_0084 (2, 210, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 88, 117, 146], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0084 transposed
suus panc_0084 not saving softmax
suus panc_0084 we moeten gekke ding doen met groot commentaar
suus panc_0084 voeg toe aan pred_gt tuples voor later
panc_0088 (2, 170, 490, 490)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 170, 490, 490)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 53, 80, 106], [0, 66, 132, 198, 264, 330], [0, 66, 132, 198, 264, 330]]
number of tiles: 180
using precomputed Gaussian
prediction done
suus panc_0088 transposed
suus panc_0088 not saving softmax
suus panc_0088 we moeten gekke ding doen met groot commentaar
suus panc_0088 voeg toe aan pred_gt tuples voor later
panc_0092 (2, 330, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 330, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 118, 148, 177, 207, 236, 266], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0092 transposed
suus panc_0092 not saving softmax
suus panc_0092 we moeten gekke ding doen met groot commentaar
suus panc_0092 voeg toe aan pred_gt tuples voor later
panc_0098 (2, 242, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 242, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 119, 148, 178], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0098 transposed
suus panc_0098 not saving softmax
suus panc_0098 we moeten gekke ding doen met groot commentaar
suus panc_0098 voeg toe aan pred_gt tuples voor later
panc_0113 (2, 228, 637, 637)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 637, 637)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 80, 159, 238, 318, 398, 477], [0, 80, 159, 238, 318, 398, 477]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0113 transposed
suus panc_0113 not saving softmax
suus panc_0113 we moeten gekke ding doen met groot commentaar
suus panc_0113 voeg toe aan pred_gt tuples voor later
panc_0116 (2, 315, 595, 595)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 315, 595, 595)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 63, 94, 126, 157, 188, 220, 251], [0, 72, 145, 218, 290, 362, 435], [0, 72, 145, 218, 290, 362, 435]]
number of tiles: 441
using precomputed Gaussian
prediction done
suus panc_0116 transposed
suus panc_0116 not saving softmax
suus panc_0116 we moeten gekke ding doen met groot commentaar
suus panc_0116 voeg toe aan pred_gt tuples voor later
panc_0121 (2, 300, 621, 621)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 300, 621, 621)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 88, 118, 148, 177, 206, 236], [0, 77, 154, 230, 307, 384, 461], [0, 77, 154, 230, 307, 384, 461]]
number of tiles: 441
using precomputed Gaussian
prediction done
suus panc_0121 transposed
suus panc_0121 not saving softmax
suus panc_0121 we moeten gekke ding doen met groot commentaar
suus panc_0121 voeg toe aan pred_gt tuples voor later
panc_0125 (2, 262, 546, 546)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 262, 546, 546)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 57, 85, 113, 141, 170, 198], [0, 77, 154, 232, 309, 386], [0, 77, 154, 232, 309, 386]]
number of tiles: 288
using precomputed Gaussian
prediction done
suus panc_0125 transposed
suus panc_0125 not saving softmax
suus panc_0125 we moeten gekke ding doen met groot commentaar
suus panc_0125 voeg toe aan pred_gt tuples voor later
panc_0153 (2, 232, 530, 530)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 232, 530, 530)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 56, 84, 112, 140, 168], [0, 74, 148, 222, 296, 370], [0, 74, 148, 222, 296, 370]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0153 transposed
suus panc_0153 not saving softmax
suus panc_0153 we moeten gekke ding doen met groot commentaar
suus panc_0153 voeg toe aan pred_gt tuples voor later
panc_0159 (2, 332, 545, 545)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 332, 545, 545)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 89, 119, 149, 179, 208, 238, 268], [0, 77, 154, 231, 308, 385], [0, 77, 154, 231, 308, 385]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0159 transposed
suus panc_0159 not saving softmax
suus panc_0159 we moeten gekke ding doen met groot commentaar
suus panc_0159 voeg toe aan pred_gt tuples voor later
panc_0160 (2, 212, 570, 570)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 212, 570, 570)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 118, 148], [0, 68, 137, 205, 273, 342, 410], [0, 68, 137, 205, 273, 342, 410]]
number of tiles: 294
using precomputed Gaussian
prediction done
suus panc_0160 transposed
suus panc_0160 not saving softmax
suus panc_0160 we moeten gekke ding doen met groot commentaar
suus panc_0160 voeg toe aan pred_gt tuples voor later
panc_0161 (2, 285, 563, 563)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 285, 563, 563)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 63, 95, 126, 158, 189, 221], [0, 67, 134, 202, 269, 336, 403], [0, 67, 134, 202, 269, 336, 403]]
number of tiles: 392
using precomputed Gaussian
prediction done
suus panc_0161 transposed
suus panc_0161 not saving softmax
suus panc_0161 we moeten gekke ding doen met groot commentaar
suus panc_0161 voeg toe aan pred_gt tuples voor later
panc_0177 (2, 335, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 335, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 90, 120, 151, 181, 211, 241, 271], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0177 transposed
suus panc_0177 not saving softmax
suus panc_0177 we moeten gekke ding doen met groot commentaar
suus panc_0177 voeg toe aan pred_gt tuples voor later
panc_0179 (2, 335, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 335, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 90, 120, 151, 181, 211, 241, 271], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0179 transposed
suus panc_0179 not saving softmax
suus panc_0179 we moeten gekke ding doen met groot commentaar
suus panc_0179 voeg toe aan pred_gt tuples voor later
panc_0181 (2, 295, 568, 568)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 295, 568, 568)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 87, 116, 144, 173, 202, 231], [0, 68, 136, 204, 272, 340, 408], [0, 68, 136, 204, 272, 340, 408]]
number of tiles: 441
using precomputed Gaussian
prediction done
suus panc_0181 transposed
suus panc_0181 not saving softmax
suus panc_0181 we moeten gekke ding doen met groot commentaar
suus panc_0181 voeg toe aan pred_gt tuples voor later
panc_0186 (2, 212, 459, 459)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 212, 459, 459)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 118, 148], [0, 75, 150, 224, 299], [0, 75, 150, 224, 299]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0186 transposed
suus panc_0186 not saving softmax
suus panc_0186 we moeten gekke ding doen met groot commentaar
suus panc_0186 voeg toe aan pred_gt tuples voor later
panc_0188 (2, 308, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 308, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 61, 92, 122, 152, 183, 214, 244], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 324
using precomputed Gaussian
prediction done
suus panc_0188 transposed
suus panc_0188 not saving softmax
suus panc_0188 we moeten gekke ding doen met groot commentaar
suus panc_0188 voeg toe aan pred_gt tuples voor later
panc_0192 (2, 308, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 308, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 61, 92, 122, 152, 183, 214, 244], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 324
using precomputed Gaussian
prediction done
suus panc_0192 transposed
suus panc_0192 not saving softmax
suus panc_0192 we moeten gekke ding doen met groot commentaar
suus panc_0192 voeg toe aan pred_gt tuples voor later
panc_0214 (2, 333, 559, 559)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 333, 559, 559)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 90, 120, 149, 179, 209, 239, 269], [0, 80, 160, 239, 319, 399], [0, 80, 160, 239, 319, 399]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0214 transposed
suus panc_0214 not saving softmax
suus panc_0214 we moeten gekke ding doen met groot commentaar
suus panc_0214 voeg toe aan pred_gt tuples voor later
panc_0217 (2, 306, 451, 451)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 306, 451, 451)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 91, 121, 151, 182, 212, 242], [0, 73, 146, 218, 291], [0, 73, 146, 218, 291]]
number of tiles: 225
using precomputed Gaussian
prediction done
suus panc_0217 transposed
suus panc_0217 not saving softmax
suus panc_0217 we moeten gekke ding doen met groot commentaar
suus panc_0217 voeg toe aan pred_gt tuples voor later
panc_0245 (2, 276, 533, 533)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 276, 533, 533)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 61, 91, 121, 151, 182, 212], [0, 75, 149, 224, 298, 373], [0, 75, 149, 224, 298, 373]]
number of tiles: 288
using precomputed Gaussian
prediction done
suus panc_0245 transposed
suus panc_0245 not saving softmax
suus panc_0245 we moeten gekke ding doen met groot commentaar
suus panc_0245 voeg toe aan pred_gt tuples voor later
panc_0297 (2, 345, 505, 505)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 345, 505, 505)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 94, 125, 156, 187, 219, 250, 281], [0, 69, 138, 207, 276, 345], [0, 69, 138, 207, 276, 345]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0297 transposed
suus panc_0297 not saving softmax
suus panc_0297 we moeten gekke ding doen met groot commentaar
suus panc_0297 voeg toe aan pred_gt tuples voor later
panc_0299 (2, 204, 511, 511)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 204, 511, 511)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 56, 84, 112, 140], [0, 70, 140, 211, 281, 351], [0, 70, 140, 211, 281, 351]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0299 transposed
suus panc_0299 not saving softmax
suus panc_0299 we moeten gekke ding doen met groot commentaar
suus panc_0299 voeg toe aan pred_gt tuples voor later
panc_0330 (2, 230, 408, 408)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 230, 408, 408)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 55, 83, 111, 138, 166], [0, 62, 124, 186, 248], [0, 62, 124, 186, 248]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0330 transposed
suus panc_0330 not saving softmax
suus panc_0330 we moeten gekke ding doen met groot commentaar
suus panc_0330 voeg toe aan pred_gt tuples voor later
panc_0332 (2, 228, 449, 449)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 449, 449)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0332 transposed
suus panc_0332 not saving softmax
suus panc_0332 we moeten gekke ding doen met groot commentaar
suus panc_0332 voeg toe aan pred_gt tuples voor later
panc_0348 (2, 317, 444, 444)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 317, 444, 444)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 63, 95, 126, 158, 190, 221, 253], [0, 71, 142, 213, 284], [0, 71, 142, 213, 284]]
number of tiles: 225
using precomputed Gaussian
prediction done
suus panc_0348 transposed
suus panc_0348 not saving softmax
suus panc_0348 we moeten gekke ding doen met groot commentaar
suus panc_0348 voeg toe aan pred_gt tuples voor later
panc_0362 (2, 265, 640, 640)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 265, 640, 640)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 57, 86, 115, 144, 172, 201], [0, 80, 160, 240, 320, 400, 480], [0, 80, 160, 240, 320, 400, 480]]
number of tiles: 392
using precomputed Gaussian
prediction done
suus panc_0362 transposed
suus panc_0362 not saving softmax
suus panc_0362 we moeten gekke ding doen met groot commentaar
suus panc_0362 voeg toe aan pred_gt tuples voor later
panc_0374 (2, 208, 631, 631)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 208, 631, 631)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 86, 115, 144], [0, 78, 157, 236, 314, 392, 471], [0, 78, 157, 236, 314, 392, 471]]
number of tiles: 294
using precomputed Gaussian
prediction done
suus panc_0374 transposed
suus panc_0374 not saving softmax
suus panc_0374 we moeten gekke ding doen met groot commentaar
suus panc_0374 voeg toe aan pred_gt tuples voor later
panc_0378 (2, 240, 623, 623)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 623, 623)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 59, 88, 117, 147, 176], [0, 77, 154, 232, 309, 386, 463], [0, 77, 154, 232, 309, 386, 463]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0378 transposed
suus panc_0378 not saving softmax
suus panc_0378 we moeten gekke ding doen met groot commentaar
suus panc_0378 voeg toe aan pred_gt tuples voor later
panc_0390 (2, 248, 611, 611)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 248, 611, 611)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 61, 92, 123, 153, 184], [0, 75, 150, 226, 301, 376, 451], [0, 75, 150, 226, 301, 376, 451]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0390 transposed
suus panc_0390 not saving softmax
suus panc_0390 we moeten gekke ding doen met groot commentaar
suus panc_0390 voeg toe aan pred_gt tuples voor later
panc_0398 (2, 205, 618, 618)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 618, 618)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 56, 85, 113, 141], [0, 76, 153, 229, 305, 382, 458], [0, 76, 153, 229, 305, 382, 458]]
number of tiles: 294
using precomputed Gaussian
prediction done
suus panc_0398 transposed
suus panc_0398 not saving softmax
suus panc_0398 we moeten gekke ding doen met groot commentaar
suus panc_0398 voeg toe aan pred_gt tuples voor later
panc_0517 (2, 115, 451, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 115, 451, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 51], [0, 73, 146, 218, 291], [0, 71, 141, 212, 282, 353]]
number of tiles: 90
using precomputed Gaussian
prediction done
suus panc_0517 transposed
suus panc_0517 not saving softmax
suus panc_0517 we moeten gekke ding doen met groot commentaar
suus panc_0517 voeg toe aan pred_gt tuples voor later
panc_0538 (2, 96, 434, 537)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 96, 434, 537)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32], [0, 68, 137, 206, 274], [0, 75, 151, 226, 302, 377]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0538 transposed
suus panc_0538 not saving softmax
suus panc_0538 voeg toe aan pred_gt tuples voor later
panc_0551 (2, 108, 415, 511)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 415, 511)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 64, 128, 191, 255], [0, 70, 140, 211, 281, 351]]
number of tiles: 90
using precomputed Gaussian
prediction done
suus panc_0551 transposed
suus panc_0551 not saving softmax
suus panc_0551 voeg toe aan pred_gt tuples voor later
panc_0557 (2, 184, 214, 536)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 214, 536)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 90, 120], [0, 54], [0, 75, 150, 226, 301, 376]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0557 transposed
suus panc_0557 not saving softmax
suus panc_0557 voeg toe aan pred_gt tuples voor later
panc_0571 (2, 108, 410, 511)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 410, 511)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 62, 125, 188, 250], [0, 70, 140, 211, 281, 351]]
number of tiles: 90
using precomputed Gaussian
prediction done
suus panc_0571 transposed
suus panc_0571 not saving softmax
suus panc_0571 voeg toe aan pred_gt tuples voor later
panc_0584 (2, 108, 409, 509)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 409, 509)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 62, 124, 187, 249], [0, 70, 140, 209, 279, 349]]
number of tiles: 90
using precomputed Gaussian
prediction done
suus panc_0584 transposed
suus panc_0584 not saving softmax
suus panc_0584 voeg toe aan pred_gt tuples voor later
panc_0592 (2, 194, 237, 536)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 194, 237, 536)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 52, 78, 104, 130], [0, 77], [0, 75, 150, 226, 301, 376]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0592 transposed
suus panc_0592 not saving softmax
suus panc_0592 voeg toe aan pred_gt tuples voor later
panc_0593 (2, 108, 415, 512)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 415, 512)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 64, 128, 191, 255], [0, 70, 141, 211, 282, 352]]
number of tiles: 90
using precomputed Gaussian
prediction done
suus panc_0593 transposed
suus panc_0593 not saving softmax
suus panc_0593 voeg toe aan pred_gt tuples voor later
panc_0596 (2, 177, 237, 534)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 177, 237, 534)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 28, 56, 85, 113], [0, 77], [0, 75, 150, 224, 299, 374]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0596 transposed
suus panc_0596 not saving softmax
suus panc_0596 voeg toe aan pred_gt tuples voor later
2022-07-01 05:44:03.872162: finished prediction
2022-07-01 05:44:03.877839: evaluation of raw predictions
2022-07-01 05:45:00.897063: determining postprocessing
Foreground vs background
before: 0.5846676466642747
after:  0.4728182376242238
1
before: 0.7836004774048332
after:  0.8060063229410218
Removing all but the largest region for class 1 improved results!
min_valid_object_sizes None
10
before: 0.49611688173863433
after:  0.47379911957379456
11
before: 0.4635439871888196
after:  0.4688841584440562
Removing all but the largest region for class 11 improved results!
min_valid_object_sizes None
12
before: 0.2997469981155583
after:  0.3382584732285759
Removing all but the largest region for class 12 improved results!
min_valid_object_sizes None
13
before: 0.3336712403461612
after:  0.3099655819805781
14
before: 0.568415346835558
after:  0.5784760140003222
Removing all but the largest region for class 14 improved results!
min_valid_object_sizes None
15
before: 0.5067035089776872
after:  0.5130538524043823
Removing all but the largest region for class 15 improved results!
min_valid_object_sizes None
2
before: 0.765917722198492
after:  0.7920630539633567
Removing all but the largest region for class 2 improved results!
min_valid_object_sizes None
3
before: 0.7267085778398599
after:  0.7825019320974883
Removing all but the largest region for class 3 improved results!
min_valid_object_sizes None
4
before: 0.5532010787334419
after:  0.5757524552916261
Removing all but the largest region for class 4 improved results!
min_valid_object_sizes None
5
before: 0.45142104845574654
after:  0.4427219785684404
6
before: 0.8944940968721102
after:  0.9087015777573771
Removing all but the largest region for class 6 improved results!
min_valid_object_sizes None
7
before: 0.5322630749730003
after:  0.5440800789605426
Removing all but the largest region for class 7 improved results!
min_valid_object_sizes None
8
before: 0.7559305488802334
after:  0.7642310992407353
Removing all but the largest region for class 8 improved results!
min_valid_object_sizes None
9
before: 0.6382801114039821
after:  0.6391918799808975
Removing all but the largest region for class 9 improved results!
min_valid_object_sizes None
done
for which classes:
[1, 11, 12, 14, 15, 2, 3, 4, 6, 7, 8, 9]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0015.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0084.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0153.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0188.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0332.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0538.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0025.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0088.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0159.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0192.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0348.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0551.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0035.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0092.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0160.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0214.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0362.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0557.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0047.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0098.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0161.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0217.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0374.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0571.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0050.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0113.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0177.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0245.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0378.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0584.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0057.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0116.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0179.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0297.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0390.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0592.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0069.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0121.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0181.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0299.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0398.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0593.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0079.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0125.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0186.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0330.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0517.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/validation_raw/panc_0596.nii.gz
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='2', validation_only=False, continue_training=False, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-01 05:59:42.902091: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-07-01 05:59:42.911941: The split file contains 5 splits.
2022-07-01 05:59:42.914252: Desired fold for training: 2
2022-07-01 05:59:42.916300: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusB run_training - zet learning rate als  
2022-07-01 05:59:46.121547: Suus1 maybe_update_lr lr: 0.01
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-01 05:59:57.464223: Unable to plot network architecture:
2022-07-01 05:59:57.510314: local variable 'g' referenced before assignment
2022-07-01 05:59:57.524384: 
printing the network instead:

2022-07-01 05:59:57.531302: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-01 05:59:57.557898: 

2022-07-01 05:59:57.574462: 
epoch:  0
2022-07-01 06:04:56.004215: train loss : 1.0718
2022-07-01 06:05:12.904835: validation loss: 0.7785
2022-07-01 06:05:12.909395: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:05:12.912208: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:05:13.406697: Suus1 maybe_update_lr lr: 0.009982
2022-07-01 06:05:13.410086: This epoch took 315.819678 s

2022-07-01 06:05:13.412800: 
epoch:  1
2022-07-01 06:09:56.069876: train loss : 0.8033
2022-07-01 06:10:12.921966: validation loss: 0.8355
2022-07-01 06:10:12.925909: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:10:12.928190: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:10:13.336386: Suus1 maybe_update_lr lr: 0.009964
2022-07-01 06:10:13.338684: This epoch took 299.922872 s

2022-07-01 06:10:13.341222: 
epoch:  2
2022-07-01 06:14:55.861416: train loss : 0.7863
2022-07-01 06:15:12.729353: validation loss: 0.8329
2022-07-01 06:15:12.733282: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:15:12.735562: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:15:13.148493: Suus1 maybe_update_lr lr: 0.009946
2022-07-01 06:15:13.151008: This epoch took 299.807575 s

2022-07-01 06:15:13.153482: 
epoch:  3
2022-07-01 06:19:56.134980: train loss : 0.7952
2022-07-01 06:20:13.021148: validation loss: 0.7887
2022-07-01 06:20:13.025111: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:20:13.027291: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:20:13.438945: Suus1 maybe_update_lr lr: 0.009928
2022-07-01 06:20:13.441404: This epoch took 300.285705 s

2022-07-01 06:20:13.443411: 
epoch:  4
2022-07-01 06:24:55.752840: train loss : 0.7423
2022-07-01 06:25:12.614277: validation loss: 0.7047
2022-07-01 06:25:12.618273: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:25:12.620489: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:25:13.043364: Suus1 maybe_update_lr lr: 0.00991
2022-07-01 06:25:13.045700: This epoch took 299.599995 s

2022-07-01 06:25:13.047746: 
epoch:  5
2022-07-01 06:29:55.522372: train loss : 0.7499
2022-07-01 06:30:12.396427: validation loss: 0.7759
2022-07-01 06:30:12.400200: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:30:12.402476: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:30:12.818733: Suus1 maybe_update_lr lr: 0.009892
2022-07-01 06:30:12.821177: This epoch took 299.771027 s

2022-07-01 06:30:12.823358: 
epoch:  6
2022-07-01 06:34:55.177097: train loss : 0.7708
2022-07-01 06:35:12.029317: validation loss: 0.7691
2022-07-01 06:35:12.033025: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.2431, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:35:12.035396: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:35:12.463163: Suus1 maybe_update_lr lr: 0.009874
2022-07-01 06:35:12.465450: saving best epoch checkpoint...
2022-07-01 06:35:12.642440: saving checkpoint...
2022-07-01 06:35:15.475346: done, saving took 3.01 seconds
2022-07-01 06:35:15.492506: This epoch took 302.666917 s

2022-07-01 06:35:15.495389: 
epoch:  7
2022-07-01 06:39:57.973284: train loss : 0.7823
2022-07-01 06:40:14.857760: validation loss: 0.7562
2022-07-01 06:40:14.862513: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:40:14.864738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:40:15.294331: Suus1 maybe_update_lr lr: 0.009856
2022-07-01 06:40:15.296789: This epoch took 299.799099 s

2022-07-01 06:40:15.298826: 
epoch:  8
2022-07-01 06:44:57.835044: train loss : 0.7488
2022-07-01 06:45:14.695603: validation loss: 0.7275
2022-07-01 06:45:14.699636: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.2848, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:45:14.702208: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:45:15.135787: Suus1 maybe_update_lr lr: 0.009838
2022-07-01 06:45:15.138493: saving best epoch checkpoint...
2022-07-01 06:45:15.338532: saving checkpoint...
2022-07-01 06:45:18.224110: done, saving took 3.08 seconds
2022-07-01 06:45:18.247674: This epoch took 302.946688 s

2022-07-01 06:45:18.249951: 
epoch:  9
2022-07-01 06:50:00.876212: train loss : 0.7419
2022-07-01 06:50:17.733655: validation loss: 0.7000
2022-07-01 06:50:17.738682: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:50:17.740678: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:50:18.170060: Suus1 maybe_update_lr lr: 0.00982
2022-07-01 06:50:18.172379: This epoch took 299.920079 s

2022-07-01 06:50:18.174429: 
epoch:  10
2022-07-01 06:55:00.586913: train loss : 0.7081
2022-07-01 06:55:17.461991: validation loss: 0.6724
2022-07-01 06:55:17.467070: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.1777, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 06:55:17.469509: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 06:55:17.910266: Suus1 maybe_update_lr lr: 0.009802
2022-07-01 06:55:17.912699: saving best epoch checkpoint...
2022-07-01 06:55:18.071302: saving checkpoint...
2022-07-01 06:55:21.130080: done, saving took 3.22 seconds
2022-07-01 06:55:21.149677: This epoch took 302.973073 s

2022-07-01 06:55:21.152580: 
epoch:  11
2022-07-01 07:00:03.345635: train loss : 0.7361
2022-07-01 07:00:20.214800: validation loss: 0.7586
2022-07-01 07:00:20.219414: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0131, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 07:00:20.222000: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 07:00:20.696196: Suus1 maybe_update_lr lr: 0.009784
2022-07-01 07:00:20.698941: This epoch took 299.543399 s

2022-07-01 07:00:20.701133: 
epoch:  12
2022-07-01 07:05:03.250392: train loss : 0.7278
2022-07-01 07:05:20.166487: validation loss: 0.7335
2022-07-01 07:05:20.170742: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.3224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 07:05:20.173253: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 07:05:20.839952: Suus1 maybe_update_lr lr: 0.009766
2022-07-01 07:05:20.842469: saving best epoch checkpoint...
2022-07-01 07:05:20.992655: saving checkpoint...
2022-07-01 07:05:24.433642: done, saving took 3.59 seconds
2022-07-01 07:05:24.450411: This epoch took 303.747167 s

2022-07-01 07:05:24.453208: 
epoch:  13
2022-07-01 07:10:06.916416: train loss : 0.7126
2022-07-01 07:10:24.357316: validation loss: 0.6831
2022-07-01 07:10:24.361670: Average global foreground Dice: [0.0, 0.0, 0.0, 0.0, 0.0, 0.2173, 0.0012, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
2022-07-01 07:10:24.363987: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-01 07:10:24.798334: Suus1 maybe_update_lr lr: 0.009748
2022-07-01 07:10:24.800758: saving best epoch checkpoint...
2022-07-01 07:10:24.936115: saving checkpoint...
2022-07-01 07:10:28.250409: done, saving took 3.45 seconds
2022-07-01 07:10:28.265120: This epoch took 303.809432 s

2022-07-01 07:10:28.267317: 
epoch:  14
2022-07-01 07:15:10.904572: train loss : 0.7210
