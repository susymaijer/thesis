
Currently Loaded Modules:
  1) tools/miniconda/python3.8/4.9.2

 

WARNING: overwriting environment variables set in the machine
overwriting variable nnUNet_raw_data_base nnUNet_preprocessed RESULTS_FOLDER OUTPUT
  Running command git clone -q https://github.com/FabianIsensee/hiddenlayer.git /tmp/pip-install-yhl6190a/hiddenlayer_646de0c8b8244ddf95fac2daac62545a
  Running command git checkout -b more_plotted_details --track origin/more_plotted_details
  Switched to a new branch 'more_plotted_details'
  Branch 'more_plotted_details' set up to track remote branch 'more_plotted_details' from 'origin'.
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/experiment/nnUNet/nnunet/run/run_training.py", line 180, in main
    trainer.run_training()
  File "/home/smaijer/experiment/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 453, in run_training
    ret = super().run_training()
  File "/home/smaijer/experiment/nnUNet/nnunet/training/network_training/nnUNetTrainer.py", line 318, in run_training
    super(nnUNetTrainer, self).run_training()
  File "/home/smaijer/experiment/nnUNet/nnunet/training/network_training/network_trainer.py", line 459, in run_training
    l = self.run_iteration(self.tr_gen, True)
  File "/home/smaijer/experiment/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 262, in run_iteration
    self.amp_grad_scaler.scale(l).backward()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 300.00 MiB (GPU 0; 23.65 GiB total capacity; 21.56 GiB already allocated; 208.31 MiB free; 22.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Exception in thread Thread-5:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
