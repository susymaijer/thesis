Starting at Fri Jun 17 00:48:59 CEST 2022
Running on hosts: res-hpc-lkeb07
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 8.
Account: div2-lkeb
Job ID: 10517810
Job name: ExpPancreasTrain
Node running script: res-hpc-lkeb07
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Fri Jun 17 15:07:25 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     Off  | 00000000:AF:00.0 Off |                  Off |
| 32%   36C    P0    60W / 260W |      0MiB / 24220MiB |      4%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer/experiment
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
nnUNet_raw_data_base = /exports/lkeb-hpc/smaijer/data/nnUNet_raw_data_base
nnUNet_preprocessed = /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed
RESULTS_FOLDER = /exports/lkeb-hpc/smaijer/results
OUTPUT = /exports/lkeb-hpc/smaijer/output
Installing hidden layer..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-xaeabtj6/hiddenlayer_2825089ba6f64f84b91708a88d2e4087
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Installing nnU-net..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_lowres', network_trainer='nnUNetTrainerV2_UNETR', task='501', fold='0', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_lowres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_UNETR.nnUNetTrainerV2_UNETR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 0 from these plans
I am using sample dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-17 15:07:46.566013: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-06-17 15:07:46.576794: The split file contains 5 splits.
2022-06-17 15:07:46.579382: Desired fold for training: 0
2022-06-17 15:07:46.581560: This split has 54 training and 14 validation cases.
unpacking dataset
done
2022-06-17 15:07:46.825779: UNETR initialising network
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
UNETR(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(32, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(32, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): UNETRDecoder(
    (decoder5): UnetrUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (conv_block): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder4): UnetrUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (conv_block): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder3): UnetrUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (conv_block): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder2): UnetrUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (conv_block): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (out): UnetOutBlock(
      (conv): Convolution(
        (conv): Conv3d(16, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-06-17 15:07:50.306077: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_lowres/Task501/nnUNetTrainerV2_UNETR__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-06-17 15:08:08.460287: Suus1 maybe_update_lr lr: (-0.315779+0.102603j)
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
2022-06-17 15:08:11.564316: Unable to plot network architecture:
2022-06-17 15:08:11.577559: No module named 'IPython'
2022-06-17 15:08:11.579764: 
printing the network instead:

2022-06-17 15:08:11.581664: UNETR(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU()
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(32, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(32, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): UNETRDecoder(
    (decoder5): UnetrUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (conv_block): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder4): UnetrUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (conv_block): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(128, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder3): UnetrUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (conv_block): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(64, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (decoder2): UnetrUpBlock(
      (transp_conv): Convolution(
        (conv): ConvTranspose3d(32, 16, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (conv_block): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(32, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (out): UnetOutBlock(
      (conv): Convolution(
        (conv): Conv3d(16, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1))
      )
    )
  )
)
2022-06-17 15:08:11.617824: 

2022-06-17 15:08:13.135216: saving checkpoint...
2022-06-17 15:08:18.234374: done, saving took 6.60 seconds
panc_0006 (2, 124, 311, 311)
debug: mirroring True mirror_axes (0, 1, 2)
Suus 3d
step_size: 0.5
do mirror: True
data shape: (1, 124, 311, 311)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 22, 44], [0, 60, 119], [0, 76, 151]]
number of tiles: 27
computing Gaussian
done
Program finished with exit code 0 at: Fri Jun 17 00:48:59 CEST 2022
