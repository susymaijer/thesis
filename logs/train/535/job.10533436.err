
Currently Loaded Modules:
  1) tools/miniconda/python3.8/4.9.2

 

WARNING: overwriting environment variables set in the machine
overwriting variable nnUNet_raw_data_base nnUNet_preprocessed RESULTS_FOLDER OUTPUT
  Running command git clone -q https://github.com/FabianIsensee/hiddenlayer.git /tmp/pip-install-5ae5xzf5/hiddenlayer_16361613a8f94ca484d94a3f41499208
  Running command git checkout -b more_plotted_details --track origin/more_plotted_details
  Switched to a new branch 'more_plotted_details'
  Branch 'more_plotted_details' set up to track remote branch 'more_plotted_details' from 'origin'.
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 180, in main
    trainer.run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 453, in run_training
    ret = super().run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainer.py", line 318, in run_training
    super(nnUNetTrainer, self).run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 459, in run_training
    l = self.run_iteration(self.tr_gen, True)
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 259, in run_iteration
    l = self.loss(output, target)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/deep_supervision.py", line 44, in forward
    l += weights[i] * self.loss(x[i], y[i])
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 346, in forward
    dc_loss = self.dc(net_output, target, loss_mask=mask) if self.weight_dice != 0 else 0
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 178, in forward
    tp, fp, fn, _ = get_tp_fp_fn_tn(x, y, axes, loss_mask, False)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 128, in get_tp_fp_fn_tn
    y_onehot.scatter_(1, gt, 1)
RuntimeError: Expected index [2, 1, 64, 80, 80] to be smaller than self [2, 16, 32, 80, 80] apart from dimension 1
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread Thread-5:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
