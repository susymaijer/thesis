
Currently Loaded Modules:
  1) tools/miniconda/python3.8/4.9.2

 

WARNING: overwriting environment variables set in the machine
overwriting variable nnUNet_raw_data_base nnUNet_preprocessed RESULTS_FOLDER OUTPUT
  Running command git clone -q https://github.com/FabianIsensee/hiddenlayer.git /tmp/pip-install-xuvzuf3z/hiddenlayer_6a8f84abdad94ee6a9fce4cc7f14b92f
  Running command git checkout -b more_plotted_details --track origin/more_plotted_details
  Switched to a new branch 'more_plotted_details'
  Branch 'more_plotted_details' set up to track remote branch 'more_plotted_details' from 'origin'.
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 180, in main
    trainer.run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 453, in run_training
    ret = super().run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainer.py", line 318, in run_training
    super(nnUNetTrainer, self).run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 459, in run_training
    l = self.run_iteration(self.tr_gen, True)
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 259, in run_iteration
    l = self.loss(output, target)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/deep_supervision.py", line 44, in forward
    l += weights[i] * self.loss(x[i], y[i])
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 346, in forward
    dc_loss = self.dc(net_output, target, loss_mask=mask) if self.weight_dice != 0 else 0
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 178, in forward
    tp, fp, fn, _ = get_tp_fp_fn_tn(x, y, axes, loss_mask, False)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 128, in get_tp_fp_fn_tn
    y_onehot.scatter_(1, gt, 1)
RuntimeError: Expected index [2, 1, 64, 80, 80] to be smaller than self [2, 16, 32, 80, 80] apart from dimension 1
Exception in thread Thread-5:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 180, in main
    trainer.run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 453, in run_training
    ret = super().run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainer.py", line 318, in run_training
    super(nnUNetTrainer, self).run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 459, in run_training
    l = self.run_iteration(self.tr_gen, True)
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 259, in run_iteration
    l = self.loss(output, target)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/deep_supervision.py", line 44, in forward
    l += weights[i] * self.loss(x[i], y[i])
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 346, in forward
    dc_loss = self.dc(net_output, target, loss_mask=mask) if self.weight_dice != 0 else 0
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 178, in forward
    tp, fp, fn, _ = get_tp_fp_fn_tn(x, y, axes, loss_mask, False)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 128, in get_tp_fp_fn_tn
    y_onehot.scatter_(1, gt, 1)
RuntimeError: Expected index [2, 1, 64, 80, 80] to be smaller than self [2, 16, 32, 80, 80] apart from dimension 1
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
Exception in thread Thread-5:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 180, in main
    trainer.run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 453, in run_training
    ret = super().run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainer.py", line 318, in run_training
    super(nnUNetTrainer, self).run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 459, in run_training
    l = self.run_iteration(self.tr_gen, True)
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 259, in run_iteration
    l = self.loss(output, target)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/deep_supervision.py", line 44, in forward
    l += weights[i] * self.loss(x[i], y[i])
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 346, in forward
    dc_loss = self.dc(net_output, target, loss_mask=mask) if self.weight_dice != 0 else 0
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 178, in forward
    tp, fp, fn, _ = get_tp_fp_fn_tn(x, y, axes, loss_mask, False)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 128, in get_tp_fp_fn_tn
    y_onehot.scatter_(1, gt, 1)
RuntimeError: Expected index [2, 1, 64, 80, 80] to be smaller than self [2, 16, 32, 80, 80] apart from dimension 1
Exception in thread Thread-5:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 180, in main
    trainer.run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 453, in run_training
    ret = super().run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainer.py", line 318, in run_training
    super(nnUNetTrainer, self).run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 459, in run_training
    l = self.run_iteration(self.tr_gen, True)
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 259, in run_iteration
    l = self.loss(output, target)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/deep_supervision.py", line 44, in forward
    l += weights[i] * self.loss(x[i], y[i])
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 346, in forward
    dc_loss = self.dc(net_output, target, loss_mask=mask) if self.weight_dice != 0 else 0
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 178, in forward
    tp, fp, fn, _ = get_tp_fp_fn_tn(x, y, axes, loss_mask, False)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 128, in get_tp_fp_fn_tn
    y_onehot.scatter_(1, gt, 1)
RuntimeError: Expected index [2, 1, 64, 80, 80] to be smaller than self [2, 16, 32, 80, 80] apart from dimension 1
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
Exception in thread Thread-5:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 180, in main
    trainer.run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 453, in run_training
    ret = super().run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainer.py", line 318, in run_training
    super(nnUNetTrainer, self).run_training()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 459, in run_training
    l = self.run_iteration(self.tr_gen, True)
  File "/home/smaijer/nnUNet/nnunet/training/network_training/nnUNetTrainerV2.py", line 259, in run_iteration
    l = self.loss(output, target)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/deep_supervision.py", line 44, in forward
    l += weights[i] * self.loss(x[i], y[i])
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 346, in forward
    dc_loss = self.dc(net_output, target, loss_mask=mask) if self.weight_dice != 0 else 0
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 178, in forward
    tp, fp, fn, _ = get_tp_fp_fn_tn(x, y, axes, loss_mask, False)
  File "/home/smaijer/nnUNet/nnunet/training/loss_functions/dice_loss.py", line 128, in get_tp_fp_fn_tn
    y_onehot.scatter_(1, gt, 1)
RuntimeError: Expected index [2, 1, 64, 80, 80] to be smaller than self [2, 16, 32, 80, 80] apart from dimension 1
Exception in thread Thread-4:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
Exception in thread Thread-5:
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/dataloading/multi_threaded_augmenter.py", line 92, in results_loop
    raise RuntimeError("Abort event was set. So someone died and we should end this madness. \nIMPORTANT: "
RuntimeError: Abort event was set. So someone died and we should end this madness. 
IMPORTANT: This is not the actual error message! Look further up to see what caused the error. Please also check whether your RAM was full
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 172, in main
    trainer.load_latest_checkpoint()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 309, in load_latest_checkpoint
    raise RuntimeError("No checkpoint found")
RuntimeError: No checkpoint found
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 172, in main
    trainer.load_latest_checkpoint()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 309, in load_latest_checkpoint
    raise RuntimeError("No checkpoint found")
RuntimeError: No checkpoint found
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 172, in main
    trainer.load_latest_checkpoint()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 309, in load_latest_checkpoint
    raise RuntimeError("No checkpoint found")
RuntimeError: No checkpoint found
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 172, in main
    trainer.load_latest_checkpoint()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 309, in load_latest_checkpoint
    raise RuntimeError("No checkpoint found")
RuntimeError: No checkpoint found
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_train", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_train')())
  File "/home/smaijer/nnUNet/nnunet/run/run_training.py", line 172, in main
    trainer.load_latest_checkpoint()
  File "/home/smaijer/nnUNet/nnunet/training/network_training/network_trainer.py", line 309, in load_latest_checkpoint
    raise RuntimeError("No checkpoint found")
RuntimeError: No checkpoint found
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_determine_postprocessing", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_determine_postprocessing')())
  File "/home/smaijer/nnUNet/nnunet/postprocessing/consolidate_postprocessing_simple.py", line 56, in main
    consolidate_folds(folder, val)
  File "/home/smaijer/nnUNet/nnunet/postprocessing/consolidate_postprocessing.py", line 61, in consolidate_folds
    collect_cv_niftis(output_folder_base, output_folder_raw, validation_folder_name,
  File "/home/smaijer/nnUNet/nnunet/postprocessing/consolidate_postprocessing.py", line 31, in collect_cv_niftis
    raise RuntimeError("some folds are missing. Please run the full 5-fold cross-validation. "
RuntimeError: some folds are missing. Please run the full 5-fold cross-validation. The following folds seem to be missing: [0, 1, 2, 3, 4]
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_predict", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_predict')())
  File "/home/smaijer/nnUNet/nnunet/inference/predict_simple.py", line 217, in main
    predict_from_folder(model_folder_name, input_folder, output_folder, folds, save_npz, num_threads_preprocessing,
  File "/home/smaijer/nnUNet/nnunet/inference/predict.py", line 658, in predict_from_folder
    return predict_cases(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,
  File "/home/smaijer/nnUNet/nnunet/inference/predict.py", line 184, in predict_cases
    trainer, params = load_model_and_checkpoint_files(model, folds, mixed_precision=mixed_precision,
  File "/home/smaijer/nnUNet/nnunet/training/model_restore.py", line 140, in load_model_and_checkpoint_files
    trainer = restore_model(join(folds[0], "%s.model.pkl" % checkpoint_name), fp16=mixed_precision)
  File "/home/smaijer/nnUNet/nnunet/training/model_restore.py", line 56, in restore_model
    info = load_pickle(pkl_file)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/utilities/file_and_folder_operations.py", line 57, in load_pickle
    with open(file, mode) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task535/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model.pkl'
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_predict", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_predict')())
  File "/home/smaijer/nnUNet/nnunet/inference/predict_simple.py", line 217, in main
    predict_from_folder(model_folder_name, input_folder, output_folder, folds, save_npz, num_threads_preprocessing,
  File "/home/smaijer/nnUNet/nnunet/inference/predict.py", line 658, in predict_from_folder
    return predict_cases(model, list_of_lists[part_id::num_parts], output_files[part_id::num_parts], folds,
  File "/home/smaijer/nnUNet/nnunet/inference/predict.py", line 184, in predict_cases
    trainer, params = load_model_and_checkpoint_files(model, folds, mixed_precision=mixed_precision,
  File "/home/smaijer/nnUNet/nnunet/training/model_restore.py", line 140, in load_model_and_checkpoint_files
    trainer = restore_model(join(folds[0], "%s.model.pkl" % checkpoint_name), fp16=mixed_precision)
  File "/home/smaijer/nnUNet/nnunet/training/model_restore.py", line 56, in restore_model
    info = load_pickle(pkl_file)
  File "/home/smaijer/.conda/envs/nn/lib/python3.9/site-packages/batchgenerators/utilities/file_and_folder_operations.py", line 57, in load_pickle
    with open(file, mode) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task535/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model.pkl'
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_evaluate_folder", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_evaluate_folder')())
  File "/home/smaijer/nnUNet/nnunet/evaluation/evaluator.py", line 483, in nnunet_evaluate_folder
    return evaluate_folder(args.ref, args.pred, args.l)
  File "/home/smaijer/nnUNet/nnunet/evaluation/evaluator.py", line 456, in evaluate_folder
    assert all([i in files_pred for i in files_gt]), "files missing in folder_with_predictions"
AssertionError: files missing in folder_with_predictions
Traceback (most recent call last):
  File "/home/smaijer/.conda/envs/nn/bin/nnUNet_evaluate_folder", line 33, in <module>
    sys.exit(load_entry_point('nnunet', 'console_scripts', 'nnUNet_evaluate_folder')())
  File "/home/smaijer/nnUNet/nnunet/evaluation/evaluator.py", line 483, in nnunet_evaluate_folder
    return evaluate_folder(args.ref, args.pred, args.l)
  File "/home/smaijer/nnUNet/nnunet/evaluation/evaluator.py", line 456, in evaluate_folder
    assert all([i in files_pred for i in files_gt]), "files missing in folder_with_predictions"
AssertionError: files missing in folder_with_predictions
