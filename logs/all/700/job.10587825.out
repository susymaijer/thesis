Starting at Mon Jun 27 16:26:32 CEST 2022
Running on hosts: res-hpc-lkeb05
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 8.
Account: div2-lkeb
Job ID: 10587825
Job name: PancreasAll
Node running script: res-hpc-lkeb05
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Thu Jun 30 00:23:51 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     Off  | 00000000:AF:00.0 Off |                  Off |
| 31%   40C    P0    62W / 260W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-kw56r4ft/hiddenlayer_2fc14e6a6c8e4d329022d5435aa68749
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Obtaining file:///home/smaijer/code/nnUNet
Requirement already satisfied: torch>1.10.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.12.0)
Requirement already satisfied: tqdm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.64.0)
Requirement already satisfied: dicom2nifti in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.4.2)
Requirement already satisfied: scikit-image>=0.14 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.19.3)
Requirement already satisfied: medpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.0)
Requirement already satisfied: scipy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.8.1)
Requirement already satisfied: batchgenerators>=0.23 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.24)
Requirement already satisfied: numpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.23.0)
Requirement already satisfied: sklearn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.0)
Requirement already satisfied: SimpleITK in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.1.1.2)
Requirement already satisfied: pandas in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.4.3)
Requirement already satisfied: requests in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.28.0)
Requirement already satisfied: nibabel in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.0.1)
Requirement already satisfied: tifffile in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2022.5.4)
Requirement already satisfied: matplotlib in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (3.5.2)
Requirement already satisfied: monai in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.9.0)
Requirement already satisfied: einops in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.1)
Requirement already satisfied: ipython in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (8.4.0)
Requirement already satisfied: graphviz in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.20)
Requirement already satisfied: future in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)
Requirement already satisfied: pillow>=7.1.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (9.1.1)
Requirement already satisfied: scikit-learn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.1)
Requirement already satisfied: unittest2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: threadpoolctl in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.1.0)
Requirement already satisfied: PyWavelets>=1.1.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.3.0)
Requirement already satisfied: packaging>=20.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (21.3)
Requirement already satisfied: imageio>=2.4.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.19.3)
Requirement already satisfied: networkx>=2.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.8.4)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from packaging>=20.0->scikit-image>=0.14->nnunet==1.7.0) (3.0.9)
Requirement already satisfied: typing-extensions in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from torch>1.10.0->nnunet==1.7.0) (4.2.0)
Requirement already satisfied: python-gdcm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (3.0.14)
Requirement already satisfied: pydicom>=2.2.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (2.3.0)
Requirement already satisfied: pickleshare in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.7.5)
Requirement already satisfied: traitlets>=5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.3.0)
Requirement already satisfied: setuptools>=18.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (58.1.0)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (3.0.30)
Requirement already satisfied: decorator in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.1.1)
Requirement already satisfied: backcall in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.2.0)
Requirement already satisfied: matplotlib-inline in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.1.3)
Requirement already satisfied: pexpect>4.3 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (4.8.0)
Requirement already satisfied: jedi>=0.16 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.18.1)
Requirement already satisfied: pygments>=2.4.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (2.12.0)
Requirement already satisfied: stack-data in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.3.0)
Requirement already satisfied: parso<0.9.0,>=0.8.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from jedi>=0.16->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: ptyprocess>=0.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pexpect>4.3->ipython->nnunet==1.7.0) (0.7.0)
Requirement already satisfied: wcwidth in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nnunet==1.7.0) (0.2.5)
Requirement already satisfied: fonttools>=4.22.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (4.33.3)
Requirement already satisfied: cycler>=0.10 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (0.11.0)
Requirement already satisfied: python-dateutil>=2.7 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (2.8.2)
Requirement already satisfied: kiwisolver>=1.0.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (1.4.3)
Requirement already satisfied: six>=1.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunet==1.7.0) (1.16.0)
Requirement already satisfied: pytz>=2020.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pandas->nnunet==1.7.0) (2022.1)
Requirement already satisfied: idna<4,>=2.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (3.3)
Requirement already satisfied: certifi>=2017.4.17 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2022.6.15)
Requirement already satisfied: charset-normalizer~=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2.0.12)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (1.26.9)
Requirement already satisfied: joblib>=1.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: pure-eval in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.2.2)
Requirement already satisfied: asttokens in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (2.0.5)
Requirement already satisfied: executing in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: traceback2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: linecache2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)
Installing collected packages: argparse, nnunet
  Attempting uninstall: nnunet
    Found existing installation: nnunet 1.7.0
    Uninstalling nnunet-1.7.0:
      Successfully uninstalled nnunet-1.7.0
  Running setup.py develop for nnunet
Successfully installed argparse-1.4.0 nnunet-1.7.0
Start preprocessing..
Done preprocessing! Start training all the folds..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='0', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-30 00:24:32.797612: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-30 00:24:32.809443: The split file contains 5 splits.
2022-06-30 00:24:32.812113: Desired fold for training: 0
2022-06-30 00:24:32.814318: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-06-30 00:24:36.271461: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/model_latest.model train= True
SuusB run_training - zet learning rate als  
2022-06-30 00:24:43.553920: Suus1 maybe_update_lr lr: 0.003384
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-06-30 00:25:01.432181: Unable to plot network architecture:
2022-06-30 00:25:01.445646: local variable 'g' referenced before assignment
2022-06-30 00:25:01.457367: 
printing the network instead:

2022-06-30 00:25:01.470338: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-06-30 00:25:01.486375: 

2022-06-30 00:25:01.494694: 
epoch:  350
2022-06-30 00:29:54.201750: train loss : 0.1975
2022-06-30 00:30:10.799113: validation loss: 0.1341
2022-06-30 00:30:10.803593: Average global foreground Dice: [0.7209, 0.6738, 0.6074, 0.351, 0.2651, 0.8601, 0.4695, 0.6712, 0.4821, 0.4573, 0.2697, 0.0, 0.2644, 0.6504, 0.294]
2022-06-30 00:30:10.805981: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:30:11.427825: Suus1 maybe_update_lr lr: 0.003364
2022-06-30 00:30:11.430295: saving best epoch checkpoint...
2022-06-30 00:30:11.845281: saving checkpoint...
2022-06-30 00:30:14.944749: done, saving took 3.51 seconds
2022-06-30 00:30:14.978802: This epoch took 313.474904 s

2022-06-30 00:30:14.981263: 
epoch:  351
2022-06-30 00:34:51.782254: train loss : 0.2086
2022-06-30 00:35:08.393200: validation loss: 0.1367
2022-06-30 00:35:08.401322: Average global foreground Dice: [0.6067, 0.6602, 0.4971, 0.4666, 0.4434, 0.8282, 0.4063, 0.7701, 0.5246, 0.4367, 0.2958, 0.0, 0.2513, 0.4185, 0.4856]
2022-06-30 00:35:08.403799: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:35:08.898914: Suus1 maybe_update_lr lr: 0.003343
2022-06-30 00:35:08.901410: saving best epoch checkpoint...
2022-06-30 00:35:09.188746: saving checkpoint...
2022-06-30 00:35:12.267140: done, saving took 3.36 seconds
2022-06-30 00:35:12.284444: This epoch took 297.300965 s

2022-06-30 00:35:12.286664: 
epoch:  352
2022-06-30 00:39:49.200966: train loss : 0.1885
2022-06-30 00:40:05.803164: validation loss: 0.1652
2022-06-30 00:40:05.808169: Average global foreground Dice: [0.5802, 0.6432, 0.5346, 0.2787, 0.3631, 0.8097, 0.4525, 0.7239, 0.4951, 0.3724, 0.247, 0.0, 0.1697, 0.5381, 0.5471]
2022-06-30 00:40:05.811164: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:40:06.485353: Suus1 maybe_update_lr lr: 0.003323
2022-06-30 00:40:06.488216: This epoch took 294.199052 s

2022-06-30 00:40:06.490691: 
epoch:  353
2022-06-30 00:44:43.446912: train loss : 0.1610
2022-06-30 00:45:00.073201: validation loss: 0.1265
2022-06-30 00:45:00.077681: Average global foreground Dice: [0.6909, 0.6572, 0.4084, 0.4077, 0.3054, 0.8014, 0.4269, 0.6544, 0.4875, 0.4579, 0.2769, 0.0, 0.2293, 0.6203, 0.601]
2022-06-30 00:45:00.080426: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:45:00.586603: Suus1 maybe_update_lr lr: 0.003303
2022-06-30 00:45:00.588848: This epoch took 294.095684 s

2022-06-30 00:45:00.590952: 
epoch:  354
2022-06-30 00:49:37.588283: train loss : 0.1865
2022-06-30 00:49:54.211590: validation loss: 0.1145
2022-06-30 00:49:54.215961: Average global foreground Dice: [0.6854, 0.6869, 0.6004, 0.482, 0.3575, 0.8586, 0.3985, 0.6665, 0.5025, 0.387, 0.2919, 0.0, 0.2187, 0.4136, 0.3678]
2022-06-30 00:49:54.218523: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:49:54.690851: Suus1 maybe_update_lr lr: 0.003282
2022-06-30 00:49:54.693186: This epoch took 294.100322 s

2022-06-30 00:49:54.695279: 
epoch:  355
2022-06-30 00:54:31.693477: train loss : 0.1944
2022-06-30 00:54:48.323466: validation loss: 0.1732
2022-06-30 00:54:48.328315: Average global foreground Dice: [0.6236, 0.5882, 0.5156, 0.437, 0.3366, 0.8056, 0.3493, 0.7109, 0.5078, 0.3922, 0.2791, 0.0, 0.2321, 0.6382, 0.5348]
2022-06-30 00:54:48.330965: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:54:48.810598: Suus1 maybe_update_lr lr: 0.003262
2022-06-30 00:54:48.813160: This epoch took 294.115788 s

2022-06-30 00:54:48.815701: 
epoch:  356
2022-06-30 00:59:25.725139: train loss : 0.1686
2022-06-30 00:59:42.335907: validation loss: 0.1415
2022-06-30 00:59:42.340191: Average global foreground Dice: [0.6285, 0.5736, 0.6591, 0.4367, 0.3675, 0.8091, 0.5379, 0.6878, 0.4944, 0.4477, 0.275, 0.0, 0.2077, 0.5723, 0.3894]
2022-06-30 00:59:42.342386: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:59:42.820753: Suus1 maybe_update_lr lr: 0.003241
2022-06-30 00:59:42.823171: This epoch took 294.004982 s

2022-06-30 00:59:42.825347: 
epoch:  357
2022-06-30 01:04:19.739253: train loss : 0.1839
2022-06-30 01:04:36.364109: validation loss: 0.1029
2022-06-30 01:04:36.368546: Average global foreground Dice: [0.7111, 0.6017, 0.594, 0.3895, 0.3262, 0.8342, 0.4973, 0.6836, 0.5455, 0.3753, 0.2677, 0.0, 0.2276, 0.6158, 0.5855]
2022-06-30 01:04:36.371341: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:04:36.845171: Suus1 maybe_update_lr lr: 0.003221
2022-06-30 01:04:36.848185: This epoch took 294.020695 s

2022-06-30 01:04:36.850684: 
epoch:  358
2022-06-30 01:09:13.709486: train loss : 0.1893
2022-06-30 01:09:30.303630: validation loss: 0.1680
2022-06-30 01:09:30.308736: Average global foreground Dice: [0.6502, 0.4937, 0.5796, 0.3264, 0.3263, 0.8211, 0.3483, 0.685, 0.4897, 0.3503, 0.1859, 0.0, 0.2337, 0.5528, 0.5908]
2022-06-30 01:09:30.312185: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:09:30.785738: Suus1 maybe_update_lr lr: 0.003201
2022-06-30 01:09:30.788441: This epoch took 293.935498 s

2022-06-30 01:09:30.791001: 
epoch:  359
2022-06-30 01:14:07.652518: train loss : 0.1642
2022-06-30 01:14:24.278033: validation loss: 0.1170
2022-06-30 01:14:24.282816: Average global foreground Dice: [0.7119, 0.6647, 0.506, 0.404, 0.2852, 0.8153, 0.378, 0.7176, 0.5722, 0.4222, 0.2583, 0.0, 0.2453, 0.5251, 0.4871]
2022-06-30 01:14:24.286355: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:14:24.787195: Suus1 maybe_update_lr lr: 0.00318
2022-06-30 01:14:24.789834: This epoch took 293.996371 s

2022-06-30 01:14:24.792503: 
epoch:  360
2022-06-30 01:19:01.744444: train loss : 0.1912
2022-06-30 01:19:18.415307: validation loss: 0.1600
2022-06-30 01:19:18.420477: Average global foreground Dice: [0.5713, 0.563, 0.4702, 0.4493, 0.2861, 0.8503, 0.3931, 0.6774, 0.4829, 0.3866, 0.228, 0.0, 0.1436, 0.6627, 0.3862]
2022-06-30 01:19:18.423110: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:19:18.907658: Suus1 maybe_update_lr lr: 0.00316
2022-06-30 01:19:18.909991: This epoch took 294.114783 s

2022-06-30 01:19:18.912159: 
epoch:  361
2022-06-30 01:23:55.940086: train loss : 0.1859
2022-06-30 01:24:12.573962: validation loss: 0.1309
2022-06-30 01:24:12.579163: Average global foreground Dice: [0.627, 0.6402, 0.5599, 0.2534, 0.3413, 0.8544, 0.3903, 0.6612, 0.5224, 0.3709, 0.3184, 0.0, 0.2588, 0.6784, 0.4564]
2022-06-30 01:24:12.582248: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:24:13.085323: Suus1 maybe_update_lr lr: 0.003139
2022-06-30 01:24:13.087769: This epoch took 294.173437 s

2022-06-30 01:24:13.090189: 
epoch:  362
2022-06-30 01:28:50.071792: train loss : 0.1538
2022-06-30 01:29:06.693454: validation loss: 0.1734
2022-06-30 01:29:06.698003: Average global foreground Dice: [0.649, 0.5801, 0.6142, 0.3817, 0.3171, 0.7786, 0.3784, 0.6768, 0.4979, 0.3859, 0.2988, 0.0, 0.2191, 0.632, 0.5601]
2022-06-30 01:29:06.700342: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:29:07.167356: Suus1 maybe_update_lr lr: 0.003119
2022-06-30 01:29:07.169741: This epoch took 294.077378 s

2022-06-30 01:29:07.172011: 
epoch:  363
2022-06-30 01:33:44.349770: train loss : 0.1783
2022-06-30 01:34:00.995963: validation loss: 0.1124
2022-06-30 01:34:01.001073: Average global foreground Dice: [0.7027, 0.6245, 0.6836, 0.4416, 0.3786, 0.8329, 0.4218, 0.6958, 0.5236, 0.3927, 0.3232, 0.0006, 0.1893, 0.5292, 0.4409]
2022-06-30 01:34:01.003850: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:34:01.500769: Suus1 maybe_update_lr lr: 0.003098
2022-06-30 01:34:01.503322: This epoch took 294.328880 s

2022-06-30 01:34:01.505619: 
epoch:  364
2022-06-30 01:38:38.679856: train loss : 0.1753
2022-06-30 01:38:55.324552: validation loss: 0.1283
2022-06-30 01:38:55.329443: Average global foreground Dice: [0.6634, 0.6345, 0.5559, 0.3926, 0.276, 0.8559, 0.5075, 0.6204, 0.5289, 0.3897, 0.2365, 0.0, 0.2234, 0.4021, 0.3364]
2022-06-30 01:38:55.332293: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:38:55.802226: Suus1 maybe_update_lr lr: 0.003078
2022-06-30 01:38:55.804952: This epoch took 294.296824 s

2022-06-30 01:38:55.807563: 
epoch:  365
2022-06-30 01:43:33.051321: train loss : 0.1865
2022-06-30 01:43:49.670709: validation loss: 0.1351
2022-06-30 01:43:49.675481: Average global foreground Dice: [0.6633, 0.6131, 0.5671, 0.4046, 0.4794, 0.8621, 0.4093, 0.7276, 0.513, 0.3629, 0.2962, 0.0, 0.2659, 0.6168, 0.5592]
2022-06-30 01:43:49.678498: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:43:50.165511: Suus1 maybe_update_lr lr: 0.003057
2022-06-30 01:43:50.187372: This epoch took 294.377453 s

2022-06-30 01:43:50.211460: 
epoch:  366
2022-06-30 01:48:27.444492: train loss : 0.2176
2022-06-30 01:48:44.084446: validation loss: 0.1055
2022-06-30 01:48:44.089886: Average global foreground Dice: [0.672, 0.6752, 0.6174, 0.4853, 0.3674, 0.8565, 0.5259, 0.6943, 0.5213, 0.459, 0.3179, 0.0038, 0.2514, 0.4965, 0.2756]
2022-06-30 01:48:44.093012: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:48:44.611954: Suus1 maybe_update_lr lr: 0.003037
2022-06-30 01:48:44.614557: This epoch took 294.382852 s

2022-06-30 01:48:44.617286: 
epoch:  367
2022-06-30 01:53:21.820845: train loss : 0.1884
2022-06-30 01:53:38.437410: validation loss: 0.1679
2022-06-30 01:53:38.442211: Average global foreground Dice: [0.7048, 0.6404, 0.631, 0.2352, 0.3153, 0.7841, 0.4671, 0.6617, 0.5056, 0.3812, 0.2113, 0.0002, 0.1855, 0.6381, 0.3187]
2022-06-30 01:53:38.444935: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:53:38.922840: Suus1 maybe_update_lr lr: 0.003016
2022-06-30 01:53:38.925455: This epoch took 294.305568 s

2022-06-30 01:53:38.927525: 
epoch:  368
2022-06-30 01:58:16.114728: train loss : 0.1586
2022-06-30 01:58:32.737089: validation loss: 0.1113
2022-06-30 01:58:32.741391: Average global foreground Dice: [0.7307, 0.6699, 0.6491, 0.4973, 0.4134, 0.8782, 0.543, 0.7412, 0.4734, 0.451, 0.3481, 0.0003, 0.2583, 0.653, 0.5867]
2022-06-30 01:58:32.743952: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:58:33.224090: Suus1 maybe_update_lr lr: 0.002996
2022-06-30 01:58:33.227485: saving best epoch checkpoint...
2022-06-30 01:58:33.413610: saving checkpoint...
2022-06-30 01:58:36.536883: done, saving took 3.31 seconds
2022-06-30 01:58:36.552370: This epoch took 297.622656 s

2022-06-30 01:58:36.554517: 
epoch:  369
2022-06-30 02:03:13.650672: train loss : 0.1655
2022-06-30 02:03:30.284028: validation loss: 0.1148
2022-06-30 02:03:30.288505: Average global foreground Dice: [0.6437, 0.6607, 0.6591, 0.3897, 0.3732, 0.8475, 0.4576, 0.713, 0.496, 0.3885, 0.2362, 0.0104, 0.2998, 0.629, 0.3667]
2022-06-30 02:03:30.291140: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:03:30.767941: Suus1 maybe_update_lr lr: 0.002975
2022-06-30 02:03:30.770362: saving best epoch checkpoint...
2022-06-30 02:03:30.955888: saving checkpoint...
2022-06-30 02:03:33.997379: done, saving took 3.22 seconds
2022-06-30 02:03:34.012073: This epoch took 297.455479 s

2022-06-30 02:03:34.014363: 
epoch:  370
2022-06-30 02:08:11.120746: train loss : 0.1490
2022-06-30 02:08:27.756191: validation loss: 0.1342
2022-06-30 02:08:27.761160: Average global foreground Dice: [0.5228, 0.5624, 0.4331, 0.402, 0.364, 0.8013, 0.5309, 0.7165, 0.5761, 0.4212, 0.2669, 0.0074, 0.2259, 0.8002, 0.6687]
2022-06-30 02:08:27.763805: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:08:28.245271: Suus1 maybe_update_lr lr: 0.002954
2022-06-30 02:08:28.247787: saving best epoch checkpoint...
2022-06-30 02:08:28.429363: saving checkpoint...
2022-06-30 02:08:31.496537: done, saving took 3.25 seconds
2022-06-30 02:08:31.511105: This epoch took 297.494752 s

2022-06-30 02:08:31.513456: 
epoch:  371
2022-06-30 02:13:10.456947: train loss : 0.1933
2022-06-30 02:13:27.428360: validation loss: 0.1443
2022-06-30 02:13:27.432981: Average global foreground Dice: [0.6365, 0.578, 0.4915, 0.4284, 0.3049, 0.8289, 0.3939, 0.6878, 0.5154, 0.3691, 0.2435, 0.0009, 0.2956, 0.6063, 0.4182]
2022-06-30 02:13:27.435578: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:13:27.917987: Suus1 maybe_update_lr lr: 0.002934
2022-06-30 02:13:27.920458: This epoch took 296.404377 s

2022-06-30 02:13:27.922719: 
epoch:  372
2022-06-30 02:18:11.664879: train loss : 0.1619
2022-06-30 02:18:28.676179: validation loss: 0.1534
2022-06-30 02:18:28.680735: Average global foreground Dice: [0.5641, 0.5775, 0.4598, 0.3164, 0.3034, 0.8505, 0.4607, 0.6623, 0.4796, 0.4117, 0.2617, 0.0107, 0.2313, 0.7652, 0.7059]
2022-06-30 02:18:28.683208: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:18:29.160155: Suus1 maybe_update_lr lr: 0.002913
2022-06-30 02:18:29.162376: This epoch took 301.237627 s

2022-06-30 02:18:29.164548: 
epoch:  373
2022-06-30 02:23:13.156670: train loss : 0.1919
2022-06-30 02:23:30.180302: validation loss: 0.2024
2022-06-30 02:23:30.185972: Average global foreground Dice: [0.6276, 0.5498, 0.6343, 0.2442, 0.3198, 0.6714, 0.3968, 0.7107, 0.4745, 0.406, 0.2768, 0.0139, 0.2486, 0.5912, 0.3338]
2022-06-30 02:23:30.189390: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:23:30.657595: Suus1 maybe_update_lr lr: 0.002892
2022-06-30 02:23:30.660094: This epoch took 301.493516 s

2022-06-30 02:23:30.662448: 
epoch:  374
2022-06-30 02:28:14.598229: train loss : 0.1833
2022-06-30 02:28:31.647265: validation loss: 0.1033
2022-06-30 02:28:31.651840: Average global foreground Dice: [0.682, 0.619, 0.6879, 0.4068, 0.4474, 0.8494, 0.4643, 0.6808, 0.4663, 0.408, 0.2936, 0.0005, 0.1992, 0.4929, 0.4177]
2022-06-30 02:28:31.653975: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:28:32.132299: Suus1 maybe_update_lr lr: 0.002872
2022-06-30 02:28:32.134367: This epoch took 301.469379 s

2022-06-30 02:28:32.136323: 
epoch:  375
2022-06-30 02:33:16.063422: train loss : 0.1892
2022-06-30 02:33:33.087521: validation loss: 0.1269
2022-06-30 02:33:33.091924: Average global foreground Dice: [0.6577, 0.5249, 0.6312, 0.3277, 0.3059, 0.844, 0.4434, 0.673, 0.4495, 0.4047, 0.3278, 0.0, 0.2495, 0.4954, 0.3313]
2022-06-30 02:33:33.094363: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:33:33.564508: Suus1 maybe_update_lr lr: 0.002851
2022-06-30 02:33:33.566968: This epoch took 301.428636 s

2022-06-30 02:33:33.569024: 
epoch:  376
2022-06-30 02:38:17.483350: train loss : 0.1915
2022-06-30 02:38:34.483885: validation loss: 0.1147
2022-06-30 02:38:34.487911: Average global foreground Dice: [0.763, 0.5615, 0.6496, 0.3587, 0.3548, 0.854, 0.5155, 0.669, 0.5409, 0.3827, 0.2368, 0.0004, 0.2524, 0.6833, 0.5578]
2022-06-30 02:38:34.490236: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:38:34.964376: Suus1 maybe_update_lr lr: 0.00283
2022-06-30 02:38:34.966731: This epoch took 301.395489 s

2022-06-30 02:38:34.968838: 
epoch:  377
2022-06-30 02:43:19.156020: train loss : 0.1561
2022-06-30 02:43:36.217749: validation loss: 0.1404
2022-06-30 02:43:36.222251: Average global foreground Dice: [0.5284, 0.5479, 0.4514, 0.4447, 0.3463, 0.8393, 0.4778, 0.6644, 0.4358, 0.4078, 0.2778, 0.0055, 0.2641, 0.5787, 0.5805]
2022-06-30 02:43:36.224543: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:43:36.713695: Suus1 maybe_update_lr lr: 0.00281
2022-06-30 02:43:36.717515: This epoch took 301.746610 s

2022-06-30 02:43:36.719858: 
epoch:  378
2022-06-30 02:48:21.032249: train loss : 0.1362
2022-06-30 02:48:38.079739: validation loss: 0.1395
2022-06-30 02:48:38.084563: Average global foreground Dice: [0.7077, 0.6582, 0.6502, 0.3841, 0.4187, 0.8262, 0.4596, 0.7225, 0.5247, 0.4349, 0.2749, 0.0104, 0.2248, 0.5005, 0.5271]
2022-06-30 02:48:38.087204: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:48:38.556400: Suus1 maybe_update_lr lr: 0.002789
2022-06-30 02:48:38.558755: This epoch took 301.836771 s

2022-06-30 02:48:38.561042: 
epoch:  379
2022-06-30 02:53:23.160390: train loss : 0.1907
2022-06-30 02:53:40.215495: validation loss: 0.1027
2022-06-30 02:53:40.219813: Average global foreground Dice: [0.6918, 0.668, 0.613, 0.4187, 0.4017, 0.8552, 0.4336, 0.6969, 0.4813, 0.3874, 0.3398, 0.0049, 0.2272, 0.4861, 0.2829]
2022-06-30 02:53:40.222335: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:53:40.695463: Suus1 maybe_update_lr lr: 0.002768
2022-06-30 02:53:40.698166: This epoch took 302.134725 s

2022-06-30 02:53:40.700264: 
epoch:  380
2022-06-30 02:58:25.358152: train loss : 0.1722
2022-06-30 02:58:42.407441: validation loss: 0.1405
2022-06-30 02:58:42.411795: Average global foreground Dice: [0.6559, 0.6785, 0.5827, 0.367, 0.3964, 0.8288, 0.2874, 0.6938, 0.566, 0.4683, 0.3542, 0.0008, 0.2415, 0.6092, 0.3778]
2022-06-30 02:58:42.414113: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 02:58:42.973593: Suus1 maybe_update_lr lr: 0.002747
2022-06-30 02:58:42.976264: This epoch took 302.273895 s

2022-06-30 02:58:42.978643: 
epoch:  381
2022-06-30 03:03:27.652632: train loss : 0.1899
2022-06-30 03:03:44.704358: validation loss: 0.1133
2022-06-30 03:03:44.708713: Average global foreground Dice: [0.6455, 0.6727, 0.579, 0.2455, 0.3773, 0.8531, 0.4979, 0.7086, 0.4798, 0.4559, 0.3883, 0.0046, 0.216, 0.5308, 0.6923]
2022-06-30 03:03:44.711370: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:03:45.181032: Suus1 maybe_update_lr lr: 0.002727
2022-06-30 03:03:45.183355: This epoch took 302.202300 s

2022-06-30 03:03:45.185378: 
epoch:  382
2022-06-30 03:08:29.719926: train loss : 0.1475
2022-06-30 03:08:46.772017: validation loss: 0.1191
2022-06-30 03:08:46.777173: Average global foreground Dice: [0.6831, 0.5864, 0.5839, 0.3847, 0.4435, 0.8369, 0.5097, 0.6892, 0.5179, 0.4298, 0.2824, 0.0082, 0.2206, 0.4964, 0.4316]
2022-06-30 03:08:46.779828: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:08:47.284344: Suus1 maybe_update_lr lr: 0.002706
2022-06-30 03:08:47.287090: This epoch took 302.099673 s

2022-06-30 03:08:47.289557: 
epoch:  383
2022-06-30 03:13:31.267003: train loss : 0.1717
2022-06-30 03:13:48.268708: validation loss: 0.0957
2022-06-30 03:13:48.272953: Average global foreground Dice: [0.6598, 0.6391, 0.616, 0.4315, 0.3752, 0.8554, 0.4724, 0.6953, 0.5585, 0.4047, 0.2944, 0.0276, 0.2268, 0.6543, 0.4306]
2022-06-30 03:13:48.275458: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:13:48.751644: Suus1 maybe_update_lr lr: 0.002685
2022-06-30 03:13:48.753980: saving best epoch checkpoint...
2022-06-30 03:13:48.947798: saving checkpoint...
2022-06-30 03:13:52.120069: done, saving took 3.36 seconds
2022-06-30 03:13:52.135771: This epoch took 304.843706 s

2022-06-30 03:13:52.138259: 
epoch:  384
2022-06-30 03:18:36.372231: train loss : 0.1673
2022-06-30 03:18:53.417965: validation loss: 0.1054
2022-06-30 03:18:53.422727: Average global foreground Dice: [0.651, 0.728, 0.7033, 0.3963, 0.5173, 0.8244, 0.3857, 0.7203, 0.5307, 0.4371, 0.3371, 0.0588, 0.1581, 0.4612, 0.6123]
2022-06-30 03:18:53.425095: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:18:53.903831: Suus1 maybe_update_lr lr: 0.002664
2022-06-30 03:18:53.906392: saving best epoch checkpoint...
2022-06-30 03:18:54.096141: saving checkpoint...
2022-06-30 03:18:57.051324: done, saving took 3.14 seconds
2022-06-30 03:18:57.066280: This epoch took 304.925732 s

2022-06-30 03:18:57.068616: 
epoch:  385
2022-06-30 03:23:41.284898: train loss : 0.1810
2022-06-30 03:23:58.359982: validation loss: 0.1216
2022-06-30 03:23:58.364384: Average global foreground Dice: [0.6116, 0.6565, 0.6039, 0.3744, 0.3244, 0.8369, 0.4113, 0.7124, 0.5308, 0.4326, 0.2568, 0.0545, 0.2445, 0.4265, 0.52]
2022-06-30 03:23:58.366691: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:23:58.839100: Suus1 maybe_update_lr lr: 0.002643
2022-06-30 03:23:58.841637: This epoch took 301.770728 s

2022-06-30 03:23:58.843827: 
epoch:  386
2022-06-30 03:28:43.373892: train loss : 0.1785
2022-06-30 03:29:00.425035: validation loss: 0.1201
2022-06-30 03:29:00.429807: Average global foreground Dice: [0.6531, 0.5438, 0.5053, 0.4911, 0.4212, 0.8107, 0.4462, 0.7194, 0.5401, 0.3936, 0.401, 0.0384, 0.2793, 0.6406, 0.4273]
2022-06-30 03:29:00.432664: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:29:00.905180: Suus1 maybe_update_lr lr: 0.002622
2022-06-30 03:29:00.907566: saving best epoch checkpoint...
2022-06-30 03:29:01.097968: saving checkpoint...
2022-06-30 03:29:04.087808: done, saving took 3.18 seconds
2022-06-30 03:29:04.102477: This epoch took 305.256121 s

2022-06-30 03:29:04.104851: 
epoch:  387
2022-06-30 03:33:48.221540: train loss : 0.1831
2022-06-30 03:34:05.287438: validation loss: 0.0883
2022-06-30 03:34:05.292115: Average global foreground Dice: [0.6983, 0.7276, 0.6442, 0.5495, 0.3709, 0.8325, 0.5025, 0.6894, 0.526, 0.483, 0.3194, 0.079, 0.2528, 0.4808, 0.523]
2022-06-30 03:34:05.294880: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:34:05.773747: Suus1 maybe_update_lr lr: 0.002601
2022-06-30 03:34:05.776097: saving best epoch checkpoint...
2022-06-30 03:34:05.958792: saving checkpoint...
2022-06-30 03:34:09.234893: done, saving took 3.46 seconds
2022-06-30 03:34:09.251377: This epoch took 305.144028 s

2022-06-30 03:34:09.253389: 
epoch:  388
2022-06-30 03:38:53.578954: train loss : 0.1636
2022-06-30 03:39:10.652019: validation loss: 0.1070
2022-06-30 03:39:10.656662: Average global foreground Dice: [0.6453, 0.6368, 0.4738, 0.4553, 0.3377, 0.8597, 0.4557, 0.6878, 0.5373, 0.457, 0.3644, 0.0515, 0.182, 0.4913, 0.3923]
2022-06-30 03:39:10.658861: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:39:11.150907: Suus1 maybe_update_lr lr: 0.002581
2022-06-30 03:39:11.153147: This epoch took 301.897792 s

2022-06-30 03:39:11.155165: 
epoch:  389
2022-06-30 03:43:55.778457: train loss : 0.1687
2022-06-30 03:44:12.831308: validation loss: 0.1664
2022-06-30 03:44:12.835672: Average global foreground Dice: [0.5464, 0.4774, 0.4612, 0.3012, 0.3666, 0.7909, 0.4722, 0.7065, 0.4984, 0.3771, 0.2434, 0.1058, 0.2325, 0.4347, 0.44]
2022-06-30 03:44:12.838332: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:44:13.310951: Suus1 maybe_update_lr lr: 0.00256
2022-06-30 03:44:13.313416: This epoch took 302.156188 s

2022-06-30 03:44:13.315837: 
epoch:  390
2022-06-30 03:48:57.882047: train loss : 0.1832
2022-06-30 03:49:14.951286: validation loss: 0.1133
2022-06-30 03:49:14.956569: Average global foreground Dice: [0.6077, 0.6939, 0.6196, 0.4409, 0.368, 0.8451, 0.4322, 0.6285, 0.5248, 0.3994, 0.2915, 0.0267, 0.1997, 0.4581, 0.2798]
2022-06-30 03:49:14.959054: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:49:15.444614: Suus1 maybe_update_lr lr: 0.002539
2022-06-30 03:49:15.447172: This epoch took 302.129148 s

2022-06-30 03:49:15.449454: 
epoch:  391
2022-06-30 03:54:00.050129: train loss : 0.1437
2022-06-30 03:54:17.110084: validation loss: 0.1088
2022-06-30 03:54:17.114714: Average global foreground Dice: [0.5484, 0.6566, 0.5976, 0.45, 0.4099, 0.8252, 0.4619, 0.7371, 0.5468, 0.4683, 0.2875, 0.0879, 0.2876, 0.4301, 0.5207]
2022-06-30 03:54:17.117318: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:54:17.589570: Suus1 maybe_update_lr lr: 0.002518
2022-06-30 03:54:17.592361: This epoch took 302.140654 s

2022-06-30 03:54:17.594886: 
epoch:  392
2022-06-30 03:59:02.155722: train loss : 0.1780
2022-06-30 03:59:19.203177: validation loss: 0.0892
2022-06-30 03:59:19.207819: Average global foreground Dice: [0.6526, 0.6479, 0.6156, 0.4687, 0.3137, 0.8505, 0.5031, 0.6989, 0.5591, 0.4331, 0.2967, 0.0693, 0.1638, 0.6206, 0.591]
2022-06-30 03:59:19.210086: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 03:59:19.690341: Suus1 maybe_update_lr lr: 0.002497
2022-06-30 03:59:19.692800: This epoch took 302.095641 s

2022-06-30 03:59:19.694991: 
epoch:  393
2022-06-30 04:04:04.249103: train loss : 0.1602
2022-06-30 04:04:21.299958: validation loss: 0.1023
2022-06-30 04:04:21.304740: Average global foreground Dice: [0.5998, 0.6597, 0.5024, 0.3154, 0.3998, 0.8609, 0.4236, 0.7584, 0.5707, 0.4183, 0.3176, 0.0831, 0.257, 0.5956, 0.605]
2022-06-30 04:04:21.307104: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:04:21.809573: Suus1 maybe_update_lr lr: 0.002476
2022-06-30 04:04:21.812010: This epoch took 302.114787 s

2022-06-30 04:04:21.814152: 
epoch:  394
2022-06-30 04:09:06.239749: train loss : 0.1484
2022-06-30 04:09:23.291238: validation loss: 0.1173
2022-06-30 04:09:23.321186: Average global foreground Dice: [0.6467, 0.5891, 0.5957, 0.425, 0.3631, 0.8526, 0.3969, 0.7312, 0.5268, 0.3668, 0.3698, 0.0428, 0.2351, 0.5837, 0.5155]
2022-06-30 04:09:23.323789: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:09:23.793773: Suus1 maybe_update_lr lr: 0.002455
2022-06-30 04:09:23.796533: This epoch took 301.980094 s

2022-06-30 04:09:23.798674: 
epoch:  395
2022-06-30 04:14:08.231701: train loss : 0.1521
2022-06-30 04:14:25.276404: validation loss: 0.0975
2022-06-30 04:14:25.281443: Average global foreground Dice: [0.6105, 0.6201, 0.6221, 0.4517, 0.3556, 0.8491, 0.4988, 0.7059, 0.5634, 0.4162, 0.3033, 0.1854, 0.2502, 0.58, 0.4456]
2022-06-30 04:14:25.284492: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:14:25.760080: Suus1 maybe_update_lr lr: 0.002434
2022-06-30 04:14:25.762597: saving best epoch checkpoint...
2022-06-30 04:14:25.948444: saving checkpoint...
2022-06-30 04:14:28.958618: done, saving took 3.19 seconds
2022-06-30 04:14:28.974762: This epoch took 305.173759 s

2022-06-30 04:14:28.976934: 
epoch:  396
2022-06-30 04:19:13.073195: train loss : 0.1436
2022-06-30 04:19:30.138519: validation loss: 0.1145
2022-06-30 04:19:30.142780: Average global foreground Dice: [0.6664, 0.6278, 0.5715, 0.3925, 0.3758, 0.8382, 0.4799, 0.6907, 0.4611, 0.3812, 0.2517, 0.1466, 0.2488, 0.7192, 0.6093]
2022-06-30 04:19:30.145210: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:19:30.618380: Suus1 maybe_update_lr lr: 0.002413
2022-06-30 04:19:30.620904: saving best epoch checkpoint...
2022-06-30 04:19:30.808476: saving checkpoint...
2022-06-30 04:19:33.861194: done, saving took 3.24 seconds
2022-06-30 04:19:33.876005: This epoch took 304.896930 s

2022-06-30 04:19:33.878286: 
epoch:  397
2022-06-30 04:24:17.979810: train loss : 0.1857
2022-06-30 04:24:35.029883: validation loss: 0.1333
2022-06-30 04:24:35.035320: Average global foreground Dice: [0.5716, 0.5175, 0.5757, 0.4586, 0.3942, 0.816, 0.5467, 0.7139, 0.4538, 0.4282, 0.2889, 0.0282, 0.2124, 0.6669, 0.5498]
2022-06-30 04:24:35.038340: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:24:35.515648: Suus1 maybe_update_lr lr: 0.002391
2022-06-30 04:24:35.518313: This epoch took 301.637906 s

2022-06-30 04:24:35.520547: 
epoch:  398
2022-06-30 04:29:19.932498: train loss : 0.1690
2022-06-30 04:29:36.988648: validation loss: 0.1103
2022-06-30 04:29:36.993556: Average global foreground Dice: [0.7537, 0.5983, 0.561, 0.4148, 0.3322, 0.8345, 0.3993, 0.6868, 0.5207, 0.3727, 0.3039, 0.0937, 0.2303, 0.4679, 0.4334]
2022-06-30 04:29:36.996251: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:29:37.466656: Suus1 maybe_update_lr lr: 0.00237
2022-06-30 04:29:37.469252: This epoch took 301.946312 s

2022-06-30 04:29:37.471481: 
epoch:  399
2022-06-30 04:34:21.922681: train loss : 0.1413
2022-06-30 04:34:38.986022: validation loss: 0.0871
2022-06-30 04:34:38.990650: Average global foreground Dice: [0.5842, 0.7462, 0.6761, 0.3895, 0.3101, 0.876, 0.454, 0.6933, 0.5349, 0.4235, 0.3262, 0.1727, 0.2649, 0.6582, 0.4637]
2022-06-30 04:34:38.994167: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:34:39.507963: Suus1 maybe_update_lr lr: 0.002349
2022-06-30 04:34:39.510404: saving scheduled checkpoint file...
2022-06-30 04:34:39.700716: saving checkpoint...
2022-06-30 04:34:42.731722: done, saving took 3.22 seconds
2022-06-30 04:34:42.749655: done
2022-06-30 04:34:42.752291: saving best epoch checkpoint...
2022-06-30 04:34:42.900723: saving checkpoint...
2022-06-30 04:34:45.870975: done, saving took 3.12 seconds
2022-06-30 04:34:45.886437: This epoch took 308.412726 s

2022-06-30 04:34:45.888964: 
epoch:  400
2022-06-30 04:39:29.744168: train loss : 0.1350
2022-06-30 04:39:46.816157: validation loss: 0.0934
2022-06-30 04:39:46.820922: Average global foreground Dice: [0.6274, 0.7065, 0.6004, 0.498, 0.3796, 0.8633, 0.3583, 0.7144, 0.5628, 0.4261, 0.3212, 0.1014, 0.2358, 0.49, 0.7084]
2022-06-30 04:39:46.823384: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:39:47.294569: Suus1 maybe_update_lr lr: 0.002328
2022-06-30 04:39:47.297268: saving best epoch checkpoint...
2022-06-30 04:39:47.480101: saving checkpoint...
2022-06-30 04:39:50.487866: done, saving took 3.19 seconds
2022-06-30 04:39:50.502760: This epoch took 304.611409 s

2022-06-30 04:39:50.505124: 
epoch:  401
2022-06-30 04:44:34.650645: train loss : 0.1516
2022-06-30 04:44:51.710360: validation loss: 0.1324
2022-06-30 04:44:51.714745: Average global foreground Dice: [0.5498, 0.6503, 0.5424, 0.3601, 0.3923, 0.8207, 0.3928, 0.6406, 0.5376, 0.4237, 0.2919, 0.0573, 0.2382, 0.6678, 0.4895]
2022-06-30 04:44:51.717189: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:44:52.199451: Suus1 maybe_update_lr lr: 0.002307
2022-06-30 04:44:52.201921: This epoch took 301.694485 s

2022-06-30 04:44:52.204131: 
epoch:  402
2022-06-30 04:49:36.643690: train loss : 0.1639
2022-06-30 04:49:53.709920: validation loss: 0.1148
2022-06-30 04:49:53.714677: Average global foreground Dice: [0.655, 0.5788, 0.5512, 0.4184, 0.418, 0.8207, 0.3605, 0.6892, 0.5591, 0.3694, 0.3099, 0.1252, 0.203, 0.5386, 0.5094]
2022-06-30 04:49:53.717229: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:49:54.203916: Suus1 maybe_update_lr lr: 0.002286
2022-06-30 04:49:54.206293: This epoch took 301.999970 s

2022-06-30 04:49:54.208578: 
epoch:  403
2022-06-30 04:54:38.679900: train loss : 0.1546
2022-06-30 04:54:55.728325: validation loss: 0.2086
2022-06-30 04:54:55.732597: Average global foreground Dice: [0.639, 0.6824, 0.4706, 0.44, 0.3095, 0.7861, 0.3562, 0.6541, 0.5135, 0.4184, 0.2529, 0.0975, 0.2389, 0.5007, 0.5395]
2022-06-30 04:54:55.735311: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:54:56.221669: Suus1 maybe_update_lr lr: 0.002264
2022-06-30 04:54:56.224342: This epoch took 302.013720 s

2022-06-30 04:54:56.227729: 
epoch:  404
2022-06-30 04:59:40.677383: train loss : 0.1617
2022-06-30 04:59:57.720548: validation loss: 0.1574
2022-06-30 04:59:57.725757: Average global foreground Dice: [0.7177, 0.579, 0.5582, 0.3402, 0.361, 0.7876, 0.4779, 0.6986, 0.4809, 0.4008, 0.3044, 0.0807, 0.2208, 0.5929, 0.5297]
2022-06-30 04:59:57.729307: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 04:59:58.220824: Suus1 maybe_update_lr lr: 0.002243
2022-06-30 04:59:58.223412: This epoch took 301.993356 s

2022-06-30 04:59:58.226110: 
epoch:  405
2022-06-30 05:04:42.669414: train loss : 0.1629
2022-06-30 05:04:59.719482: validation loss: 0.1194
2022-06-30 05:04:59.724641: Average global foreground Dice: [0.6374, 0.6412, 0.5758, 0.3932, 0.3416, 0.8333, 0.35, 0.6748, 0.5109, 0.4086, 0.2852, 0.1447, 0.2499, 0.5813, 0.5567]
2022-06-30 05:04:59.727551: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:05:00.202952: Suus1 maybe_update_lr lr: 0.002222
2022-06-30 05:05:00.205732: This epoch took 301.977254 s

2022-06-30 05:05:00.208623: 
epoch:  406
2022-06-30 05:09:44.687402: train loss : 0.1703
2022-06-30 05:10:01.734277: validation loss: 0.1042
2022-06-30 05:10:01.739284: Average global foreground Dice: [0.7311, 0.6216, 0.6401, 0.4117, 0.3424, 0.8529, 0.4398, 0.6549, 0.5441, 0.4001, 0.2845, 0.0871, 0.207, 0.6303, 0.6456]
2022-06-30 05:10:01.741804: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:10:02.229960: Suus1 maybe_update_lr lr: 0.002201
2022-06-30 05:10:02.232434: This epoch took 302.021232 s

2022-06-30 05:10:02.234701: 
epoch:  407
2022-06-30 05:14:46.683240: train loss : 0.1412
2022-06-30 05:15:03.718818: validation loss: 0.1460
2022-06-30 05:15:03.723531: Average global foreground Dice: [0.6112, 0.5929, 0.535, 0.3425, 0.3682, 0.8022, 0.4862, 0.6751, 0.5452, 0.3566, 0.3746, 0.0966, 0.2723, 0.6014, 0.5108]
2022-06-30 05:15:03.726122: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:15:04.241626: Suus1 maybe_update_lr lr: 0.002179
2022-06-30 05:15:04.243859: This epoch took 302.007080 s

2022-06-30 05:15:04.246145: 
epoch:  408
2022-06-30 05:19:48.707698: train loss : 0.1522
2022-06-30 05:20:05.745885: validation loss: 0.1008
2022-06-30 05:20:05.750538: Average global foreground Dice: [0.6101, 0.6407, 0.5987, 0.4427, 0.424, 0.8286, 0.5496, 0.7377, 0.5732, 0.3954, 0.3383, 0.1013, 0.2425, 0.6848, 0.5513]
2022-06-30 05:20:05.753259: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:20:06.256314: Suus1 maybe_update_lr lr: 0.002158
2022-06-30 05:20:06.258860: This epoch took 302.010539 s

2022-06-30 05:20:06.261452: 
epoch:  409
2022-06-30 05:24:50.749109: train loss : 0.1702
2022-06-30 05:25:07.786968: validation loss: 0.1202
2022-06-30 05:25:07.791584: Average global foreground Dice: [0.6343, 0.6226, 0.606, 0.3913, 0.4024, 0.8311, 0.3959, 0.6658, 0.5277, 0.4385, 0.2597, 0.1145, 0.2829, 0.6036, 0.6378]
2022-06-30 05:25:07.793916: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:25:08.282519: Suus1 maybe_update_lr lr: 0.002137
2022-06-30 05:25:08.284878: saving best epoch checkpoint...
2022-06-30 05:25:08.468976: saving checkpoint...
2022-06-30 05:25:11.366551: done, saving took 3.08 seconds
2022-06-30 05:25:11.380915: This epoch took 305.117221 s

2022-06-30 05:25:11.383146: 
epoch:  410
2022-06-30 05:29:55.105584: train loss : 0.1557
2022-06-30 05:30:12.142971: validation loss: 0.1095
2022-06-30 05:30:12.147621: Average global foreground Dice: [0.7373, 0.6263, 0.5508, 0.3865, 0.4076, 0.8441, 0.4556, 0.732, 0.5126, 0.4741, 0.3096, 0.1579, 0.283, 0.48, 0.667]
2022-06-30 05:30:12.150149: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:30:12.644142: Suus1 maybe_update_lr lr: 0.002115
2022-06-30 05:30:12.646495: saving best epoch checkpoint...
2022-06-30 05:30:12.830587: saving checkpoint...
2022-06-30 05:30:15.734905: done, saving took 3.09 seconds
2022-06-30 05:30:15.749769: This epoch took 304.364357 s

2022-06-30 05:30:15.751855: 
epoch:  411
2022-06-30 05:34:59.610968: train loss : 0.1650
2022-06-30 05:35:16.651001: validation loss: 0.0881
2022-06-30 05:35:16.655202: Average global foreground Dice: [0.5485, 0.67, 0.5119, 0.4097, 0.2655, 0.8683, 0.5077, 0.682, 0.5801, 0.4525, 0.3241, 0.0328, 0.2468, 0.6942, 0.4476]
2022-06-30 05:35:16.657555: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:35:17.321821: Suus1 maybe_update_lr lr: 0.002094
2022-06-30 05:35:17.324167: This epoch took 301.570216 s

2022-06-30 05:35:17.326098: 
epoch:  412
2022-06-30 05:40:01.657597: train loss : 0.1470
2022-06-30 05:40:18.700323: validation loss: 0.1074
2022-06-30 05:40:18.704993: Average global foreground Dice: [0.7186, 0.5828, 0.643, 0.3011, 0.35, 0.8403, 0.4889, 0.693, 0.5443, 0.4502, 0.276, 0.1323, 0.267, 0.7054, 0.5655]
2022-06-30 05:40:18.707410: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:40:19.171943: Suus1 maybe_update_lr lr: 0.002072
2022-06-30 05:40:19.174172: saving best epoch checkpoint...
2022-06-30 05:40:19.361656: saving checkpoint...
2022-06-30 05:40:22.207408: done, saving took 3.03 seconds
2022-06-30 05:40:22.222353: This epoch took 304.894303 s

2022-06-30 05:40:22.225422: 
epoch:  413
2022-06-30 05:45:06.092623: train loss : 0.1439
2022-06-30 05:45:23.132121: validation loss: 0.0768
2022-06-30 05:45:23.137237: Average global foreground Dice: [0.6437, 0.7034, 0.6868, 0.3813, 0.3954, 0.876, 0.48, 0.6954, 0.5329, 0.4308, 0.354, 0.1824, 0.2539, 0.6141, 0.4508]
2022-06-30 05:45:23.139632: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:45:23.631686: Suus1 maybe_update_lr lr: 0.002051
2022-06-30 05:45:23.634015: saving best epoch checkpoint...
2022-06-30 05:45:23.819228: saving checkpoint...
2022-06-30 05:45:26.927259: done, saving took 3.29 seconds
2022-06-30 05:45:26.941517: This epoch took 304.713745 s

2022-06-30 05:45:26.943563: 
epoch:  414
2022-06-30 05:50:10.810309: train loss : 0.1293
2022-06-30 05:50:27.849736: validation loss: 0.0978
2022-06-30 05:50:27.853697: Average global foreground Dice: [0.6841, 0.608, 0.5716, 0.4, 0.3835, 0.8434, 0.4705, 0.747, 0.5365, 0.5013, 0.3316, 0.1943, 0.3026, 0.6889, 0.6158]
2022-06-30 05:50:27.856411: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:50:28.322357: Suus1 maybe_update_lr lr: 0.00203
2022-06-30 05:50:28.324642: saving best epoch checkpoint...
2022-06-30 05:50:28.516105: saving checkpoint...
2022-06-30 05:50:31.394884: done, saving took 3.07 seconds
2022-06-30 05:50:31.409373: This epoch took 304.463839 s

2022-06-30 05:50:31.411889: 
epoch:  415
2022-06-30 05:55:15.438420: train loss : 0.1545
2022-06-30 05:55:32.471047: validation loss: 0.1440
2022-06-30 05:55:32.475046: Average global foreground Dice: [0.6584, 0.6352, 0.602, 0.2736, 0.3493, 0.8229, 0.4229, 0.6799, 0.5432, 0.4434, 0.3693, 0.1485, 0.2286, 0.6595, 0.4559]
2022-06-30 05:55:32.477427: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 05:55:32.979906: Suus1 maybe_update_lr lr: 0.002008
2022-06-30 05:55:32.982403: This epoch took 301.568482 s

2022-06-30 05:55:32.984688: 
epoch:  416
2022-06-30 06:00:17.321880: train loss : 0.1577
2022-06-30 06:00:34.399120: validation loss: 0.1293
2022-06-30 06:00:34.403157: Average global foreground Dice: [0.7444, 0.6328, 0.649, 0.4942, 0.3416, 0.814, 0.4749, 0.7261, 0.5137, 0.4822, 0.2674, 0.1348, 0.2659, 0.3465, 0.5396]
2022-06-30 06:00:34.405362: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:00:34.879575: Suus1 maybe_update_lr lr: 0.001987
2022-06-30 06:00:34.881733: This epoch took 301.894960 s

2022-06-30 06:00:34.883613: 
epoch:  417
2022-06-30 06:05:19.253369: train loss : 0.1392
2022-06-30 06:05:36.286746: validation loss: 0.1281
2022-06-30 06:05:36.291781: Average global foreground Dice: [0.6068, 0.552, 0.6218, 0.4073, 0.4042, 0.8429, 0.5026, 0.7333, 0.5208, 0.4114, 0.2737, 0.1494, 0.2592, 0.6094, 0.3729]
2022-06-30 06:05:36.294328: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:05:36.757796: Suus1 maybe_update_lr lr: 0.001965
2022-06-30 06:05:36.759991: This epoch took 301.874517 s

2022-06-30 06:05:36.761984: 
epoch:  418
2022-06-30 06:10:21.199658: train loss : 0.1679
2022-06-30 06:10:38.250411: validation loss: 0.0512
2022-06-30 06:10:38.254929: Average global foreground Dice: [0.6263, 0.7192, 0.6294, 0.4875, 0.4566, 0.8531, 0.5091, 0.7222, 0.5926, 0.4089, 0.299, 0.1346, 0.2403, 0.6103, 0.6128]
2022-06-30 06:10:38.257488: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:10:38.729729: Suus1 maybe_update_lr lr: 0.001943
2022-06-30 06:10:38.732082: saving best epoch checkpoint...
2022-06-30 06:10:38.918036: saving checkpoint...
2022-06-30 06:10:41.791219: done, saving took 3.06 seconds
2022-06-30 06:10:41.805736: This epoch took 305.041770 s

2022-06-30 06:10:41.807884: 
epoch:  419
2022-06-30 06:15:25.864667: train loss : 0.1425
2022-06-30 06:15:42.920113: validation loss: 0.0942
2022-06-30 06:15:42.924574: Average global foreground Dice: [0.6464, 0.6552, 0.6131, 0.5368, 0.3764, 0.851, 0.3675, 0.7263, 0.5784, 0.4471, 0.4311, 0.1365, 0.2943, 0.4918, 0.4756]
2022-06-30 06:15:42.926876: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:15:43.391006: Suus1 maybe_update_lr lr: 0.001922
2022-06-30 06:15:43.393421: saving best epoch checkpoint...
2022-06-30 06:15:43.577120: saving checkpoint...
2022-06-30 06:15:46.419831: done, saving took 3.02 seconds
2022-06-30 06:15:46.434192: This epoch took 304.624271 s

2022-06-30 06:15:46.436308: 
epoch:  420
2022-06-30 06:20:30.387540: train loss : 0.1475
2022-06-30 06:20:47.432166: validation loss: 0.1058
2022-06-30 06:20:47.436170: Average global foreground Dice: [0.6997, 0.6546, 0.6193, 0.4147, 0.4354, 0.8038, 0.4571, 0.7082, 0.5461, 0.4619, 0.3413, 0.1532, 0.2188, 0.4592, 0.6052]
2022-06-30 06:20:47.438237: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:20:47.912376: Suus1 maybe_update_lr lr: 0.0019
2022-06-30 06:20:47.914534: saving best epoch checkpoint...
2022-06-30 06:20:48.123856: saving checkpoint...
2022-06-30 06:20:50.964092: done, saving took 3.05 seconds
2022-06-30 06:20:50.978177: This epoch took 304.539807 s

2022-06-30 06:20:50.980299: 
epoch:  421
2022-06-30 06:25:34.356484: train loss : 0.1212
2022-06-30 06:25:51.404405: validation loss: 0.1492
2022-06-30 06:25:51.409627: Average global foreground Dice: [0.6341, 0.6549, 0.4686, 0.3735, 0.3846, 0.7805, 0.4114, 0.7091, 0.4614, 0.4288, 0.3047, 0.1603, 0.2355, 0.708, 0.7107]
2022-06-30 06:25:51.411914: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:25:52.117832: Suus1 maybe_update_lr lr: 0.001879
2022-06-30 06:25:52.119864: This epoch took 301.137446 s

2022-06-30 06:25:52.121866: 
epoch:  422
2022-06-30 06:30:36.510115: train loss : 0.1494
2022-06-30 06:30:53.551011: validation loss: 0.1362
2022-06-30 06:30:53.556072: Average global foreground Dice: [0.5765, 0.5555, 0.6002, 0.2983, 0.3411, 0.8556, 0.4495, 0.6546, 0.4986, 0.3672, 0.2897, 0.1039, 0.258, 0.8068, 0.6241]
2022-06-30 06:30:53.558609: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:30:54.030740: Suus1 maybe_update_lr lr: 0.001857
2022-06-30 06:30:54.032932: This epoch took 301.909003 s

2022-06-30 06:30:54.034854: 
epoch:  423
2022-06-30 06:35:38.432599: train loss : 0.1524
2022-06-30 06:35:55.491011: validation loss: 0.1222
2022-06-30 06:35:55.495992: Average global foreground Dice: [0.7368, 0.5669, 0.6224, 0.2746, 0.4606, 0.8685, 0.4914, 0.7121, 0.536, 0.4525, 0.2939, 0.1322, 0.2498, 0.3751, 0.5194]
2022-06-30 06:35:55.498105: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:35:55.964270: Suus1 maybe_update_lr lr: 0.001835
2022-06-30 06:35:55.966938: This epoch took 301.929993 s

2022-06-30 06:35:55.969440: 
epoch:  424
2022-06-30 06:40:40.356032: train loss : 0.1526
2022-06-30 06:40:57.415660: validation loss: 0.0953
2022-06-30 06:40:57.420682: Average global foreground Dice: [0.7403, 0.7182, 0.7122, 0.4216, 0.3474, 0.8402, 0.466, 0.6858, 0.5275, 0.435, 0.3393, 0.1581, 0.3261, 0.7103, 0.6103]
2022-06-30 06:40:57.423406: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:40:57.969621: Suus1 maybe_update_lr lr: 0.001813
2022-06-30 06:40:57.971575: saving best epoch checkpoint...
2022-06-30 06:40:58.178877: saving checkpoint...
2022-06-30 06:41:01.070340: done, saving took 3.10 seconds
2022-06-30 06:41:01.084686: This epoch took 305.112945 s

2022-06-30 06:41:01.086664: 
epoch:  425
2022-06-30 06:45:45.109454: train loss : 0.1272
2022-06-30 06:46:02.171017: validation loss: 0.0821
2022-06-30 06:46:02.176770: Average global foreground Dice: [0.6396, 0.5435, 0.6043, 0.4501, 0.3927, 0.8307, 0.5495, 0.7073, 0.571, 0.4583, 0.3437, 0.1629, 0.3016, 0.7102, 0.3549]
2022-06-30 06:46:02.179264: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:46:02.666889: Suus1 maybe_update_lr lr: 0.001792
2022-06-30 06:46:02.669247: saving best epoch checkpoint...
2022-06-30 06:46:02.850479: saving checkpoint...
2022-06-30 06:46:05.899318: done, saving took 3.23 seconds
2022-06-30 06:46:05.913429: This epoch took 304.824672 s

2022-06-30 06:46:05.915417: 
epoch:  426
2022-06-30 06:50:49.927748: train loss : 0.1306
2022-06-30 06:51:06.966974: validation loss: 0.0308
2022-06-30 06:51:06.971473: Average global foreground Dice: [0.8269, 0.6679, 0.7298, 0.5598, 0.4293, 0.8761, 0.5767, 0.7365, 0.5683, 0.5102, 0.3117, 0.1762, 0.296, 0.7001, 0.4701]
2022-06-30 06:51:06.973839: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:51:07.442271: Suus1 maybe_update_lr lr: 0.00177
2022-06-30 06:51:07.444626: saving best epoch checkpoint...
2022-06-30 06:51:07.629229: saving checkpoint...
2022-06-30 06:51:10.544014: done, saving took 3.10 seconds
2022-06-30 06:51:10.558903: This epoch took 304.641576 s

2022-06-30 06:51:10.561024: 
epoch:  427
2022-06-30 06:55:54.546397: train loss : 0.1421
2022-06-30 06:56:11.581872: validation loss: 0.0561
2022-06-30 06:56:11.586651: Average global foreground Dice: [0.7363, 0.6594, 0.6673, 0.4694, 0.4899, 0.86, 0.5105, 0.7858, 0.5906, 0.4385, 0.3525, 0.2358, 0.2837, 0.7263, 0.5926]
2022-06-30 06:56:11.588979: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 06:56:12.054713: Suus1 maybe_update_lr lr: 0.001748
2022-06-30 06:56:12.057208: saving best epoch checkpoint...
2022-06-30 06:56:12.261323: saving checkpoint...
2022-06-30 06:56:15.154104: done, saving took 3.09 seconds
2022-06-30 06:56:15.168471: This epoch took 304.605357 s

2022-06-30 06:56:15.170552: 
epoch:  428
2022-06-30 07:00:59.126365: train loss : 0.1238
2022-06-30 07:01:16.175875: validation loss: 0.1026
2022-06-30 07:01:16.180738: Average global foreground Dice: [0.6877, 0.5575, 0.6513, 0.3927, 0.3983, 0.8424, 0.4392, 0.6342, 0.5567, 0.3989, 0.4112, 0.1113, 0.2835, 0.5706, 0.4193]
2022-06-30 07:01:16.182811: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:01:16.653457: Suus1 maybe_update_lr lr: 0.001726
2022-06-30 07:01:16.655662: This epoch took 301.482931 s

2022-06-30 07:01:16.657681: 
epoch:  429
2022-06-30 07:06:01.032904: train loss : 0.1131
2022-06-30 07:06:18.092434: validation loss: 0.0930
2022-06-30 07:06:18.097406: Average global foreground Dice: [0.4602, 0.6369, 0.5167, 0.4079, 0.4292, 0.7927, 0.4483, 0.7585, 0.5612, 0.4526, 0.328, 0.1807, 0.2871, 0.4489, 0.4784]
2022-06-30 07:06:18.099734: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:06:18.567037: Suus1 maybe_update_lr lr: 0.001704
2022-06-30 07:06:18.571284: This epoch took 301.911568 s

2022-06-30 07:06:18.573100: 
epoch:  430
2022-06-30 07:11:03.004419: train loss : 0.1392
2022-06-30 07:11:20.066008: validation loss: 0.0946
2022-06-30 07:11:20.071193: Average global foreground Dice: [0.5901, 0.6632, 0.5669, 0.45, 0.3589, 0.8377, 0.4723, 0.6825, 0.5021, 0.451, 0.3359, 0.177, 0.2349, 0.5309, 0.4383]
2022-06-30 07:11:20.073723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:11:20.556230: Suus1 maybe_update_lr lr: 0.001682
2022-06-30 07:11:20.558598: This epoch took 301.983696 s

2022-06-30 07:11:20.560549: 
epoch:  431
2022-06-30 07:16:04.979286: train loss : 0.1286
2022-06-30 07:16:22.037541: validation loss: 0.0878
2022-06-30 07:16:22.041529: Average global foreground Dice: [0.7411, 0.5664, 0.6378, 0.3945, 0.3371, 0.8319, 0.4778, 0.6995, 0.5246, 0.4128, 0.2691, 0.1249, 0.239, 0.7098, 0.6591]
2022-06-30 07:16:22.043669: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:16:22.542689: Suus1 maybe_update_lr lr: 0.00166
2022-06-30 07:16:22.544907: This epoch took 301.982402 s

2022-06-30 07:16:22.546716: 
epoch:  432
2022-06-30 07:21:07.106830: train loss : 0.1196
2022-06-30 07:21:24.155138: validation loss: 0.0890
2022-06-30 07:21:24.159138: Average global foreground Dice: [0.6491, 0.6703, 0.662, 0.4277, 0.4713, 0.8386, 0.4528, 0.6771, 0.5223, 0.3892, 0.3369, 0.0829, 0.2374, 0.5513, 0.5856]
2022-06-30 07:21:24.161503: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:21:24.654404: Suus1 maybe_update_lr lr: 0.001638
2022-06-30 07:21:24.656806: This epoch took 302.108202 s

2022-06-30 07:21:24.658944: 
epoch:  433
2022-06-30 07:26:09.135927: train loss : 0.1339
2022-06-30 07:26:26.199547: validation loss: 0.1216
2022-06-30 07:26:26.204129: Average global foreground Dice: [0.6691, 0.6616, 0.5818, 0.2924, 0.3952, 0.829, 0.4266, 0.7009, 0.513, 0.4127, 0.3119, 0.1219, 0.2029, 0.7712, 0.511]
2022-06-30 07:26:26.206259: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:26:26.671782: Suus1 maybe_update_lr lr: 0.001616
2022-06-30 07:26:26.673924: This epoch took 302.013139 s

2022-06-30 07:26:26.675659: 
epoch:  434
2022-06-30 07:31:11.152159: train loss : 0.1286
2022-06-30 07:31:28.204476: validation loss: 0.0710
2022-06-30 07:31:28.232720: Average global foreground Dice: [0.752, 0.7301, 0.6872, 0.4281, 0.3877, 0.8412, 0.5015, 0.6893, 0.5601, 0.4324, 0.4195, 0.1662, 0.3243, 0.743, 0.6308]
2022-06-30 07:31:28.234896: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:31:28.708239: Suus1 maybe_update_lr lr: 0.001594
2022-06-30 07:31:28.710486: This epoch took 302.032879 s

2022-06-30 07:31:28.712634: 
epoch:  435
2022-06-30 07:36:13.188164: train loss : 0.1481
2022-06-30 07:36:30.242022: validation loss: 0.0654
2022-06-30 07:36:30.246968: Average global foreground Dice: [0.7622, 0.7784, 0.6216, 0.507, 0.4339, 0.8561, 0.4755, 0.6954, 0.5201, 0.4431, 0.3026, 0.1249, 0.2742, 0.605, 0.7071]
2022-06-30 07:36:30.249158: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:36:30.711633: Suus1 maybe_update_lr lr: 0.001572
2022-06-30 07:36:30.713872: This epoch took 301.999433 s

2022-06-30 07:36:30.715644: 
epoch:  436
2022-06-30 07:41:15.231466: train loss : 0.1294
2022-06-30 07:41:32.286365: validation loss: 0.0878
2022-06-30 07:41:32.291075: Average global foreground Dice: [0.7882, 0.6773, 0.6266, 0.4459, 0.4088, 0.8516, 0.5058, 0.7624, 0.5748, 0.5202, 0.3768, 0.1995, 0.2575, 0.2421, 0.6426]
2022-06-30 07:41:32.293111: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:41:32.758829: Suus1 maybe_update_lr lr: 0.00155
2022-06-30 07:41:32.760907: saving best epoch checkpoint...
2022-06-30 07:41:32.946394: saving checkpoint...
2022-06-30 07:41:35.785236: done, saving took 3.02 seconds
2022-06-30 07:41:35.799256: This epoch took 305.081530 s

2022-06-30 07:41:35.801167: 
epoch:  437
2022-06-30 07:46:20.078974: train loss : 0.1353
2022-06-30 07:46:37.133923: validation loss: 0.0448
2022-06-30 07:46:37.138929: Average global foreground Dice: [0.7373, 0.6498, 0.656, 0.4796, 0.3858, 0.8163, 0.4337, 0.7156, 0.5383, 0.4346, 0.2465, 0.1619, 0.2474, 0.7199, 0.5874]
2022-06-30 07:46:37.140984: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:46:37.608578: Suus1 maybe_update_lr lr: 0.001528
2022-06-30 07:46:37.610794: saving best epoch checkpoint...
2022-06-30 07:46:37.795967: saving checkpoint...
2022-06-30 07:46:40.685380: done, saving took 3.07 seconds
2022-06-30 07:46:40.699227: This epoch took 304.896092 s

2022-06-30 07:46:40.701504: 
epoch:  438
2022-06-30 07:51:24.876762: train loss : 0.1431
2022-06-30 07:51:41.947906: validation loss: 0.0741
2022-06-30 07:51:41.952346: Average global foreground Dice: [0.7446, 0.6835, 0.6958, 0.4394, 0.4057, 0.8654, 0.4687, 0.6699, 0.5317, 0.4894, 0.3919, 0.2451, 0.2384, 0.558, 0.6427]
2022-06-30 07:51:41.954646: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:51:42.422063: Suus1 maybe_update_lr lr: 0.001506
2022-06-30 07:51:42.424430: saving best epoch checkpoint...
2022-06-30 07:51:42.613166: saving checkpoint...
2022-06-30 07:51:45.608577: done, saving took 3.18 seconds
2022-06-30 07:51:45.623068: This epoch took 304.919560 s

2022-06-30 07:51:45.625296: 
epoch:  439
2022-06-30 07:56:29.591130: train loss : 0.1359
2022-06-30 07:56:46.662027: validation loss: 0.0755
2022-06-30 07:56:46.667161: Average global foreground Dice: [0.6724, 0.7029, 0.6514, 0.4372, 0.2991, 0.8639, 0.3336, 0.7046, 0.5487, 0.4059, 0.3721, 0.1557, 0.2639, 0.6915, 0.5744]
2022-06-30 07:56:46.669668: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 07:56:47.149433: Suus1 maybe_update_lr lr: 0.001483
2022-06-30 07:56:47.151610: This epoch took 301.523764 s

2022-06-30 07:56:47.153533: 
epoch:  440
2022-06-30 08:01:31.605572: train loss : 0.1512
2022-06-30 08:01:48.646098: validation loss: 0.0807
2022-06-30 08:01:48.650704: Average global foreground Dice: [0.6481, 0.7197, 0.6037, 0.4961, 0.3309, 0.8667, 0.4359, 0.73, 0.5374, 0.4261, 0.3089, 0.194, 0.2882, 0.6235, 0.4487]
2022-06-30 08:01:48.652973: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:01:49.129481: Suus1 maybe_update_lr lr: 0.001461
2022-06-30 08:01:49.131583: This epoch took 301.976727 s

2022-06-30 08:01:49.133548: 
epoch:  441
2022-06-30 08:06:33.602939: train loss : 0.1336
2022-06-30 08:06:50.650163: validation loss: 0.0566
2022-06-30 08:06:50.653828: Average global foreground Dice: [0.7209, 0.6353, 0.6323, 0.429, 0.4223, 0.8665, 0.4854, 0.7352, 0.6054, 0.4605, 0.3833, 0.1247, 0.27, 0.7135, 0.4988]
2022-06-30 08:06:50.655944: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:06:51.187364: Suus1 maybe_update_lr lr: 0.001439
2022-06-30 08:06:51.189851: saving best epoch checkpoint...
2022-06-30 08:06:51.402031: saving checkpoint...
2022-06-30 08:06:54.398628: done, saving took 3.21 seconds
2022-06-30 08:06:54.412734: This epoch took 305.277484 s

2022-06-30 08:06:54.414911: 
epoch:  442
2022-06-30 08:11:38.260968: train loss : 0.1324
2022-06-30 08:11:55.297946: validation loss: 0.0769
2022-06-30 08:11:55.302223: Average global foreground Dice: [0.6504, 0.7696, 0.6219, 0.4166, 0.4135, 0.8806, 0.5526, 0.6997, 0.5449, 0.4552, 0.3772, 0.2154, 0.2889, 0.6014, 0.4789]
2022-06-30 08:11:55.304827: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:11:55.829201: Suus1 maybe_update_lr lr: 0.001416
2022-06-30 08:11:55.831265: saving best epoch checkpoint...
2022-06-30 08:11:56.017988: saving checkpoint...
2022-06-30 08:11:58.969190: done, saving took 3.14 seconds
2022-06-30 08:11:58.982668: This epoch took 304.565669 s

2022-06-30 08:11:58.984631: 
epoch:  443
2022-06-30 08:16:42.594920: train loss : 0.1189
2022-06-30 08:16:59.616958: validation loss: 0.0915
2022-06-30 08:16:59.621688: Average global foreground Dice: [0.7228, 0.7574, 0.6544, 0.4819, 0.3279, 0.853, 0.5057, 0.758, 0.5031, 0.3839, 0.381, 0.2345, 0.2378, 0.7329, 0.5449]
2022-06-30 08:16:59.623903: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:17:00.099352: Suus1 maybe_update_lr lr: 0.001394
2022-06-30 08:17:00.101699: saving best epoch checkpoint...
2022-06-30 08:17:00.285997: saving checkpoint...
2022-06-30 08:17:03.182607: done, saving took 3.08 seconds
2022-06-30 08:17:03.196702: This epoch took 304.209779 s

2022-06-30 08:17:03.198838: 
epoch:  444
2022-06-30 08:21:47.149162: train loss : 0.1137
2022-06-30 08:22:04.203482: validation loss: 0.0397
2022-06-30 08:22:04.207984: Average global foreground Dice: [0.7809, 0.6857, 0.6262, 0.5248, 0.4447, 0.8616, 0.5894, 0.7745, 0.5829, 0.5057, 0.3834, 0.1862, 0.2576, 0.6172, 0.5805]
2022-06-30 08:22:04.210207: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:22:04.708274: Suus1 maybe_update_lr lr: 0.001372
2022-06-30 08:22:04.710718: saving best epoch checkpoint...
2022-06-30 08:22:04.914582: saving checkpoint...
2022-06-30 08:22:07.805349: done, saving took 3.09 seconds
2022-06-30 08:22:07.819232: This epoch took 304.618278 s

2022-06-30 08:22:07.821265: 
epoch:  445
2022-06-30 08:26:51.846832: train loss : 0.1400
2022-06-30 08:27:08.909222: validation loss: 0.0989
2022-06-30 08:27:08.914062: Average global foreground Dice: [0.7109, 0.6929, 0.5945, 0.4442, 0.3884, 0.8239, 0.5141, 0.7065, 0.5331, 0.4279, 0.2976, 0.1584, 0.2319, 0.5543, 0.3569]
2022-06-30 08:27:08.916364: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:27:09.384858: Suus1 maybe_update_lr lr: 0.001349
2022-06-30 08:27:09.387086: This epoch took 301.563760 s

2022-06-30 08:27:09.388986: 
epoch:  446
2022-06-30 08:31:53.908897: train loss : 0.1409
2022-06-30 08:32:10.963473: validation loss: 0.0817
2022-06-30 08:32:10.968548: Average global foreground Dice: [0.6241, 0.6092, 0.5367, 0.3298, 0.3521, 0.8438, 0.411, 0.6934, 0.556, 0.4533, 0.2784, 0.1522, 0.3133, 0.7359, 0.6211]
2022-06-30 08:32:10.970712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:32:11.446106: Suus1 maybe_update_lr lr: 0.001327
2022-06-30 08:32:11.448467: This epoch took 302.057564 s

2022-06-30 08:32:11.450434: 
epoch:  447
2022-06-30 08:36:55.935395: train loss : 0.1260
2022-06-30 08:37:12.991144: validation loss: 0.0459
2022-06-30 08:37:12.994976: Average global foreground Dice: [0.8403, 0.6491, 0.6908, 0.4952, 0.3356, 0.8812, 0.4554, 0.7184, 0.5714, 0.4559, 0.2704, 0.1773, 0.2819, 0.5963, 0.6156]
2022-06-30 08:37:12.997221: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:37:13.468738: Suus1 maybe_update_lr lr: 0.001304
2022-06-30 08:37:13.471084: This epoch took 302.018431 s

2022-06-30 08:37:13.473253: 
epoch:  448
2022-06-30 08:41:58.027780: train loss : 0.1023
2022-06-30 08:42:15.075640: validation loss: 0.0572
2022-06-30 08:42:15.080778: Average global foreground Dice: [0.7803, 0.7305, 0.7019, 0.4772, 0.3592, 0.8575, 0.4438, 0.6435, 0.5496, 0.4765, 0.3109, 0.2028, 0.276, 0.7319, 0.565]
2022-06-30 08:42:15.083030: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:42:15.548368: Suus1 maybe_update_lr lr: 0.001282
2022-06-30 08:42:15.550782: This epoch took 302.075521 s

2022-06-30 08:42:15.552577: 
epoch:  449
2022-06-30 08:47:00.045703: train loss : 0.1352
2022-06-30 08:47:17.095055: validation loss: 0.0387
2022-06-30 08:47:17.099411: Average global foreground Dice: [0.7865, 0.7349, 0.6778, 0.5134, 0.4056, 0.8652, 0.4864, 0.7246, 0.6142, 0.5117, 0.3567, 0.1942, 0.2897, 0.7412, 0.5324]
2022-06-30 08:47:17.101506: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:47:17.568517: Suus1 maybe_update_lr lr: 0.001259
2022-06-30 08:47:17.570816: saving scheduled checkpoint file...
2022-06-30 08:47:17.756075: saving checkpoint...
2022-06-30 08:47:20.595093: done, saving took 3.02 seconds
2022-06-30 08:47:20.611454: done
2022-06-30 08:47:20.613420: saving best epoch checkpoint...
2022-06-30 08:47:20.709113: saving checkpoint...
2022-06-30 08:47:23.497856: done, saving took 2.88 seconds
2022-06-30 08:47:23.511889: This epoch took 307.957427 s

2022-06-30 08:47:23.513903: 
epoch:  450
2022-06-30 08:52:07.523342: train loss : 0.1314
2022-06-30 08:52:24.576609: validation loss: 0.0407
2022-06-30 08:52:24.581412: Average global foreground Dice: [0.7215, 0.6589, 0.6368, 0.5353, 0.4876, 0.8624, 0.5782, 0.7575, 0.5769, 0.4751, 0.3986, 0.2286, 0.3351, 0.7246, 0.5198]
2022-06-30 08:52:24.583717: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:52:25.131761: Suus1 maybe_update_lr lr: 0.001236
2022-06-30 08:52:25.134091: saving best epoch checkpoint...
2022-06-30 08:52:25.342402: saving checkpoint...
2022-06-30 08:52:28.216437: done, saving took 3.08 seconds
2022-06-30 08:52:28.230506: This epoch took 304.714625 s

2022-06-30 08:52:28.232514: 
epoch:  451
2022-06-30 08:57:12.077242: train loss : 0.1445
2022-06-30 08:57:29.088395: validation loss: 0.1039
2022-06-30 08:57:29.092362: Average global foreground Dice: [0.6484, 0.6268, 0.6861, 0.4449, 0.39, 0.7558, 0.4952, 0.7262, 0.5492, 0.4392, 0.2657, 0.1684, 0.257, 0.3319, 0.2826]
2022-06-30 08:57:29.094516: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 08:57:29.778802: Suus1 maybe_update_lr lr: 0.001214
2022-06-30 08:57:29.780853: This epoch took 301.546323 s

2022-06-30 08:57:29.782971: 
epoch:  452
2022-06-30 09:02:13.495975: train loss : 0.1088
2022-06-30 09:02:30.503884: validation loss: 0.0908
2022-06-30 09:02:30.508133: Average global foreground Dice: [0.6649, 0.6991, 0.6235, 0.4391, 0.4105, 0.8277, 0.4599, 0.7447, 0.5911, 0.4684, 0.3291, 0.1876, 0.3099, 0.5972, 0.3365]
2022-06-30 09:02:30.510249: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:02:30.979080: Suus1 maybe_update_lr lr: 0.001191
2022-06-30 09:02:30.981257: This epoch took 301.196273 s

2022-06-30 09:02:30.983188: 
epoch:  453
2022-06-30 09:07:14.637552: train loss : 0.1253
2022-06-30 09:07:31.644408: validation loss: 0.0682
2022-06-30 09:07:31.648778: Average global foreground Dice: [0.6495, 0.7152, 0.6122, 0.449, 0.3737, 0.8684, 0.548, 0.6667, 0.5457, 0.4731, 0.3364, 0.1581, 0.3151, 0.5253, 0.5327]
2022-06-30 09:07:31.651359: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:07:32.120334: Suus1 maybe_update_lr lr: 0.001168
2022-06-30 09:07:32.122546: This epoch took 301.137562 s

2022-06-30 09:07:32.124471: 
epoch:  454
2022-06-30 09:12:15.748727: train loss : 0.1125
2022-06-30 09:12:32.728808: validation loss: 0.0378
2022-06-30 09:12:32.732781: Average global foreground Dice: [0.6864, 0.7071, 0.6526, 0.4203, 0.4763, 0.8618, 0.5938, 0.7525, 0.5969, 0.4542, 0.4305, 0.202, 0.2673, 0.7281, 0.5595]
2022-06-30 09:12:32.734759: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:12:33.203300: Suus1 maybe_update_lr lr: 0.001145
2022-06-30 09:12:33.205609: This epoch took 301.078840 s

2022-06-30 09:12:33.207707: 
epoch:  455
2022-06-30 09:17:16.869985: train loss : 0.0864
2022-06-30 09:17:33.863599: validation loss: 0.0806
2022-06-30 09:17:33.867747: Average global foreground Dice: [0.7721, 0.6453, 0.7078, 0.4101, 0.349, 0.8629, 0.4994, 0.6904, 0.5161, 0.4164, 0.3654, 0.1694, 0.289, 0.623, 0.6322]
2022-06-30 09:17:33.869989: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:17:34.338279: Suus1 maybe_update_lr lr: 0.001122
2022-06-30 09:17:34.340461: This epoch took 301.130836 s

2022-06-30 09:17:34.342723: 
epoch:  456
2022-06-30 09:22:18.023918: train loss : 0.1572
2022-06-30 09:22:35.028085: validation loss: 0.0643
2022-06-30 09:22:35.032442: Average global foreground Dice: [0.7189, 0.6321, 0.6145, 0.4316, 0.4005, 0.8276, 0.4316, 0.7359, 0.619, 0.4435, 0.2848, 0.1827, 0.3241, 0.4291, 0.4552]
2022-06-30 09:22:35.034784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:22:35.500819: Suus1 maybe_update_lr lr: 0.001099
2022-06-30 09:22:35.503254: This epoch took 301.158424 s

2022-06-30 09:22:35.505477: 
epoch:  457
2022-06-30 09:27:19.237033: train loss : 0.1164
2022-06-30 09:27:36.241435: validation loss: 0.0949
2022-06-30 09:27:36.246604: Average global foreground Dice: [0.5528, 0.6922, 0.5742, 0.466, 0.3583, 0.8588, 0.4104, 0.7229, 0.5088, 0.4478, 0.3533, 0.223, 0.2405, 0.6756, 0.5393]
2022-06-30 09:27:36.248902: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:27:36.726297: Suus1 maybe_update_lr lr: 0.001076
2022-06-30 09:27:36.728426: This epoch took 301.220719 s

2022-06-30 09:27:36.730406: 
epoch:  458
2022-06-30 09:32:20.348646: train loss : 0.1217
2022-06-30 09:32:37.341599: validation loss: 0.0365
2022-06-30 09:32:37.346563: Average global foreground Dice: [0.7884, 0.631, 0.6367, 0.3994, 0.4362, 0.8732, 0.5634, 0.6878, 0.5765, 0.4288, 0.3409, 0.21, 0.2634, 0.6803, 0.6806]
2022-06-30 09:32:37.349231: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:32:37.821640: Suus1 maybe_update_lr lr: 0.001053
2022-06-30 09:32:37.823692: This epoch took 301.091244 s

2022-06-30 09:32:37.825528: 
epoch:  459
2022-06-30 09:37:21.561853: train loss : 0.1133
2022-06-30 09:37:38.580713: validation loss: 0.0455
2022-06-30 09:37:38.585109: Average global foreground Dice: [0.6053, 0.769, 0.5999, 0.5475, 0.3968, 0.859, 0.4458, 0.7323, 0.6052, 0.4587, 0.3906, 0.2129, 0.2806, 0.7824, 0.706]
2022-06-30 09:37:38.587258: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:37:39.056518: Suus1 maybe_update_lr lr: 0.00103
2022-06-30 09:37:39.058698: This epoch took 301.231848 s

2022-06-30 09:37:39.060915: 
epoch:  460
2022-06-30 09:42:22.976656: train loss : 0.1045
2022-06-30 09:42:40.031239: validation loss: 0.0693
2022-06-30 09:42:40.035431: Average global foreground Dice: [0.7972, 0.5909, 0.7202, 0.2801, 0.3981, 0.8572, 0.5692, 0.7427, 0.5551, 0.382, 0.3376, 0.1827, 0.2318, 0.6573, 0.5604]
2022-06-30 09:42:40.037576: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:42:40.511342: Suus1 maybe_update_lr lr: 0.001007
2022-06-30 09:42:40.513526: This epoch took 301.450609 s

2022-06-30 09:42:40.515717: 
epoch:  461
2022-06-30 09:47:24.966296: train loss : 0.1022
2022-06-30 09:47:42.010796: validation loss: 0.0576
2022-06-30 09:47:42.015029: Average global foreground Dice: [0.7226, 0.6469, 0.7005, 0.5451, 0.4108, 0.8556, 0.5189, 0.7332, 0.5567, 0.4616, 0.2815, 0.2222, 0.1848, 0.4372, 0.3257]
2022-06-30 09:47:42.017324: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:47:42.515285: Suus1 maybe_update_lr lr: 0.000983
2022-06-30 09:47:42.517286: This epoch took 301.999601 s

2022-06-30 09:47:42.519097: 
epoch:  462
2022-06-30 09:52:27.009248: train loss : 0.0997
2022-06-30 09:52:44.076072: validation loss: 0.0623
2022-06-30 09:52:44.081147: Average global foreground Dice: [0.6725, 0.7835, 0.6773, 0.5575, 0.3857, 0.869, 0.5165, 0.7236, 0.5594, 0.444, 0.3536, 0.1842, 0.3428, 0.8515, 0.6406]
2022-06-30 09:52:44.083445: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:52:44.553753: Suus1 maybe_update_lr lr: 0.00096
2022-06-30 09:52:44.555996: This epoch took 302.035055 s

2022-06-30 09:52:44.557894: 
epoch:  463
2022-06-30 09:57:29.057048: train loss : 0.1042
2022-06-30 09:57:46.101132: validation loss: 0.0306
2022-06-30 09:57:46.105971: Average global foreground Dice: [0.7756, 0.7454, 0.7069, 0.444, 0.4478, 0.8717, 0.5709, 0.7745, 0.5892, 0.5343, 0.3838, 0.272, 0.2614, 0.5168, 0.5801]
2022-06-30 09:57:46.108239: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 09:57:46.667055: Suus1 maybe_update_lr lr: 0.000937
2022-06-30 09:57:46.669436: saving best epoch checkpoint...
2022-06-30 09:57:46.857533: saving checkpoint...
2022-06-30 09:57:49.740103: done, saving took 3.07 seconds
2022-06-30 09:57:49.754437: This epoch took 305.194789 s

2022-06-30 09:57:49.756452: 
epoch:  464
2022-06-30 10:02:33.866308: train loss : 0.1084
2022-06-30 10:02:50.912990: validation loss: 0.0620
2022-06-30 10:02:50.917005: Average global foreground Dice: [0.744, 0.7595, 0.6524, 0.4737, 0.3959, 0.8609, 0.3934, 0.7461, 0.6093, 0.4955, 0.4145, 0.193, 0.3463, 0.5764, 0.379]
2022-06-30 10:02:50.919270: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:02:51.395483: Suus1 maybe_update_lr lr: 0.000913
2022-06-30 10:02:51.398378: saving best epoch checkpoint...
2022-06-30 10:02:51.583886: saving checkpoint...
2022-06-30 10:02:54.537349: done, saving took 3.14 seconds
2022-06-30 10:02:54.552051: This epoch took 304.793622 s

2022-06-30 10:02:54.554240: 
epoch:  465
2022-06-30 10:07:38.580513: train loss : 0.0798
2022-06-30 10:07:55.664152: validation loss: 0.0987
2022-06-30 10:07:55.668749: Average global foreground Dice: [0.5971, 0.6567, 0.6355, 0.4824, 0.3984, 0.8629, 0.5374, 0.763, 0.5435, 0.47, 0.348, 0.2181, 0.2894, 0.6846, 0.5884]
2022-06-30 10:07:55.670987: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:07:56.137436: Suus1 maybe_update_lr lr: 0.00089
2022-06-30 10:07:56.139835: saving best epoch checkpoint...
2022-06-30 10:07:56.327972: saving checkpoint...
2022-06-30 10:07:59.154333: done, saving took 3.01 seconds
2022-06-30 10:07:59.169795: This epoch took 304.613573 s

2022-06-30 10:07:59.172089: 
epoch:  466
2022-06-30 10:12:43.561541: train loss : 0.1037
2022-06-30 10:13:00.634327: validation loss: 0.0270
2022-06-30 10:13:00.638415: Average global foreground Dice: [0.7458, 0.6686, 0.7002, 0.5242, 0.4472, 0.8664, 0.576, 0.7633, 0.5889, 0.4692, 0.4106, 0.2517, 0.307, 0.6312, 0.4953]
2022-06-30 10:13:00.640659: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:13:01.111330: Suus1 maybe_update_lr lr: 0.000866
2022-06-30 10:13:01.114081: saving best epoch checkpoint...
2022-06-30 10:13:01.300956: saving checkpoint...
2022-06-30 10:13:04.232858: done, saving took 3.12 seconds
2022-06-30 10:13:04.247885: This epoch took 305.073642 s

2022-06-30 10:13:04.250069: 
epoch:  467
2022-06-30 10:17:48.626765: train loss : 0.1110
2022-06-30 10:18:05.689982: validation loss: 0.0344
2022-06-30 10:18:05.694754: Average global foreground Dice: [0.7737, 0.7213, 0.6759, 0.5076, 0.3721, 0.8493, 0.4886, 0.7329, 0.6266, 0.5252, 0.3583, 0.2847, 0.3017, 0.5879, 0.6027]
2022-06-30 10:18:05.697068: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:18:06.174876: Suus1 maybe_update_lr lr: 0.000842
2022-06-30 10:18:06.177346: saving best epoch checkpoint...
2022-06-30 10:18:06.362846: saving checkpoint...
2022-06-30 10:18:09.272187: done, saving took 3.09 seconds
2022-06-30 10:18:09.286227: This epoch took 305.034218 s

2022-06-30 10:18:09.288181: 
epoch:  468
2022-06-30 10:22:53.075505: train loss : 0.1044
2022-06-30 10:23:10.078511: validation loss: 0.0527
2022-06-30 10:23:10.083193: Average global foreground Dice: [0.7234, 0.7492, 0.6747, 0.5281, 0.3817, 0.8714, 0.5334, 0.7221, 0.5573, 0.4845, 0.3948, 0.2116, 0.301, 0.7044, 0.5166]
2022-06-30 10:23:10.085408: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:23:10.555610: Suus1 maybe_update_lr lr: 0.000819
2022-06-30 10:23:10.557711: saving best epoch checkpoint...
2022-06-30 10:23:10.743504: saving checkpoint...
2022-06-30 10:23:13.627872: done, saving took 3.07 seconds
2022-06-30 10:23:13.642304: This epoch took 304.352017 s

2022-06-30 10:23:13.644601: 
epoch:  469
2022-06-30 10:27:57.276064: train loss : 0.0931
2022-06-30 10:28:14.272115: validation loss: 0.0392
2022-06-30 10:28:14.276496: Average global foreground Dice: [0.7649, 0.7476, 0.7156, 0.4531, 0.4053, 0.8647, 0.5324, 0.7084, 0.5396, 0.442, 0.3199, 0.259, 0.3031, 0.67, 0.6459]
2022-06-30 10:28:14.278659: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:28:14.747649: Suus1 maybe_update_lr lr: 0.000795
2022-06-30 10:28:14.749641: saving best epoch checkpoint...
2022-06-30 10:28:14.933226: saving checkpoint...
2022-06-30 10:28:17.857124: done, saving took 3.11 seconds
2022-06-30 10:28:17.880061: This epoch took 304.233412 s

2022-06-30 10:28:17.882275: 
epoch:  470
2022-06-30 10:33:01.485973: train loss : 0.1141
2022-06-30 10:33:18.487793: validation loss: 0.0384
2022-06-30 10:33:18.492141: Average global foreground Dice: [0.7685, 0.6749, 0.7206, 0.4217, 0.4468, 0.8606, 0.6322, 0.7696, 0.583, 0.5275, 0.3289, 0.2449, 0.269, 0.6117, 0.5435]
2022-06-30 10:33:18.494549: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:33:18.959059: Suus1 maybe_update_lr lr: 0.000771
2022-06-30 10:33:18.961536: saving best epoch checkpoint...
2022-06-30 10:33:19.144688: saving checkpoint...
2022-06-30 10:33:22.050131: done, saving took 3.09 seconds
2022-06-30 10:33:22.064153: This epoch took 304.179726 s

2022-06-30 10:33:22.066237: 
epoch:  471
2022-06-30 10:38:05.640671: train loss : 0.1111
2022-06-30 10:38:22.637343: validation loss: 0.0975
2022-06-30 10:38:22.641764: Average global foreground Dice: [0.7345, 0.5774, 0.6062, 0.3515, 0.3908, 0.8248, 0.4667, 0.7606, 0.5603, 0.4853, 0.3766, 0.2315, 0.248, 0.5813, 0.4824]
2022-06-30 10:38:22.643935: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:38:23.267405: Suus1 maybe_update_lr lr: 0.000747
2022-06-30 10:38:23.269792: This epoch took 301.201578 s

2022-06-30 10:38:23.271711: 
epoch:  472
2022-06-30 10:43:06.860519: train loss : 0.1086
2022-06-30 10:43:23.842740: validation loss: 0.0381
2022-06-30 10:43:23.847672: Average global foreground Dice: [0.7702, 0.7277, 0.6866, 0.5163, 0.4571, 0.8611, 0.4882, 0.7308, 0.5709, 0.477, 0.361, 0.2412, 0.2628, 0.6764, 0.5825]
2022-06-30 10:43:23.849869: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:43:24.356745: Suus1 maybe_update_lr lr: 0.000723
2022-06-30 10:43:24.359053: This epoch took 301.085347 s

2022-06-30 10:43:24.361124: 
epoch:  473
2022-06-30 10:48:07.799684: train loss : 0.1083
2022-06-30 10:48:24.774217: validation loss: 0.0393
2022-06-30 10:48:24.778726: Average global foreground Dice: [0.6591, 0.6911, 0.6905, 0.4858, 0.4467, 0.8651, 0.5382, 0.7407, 0.5544, 0.5018, 0.3572, 0.276, 0.3582, 0.7132, 0.6542]
2022-06-30 10:48:24.780715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:48:25.255631: Suus1 maybe_update_lr lr: 0.000699
2022-06-30 10:48:25.257874: saving best epoch checkpoint...
2022-06-30 10:48:25.439659: saving checkpoint...
2022-06-30 10:48:28.199556: done, saving took 2.94 seconds
2022-06-30 10:48:28.213789: This epoch took 303.850573 s

2022-06-30 10:48:28.215777: 
epoch:  474
2022-06-30 10:53:11.652203: train loss : 0.0840
2022-06-30 10:53:28.652356: validation loss: 0.0379
2022-06-30 10:53:28.657343: Average global foreground Dice: [0.7775, 0.691, 0.6878, 0.4215, 0.4259, 0.8606, 0.5402, 0.752, 0.5977, 0.513, 0.3449, 0.2091, 0.2928, 0.7199, 0.5889]
2022-06-30 10:53:28.659752: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:53:29.130579: Suus1 maybe_update_lr lr: 0.000675
2022-06-30 10:53:29.132784: saving best epoch checkpoint...
2022-06-30 10:53:29.317578: saving checkpoint...
2022-06-30 10:53:32.186305: done, saving took 3.05 seconds
2022-06-30 10:53:32.200722: This epoch took 303.982980 s

2022-06-30 10:53:32.202897: 
epoch:  475
2022-06-30 10:58:15.723366: train loss : 0.1138
2022-06-30 10:58:32.718144: validation loss: 0.0144
2022-06-30 10:58:32.723517: Average global foreground Dice: [0.8094, 0.7777, 0.7422, 0.5447, 0.5336, 0.8907, 0.5978, 0.7217, 0.5762, 0.4349, 0.3278, 0.254, 0.2714, 0.8276, 0.7293]
2022-06-30 10:58:32.725813: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 10:58:33.196587: Suus1 maybe_update_lr lr: 0.00065
2022-06-30 10:58:33.199021: saving best epoch checkpoint...
2022-06-30 10:58:33.384872: saving checkpoint...
2022-06-30 10:58:36.240192: done, saving took 3.04 seconds
2022-06-30 10:58:36.254852: This epoch took 304.049879 s

2022-06-30 10:58:36.257030: 
epoch:  476
2022-06-30 11:03:19.507491: train loss : 0.0821
2022-06-30 11:03:36.493599: validation loss: 0.0473
2022-06-30 11:03:36.498429: Average global foreground Dice: [0.7747, 0.7081, 0.7014, 0.4961, 0.4225, 0.8518, 0.57, 0.7602, 0.5614, 0.516, 0.3176, 0.2306, 0.2509, 0.6261, 0.4899]
2022-06-30 11:03:36.500801: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:03:36.970155: Suus1 maybe_update_lr lr: 0.000626
2022-06-30 11:03:36.972430: This epoch took 300.713112 s

2022-06-30 11:03:36.974553: 
epoch:  477
2022-06-30 11:08:20.100876: train loss : 0.1009
2022-06-30 11:08:37.057020: validation loss: 0.0975
2022-06-30 11:08:37.061997: Average global foreground Dice: [0.5612, 0.6804, 0.462, 0.4812, 0.415, 0.8351, 0.452, 0.7175, 0.5489, 0.4535, 0.2922, 0.1685, 0.313, 0.7279, 0.6192]
2022-06-30 11:08:37.064356: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:08:37.606000: Suus1 maybe_update_lr lr: 0.000601
2022-06-30 11:08:37.608673: This epoch took 300.632101 s

2022-06-30 11:08:37.611528: 
epoch:  478
2022-06-30 11:13:20.261817: train loss : 0.0969
2022-06-30 11:13:38.830508: validation loss: 0.0327
2022-06-30 11:13:38.835075: Average global foreground Dice: [0.7771, 0.6222, 0.7513, 0.3989, 0.4795, 0.8704, 0.544, 0.7212, 0.6022, 0.4834, 0.4161, 0.2278, 0.2814, 0.6425, 0.5995]
2022-06-30 11:13:38.837428: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:13:39.315323: Suus1 maybe_update_lr lr: 0.000577
2022-06-30 11:13:39.317783: This epoch took 301.703504 s

2022-06-30 11:13:39.320035: 
epoch:  479
2022-06-30 11:18:21.449067: train loss : 0.0945
2022-06-30 11:18:39.319803: validation loss: 0.0452
2022-06-30 11:18:39.325359: Average global foreground Dice: [0.7343, 0.7496, 0.724, 0.4852, 0.3666, 0.8675, 0.4764, 0.7591, 0.6102, 0.5053, 0.4339, 0.1696, 0.31, 0.8022, 0.6626]
2022-06-30 11:18:39.327790: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:18:39.799355: Suus1 maybe_update_lr lr: 0.000552
2022-06-30 11:18:39.801901: This epoch took 300.478740 s

2022-06-30 11:18:39.804206: 
epoch:  480
2022-06-30 11:23:22.026823: train loss : 0.0947
2022-06-30 11:23:43.866935: validation loss: 0.0234
2022-06-30 11:23:43.872136: Average global foreground Dice: [0.7351, 0.692, 0.6498, 0.4125, 0.4839, 0.8861, 0.553, 0.7802, 0.5867, 0.4341, 0.3536, 0.231, 0.2769, 0.6821, 0.5619]
2022-06-30 11:23:43.874698: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:23:44.343895: Suus1 maybe_update_lr lr: 0.000527
2022-06-30 11:23:44.346431: saving best epoch checkpoint...
2022-06-30 11:23:44.551949: saving checkpoint...
2022-06-30 11:23:47.823527: done, saving took 3.47 seconds
2022-06-30 11:23:47.839991: This epoch took 308.033722 s

2022-06-30 11:23:47.842440: 
epoch:  481
2022-06-30 11:28:30.039797: train loss : 0.0976
2022-06-30 11:28:50.497122: validation loss: 0.0815
2022-06-30 11:28:50.502001: Average global foreground Dice: [0.6841, 0.6587, 0.6267, 0.4645, 0.3634, 0.8537, 0.4711, 0.6269, 0.5218, 0.4596, 0.3249, 0.2056, 0.2189, 0.6994, 0.6641]
2022-06-30 11:28:50.504688: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:28:51.133445: Suus1 maybe_update_lr lr: 0.000502
2022-06-30 11:28:51.135937: This epoch took 303.291170 s

2022-06-30 11:28:51.138319: 
epoch:  482
2022-06-30 11:33:33.765334: train loss : 0.0990
2022-06-30 11:33:52.548895: validation loss: 0.0224
2022-06-30 11:33:52.553698: Average global foreground Dice: [0.7594, 0.811, 0.7064, 0.4368, 0.3801, 0.8806, 0.5661, 0.6809, 0.6656, 0.4923, 0.4012, 0.1987, 0.3424, 0.7238, 0.7031]
2022-06-30 11:33:52.556470: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:33:53.031316: Suus1 maybe_update_lr lr: 0.000477
2022-06-30 11:33:53.033529: saving best epoch checkpoint...
2022-06-30 11:33:53.223053: saving checkpoint...
2022-06-30 11:33:56.492879: done, saving took 3.46 seconds
2022-06-30 11:33:56.508783: This epoch took 305.368157 s

2022-06-30 11:33:56.511424: 
epoch:  483
2022-06-30 11:38:39.215649: train loss : 0.0745
2022-06-30 11:38:59.924131: validation loss: 0.0575
2022-06-30 11:38:59.929012: Average global foreground Dice: [0.7209, 0.6818, 0.6441, 0.4368, 0.3791, 0.8583, 0.5365, 0.755, 0.5465, 0.4451, 0.3636, 0.2159, 0.3152, 0.7566, 0.5656]
2022-06-30 11:38:59.931275: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:39:00.400017: Suus1 maybe_update_lr lr: 0.000451
2022-06-30 11:39:00.402292: This epoch took 303.888302 s

2022-06-30 11:39:00.404302: 
epoch:  484
2022-06-30 11:43:43.109392: train loss : 0.0703
2022-06-30 11:44:03.624260: validation loss: 0.0263
2022-06-30 11:44:03.629370: Average global foreground Dice: [0.6571, 0.6996, 0.6854, 0.466, 0.4194, 0.8793, 0.5251, 0.7618, 0.622, 0.5035, 0.3546, 0.2121, 0.2767, 0.7084, 0.6161]
2022-06-30 11:44:03.631623: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:44:04.105397: Suus1 maybe_update_lr lr: 0.000426
2022-06-30 11:44:04.107761: saving best epoch checkpoint...
2022-06-30 11:44:04.291010: saving checkpoint...
2022-06-30 11:44:07.922965: done, saving took 3.81 seconds
2022-06-30 11:44:07.939303: This epoch took 307.532939 s

2022-06-30 11:44:07.942098: 
epoch:  485
2022-06-30 11:48:50.475377: train loss : 0.1000
2022-06-30 11:49:08.406737: validation loss: 0.0575
2022-06-30 11:49:08.411459: Average global foreground Dice: [0.7897, 0.7281, 0.6741, 0.4808, 0.408, 0.8219, 0.4577, 0.7289, 0.5946, 0.4845, 0.4215, 0.2568, 0.2955, 0.8213, 0.6517]
2022-06-30 11:49:08.413871: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:49:08.884443: Suus1 maybe_update_lr lr: 0.0004
2022-06-30 11:49:08.887031: saving best epoch checkpoint...
2022-06-30 11:49:09.084522: saving checkpoint...
2022-06-30 11:49:12.605295: done, saving took 3.72 seconds
2022-06-30 11:49:12.620720: This epoch took 304.675793 s

2022-06-30 11:49:12.622981: 
epoch:  486
2022-06-30 11:53:55.332748: train loss : 0.1104
2022-06-30 11:54:15.623878: validation loss: 0.0519
2022-06-30 11:54:15.628459: Average global foreground Dice: [0.6556, 0.6809, 0.7036, 0.4577, 0.3683, 0.8298, 0.5375, 0.7386, 0.5675, 0.4977, 0.3772, 0.2688, 0.2837, 0.4393, 0.4393]
2022-06-30 11:54:15.631019: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:54:16.105006: Suus1 maybe_update_lr lr: 0.000375
2022-06-30 11:54:16.107662: This epoch took 303.482510 s

2022-06-30 11:54:16.110159: 
epoch:  487
2022-06-30 11:58:58.977382: train loss : 0.0893
2022-06-30 11:59:16.861100: validation loss: 0.0276
2022-06-30 11:59:16.865811: Average global foreground Dice: [0.7083, 0.6853, 0.6141, 0.5123, 0.504, 0.8694, 0.5521, 0.7206, 0.585, 0.4627, 0.3772, 0.2573, 0.2395, 0.6191, 0.7154]
2022-06-30 11:59:16.868529: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 11:59:17.344820: Suus1 maybe_update_lr lr: 0.000348
2022-06-30 11:59:17.347180: This epoch took 301.234668 s

2022-06-30 11:59:17.349271: 
epoch:  488
2022-06-30 12:04:00.257231: train loss : 0.0751
2022-06-30 12:04:17.756423: validation loss: 0.0132
2022-06-30 12:04:17.761249: Average global foreground Dice: [0.7141, 0.6783, 0.6795, 0.4902, 0.4223, 0.891, 0.5675, 0.765, 0.5906, 0.5298, 0.3532, 0.2696, 0.2712, 0.6987, 0.6669]
2022-06-30 12:04:17.763957: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:04:18.237172: Suus1 maybe_update_lr lr: 0.000322
2022-06-30 12:04:18.239529: This epoch took 300.888116 s

2022-06-30 12:04:18.241858: 
epoch:  489
2022-06-30 12:09:01.221740: train loss : 0.0766
2022-06-30 12:09:18.186813: validation loss: 0.0283
2022-06-30 12:09:18.191397: Average global foreground Dice: [0.7518, 0.698, 0.6736, 0.5594, 0.3948, 0.8771, 0.5396, 0.7492, 0.5696, 0.5101, 0.3945, 0.272, 0.3129, 0.4388, 0.4254]
2022-06-30 12:09:18.193677: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:09:18.739628: Suus1 maybe_update_lr lr: 0.000296
2022-06-30 12:09:18.741956: This epoch took 300.497914 s

2022-06-30 12:09:18.744274: 
epoch:  490
2022-06-30 12:14:01.744426: train loss : 0.0955
2022-06-30 12:14:18.693945: validation loss: 0.0327
2022-06-30 12:14:18.698941: Average global foreground Dice: [0.6898, 0.7023, 0.6608, 0.5056, 0.4473, 0.8618, 0.4707, 0.7604, 0.5776, 0.5176, 0.3631, 0.277, 0.3058, 0.6151, 0.6024]
2022-06-30 12:14:18.701405: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:14:19.176669: Suus1 maybe_update_lr lr: 0.000269
2022-06-30 12:14:19.179143: This epoch took 300.432547 s

2022-06-30 12:14:19.181350: 
epoch:  491
2022-06-30 12:19:02.375711: train loss : 0.0769
2022-06-30 12:19:21.457074: validation loss: 0.0475
2022-06-30 12:19:21.461151: Average global foreground Dice: [0.7997, 0.7421, 0.7237, 0.4772, 0.3676, 0.8732, 0.5544, 0.6881, 0.58, 0.464, 0.3745, 0.1905, 0.2632, 0.6781, 0.4911]
2022-06-30 12:19:21.463236: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:19:21.938525: Suus1 maybe_update_lr lr: 0.000242
2022-06-30 12:19:21.940749: This epoch took 302.757458 s

2022-06-30 12:19:21.942846: 
epoch:  492
2022-06-30 12:24:05.048740: train loss : 0.0909
2022-06-30 12:24:24.721865: validation loss: 0.0046
2022-06-30 12:24:24.727124: Average global foreground Dice: [0.843, 0.6834, 0.6757, 0.5484, 0.4104, 0.8884, 0.6108, 0.7596, 0.5965, 0.5018, 0.4146, 0.2579, 0.3103, 0.815, 0.5085]
2022-06-30 12:24:24.729878: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:24:25.200020: Suus1 maybe_update_lr lr: 0.000215
2022-06-30 12:24:25.202347: saving best epoch checkpoint...
2022-06-30 12:24:25.398613: saving checkpoint...
2022-06-30 12:24:28.868046: done, saving took 3.66 seconds
2022-06-30 12:24:28.884844: This epoch took 306.939747 s

2022-06-30 12:24:28.887212: 
epoch:  493
2022-06-30 12:29:11.881045: train loss : 0.0758
2022-06-30 12:29:29.763761: validation loss: 0.0658
2022-06-30 12:29:29.768373: Average global foreground Dice: [0.7534, 0.6759, 0.7, 0.4687, 0.4148, 0.8418, 0.4714, 0.7068, 0.5725, 0.4415, 0.3624, 0.2908, 0.3031, 0.6721, 0.5075]
2022-06-30 12:29:29.771196: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:29:30.289187: Suus1 maybe_update_lr lr: 0.000187
2022-06-30 12:29:30.291648: This epoch took 301.401856 s

2022-06-30 12:29:30.294029: 
epoch:  494
2022-06-30 12:34:13.638854: train loss : 0.0689
2022-06-30 12:34:33.107339: validation loss: 0.0175
2022-06-30 12:34:33.112221: Average global foreground Dice: [0.7897, 0.6371, 0.6642, 0.5136, 0.4478, 0.8557, 0.5477, 0.7738, 0.6225, 0.4786, 0.3751, 0.1937, 0.3621, 0.8554, 0.7063]
2022-06-30 12:34:33.115513: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:34:33.596174: Suus1 maybe_update_lr lr: 0.000158
2022-06-30 12:34:33.598483: saving best epoch checkpoint...
2022-06-30 12:34:33.786982: saving checkpoint...
2022-06-30 12:34:37.159383: done, saving took 3.56 seconds
2022-06-30 12:34:37.174956: This epoch took 306.878759 s

2022-06-30 12:34:37.177634: 
epoch:  495
2022-06-30 12:39:20.323907: train loss : 0.0630
2022-06-30 12:39:39.637051: validation loss: 0.0538
2022-06-30 12:39:39.641605: Average global foreground Dice: [0.7493, 0.7079, 0.7084, 0.4738, 0.4234, 0.8498, 0.5136, 0.7639, 0.5461, 0.4893, 0.3059, 0.2335, 0.3062, 0.7931, 0.7568]
2022-06-30 12:39:39.643827: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:39:40.128255: Suus1 maybe_update_lr lr: 0.00013
2022-06-30 12:39:40.130544: saving best epoch checkpoint...
2022-06-30 12:39:40.316165: saving checkpoint...
2022-06-30 12:39:43.750587: done, saving took 3.62 seconds
2022-06-30 12:39:43.765756: This epoch took 306.585500 s

2022-06-30 12:39:43.768241: 
epoch:  496
2022-06-30 12:44:27.046323: train loss : 0.0775
2022-06-30 12:44:45.850028: validation loss: 0.0484
2022-06-30 12:44:45.858516: Average global foreground Dice: [0.7177, 0.7511, 0.6613, 0.5211, 0.3862, 0.8893, 0.4794, 0.7097, 0.5787, 0.5006, 0.3674, 0.1927, 0.2561, 0.668, 0.538]
2022-06-30 12:44:45.860881: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:44:46.331039: Suus1 maybe_update_lr lr: 0.0001
2022-06-30 12:44:46.333588: This epoch took 302.563132 s

2022-06-30 12:44:46.335773: 
epoch:  497
2022-06-30 12:49:29.686069: train loss : 0.0802
2022-06-30 12:49:50.704648: validation loss: 0.0356
2022-06-30 12:49:50.709216: Average global foreground Dice: [0.8213, 0.6509, 0.7279, 0.4281, 0.3723, 0.8505, 0.5606, 0.7254, 0.5957, 0.4736, 0.3322, 0.2813, 0.3227, 0.6176, 0.4927]
2022-06-30 12:49:50.711777: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:49:51.192113: Suus1 maybe_update_lr lr: 6.9e-05
2022-06-30 12:49:51.194539: This epoch took 304.856589 s

2022-06-30 12:49:51.196839: 
epoch:  498
2022-06-30 12:54:34.340396: train loss : 0.0573
2022-06-30 12:54:56.224400: validation loss: 0.0154
2022-06-30 12:54:56.229572: Average global foreground Dice: [0.8023, 0.7441, 0.7101, 0.4997, 0.4346, 0.8819, 0.5149, 0.7124, 0.6171, 0.506, 0.4129, 0.1532, 0.3404, 0.8119, 0.64]
2022-06-30 12:54:56.232455: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 12:54:56.708335: Suus1 maybe_update_lr lr: 3.7e-05
2022-06-30 12:54:56.710864: saving best epoch checkpoint...
2022-06-30 12:54:56.896753: saving checkpoint...
2022-06-30 12:55:00.194286: done, saving took 3.48 seconds
2022-06-30 12:55:00.210207: This epoch took 309.009902 s

2022-06-30 12:55:00.212654: 
epoch:  499
2022-06-30 12:59:43.574511: train loss : 0.0653
2022-06-30 13:00:04.253984: validation loss: 0.0402
2022-06-30 13:00:04.258633: Average global foreground Dice: [0.7824, 0.7203, 0.7112, 0.3944, 0.4492, 0.8866, 0.5336, 0.7559, 0.5848, 0.4693, 0.3784, 0.1934, 0.333, 0.4907, 0.4976]
2022-06-30 13:00:04.260857: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:00:04.743048: Suus1 maybe_update_lr lr: 0.0
2022-06-30 13:00:04.745525: saving scheduled checkpoint file...
2022-06-30 13:00:04.900501: saving checkpoint...
2022-06-30 13:00:08.262640: done, saving took 3.51 seconds
2022-06-30 13:00:08.279721: done
2022-06-30 13:00:08.282620: This epoch took 308.067602 s

2022-06-30 13:00:08.381415: saving checkpoint...
2022-06-30 13:00:11.756920: done, saving took 3.47 seconds


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='1', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-30 13:10:37.473563: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-30 13:10:37.488603: The split file contains 5 splits.
2022-06-30 13:10:37.491130: Desired fold for training: 1
2022-06-30 13:10:37.493148: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-06-30 13:10:40.742083: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_1/model_latest.model train= True
SuusB run_training - zet learning rate als  
2022-06-30 13:11:04.242501: Suus1 maybe_update_lr lr: 0.001259
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-06-30 13:11:20.528354: Unable to plot network architecture:
2022-06-30 13:11:20.586346: local variable 'g' referenced before assignment
2022-06-30 13:11:20.608293: 
printing the network instead:

2022-06-30 13:11:20.614128: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-06-30 13:11:20.631731: 

2022-06-30 13:11:20.635478: 
epoch:  450
2022-06-30 13:16:20.495351: train loss : 0.1092
2022-06-30 13:16:37.491850: validation loss: 0.1873
2022-06-30 13:16:37.497111: Average global foreground Dice: [0.6392, 0.5121, 0.5833, 0.3357, 0.4563, 0.8319, 0.5123, 0.688, 0.5228, 0.3835, 0.3146, 0.2436, 0.2515, 0.3005, 0.3512]
2022-06-30 13:16:37.499349: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:16:38.161748: Suus1 maybe_update_lr lr: 0.001236
2022-06-30 13:16:38.164640: This epoch took 317.524977 s

2022-06-30 13:16:38.167011: 
epoch:  451
2022-06-30 13:21:21.740912: train loss : 0.1172
2022-06-30 13:21:38.706742: validation loss: 0.1604
2022-06-30 13:21:38.712747: Average global foreground Dice: [0.6835, 0.563, 0.6671, 0.4782, 0.4444, 0.8633, 0.5049, 0.6478, 0.5962, 0.4603, 0.3632, 0.2448, 0.2867, 0.6964, 0.4476]
2022-06-30 13:21:38.715128: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:21:39.220737: Suus1 maybe_update_lr lr: 0.001214
2022-06-30 13:21:39.225195: This epoch took 301.055683 s

2022-06-30 13:21:39.230738: 
epoch:  452
2022-06-30 13:26:22.729248: train loss : 0.0834
2022-06-30 13:26:39.701374: validation loss: 0.1560
2022-06-30 13:26:39.706512: Average global foreground Dice: [0.7435, 0.6781, 0.7263, 0.4647, 0.463, 0.8516, 0.5687, 0.7391, 0.5602, 0.4224, 0.3737, 0.2787, 0.2873, 0.5097, 0.5814]
2022-06-30 13:26:39.709026: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:26:40.225714: Suus1 maybe_update_lr lr: 0.001191
2022-06-30 13:26:40.228721: This epoch took 300.993281 s

2022-06-30 13:26:40.231252: 
epoch:  453
2022-06-30 13:31:23.733916: train loss : 0.1300
2022-06-30 13:31:40.812184: validation loss: 0.1503
2022-06-30 13:31:40.817170: Average global foreground Dice: [0.7993, 0.6863, 0.6791, 0.3665, 0.3564, 0.8142, 0.5622, 0.6603, 0.5679, 0.4767, 0.3407, 0.2829, 0.3134, 0.6821, 0.5359]
2022-06-30 13:31:40.819798: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:31:41.304374: Suus1 maybe_update_lr lr: 0.001168
2022-06-30 13:31:41.306796: This epoch took 301.073181 s

2022-06-30 13:31:41.309299: 
epoch:  454
2022-06-30 13:36:24.593396: train loss : 0.0803
2022-06-30 13:36:41.570742: validation loss: 0.0890
2022-06-30 13:36:41.575594: Average global foreground Dice: [0.6763, 0.6807, 0.581, 0.3806, 0.3724, 0.8841, 0.4691, 0.6832, 0.5974, 0.3991, 0.3466, 0.2455, 0.3199, 0.5944, 0.4412]
2022-06-30 13:36:41.577892: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:36:42.088827: Suus1 maybe_update_lr lr: 0.001145
2022-06-30 13:36:42.091226: This epoch took 300.780047 s

2022-06-30 13:36:42.093461: 
epoch:  455
2022-06-30 13:41:25.502192: train loss : 0.1044
2022-06-30 13:41:45.039149: validation loss: 0.1984
2022-06-30 13:41:45.045003: Average global foreground Dice: [0.6791, 0.5357, 0.6052, 0.3957, 0.423, 0.7948, 0.5271, 0.6796, 0.5626, 0.4229, 0.4307, 0.1816, 0.2968, 0.4088, 0.2358]
2022-06-30 13:41:45.047417: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:41:45.519628: Suus1 maybe_update_lr lr: 0.001122
2022-06-30 13:41:45.522356: This epoch took 303.426237 s

2022-06-30 13:41:45.524536: 
epoch:  456
2022-06-30 13:46:28.801804: train loss : 0.1236
2022-06-30 13:46:45.800851: validation loss: 0.1972
2022-06-30 13:46:45.806703: Average global foreground Dice: [0.6279, 0.6136, 0.485, 0.4235, 0.4154, 0.8467, 0.4624, 0.6813, 0.561, 0.4585, 0.3346, 0.2036, 0.3336, 0.5596, 0.32]
2022-06-30 13:46:45.809338: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:46:46.308708: Suus1 maybe_update_lr lr: 0.001099
2022-06-30 13:46:46.311167: This epoch took 300.784382 s

2022-06-30 13:46:46.313448: 
epoch:  457
2022-06-30 13:51:29.812690: train loss : 0.1102
2022-06-30 13:51:46.824616: validation loss: 0.1086
2022-06-30 13:51:46.830610: Average global foreground Dice: [0.7703, 0.6681, 0.739, 0.4426, 0.4866, 0.8602, 0.5075, 0.7437, 0.5683, 0.427, 0.2695, 0.1932, 0.2755, 0.739, 0.68]
2022-06-30 13:51:46.832956: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:51:47.333945: Suus1 maybe_update_lr lr: 0.001076
2022-06-30 13:51:47.336464: This epoch took 301.020932 s

2022-06-30 13:51:47.338808: 
epoch:  458
2022-06-30 13:56:30.766494: train loss : 0.0961
2022-06-30 13:56:47.731947: validation loss: 0.1429
2022-06-30 13:56:47.736323: Average global foreground Dice: [0.7648, 0.6423, 0.6361, 0.2942, 0.4174, 0.8667, 0.5367, 0.6852, 0.5917, 0.4171, 0.3771, 0.1693, 0.3122, 0.6636, 0.5856]
2022-06-30 13:56:47.738749: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 13:56:48.234012: Suus1 maybe_update_lr lr: 0.001053
2022-06-30 13:56:48.236370: This epoch took 300.895209 s

2022-06-30 13:56:48.238687: 
epoch:  459
2022-06-30 14:01:31.741650: train loss : 0.0840
2022-06-30 14:01:48.719191: validation loss: 0.1060
2022-06-30 14:01:48.725322: Average global foreground Dice: [0.6856, 0.7232, 0.6506, 0.3808, 0.4357, 0.824, 0.4876, 0.672, 0.5157, 0.4004, 0.3182, 0.2148, 0.2923, 0.4461, 0.6186]
2022-06-30 14:01:48.727749: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:01:49.234260: Suus1 maybe_update_lr lr: 0.00103
2022-06-30 14:01:49.236796: This epoch took 300.995770 s

2022-06-30 14:01:49.239149: 
epoch:  460
2022-06-30 14:06:32.710119: train loss : 0.0860
2022-06-30 14:06:49.687685: validation loss: 0.1627
2022-06-30 14:06:49.692095: Average global foreground Dice: [0.7911, 0.6078, 0.6565, 0.4404, 0.4128, 0.8564, 0.573, 0.7262, 0.604, 0.4769, 0.3323, 0.2994, 0.2396, 0.4033, 0.4701]
2022-06-30 14:06:49.694213: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:06:50.191196: Suus1 maybe_update_lr lr: 0.001007
2022-06-30 14:06:50.193383: This epoch took 300.951746 s

2022-06-30 14:06:50.195424: 
epoch:  461
2022-06-30 14:11:33.667116: train loss : 0.1156
2022-06-30 14:11:50.676050: validation loss: 0.1417
2022-06-30 14:11:50.681026: Average global foreground Dice: [0.6333, 0.6228, 0.5132, 0.4242, 0.4497, 0.8372, 0.4175, 0.6934, 0.5711, 0.3945, 0.3044, 0.2177, 0.2574, 0.4976, 0.5512]
2022-06-30 14:11:50.683112: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:11:51.182491: Suus1 maybe_update_lr lr: 0.000983
2022-06-30 14:11:51.184602: This epoch took 300.987184 s

2022-06-30 14:11:51.186490: 
epoch:  462
2022-06-30 14:16:34.727760: train loss : 0.0938
2022-06-30 14:16:51.706049: validation loss: 0.0814
2022-06-30 14:16:51.710764: Average global foreground Dice: [0.7684, 0.6704, 0.6988, 0.4116, 0.5069, 0.8546, 0.5523, 0.7293, 0.5905, 0.4905, 0.3676, 0.2225, 0.3034, 0.4242, 0.5815]
2022-06-30 14:16:51.712982: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:16:52.206537: Suus1 maybe_update_lr lr: 0.00096
2022-06-30 14:16:52.209121: This epoch took 301.020720 s

2022-06-30 14:16:52.211496: 
epoch:  463
2022-06-30 14:21:35.692415: train loss : 0.0750
2022-06-30 14:22:10.803838: validation loss: 0.1179
2022-06-30 14:22:10.808377: Average global foreground Dice: [0.7186, 0.6869, 0.691, 0.3507, 0.4187, 0.8625, 0.5329, 0.6587, 0.5376, 0.4638, 0.3309, 0.2542, 0.2377, 0.5037, 0.4307]
2022-06-30 14:22:10.810809: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:22:11.306637: Suus1 maybe_update_lr lr: 0.000937
2022-06-30 14:22:11.309193: This epoch took 319.095564 s

2022-06-30 14:22:11.311382: 
epoch:  464
2022-06-30 14:26:54.449066: train loss : 0.1002
2022-06-30 14:27:18.065870: validation loss: 0.0525
2022-06-30 14:27:18.070946: Average global foreground Dice: [0.7904, 0.6064, 0.7009, 0.4068, 0.4001, 0.8905, 0.5555, 0.68, 0.5936, 0.4881, 0.2852, 0.2013, 0.2832, 0.5476, 0.4973]
2022-06-30 14:27:18.073378: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:27:18.545083: Suus1 maybe_update_lr lr: 0.000913
2022-06-30 14:27:18.547661: This epoch took 307.233975 s

2022-06-30 14:27:18.549944: 
epoch:  465
2022-06-30 14:32:01.931508: train loss : 0.0973
2022-06-30 14:32:21.123500: validation loss: 0.0535
2022-06-30 14:32:21.128119: Average global foreground Dice: [0.7409, 0.7374, 0.6824, 0.3766, 0.4227, 0.8572, 0.5096, 0.7268, 0.5726, 0.5016, 0.3817, 0.2631, 0.3202, 0.4335, 0.4507]
2022-06-30 14:32:21.130275: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:32:21.603654: Suus1 maybe_update_lr lr: 0.00089
2022-06-30 14:32:21.606172: This epoch took 303.053963 s

2022-06-30 14:32:21.608318: 
epoch:  466
2022-06-30 14:37:05.238568: train loss : 0.0692
2022-06-30 14:37:24.817624: validation loss: 0.0675
2022-06-30 14:37:24.821939: Average global foreground Dice: [0.6825, 0.6805, 0.6782, 0.4893, 0.4209, 0.8612, 0.5107, 0.7206, 0.5779, 0.4772, 0.3313, 0.2394, 0.3165, 0.6698, 0.5389]
2022-06-30 14:37:24.824349: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:37:25.313715: Suus1 maybe_update_lr lr: 0.000866
2022-06-30 14:37:25.315918: This epoch took 303.705570 s

2022-06-30 14:37:25.318106: 
epoch:  467
2022-06-30 14:42:08.962138: train loss : 0.0954
2022-06-30 14:42:32.555611: validation loss: 0.1024
2022-06-30 14:42:32.560297: Average global foreground Dice: [0.705, 0.7012, 0.6747, 0.4262, 0.4064, 0.8491, 0.5581, 0.6607, 0.5864, 0.4006, 0.3559, 0.1984, 0.3212, 0.5287, 0.5909]
2022-06-30 14:42:32.562871: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:42:33.038811: Suus1 maybe_update_lr lr: 0.000842
2022-06-30 14:42:33.041565: This epoch took 307.721425 s

2022-06-30 14:42:33.043816: 
epoch:  468
2022-06-30 14:47:16.425910: train loss : 0.0786
2022-06-30 14:47:54.666411: validation loss: 0.1347
2022-06-30 14:47:54.671570: Average global foreground Dice: [0.7832, 0.7114, 0.6588, 0.431, 0.4034, 0.8406, 0.4857, 0.7033, 0.6003, 0.4465, 0.3607, 0.1974, 0.2965, 0.2211, 0.3402]
2022-06-30 14:47:54.674340: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:47:55.206630: Suus1 maybe_update_lr lr: 0.000819
2022-06-30 14:47:55.212337: This epoch took 322.166219 s

2022-06-30 14:47:55.217468: 
epoch:  469
2022-06-30 14:52:38.255911: train loss : 0.0782
2022-06-30 14:53:01.835366: validation loss: 0.0810
2022-06-30 14:53:01.839941: Average global foreground Dice: [0.6453, 0.6707, 0.6023, 0.557, 0.4373, 0.8873, 0.4601, 0.7383, 0.6125, 0.5231, 0.399, 0.2532, 0.3442, 0.3, 0.4839]
2022-06-30 14:53:01.842177: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:53:02.320478: Suus1 maybe_update_lr lr: 0.000795
2022-06-30 14:53:02.322843: This epoch took 307.097476 s

2022-06-30 14:53:02.325200: 
epoch:  470
2022-06-30 14:57:45.696960: train loss : 0.0954
2022-06-30 14:58:18.593797: validation loss: 0.1740
2022-06-30 14:58:18.598892: Average global foreground Dice: [0.6168, 0.6141, 0.6549, 0.4621, 0.3838, 0.8416, 0.5147, 0.6421, 0.5287, 0.4221, 0.3124, 0.2147, 0.2669, 0.509, 0.5758]
2022-06-30 14:58:18.600974: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 14:58:19.074393: Suus1 maybe_update_lr lr: 0.000771
2022-06-30 14:58:19.076782: This epoch took 316.749138 s

2022-06-30 14:58:19.079081: 
epoch:  471
2022-06-30 15:03:02.304191: train loss : 0.0922
2022-06-30 15:03:32.737831: validation loss: 0.0701
2022-06-30 15:03:32.742610: Average global foreground Dice: [0.7322, 0.6591, 0.6996, 0.3919, 0.4549, 0.869, 0.5193, 0.7409, 0.572, 0.3868, 0.3692, 0.3061, 0.3134, 0.6744, 0.5677]
2022-06-30 15:03:32.744937: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:03:33.222649: Suus1 maybe_update_lr lr: 0.000747
2022-06-30 15:03:33.225172: This epoch took 314.143966 s

2022-06-30 15:03:33.227589: 
epoch:  472
2022-06-30 15:08:16.453105: train loss : 0.0978
2022-06-30 15:08:53.487617: validation loss: 0.0670
2022-06-30 15:08:53.492693: Average global foreground Dice: [0.7853, 0.6425, 0.7411, 0.4561, 0.4553, 0.847, 0.5601, 0.7058, 0.605, 0.4649, 0.2827, 0.2589, 0.2637, 0.7724, 0.5917]
2022-06-30 15:08:53.495224: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:08:53.971809: Suus1 maybe_update_lr lr: 0.000723
2022-06-30 15:08:53.974553: saving best epoch checkpoint...
2022-06-30 15:08:54.334117: saving checkpoint...
2022-06-30 15:08:57.796501: done, saving took 3.82 seconds
2022-06-30 15:08:57.816766: This epoch took 324.586992 s

2022-06-30 15:08:57.819651: 
epoch:  473
2022-06-30 15:13:40.735016: train loss : 0.0800
2022-06-30 15:14:14.862190: validation loss: 0.1167
2022-06-30 15:14:14.867536: Average global foreground Dice: [0.7536, 0.6217, 0.688, 0.4145, 0.3937, 0.8628, 0.4831, 0.6594, 0.5551, 0.4505, 0.2922, 0.268, 0.3304, 0.7033, 0.4335]
2022-06-30 15:14:14.870187: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:14:15.346522: Suus1 maybe_update_lr lr: 0.000699
2022-06-30 15:14:15.349263: saving best epoch checkpoint...
2022-06-30 15:14:15.538966: saving checkpoint...
2022-06-30 15:14:18.830639: done, saving took 3.48 seconds
2022-06-30 15:14:18.847182: This epoch took 321.025145 s

2022-06-30 15:14:18.849334: 
epoch:  474
2022-06-30 15:19:02.294819: train loss : 0.0797
2022-06-30 15:19:33.882743: validation loss: 0.1315
2022-06-30 15:19:33.887639: Average global foreground Dice: [0.7716, 0.6466, 0.7284, 0.449, 0.3925, 0.8352, 0.5738, 0.7087, 0.6035, 0.4385, 0.3176, 0.2123, 0.3048, 0.6092, 0.4406]
2022-06-30 15:19:33.890007: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:19:34.428850: Suus1 maybe_update_lr lr: 0.000675
2022-06-30 15:19:34.431874: saving best epoch checkpoint...
2022-06-30 15:19:34.622251: saving checkpoint...
2022-06-30 15:19:38.353306: done, saving took 3.92 seconds
2022-06-30 15:19:38.367743: This epoch took 319.516353 s

2022-06-30 15:19:38.370074: 
epoch:  475
2022-06-30 15:24:21.938452: train loss : 0.0883
2022-06-30 15:25:01.883356: validation loss: 0.0841
2022-06-30 15:25:01.909910: Average global foreground Dice: [0.8068, 0.6652, 0.7068, 0.5491, 0.5175, 0.8716, 0.5255, 0.689, 0.5591, 0.4702, 0.3348, 0.2573, 0.3295, 0.7812, 0.6316]
2022-06-30 15:25:01.912601: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:25:02.391750: Suus1 maybe_update_lr lr: 0.00065
2022-06-30 15:25:02.394476: saving best epoch checkpoint...
2022-06-30 15:25:02.586901: saving checkpoint...
2022-06-30 15:25:06.178084: done, saving took 3.78 seconds
2022-06-30 15:25:06.193431: This epoch took 327.821043 s

2022-06-30 15:25:06.197700: 
epoch:  476
2022-06-30 15:30:05.156940: train loss : 0.0775
2022-06-30 15:30:46.608201: validation loss: 0.1022
2022-06-30 15:30:46.617335: Average global foreground Dice: [0.7503, 0.672, 0.6671, 0.5122, 0.3519, 0.8792, 0.5225, 0.6801, 0.5752, 0.4533, 0.2952, 0.2972, 0.277, 0.5917, 0.4091]
2022-06-30 15:30:46.619628: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:30:47.093971: Suus1 maybe_update_lr lr: 0.000626
2022-06-30 15:30:47.096241: This epoch took 340.896125 s

2022-06-30 15:30:47.098393: 
epoch:  477
2022-06-30 15:35:30.428893: train loss : 0.0883
2022-06-30 15:36:10.967295: validation loss: 0.0330
2022-06-30 15:36:10.973137: Average global foreground Dice: [0.6943, 0.6702, 0.6612, 0.4047, 0.5046, 0.8718, 0.567, 0.7353, 0.6474, 0.4863, 0.3746, 0.2729, 0.3176, 0.6449, 0.4321]
2022-06-30 15:36:10.975857: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:36:11.513838: Suus1 maybe_update_lr lr: 0.000601
2022-06-30 15:36:11.517069: saving best epoch checkpoint...
2022-06-30 15:36:11.707915: saving checkpoint...
2022-06-30 15:36:15.525320: done, saving took 4.01 seconds
2022-06-30 15:36:15.543329: This epoch took 328.442820 s

2022-06-30 15:36:15.546297: 
epoch:  478
2022-06-30 15:41:54.012156: train loss : 0.0980
2022-06-30 15:42:36.759940: validation loss: 0.0815
2022-06-30 15:42:36.764745: Average global foreground Dice: [0.818, 0.6933, 0.7417, 0.5063, 0.3938, 0.8863, 0.4828, 0.7138, 0.5812, 0.4732, 0.3811, 0.2331, 0.3504, 0.5843, 0.5051]
2022-06-30 15:42:36.767134: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:42:37.269965: Suus1 maybe_update_lr lr: 0.000577
2022-06-30 15:42:37.274761: saving best epoch checkpoint...
2022-06-30 15:42:37.475615: saving checkpoint...
2022-06-30 15:42:41.062126: done, saving took 3.77 seconds
2022-06-30 15:42:41.078577: This epoch took 385.529097 s

2022-06-30 15:42:41.080736: 
epoch:  479
2022-06-30 15:47:33.160181: train loss : 0.0783
2022-06-30 15:48:21.130755: validation loss: 0.0590
2022-06-30 15:48:21.136192: Average global foreground Dice: [0.7459, 0.7095, 0.6003, 0.4835, 0.461, 0.8886, 0.5379, 0.7464, 0.616, 0.4948, 0.3929, 0.2486, 0.307, 0.7682, 0.7284]
2022-06-30 15:48:21.138996: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:48:21.621591: Suus1 maybe_update_lr lr: 0.000552
2022-06-30 15:48:21.624469: saving best epoch checkpoint...
2022-06-30 15:48:21.815610: saving checkpoint...
2022-06-30 15:48:25.846539: done, saving took 4.22 seconds
2022-06-30 15:48:25.863572: This epoch took 344.780662 s

2022-06-30 15:48:25.866286: 
epoch:  480
2022-06-30 15:53:08.758136: train loss : 0.0957
2022-06-30 15:53:48.436006: validation loss: 0.0796
2022-06-30 15:53:48.440809: Average global foreground Dice: [0.703, 0.6708, 0.6387, 0.4461, 0.4885, 0.8655, 0.5286, 0.7224, 0.6046, 0.4539, 0.3373, 0.2957, 0.2987, 0.753, 0.6017]
2022-06-30 15:53:48.443295: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:53:48.939264: Suus1 maybe_update_lr lr: 0.000527
2022-06-30 15:53:48.941812: saving best epoch checkpoint...
2022-06-30 15:53:49.145465: saving checkpoint...
2022-06-30 15:53:52.911835: done, saving took 3.97 seconds
2022-06-30 15:53:52.928312: This epoch took 327.059013 s

2022-06-30 15:53:52.930985: 
epoch:  481
2022-06-30 15:58:35.943406: train loss : 0.0643
2022-06-30 15:59:07.896249: validation loss: 0.0899
2022-06-30 15:59:07.900942: Average global foreground Dice: [0.7499, 0.6427, 0.753, 0.3971, 0.4711, 0.837, 0.553, 0.7725, 0.6035, 0.4274, 0.365, 0.299, 0.2753, 0.6627, 0.4588]
2022-06-30 15:59:07.903415: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 15:59:08.393398: Suus1 maybe_update_lr lr: 0.000502
2022-06-30 15:59:08.396029: saving best epoch checkpoint...
2022-06-30 15:59:08.592319: saving checkpoint...
2022-06-30 15:59:11.996928: done, saving took 3.60 seconds
2022-06-30 15:59:12.014261: This epoch took 319.080586 s

2022-06-30 15:59:12.016874: 
epoch:  482
2022-06-30 16:03:55.103229: train loss : 0.0562
2022-06-30 16:04:34.682208: validation loss: 0.0581
2022-06-30 16:04:34.698301: Average global foreground Dice: [0.7901, 0.6505, 0.7763, 0.4569, 0.4297, 0.8917, 0.4557, 0.7566, 0.568, 0.4319, 0.3378, 0.3469, 0.2625, 0.7207, 0.5175]
2022-06-30 16:04:34.715241: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:04:35.216661: Suus1 maybe_update_lr lr: 0.000477
2022-06-30 16:04:35.219438: saving best epoch checkpoint...
2022-06-30 16:04:35.410662: saving checkpoint...
2022-06-30 16:04:38.866688: done, saving took 3.64 seconds
2022-06-30 16:04:38.884670: This epoch took 326.865161 s

2022-06-30 16:04:38.887270: 
epoch:  483
2022-06-30 16:09:22.087532: train loss : 0.0782
2022-06-30 16:10:05.617024: validation loss: 0.1416
2022-06-30 16:10:05.622061: Average global foreground Dice: [0.6256, 0.6715, 0.6704, 0.5006, 0.4686, 0.8432, 0.5252, 0.7402, 0.6091, 0.5424, 0.3641, 0.2687, 0.3438, 0.5822, 0.504]
2022-06-30 16:10:05.624738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:10:06.251919: Suus1 maybe_update_lr lr: 0.000451
2022-06-30 16:10:06.255326: saving best epoch checkpoint...
2022-06-30 16:10:06.442535: saving checkpoint...
2022-06-30 16:10:09.380075: done, saving took 3.12 seconds
2022-06-30 16:10:09.394843: This epoch took 330.505195 s

2022-06-30 16:10:09.397355: 
epoch:  484
2022-06-30 16:15:06.071014: train loss : 0.1069
2022-06-30 16:16:23.114316: validation loss: 0.1042
2022-06-30 16:16:23.121819: Average global foreground Dice: [0.8068, 0.7674, 0.751, 0.4284, 0.3885, 0.8578, 0.5195, 0.7255, 0.5778, 0.4743, 0.3167, 0.2731, 0.3748, 0.5276, 0.504]
2022-06-30 16:16:23.124519: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:16:23.602917: Suus1 maybe_update_lr lr: 0.000426
2022-06-30 16:16:23.605560: saving best epoch checkpoint...
2022-06-30 16:16:23.793829: saving checkpoint...
2022-06-30 16:16:26.908569: done, saving took 3.30 seconds
2022-06-30 16:16:26.923573: This epoch took 377.523835 s

2022-06-30 16:16:26.925813: 
epoch:  485
2022-06-30 16:22:40.877208: train loss : 0.0856
2022-06-30 16:23:42.756708: validation loss: 0.0211
2022-06-30 16:23:42.763363: Average global foreground Dice: [0.7501, 0.7691, 0.6642, 0.5227, 0.3788, 0.898, 0.5071, 0.7318, 0.6098, 0.4859, 0.341, 0.2812, 0.3186, 0.7163, 0.636]
2022-06-30 16:23:42.765753: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:23:43.291610: Suus1 maybe_update_lr lr: 0.0004
2022-06-30 16:23:43.294334: saving best epoch checkpoint...
2022-06-30 16:23:43.482372: saving checkpoint...
2022-06-30 16:23:46.927406: done, saving took 3.63 seconds
2022-06-30 16:23:46.945173: This epoch took 440.016960 s

2022-06-30 16:23:46.947764: 
epoch:  486
2022-06-30 16:28:49.262237: train loss : 0.0813
2022-06-30 16:29:33.709834: validation loss: 0.0892
2022-06-30 16:29:33.714543: Average global foreground Dice: [0.6454, 0.7255, 0.7474, 0.5099, 0.4256, 0.9062, 0.5239, 0.7465, 0.6019, 0.4483, 0.3604, 0.2558, 0.3161, 0.5248, 0.5227]
2022-06-30 16:29:33.716642: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:29:34.194162: Suus1 maybe_update_lr lr: 0.000375
2022-06-30 16:29:34.196361: saving best epoch checkpoint...
2022-06-30 16:29:34.384273: saving checkpoint...
2022-06-30 16:29:37.377515: done, saving took 3.18 seconds
2022-06-30 16:29:37.393201: This epoch took 350.443074 s

2022-06-30 16:29:37.395753: 
epoch:  487
2022-06-30 16:34:20.282649: train loss : 0.0605
2022-06-30 16:34:52.103360: validation loss: 0.1296
2022-06-30 16:34:52.108594: Average global foreground Dice: [0.8454, 0.7451, 0.741, 0.5109, 0.3567, 0.8592, 0.5916, 0.6711, 0.5666, 0.4484, 0.3652, 0.2648, 0.2679, 0.2831, 0.537]
2022-06-30 16:34:52.111098: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:34:52.589090: Suus1 maybe_update_lr lr: 0.000348
2022-06-30 16:34:52.592119: This epoch took 315.194067 s

2022-06-30 16:34:52.594754: 
epoch:  488
2022-06-30 16:39:35.764367: train loss : 0.0502
2022-06-30 16:40:05.838020: validation loss: 0.0969
2022-06-30 16:40:05.842932: Average global foreground Dice: [0.7876, 0.6571, 0.6582, 0.562, 0.4538, 0.8712, 0.5869, 0.6962, 0.5852, 0.4976, 0.3384, 0.2316, 0.3191, 0.7341, 0.6192]
2022-06-30 16:40:05.845177: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:40:06.328488: Suus1 maybe_update_lr lr: 0.000322
2022-06-30 16:40:06.331061: saving best epoch checkpoint...
2022-06-30 16:40:06.525176: saving checkpoint...
2022-06-30 16:40:09.742879: done, saving took 3.41 seconds
2022-06-30 16:40:09.757623: This epoch took 317.160615 s

2022-06-30 16:40:09.759950: 
epoch:  489
2022-06-30 16:44:52.903262: train loss : 0.0673
2022-06-30 16:45:21.813060: validation loss: 0.1241
2022-06-30 16:45:21.817316: Average global foreground Dice: [0.7867, 0.6406, 0.6859, 0.5295, 0.3863, 0.8448, 0.4492, 0.6329, 0.5357, 0.389, 0.4145, 0.2504, 0.3533, 0.5495, 0.5433]
2022-06-30 16:45:21.819631: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:45:22.302946: Suus1 maybe_update_lr lr: 0.000296
2022-06-30 16:45:22.305506: This epoch took 312.543434 s

2022-06-30 16:45:22.307899: 
epoch:  490
2022-06-30 16:50:05.419540: train loss : 0.0876
2022-06-30 16:50:39.719862: validation loss: 0.0498
2022-06-30 16:50:39.724563: Average global foreground Dice: [0.798, 0.6536, 0.6987, 0.381, 0.4314, 0.8575, 0.5096, 0.6807, 0.6052, 0.4382, 0.3356, 0.1977, 0.3129, 0.7731, 0.6813]
2022-06-30 16:50:39.726784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:50:40.201774: Suus1 maybe_update_lr lr: 0.000269
2022-06-30 16:50:40.203882: This epoch took 317.893662 s

2022-06-30 16:50:40.206244: 
epoch:  491
2022-06-30 16:55:23.332940: train loss : 0.0632
2022-06-30 16:55:55.405770: validation loss: 0.0562
2022-06-30 16:55:55.410513: Average global foreground Dice: [0.7071, 0.6851, 0.6659, 0.2917, 0.4792, 0.8877, 0.5288, 0.713, 0.6166, 0.5268, 0.3778, 0.2634, 0.33, 0.6526, 0.5921]
2022-06-30 16:55:55.412849: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 16:55:55.896237: Suus1 maybe_update_lr lr: 0.000242
2022-06-30 16:55:55.898938: This epoch took 315.690546 s

2022-06-30 16:55:55.901490: 
epoch:  492
2022-06-30 17:00:39.056631: train loss : 0.0658
2022-06-30 17:01:11.949303: validation loss: 0.0410
2022-06-30 17:01:11.954612: Average global foreground Dice: [0.7841, 0.6818, 0.6841, 0.5321, 0.4631, 0.8863, 0.5777, 0.7527, 0.6123, 0.5066, 0.3915, 0.2958, 0.3697, 0.5027, 0.4927]
2022-06-30 17:01:11.956762: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 17:01:12.435836: Suus1 maybe_update_lr lr: 0.000215
2022-06-30 17:01:12.438908: saving best epoch checkpoint...
2022-06-30 17:01:12.636870: saving checkpoint...
2022-06-30 17:01:16.266367: done, saving took 3.83 seconds
2022-06-30 17:01:16.286673: This epoch took 320.382802 s

2022-06-30 17:01:16.288777: 
epoch:  493
2022-06-30 17:05:59.461340: train loss : 0.0479
2022-06-30 17:06:23.256545: validation loss: 0.0616
2022-06-30 17:06:23.262180: Average global foreground Dice: [0.713, 0.6858, 0.6791, 0.365, 0.4056, 0.8942, 0.5566, 0.7255, 0.6136, 0.4814, 0.3733, 0.2472, 0.3415, 0.5962, 0.5381]
2022-06-30 17:06:23.264437: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 17:06:23.938477: Suus1 maybe_update_lr lr: 0.000187
2022-06-30 17:06:23.940820: This epoch took 307.649964 s

2022-06-30 17:06:23.943219: 
epoch:  494
2022-06-30 17:11:07.458683: train loss : 0.0525
2022-06-30 17:11:29.875696: validation loss: 0.1228
2022-06-30 17:11:29.885556: Average global foreground Dice: [0.7703, 0.6802, 0.6337, 0.4855, 0.4877, 0.8716, 0.5357, 0.7361, 0.6151, 0.4546, 0.3406, 0.2625, 0.3281, 0.6418, 0.3489]
2022-06-30 17:11:29.888219: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 17:11:30.365484: Suus1 maybe_update_lr lr: 0.000158
2022-06-30 17:11:30.367979: This epoch took 306.422463 s

2022-06-30 17:11:30.370246: 
epoch:  495
2022-06-30 17:16:13.907911: train loss : 0.0720
2022-06-30 17:16:31.758610: validation loss: 0.0242
2022-06-30 17:16:31.763152: Average global foreground Dice: [0.6644, 0.7198, 0.6939, 0.4485, 0.5386, 0.886, 0.6095, 0.7714, 0.6356, 0.5436, 0.3201, 0.3406, 0.3157, 0.6652, 0.4728]
2022-06-30 17:16:31.765413: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 17:16:32.306479: Suus1 maybe_update_lr lr: 0.00013
2022-06-30 17:16:32.308994: saving best epoch checkpoint...
2022-06-30 17:16:32.515316: saving checkpoint...
2022-06-30 17:16:37.105864: done, saving took 4.79 seconds
2022-06-30 17:16:37.121963: This epoch took 306.749402 s

2022-06-30 17:16:37.124352: 
epoch:  496
2022-06-30 17:21:20.750300: train loss : 0.0482
2022-06-30 17:21:43.585491: validation loss: 0.0677
2022-06-30 17:21:43.590381: Average global foreground Dice: [0.8349, 0.5954, 0.7301, 0.3358, 0.4689, 0.87, 0.4425, 0.7039, 0.6018, 0.4396, 0.3295, 0.3413, 0.2763, 0.5737, 0.5062]
2022-06-30 17:21:43.592773: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 17:21:44.106541: Suus1 maybe_update_lr lr: 0.0001
2022-06-30 17:21:44.109045: This epoch took 306.982297 s

2022-06-30 17:21:44.111465: 
epoch:  497
2022-06-30 17:26:27.666577: train loss : 0.0724
2022-06-30 17:26:53.074914: validation loss: 0.0466
2022-06-30 17:26:53.079469: Average global foreground Dice: [0.7608, 0.7421, 0.7608, 0.4492, 0.4709, 0.9079, 0.5543, 0.787, 0.6004, 0.4594, 0.365, 0.2715, 0.3295, 0.6446, 0.6392]
2022-06-30 17:26:53.081874: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 17:26:53.558295: Suus1 maybe_update_lr lr: 6.9e-05
2022-06-30 17:26:53.560869: saving best epoch checkpoint...
2022-06-30 17:26:53.753780: saving checkpoint...
2022-06-30 17:26:57.142636: done, saving took 3.58 seconds
2022-06-30 17:26:57.156353: This epoch took 313.042809 s

2022-06-30 17:26:57.158212: 
epoch:  498
2022-06-30 17:31:40.480050: train loss : 0.0613
2022-06-30 17:32:01.587726: validation loss: 0.0807
2022-06-30 17:32:01.592746: Average global foreground Dice: [0.772, 0.6716, 0.7287, 0.4155, 0.465, 0.8681, 0.4568, 0.7887, 0.6486, 0.5095, 0.3623, 0.335, 0.3317, 0.5053, 0.5908]
2022-06-30 17:32:01.595456: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 17:32:02.109205: Suus1 maybe_update_lr lr: 3.7e-05
2022-06-30 17:32:02.113325: saving best epoch checkpoint...
2022-06-30 17:32:02.302629: saving checkpoint...
2022-06-30 17:32:06.007182: done, saving took 3.89 seconds
2022-06-30 17:32:06.022622: This epoch took 308.862555 s

2022-06-30 17:32:06.024811: 
epoch:  499
2022-06-30 17:36:49.467800: train loss : 0.0687
2022-06-30 17:37:06.465873: validation loss: 0.1207
2022-06-30 17:37:06.470108: Average global foreground Dice: [0.7246, 0.7229, 0.6942, 0.3743, 0.4174, 0.8916, 0.5165, 0.6733, 0.6248, 0.5028, 0.3016, 0.3034, 0.2565, 0.7036, 0.612]
2022-06-30 17:37:06.472339: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 17:37:06.949717: Suus1 maybe_update_lr lr: 0.0
2022-06-30 17:37:06.952097: saving scheduled checkpoint file...
2022-06-30 17:37:07.142732: saving checkpoint...
2022-06-30 17:37:10.521270: done, saving took 3.57 seconds
2022-06-30 17:37:10.537872: done
2022-06-30 17:37:10.541339: This epoch took 304.514115 s

2022-06-30 17:37:10.641881: saving checkpoint...
2022-06-30 17:37:13.738371: done, saving took 3.19 seconds


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='2', validation_only=False, continue_training=False, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-30 17:39:38.434750: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-30 17:39:38.448606: The split file contains 5 splits.
2022-06-30 17:39:38.451586: Desired fold for training: 2
2022-06-30 17:39:38.454471: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusB run_training - zet learning rate als  
2022-06-30 17:39:41.923752: Suus1 maybe_update_lr lr: 0.01
SuusC - run_training!
using pin_memory on device 0


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='3', validation_only=False, continue_training=False, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-30 17:40:20.726575: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-30 17:40:20.736621: The split file contains 5 splits.
2022-06-30 17:40:20.738837: Desired fold for training: 3
2022-06-30 17:40:20.740976: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusB run_training - zet learning rate als  
2022-06-30 17:40:24.373168: Suus1 maybe_update_lr lr: 0.01
SuusC - run_training!
using pin_memory on device 0


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid', task='700', fold='4', validation_only=False, continue_training=False, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid.nnUNetTrainerV2_Hybrid'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-30 17:40:55.332412: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-30 17:40:55.342437: The split file contains 5 splits.
2022-06-30 17:40:55.344473: Desired fold for training: 4
2022-06-30 17:40:55.346334: This split has 192 training and 48 validation cases.
unpacking dataset
done
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusB run_training - zet learning rate als  
2022-06-30 17:40:59.367980: Suus1 maybe_update_lr lr: 0.01
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-06-30 17:41:13.735871: Unable to plot network architecture:
2022-06-30 17:41:13.747870: local variable 'g' referenced before assignment
2022-06-30 17:41:13.757160: 
printing the network instead:

2022-06-30 17:41:13.767970: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-06-30 17:41:13.778213: 

2022-06-30 17:41:13.784338: 
epoch:  0
Start postprocessing..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Done postprocessing! Now start inferencing its own train and test files.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

using model stored in  /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1
This model expects 1 input modalities for each image
Found 120 unique case ids, here are some examples: ['panc_0304' 'panc_0208' 'panc_0063' 'panc_0572' 'panc_0132' 'panc_0063'
 'panc_0207' 'panc_0041' 'panc_0191' 'panc_0258']
If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc
number of cases: 120
number of cases that still need to be predicted: 120
emptying cuda cache
loading parameters for folds, None
folds is None so we will automatically look for output folders (not using 'all'!)
found the following folds:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_1', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_2', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_3', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_4']
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus5 - zet de plans properties
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
Img size: [ 64 160 160]
Patch size: (8, 16, 16)
Feature size: (8, 10, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
using the following model files:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2_Hybrid__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']
