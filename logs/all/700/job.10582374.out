Starting at Mon Jun 20 02:24:17 CEST 2022
Running on hosts: res-hpc-lkeb06
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 6.
Account: div2-lkeb
Job ID: 10582374
Job name: PancreasAll
Node running script: res-hpc-lkeb06
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Wed Jun 29 11:12:33 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     On   | 00000000:D8:00.0 Off |                  Off |
| 33%   25C    P8     4W / 260W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-czg_mrpc/hiddenlayer_d42798f4e4fb4417b50f7379e3e566d3
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Obtaining file:///home/smaijer/code/nnUNet
Requirement already satisfied: torch>1.10.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.12.0)
Requirement already satisfied: tqdm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.64.0)
Requirement already satisfied: dicom2nifti in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.4.2)
Requirement already satisfied: scikit-image>=0.14 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.19.3)
Requirement already satisfied: medpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.0)
Requirement already satisfied: scipy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.8.1)
Requirement already satisfied: batchgenerators>=0.23 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.24)
Requirement already satisfied: numpy in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.23.0)
Requirement already satisfied: sklearn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.0)
Requirement already satisfied: SimpleITK in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.1.1.2)
Requirement already satisfied: pandas in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (1.4.3)
Requirement already satisfied: requests in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2.28.0)
Requirement already satisfied: nibabel in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (4.0.1)
Requirement already satisfied: tifffile in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (2022.5.4)
Requirement already satisfied: matplotlib in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (3.5.2)
Requirement already satisfied: monai in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.9.0)
Requirement already satisfied: einops in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.4.1)
Requirement already satisfied: ipython in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (8.4.0)
Requirement already satisfied: graphviz in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from nnunet==1.7.0) (0.20)
Requirement already satisfied: future in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (0.18.2)
Requirement already satisfied: scikit-learn in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.1)
Requirement already satisfied: threadpoolctl in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (3.1.0)
Requirement already satisfied: unittest2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: pillow>=7.1.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from batchgenerators>=0.23->nnunet==1.7.0) (9.1.1)
Requirement already satisfied: networkx>=2.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.8.4)
Requirement already satisfied: imageio>=2.4.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (2.19.3)
Requirement already satisfied: PyWavelets>=1.1.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (1.3.0)
Requirement already satisfied: packaging>=20.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-image>=0.14->nnunet==1.7.0) (21.3)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from packaging>=20.0->scikit-image>=0.14->nnunet==1.7.0) (3.0.9)
Requirement already satisfied: typing-extensions in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from torch>1.10.0->nnunet==1.7.0) (4.2.0)
Requirement already satisfied: pydicom>=2.2.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (2.3.0)
Requirement already satisfied: python-gdcm in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from dicom2nifti->nnunet==1.7.0) (3.0.14)
Requirement already satisfied: backcall in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.2.0)
Requirement already satisfied: jedi>=0.16 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.18.1)
Requirement already satisfied: stack-data in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.3.0)
Requirement already satisfied: matplotlib-inline in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.1.3)
Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (3.0.30)
Requirement already satisfied: pexpect>4.3 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (4.8.0)
Requirement already satisfied: setuptools>=18.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (58.1.0)
Requirement already satisfied: pickleshare in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (0.7.5)
Requirement already satisfied: pygments>=2.4.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (2.12.0)
Requirement already satisfied: decorator in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.1.1)
Requirement already satisfied: traitlets>=5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from ipython->nnunet==1.7.0) (5.3.0)
Requirement already satisfied: parso<0.9.0,>=0.8.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from jedi>=0.16->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: ptyprocess>=0.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pexpect>4.3->ipython->nnunet==1.7.0) (0.7.0)
Requirement already satisfied: wcwidth in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nnunet==1.7.0) (0.2.5)
Requirement already satisfied: kiwisolver>=1.0.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (1.4.3)
Requirement already satisfied: fonttools>=4.22.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (4.33.3)
Requirement already satisfied: cycler>=0.10 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (0.11.0)
Requirement already satisfied: python-dateutil>=2.7 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from matplotlib->nnunet==1.7.0) (2.8.2)
Requirement already satisfied: six>=1.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->nnunet==1.7.0) (1.16.0)
Requirement already satisfied: pytz>=2020.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from pandas->nnunet==1.7.0) (2022.1)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (1.26.9)
Requirement already satisfied: idna<4,>=2.5 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (3.3)
Requirement already satisfied: charset-normalizer~=2.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2.0.12)
Requirement already satisfied: certifi>=2017.4.17 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from requests->nnunet==1.7.0) (2022.6.15)
Requirement already satisfied: joblib>=1.0.0 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from scikit-learn->batchgenerators>=0.23->nnunet==1.7.0) (1.1.0)
Requirement already satisfied: executing in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.8.3)
Requirement already satisfied: pure-eval in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (0.2.2)
Requirement already satisfied: asttokens in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from stack-data->ipython->nnunet==1.7.0) (2.0.5)
Requirement already satisfied: traceback2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.4.0)
Collecting argparse
  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: linecache2 in /exports/lkeb-hpc/smaijer/venv_environments/pancreasThesis/lib/python3.10/site-packages (from traceback2->unittest2->batchgenerators>=0.23->nnunet==1.7.0) (1.0.0)
Installing collected packages: argparse, nnunet
  Attempting uninstall: nnunet
    Found existing installation: nnunet 1.7.0
    Uninstalling nnunet-1.7.0:
      Successfully uninstalled nnunet-1.7.0
  Running setup.py develop for nnunet
Successfully installed argparse-1.4.0 nnunet-1.7.0


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2', task='700', fold='0', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-29 11:14:02.984058: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-29 11:14:02.996588: The split file contains 5 splits.
2022-06-29 11:14:02.998816: Desired fold for training: 0
2022-06-29 11:14:03.000947: This split has 192 training and 48 validation cases.
unpacking dataset
done
Suus8 - Maak network aan (BELANGRIJK!)
SuusB - first stride 
Suus10 - StackedConvLayers, input: 1 en output: 32, first_stride: None, num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [1, 3, 3], 'padding': [0, 1, 1]}
SuusA - first_stride [1, 2, 2]
Suus10 - StackedConvLayers, input: 32 en output: 64, first_stride: [1, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 64 en output: 128, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 128 en output: 256, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 256 en output: 320, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: [2, 2, 2], num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 640 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Generic_UNet(
  (encoder): Generic_UNETEncoder()
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(320, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (4): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-06-29 11:14:16.982256: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-06-29 11:14:27.383373: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-06-29 11:14:56.171250: Unable to plot network architecture:
2022-06-29 11:14:56.183237: local variable 'g' referenced before assignment
2022-06-29 11:14:56.194873: 
printing the network instead:

2022-06-29 11:14:56.198925: Generic_UNet(
  (encoder): Generic_UNETEncoder()
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(320, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (4): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-06-29 11:14:56.204289: 

2022-06-29 11:14:56.287894: saving checkpoint...
2022-06-29 11:14:58.099405: done, saving took 1.89 seconds
suus panc_0015 voeg toe aan pred_gt tuples voor later
suus panc_0025 voeg toe aan pred_gt tuples voor later
suus panc_0035 voeg toe aan pred_gt tuples voor later
suus panc_0047 voeg toe aan pred_gt tuples voor later
suus panc_0050 voeg toe aan pred_gt tuples voor later
suus panc_0057 voeg toe aan pred_gt tuples voor later
suus panc_0069 voeg toe aan pred_gt tuples voor later
suus panc_0079 voeg toe aan pred_gt tuples voor later
suus panc_0084 voeg toe aan pred_gt tuples voor later
suus panc_0088 voeg toe aan pred_gt tuples voor later
suus panc_0092 voeg toe aan pred_gt tuples voor later
suus panc_0098 voeg toe aan pred_gt tuples voor later
suus panc_0113 voeg toe aan pred_gt tuples voor later
suus panc_0116 voeg toe aan pred_gt tuples voor later
suus panc_0121 voeg toe aan pred_gt tuples voor later
suus panc_0125 voeg toe aan pred_gt tuples voor later
suus panc_0153 voeg toe aan pred_gt tuples voor later
suus panc_0159 voeg toe aan pred_gt tuples voor later
suus panc_0160 voeg toe aan pred_gt tuples voor later
suus panc_0161 voeg toe aan pred_gt tuples voor later
suus panc_0177 voeg toe aan pred_gt tuples voor later
suus panc_0179 voeg toe aan pred_gt tuples voor later
suus panc_0181 voeg toe aan pred_gt tuples voor later
suus panc_0186 voeg toe aan pred_gt tuples voor later
suus panc_0188 voeg toe aan pred_gt tuples voor later
suus panc_0192 voeg toe aan pred_gt tuples voor later
suus panc_0214 voeg toe aan pred_gt tuples voor later
suus panc_0217 voeg toe aan pred_gt tuples voor later
suus panc_0245 voeg toe aan pred_gt tuples voor later
suus panc_0297 voeg toe aan pred_gt tuples voor later
suus panc_0299 voeg toe aan pred_gt tuples voor later
suus panc_0330 voeg toe aan pred_gt tuples voor later
suus panc_0332 voeg toe aan pred_gt tuples voor later
suus panc_0348 voeg toe aan pred_gt tuples voor later
suus panc_0362 voeg toe aan pred_gt tuples voor later
suus panc_0374 voeg toe aan pred_gt tuples voor later
suus panc_0378 voeg toe aan pred_gt tuples voor later
suus panc_0390 voeg toe aan pred_gt tuples voor later
suus panc_0398 voeg toe aan pred_gt tuples voor later
suus panc_0517 voeg toe aan pred_gt tuples voor later
suus panc_0538 voeg toe aan pred_gt tuples voor later
suus panc_0551 voeg toe aan pred_gt tuples voor later
suus panc_0557 voeg toe aan pred_gt tuples voor later
suus panc_0571 voeg toe aan pred_gt tuples voor later
suus panc_0584 voeg toe aan pred_gt tuples voor later
suus panc_0592 voeg toe aan pred_gt tuples voor later
suus panc_0593 voeg toe aan pred_gt tuples voor later
suus panc_0596 voeg toe aan pred_gt tuples voor later
2022-06-29 11:14:59.314687: finished prediction
2022-06-29 11:14:59.317765: evaluation of raw predictions
2022-06-29 11:16:07.140653: determining postprocessing
Foreground vs background
before: 0.8523476078761535
after:  0.6794385440098679
1
before: 0.9247826668783107
after:  0.9274992923306704
Removing all but the largest region for class 1 improved results!
min_valid_object_sizes None
10
before: 0.8366511538468808
after:  0.8316428601748346
11
before: 0.7087458997288129
after:  0.7082941469474434
12
before: 0.7256522388900867
after:  0.7280270170999028
Removing all but the largest region for class 12 improved results!
min_valid_object_sizes None
13
before: 0.7707482861767537
after:  0.7679402687069353
14
before: 0.7998868596029378
after:  0.8015516742800133
Removing all but the largest region for class 14 improved results!
min_valid_object_sizes None
15
before: 0.8081444303834127
after:  0.8039860741886783
2
before: 0.9571061965771969
after:  0.959196367656079
Removing all but the largest region for class 2 improved results!
min_valid_object_sizes None
3
before: 0.9533862224209791
after:  0.9584612618013001
Removing all but the largest region for class 3 improved results!
min_valid_object_sizes None
4
before: 0.8164382163129887
after:  0.8159608454180783
5
before: 0.7995711966434552
after:  0.7974539374677797
6
before: 0.9723985463261019
after:  0.9729982876980725
Removing all but the largest region for class 6 improved results!
min_valid_object_sizes None
7
before: 0.875048215349655
after:  0.8801840371696171
Removing all but the largest region for class 7 improved results!
min_valid_object_sizes None
8
before: 0.9406110404321851
after:  0.9370877415445339
9
before: 0.8960429485725429
after:  0.894570054084595
done
for which classes:
[1, 12, 14, 2, 3, 6, 7]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2', task='700', fold='1', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-29 11:21:22.245614: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-29 11:21:22.263335: The split file contains 5 splits.
2022-06-29 11:21:22.265800: Desired fold for training: 1
2022-06-29 11:21:22.267861: This split has 192 training and 48 validation cases.
unpacking dataset
done
Suus8 - Maak network aan (BELANGRIJK!)
SuusB - first stride 
Suus10 - StackedConvLayers, input: 1 en output: 32, first_stride: None, num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [1, 3, 3], 'padding': [0, 1, 1]}
SuusA - first_stride [1, 2, 2]
Suus10 - StackedConvLayers, input: 32 en output: 64, first_stride: [1, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 64 en output: 128, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 128 en output: 256, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 256 en output: 320, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: [2, 2, 2], num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 640 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Generic_UNet(
  (encoder): Generic_UNETEncoder()
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(320, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (4): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-06-29 11:21:25.057999: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-06-29 11:21:34.993763: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-06-29 11:21:54.531711: Unable to plot network architecture:
2022-06-29 11:21:54.539456: local variable 'g' referenced before assignment
2022-06-29 11:21:54.545398: 
printing the network instead:

2022-06-29 11:21:54.550992: Generic_UNet(
  (encoder): Generic_UNETEncoder()
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(320, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (4): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-06-29 11:21:54.557767: 

2022-06-29 11:21:54.660453: saving checkpoint...
2022-06-29 11:21:56.247268: done, saving took 1.68 seconds
panc_0016 (2, 228, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 252
computing Gaussian
done
prediction done
suus panc_0016 transposed
suus panc_0016 not saving softmax
suus panc_0016 we moeten gekke ding doen met groot commentaar
suus panc_0016 voeg toe aan pred_gt tuples voor later
panc_0023 (2, 215, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 91, 121, 151], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0023 transposed
suus panc_0023 not saving softmax
suus panc_0023 we moeten gekke ding doen met groot commentaar
suus panc_0023 voeg toe aan pred_gt tuples voor later
panc_0027 (2, 310, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 92, 123, 154, 184, 215, 246], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 324
using precomputed Gaussian
prediction done
suus panc_0027 transposed
suus panc_0027 not saving softmax
suus panc_0027 we moeten gekke ding doen met groot commentaar
suus panc_0027 voeg toe aan pred_gt tuples voor later
panc_0033 (2, 265, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 265, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 57, 86, 115, 144, 172, 201], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 288
using precomputed Gaussian
prediction done
suus panc_0033 transposed
suus panc_0033 not saving softmax
suus panc_0033 we moeten gekke ding doen met groot commentaar
suus panc_0033 voeg toe aan pred_gt tuples voor later
panc_0043 (2, 212, 553, 553)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 212, 553, 553)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 118, 148], [0, 79, 157, 236, 314, 393], [0, 79, 157, 236, 314, 393]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0043 transposed
suus panc_0043 not saving softmax
suus panc_0043 we moeten gekke ding doen met groot commentaar
suus panc_0043 voeg toe aan pred_gt tuples voor later
panc_0045 (2, 228, 641, 641)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 641, 641)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 69, 137, 206, 275, 344, 412, 481], [0, 69, 137, 206, 275, 344, 412, 481]]
number of tiles: 448
using precomputed Gaussian
prediction done
suus panc_0045 transposed
suus panc_0045 not saving softmax
suus panc_0045 we moeten gekke ding doen met groot commentaar
suus panc_0045 voeg toe aan pred_gt tuples voor later
panc_0059 (2, 195, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 52, 79, 105, 131], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0059 transposed
suus panc_0059 not saving softmax
suus panc_0059 we moeten gekke ding doen met groot commentaar
suus panc_0059 voeg toe aan pred_gt tuples voor later
panc_0072 (2, 225, 579, 579)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 225, 579, 579)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 54, 80, 107, 134, 161], [0, 70, 140, 210, 279, 349, 419], [0, 70, 140, 210, 279, 349, 419]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0072 transposed
suus panc_0072 not saving softmax
suus panc_0072 we moeten gekke ding doen met groot commentaar
suus panc_0072 voeg toe aan pred_gt tuples voor later
panc_0083 (2, 250, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 250, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 93, 124, 155, 186], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0083 transposed
suus panc_0083 not saving softmax
suus panc_0083 we moeten gekke ding doen met groot commentaar
suus panc_0083 voeg toe aan pred_gt tuples voor later
panc_0086 (2, 200, 563, 563)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 200, 563, 563)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 54, 82, 109, 136], [0, 67, 134, 202, 269, 336, 403], [0, 67, 134, 202, 269, 336, 403]]
number of tiles: 294
using precomputed Gaussian
prediction done
suus panc_0086 transposed
suus panc_0086 not saving softmax
suus panc_0086 we moeten gekke ding doen met groot commentaar
suus panc_0086 voeg toe aan pred_gt tuples voor later
panc_0094 (2, 228, 541, 541)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 541, 541)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 76, 152, 229, 305, 381], [0, 76, 152, 229, 305, 381]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0094 transposed
suus panc_0094 not saving softmax
suus panc_0094 we moeten gekke ding doen met groot commentaar
suus panc_0094 voeg toe aan pred_gt tuples voor later
panc_0097 (2, 315, 701, 701)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 315, 701, 701)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 63, 94, 126, 157, 188, 220, 251], [0, 77, 155, 232, 309, 386, 464, 541], [0, 77, 155, 232, 309, 386, 464, 541]]
number of tiles: 576
using precomputed Gaussian
prediction done
suus panc_0097 transposed
suus panc_0097 not saving softmax
suus panc_0097 we moeten gekke ding doen met groot commentaar
suus panc_0097 voeg toe aan pred_gt tuples voor later
panc_0104 (2, 220, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 220, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 94, 125, 156], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0104 transposed
suus panc_0104 not saving softmax
suus panc_0104 we moeten gekke ding doen met groot commentaar
suus panc_0104 voeg toe aan pred_gt tuples voor later
panc_0115 (2, 220, 465, 465)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 220, 465, 465)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 94, 125, 156], [0, 76, 152, 229, 305], [0, 76, 152, 229, 305]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0115 transposed
suus panc_0115 not saving softmax
suus panc_0115 we moeten gekke ding doen met groot commentaar
suus panc_0115 voeg toe aan pred_gt tuples voor later
panc_0119 (2, 350, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 350, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 95, 127, 159, 191, 222, 254, 286], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0119 transposed
suus panc_0119 not saving softmax
suus panc_0119 we moeten gekke ding doen met groot commentaar
suus panc_0119 voeg toe aan pred_gt tuples voor later
panc_0133 (2, 170, 446, 446)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 170, 446, 446)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 53, 80, 106], [0, 72, 143, 214, 286], [0, 72, 143, 214, 286]]
number of tiles: 125
using precomputed Gaussian
prediction done
suus panc_0133 transposed
suus panc_0133 not saving softmax
suus panc_0133 we moeten gekke ding doen met groot commentaar
suus panc_0133 voeg toe aan pred_gt tuples voor later
panc_0143 (2, 180, 522, 522)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 180, 522, 522)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 87, 116], [0, 72, 145, 217, 290, 362], [0, 72, 145, 217, 290, 362]]
number of tiles: 180
using precomputed Gaussian
prediction done
suus panc_0143 transposed
suus panc_0143 not saving softmax
suus panc_0143 we moeten gekke ding doen met groot commentaar
suus panc_0143 voeg toe aan pred_gt tuples voor later
panc_0158 (2, 225, 513, 513)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 225, 513, 513)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 54, 80, 107, 134, 161], [0, 71, 141, 212, 282, 353], [0, 71, 141, 212, 282, 353]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0158 transposed
suus panc_0158 not saving softmax
suus panc_0158 we moeten gekke ding doen met groot commentaar
suus panc_0158 voeg toe aan pred_gt tuples voor later
panc_0171 (2, 242, 641, 641)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 242, 641, 641)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 119, 148, 178], [0, 69, 137, 206, 275, 344, 412, 481], [0, 69, 137, 206, 275, 344, 412, 481]]
number of tiles: 448
using precomputed Gaussian
prediction done
suus panc_0171 transposed
suus panc_0171 not saving softmax
suus panc_0171 we moeten gekke ding doen met groot commentaar
suus panc_0171 voeg toe aan pred_gt tuples voor later
panc_0172 (2, 238, 574, 574)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 238, 574, 574)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 87, 116, 145, 174], [0, 69, 138, 207, 276, 345, 414], [0, 69, 138, 207, 276, 345, 414]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0172 transposed
suus panc_0172 not saving softmax
suus panc_0172 we moeten gekke ding doen met groot commentaar
suus panc_0172 voeg toe aan pred_gt tuples voor later
panc_0235 (2, 197, 451, 451)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 197, 451, 451)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 53, 80, 106, 133], [0, 73, 146, 218, 291], [0, 73, 146, 218, 291]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0235 transposed
suus panc_0235 not saving softmax
suus panc_0235 we moeten gekke ding doen met groot commentaar
suus panc_0235 voeg toe aan pred_gt tuples voor later
panc_0249 (2, 298, 415, 415)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 298, 415, 415)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 88, 117, 146, 176, 205, 234], [0, 64, 128, 191, 255], [0, 64, 128, 191, 255]]
number of tiles: 225
using precomputed Gaussian
prediction done
suus panc_0249 transposed
suus panc_0249 not saving softmax
suus panc_0249 we moeten gekke ding doen met groot commentaar
suus panc_0249 voeg toe aan pred_gt tuples voor later
panc_0254 (2, 310, 392, 392)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 392, 392)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 92, 123, 154, 184, 215, 246], [0, 77, 155, 232], [0, 77, 155, 232]]
number of tiles: 144
using precomputed Gaussian
prediction done
suus panc_0254 transposed
suus panc_0254 not saving softmax
suus panc_0254 we moeten gekke ding doen met groot commentaar
suus panc_0254 voeg toe aan pred_gt tuples voor later
panc_0259 (2, 195, 428, 428)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 428, 428)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26, 52, 79, 105, 131], [0, 67, 134, 201, 268], [0, 67, 134, 201, 268]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0259 transposed
suus panc_0259 not saving softmax
suus panc_0259 we moeten gekke ding doen met groot commentaar
suus panc_0259 voeg toe aan pred_gt tuples voor later
panc_0268 (2, 191, 426, 426)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 191, 426, 426)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 95, 127], [0, 66, 133, 200, 266], [0, 66, 133, 200, 266]]
number of tiles: 125
using precomputed Gaussian
prediction done
suus panc_0268 transposed
suus panc_0268 not saving softmax
suus panc_0268 we moeten gekke ding doen met groot commentaar
suus panc_0268 voeg toe aan pred_gt tuples voor later
panc_0288 (2, 328, 490, 490)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 328, 490, 490)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 59, 88, 117, 147, 176, 205, 235, 264], [0, 66, 132, 198, 264, 330], [0, 66, 132, 198, 264, 330]]
number of tiles: 360
using precomputed Gaussian
prediction done
suus panc_0288 transposed
suus panc_0288 not saving softmax
suus panc_0288 we moeten gekke ding doen met groot commentaar
suus panc_0288 voeg toe aan pred_gt tuples voor later
panc_0294 (2, 239, 445, 445)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 239, 445, 445)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 88, 117, 146, 175], [0, 71, 142, 214, 285], [0, 71, 142, 214, 285]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0294 transposed
suus panc_0294 not saving softmax
suus panc_0294 we moeten gekke ding doen met groot commentaar
suus panc_0294 voeg toe aan pred_gt tuples voor later
panc_0320 (2, 322, 410, 410)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 322, 410, 410)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 57, 86, 115, 143, 172, 201, 229, 258], [0, 62, 125, 188, 250], [0, 62, 125, 188, 250]]
number of tiles: 250
using precomputed Gaussian
prediction done
suus panc_0320 transposed
suus panc_0320 not saving softmax
suus panc_0320 we moeten gekke ding doen met groot commentaar
suus panc_0320 voeg toe aan pred_gt tuples voor later
panc_0321 (2, 192, 472, 472)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 192, 472, 472)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 96, 128], [0, 78, 156, 234, 312], [0, 78, 156, 234, 312]]
number of tiles: 125
using precomputed Gaussian
prediction done
suus panc_0321 transposed
suus panc_0321 not saving softmax
suus panc_0321 we moeten gekke ding doen met groot commentaar
suus panc_0321 voeg toe aan pred_gt tuples voor later
panc_0341 (2, 302, 400, 400)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 302, 400, 400)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 89, 119, 149, 178, 208, 238], [0, 80, 160, 240], [0, 80, 160, 240]]
number of tiles: 144
using precomputed Gaussian
prediction done
suus panc_0341 transposed
suus panc_0341 not saving softmax
suus panc_0341 we moeten gekke ding doen met groot commentaar
suus panc_0341 voeg toe aan pred_gt tuples voor later
panc_0350 (2, 237, 408, 408)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 408, 408)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 86, 115, 144, 173], [0, 62, 124, 186, 248], [0, 62, 124, 186, 248]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0350 transposed
suus panc_0350 not saving softmax
suus panc_0350 we moeten gekke ding doen met groot commentaar
suus panc_0350 voeg toe aan pred_gt tuples voor later
panc_0353 (2, 217, 427, 427)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 217, 427, 427)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 61, 92, 122, 153], [0, 67, 134, 200, 267], [0, 67, 134, 200, 267]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0353 transposed
suus panc_0353 not saving softmax
suus panc_0353 we moeten gekke ding doen met groot commentaar
suus panc_0353 voeg toe aan pred_gt tuples voor later
panc_0366 (2, 215, 527, 527)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 527, 527)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 60, 91, 121, 151], [0, 73, 147, 220, 294, 367], [0, 73, 147, 220, 294, 367]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0366 transposed
suus panc_0366 not saving softmax
suus panc_0366 we moeten gekke ding doen met groot commentaar
suus panc_0366 voeg toe aan pred_gt tuples voor later
panc_0367 (2, 228, 641, 641)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 641, 641)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 55, 82, 109, 137, 164], [0, 69, 137, 206, 275, 344, 412, 481], [0, 69, 137, 206, 275, 344, 412, 481]]
number of tiles: 448
using precomputed Gaussian
prediction done
suus panc_0367 transposed
suus panc_0367 not saving softmax
suus panc_0367 we moeten gekke ding doen met groot commentaar
suus panc_0367 voeg toe aan pred_gt tuples voor later
panc_0383 (2, 242, 596, 596)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 242, 596, 596)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 119, 148, 178], [0, 73, 145, 218, 291, 363, 436], [0, 73, 145, 218, 291, 363, 436]]
number of tiles: 343
using precomputed Gaussian
prediction done
suus panc_0383 transposed
suus panc_0383 not saving softmax
suus panc_0383 we moeten gekke ding doen met groot commentaar
suus panc_0383 voeg toe aan pred_gt tuples voor later
panc_0392 (2, 212, 528, 528)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 212, 528, 528)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 30, 59, 89, 118, 148], [0, 74, 147, 221, 294, 368], [0, 74, 147, 221, 294, 368]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0392 transposed
suus panc_0392 not saving softmax
suus panc_0392 we moeten gekke ding doen met groot commentaar
suus panc_0392 voeg toe aan pred_gt tuples voor later
panc_0395 (2, 208, 455, 455)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 208, 455, 455)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 86, 115, 144], [0, 74, 148, 221, 295], [0, 74, 148, 221, 295]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0395 transposed
suus panc_0395 not saving softmax
suus panc_0395 we moeten gekke ding doen met groot commentaar
suus panc_0395 voeg toe aan pred_gt tuples voor later
panc_0403 (2, 218, 554, 554)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 554, 554)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 31, 62, 92, 123, 154], [0, 79, 158, 236, 315, 394], [0, 79, 158, 236, 315, 394]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0403 transposed
suus panc_0403 not saving softmax
suus panc_0403 we moeten gekke ding doen met groot commentaar
suus panc_0403 voeg toe aan pred_gt tuples voor later
panc_0404 (2, 210, 527, 527)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 527, 527)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 88, 117, 146], [0, 73, 147, 220, 294, 367], [0, 73, 147, 220, 294, 367]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0404 transposed
suus panc_0404 not saving softmax
suus panc_0404 we moeten gekke ding doen met groot commentaar
suus panc_0404 voeg toe aan pred_gt tuples voor later
panc_0410 (2, 268, 574, 574)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 268, 574, 574)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 29, 58, 87, 117, 146, 175, 204], [0, 69, 138, 207, 276, 345, 414], [0, 69, 138, 207, 276, 345, 414]]
number of tiles: 392
using precomputed Gaussian
prediction done
suus panc_0410 transposed
suus panc_0410 not saving softmax
suus panc_0410 we moeten gekke ding doen met groot commentaar
suus panc_0410 voeg toe aan pred_gt tuples voor later
panc_0507 (2, 90, 440, 486)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 90, 440, 486)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 26], [0, 70, 140, 210, 280], [0, 65, 130, 196, 261, 326]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0507 transposed
suus panc_0507 not saving softmax
suus panc_0507 voeg toe aan pred_gt tuples voor later
panc_0510 (2, 224, 200, 467)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 200, 467)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 96, 128, 160], [0, 40], [0, 77, 154, 230, 307]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0510 transposed
suus panc_0510 not saving softmax
suus panc_0510 voeg toe aan pred_gt tuples voor later
panc_0514 (2, 224, 215, 467)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 215, 467)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 32, 64, 96, 128, 160], [0, 55], [0, 77, 154, 230, 307]]
number of tiles: 60
using precomputed Gaussian
prediction done
suus panc_0514 transposed
suus panc_0514 not saving softmax
suus panc_0514 voeg toe aan pred_gt tuples voor later
panc_0522 (2, 108, 453, 562)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 453, 562)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 73, 146, 220, 293], [0, 67, 134, 201, 268, 335, 402]]
number of tiles: 105
using precomputed Gaussian
prediction done
suus panc_0522 transposed
suus panc_0522 not saving softmax
suus panc_0522 we moeten gekke ding doen met groot commentaar
suus panc_0522 voeg toe aan pred_gt tuples voor later
panc_0570 (2, 108, 393, 484)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 393, 484)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 78, 155, 233], [0, 65, 130, 194, 259, 324]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0570 transposed
suus panc_0570 not saving softmax
suus panc_0570 voeg toe aan pred_gt tuples voor later
panc_0587 (2, 108, 394, 486)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 394, 486)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 78, 156, 234], [0, 65, 130, 196, 261, 326]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0587 transposed
suus panc_0587 not saving softmax
suus panc_0587 voeg toe aan pred_gt tuples voor later
panc_0594 (2, 200, 237, 538)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 200, 237, 538)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 27, 54, 82, 109, 136], [0, 77], [0, 76, 151, 227, 302, 378]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0594 transposed
suus panc_0594 not saving softmax
suus panc_0594 voeg toe aan pred_gt tuples voor later
panc_0599 (2, 108, 393, 486)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 108, 393, 486)
patch size: [ 64 160 160]
steps (x, y, and z): [[0, 22, 44], [0, 78, 155, 233], [0, 65, 130, 196, 261, 326]]
number of tiles: 72
using precomputed Gaussian
prediction done
suus panc_0599 transposed
suus panc_0599 not saving softmax
suus panc_0599 voeg toe aan pred_gt tuples voor later
2022-06-29 16:36:19.213301: finished prediction
2022-06-29 16:36:19.229773: evaluation of raw predictions
2022-06-29 16:37:21.554713: determining postprocessing
Foreground vs background
before: 0.8376698193297057
after:  0.6969497401390246
1
before: 0.9154792847620211
after:  0.9188218863535402
Removing all but the largest region for class 1 improved results!
min_valid_object_sizes None
10
before: 0.8054191186204521
after:  0.7961287464908109
11
before: 0.7239853104765291
after:  0.7232274570686205
12
before: 0.7318550153401766
after:  0.7325015970355141
Removing all but the largest region for class 12 improved results!
min_valid_object_sizes None
13
before: 0.7591850734721938
after:  0.7546096627663061
14
before: 0.8593200764073634
after:  0.8505486329375024
15
before: 0.7700790123736576
after:  0.7800572468894592
Removing all but the largest region for class 15 improved results!
min_valid_object_sizes None
2
before: 0.9015829091242212
after:  0.9021118731706718
Removing all but the largest region for class 2 improved results!
min_valid_object_sizes None
3
before: 0.9078410582037142
after:  0.9141148960605602
Removing all but the largest region for class 3 improved results!
min_valid_object_sizes None
4
before: 0.7775234574904
after:  0.7794833132156558
Removing all but the largest region for class 4 improved results!
min_valid_object_sizes None
5
before: 0.7779753147958921
after:  0.7773672800641783
6
before: 0.944127135737198
after:  0.9335617662333057
7
before: 0.8942637263470341
after:  0.9045582289557491
Removing all but the largest region for class 7 improved results!
min_valid_object_sizes None
8
before: 0.923062961674796
after:  0.9238043131674919
Removing all but the largest region for class 8 improved results!
min_valid_object_sizes None
9
before: 0.8733478351199345
after:  0.8717535731800977
done
for which classes:
[1, 12, 15, 2, 3, 4, 7, 8]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0072.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0133.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0259.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0353.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0410.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0023.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0086.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0158.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0288.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0367.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0510.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0027.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0094.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0171.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0294.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0383.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0514.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0033.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0097.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0172.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0320.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0392.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0522.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0043.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0104.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0235.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0321.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0395.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0570.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0045.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0115.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0249.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0341.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0403.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0587.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0059.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0119.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0254.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0350.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0404.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0594.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0016.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0083.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0143.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0268.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0366.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0507.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_1/validation_raw/panc_0599.nii.gz
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2', task='700', fold='2', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2'>
For that I will be using the following configuration:
num_classes:  15
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 160, 160]), 'median_patient_size_in_voxels': array([138, 243, 243]), 'current_spacing': array([3.28926364, 1.64543342, 1.64543342]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 64, 160, 160]), 'median_patient_size_in_voxels': array([228, 513, 513]), 'current_spacing': array([2.        , 0.78014851, 0.78014851]), 'original_spacing': array([2.        , 0.78014851, 0.78014851]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-06-29 16:42:48.298023: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task700/splits_final.pkl
2022-06-29 16:42:48.311418: The split file contains 5 splits.
2022-06-29 16:42:48.314790: Desired fold for training: 2
2022-06-29 16:42:48.318147: This split has 192 training and 48 validation cases.
unpacking dataset
done
Suus8 - Maak network aan (BELANGRIJK!)
SuusB - first stride 
Suus10 - StackedConvLayers, input: 1 en output: 32, first_stride: None, num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [1, 3, 3], 'padding': [0, 1, 1]}
SuusA - first_stride [1, 2, 2]
Suus10 - StackedConvLayers, input: 32 en output: 64, first_stride: [1, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 64 en output: 128, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 128 en output: 256, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
SuusA - first_stride [2, 2, 2]
Suus10 - StackedConvLayers, input: 256 en output: 320, first_stride: [2, 2, 2], num_convs: 2, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: [2, 2, 2], num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 640 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 320 en output: 320, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Generic_UNet(
  (encoder): Generic_UNETEncoder()
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(320, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (4): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-06-29 16:42:51.808278: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task700/nnUNetTrainerV2__nnUNetPlansv2.1/fold_2/model_latest.model train= True
SuusB run_training - zet learning rate als  
2022-06-29 16:43:01.974803: Suus1 maybe_update_lr lr: 0.004384
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-06-29 16:43:16.676890: Unable to plot network architecture:
2022-06-29 16:43:16.681808: local variable 'g' referenced before assignment
2022-06-29 16:43:16.684569: 
printing the network instead:

2022-06-29 16:43:16.687474: Generic_UNet(
  (encoder): Generic_UNETEncoder()
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(640, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(1, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))
          (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv3d(256, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
          (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(320, 320, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose3d(320, 320, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(320, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (4): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(320, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(256, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(128, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(64, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (4): Conv3d(32, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-06-29 16:43:16.693347: 

2022-06-29 16:43:16.698710: 
epoch:  300
2022-06-29 16:49:02.451537: train loss : -0.2493
2022-06-29 16:49:20.982598: validation loss: -0.3733
2022-06-29 16:49:20.986353: Average global foreground Dice: [0.9296, 0.9234, 0.907, 0.7784, 0.7082, 0.9567, 0.9018, 0.9157, 0.8492, 0.8066, 0.6716, 0.6665, 0.6914, 0.8871, 0.7861]
2022-06-29 16:49:20.988677: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 16:49:21.564661: Suus1 maybe_update_lr lr: 0.004364
2022-06-29 16:49:21.568113: saving best epoch checkpoint...
2022-06-29 16:49:21.625886: saving checkpoint...
2022-06-29 16:49:22.962822: done, saving took 1.39 seconds
2022-06-29 16:49:22.984371: This epoch took 366.279857 s

2022-06-29 16:49:22.990890: 
epoch:  301
2022-06-29 16:54:51.506638: train loss : -0.2860
2022-06-29 16:55:10.040778: validation loss: -0.3471
2022-06-29 16:55:10.047037: Average global foreground Dice: [0.8438, 0.8793, 0.8855, 0.8145, 0.6924, 0.9399, 0.8794, 0.9262, 0.8216, 0.8199, 0.6911, 0.6191, 0.7108, 0.8816, 0.8375]
2022-06-29 16:55:10.052876: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 16:55:10.478903: Suus1 maybe_update_lr lr: 0.004344
2022-06-29 16:55:10.481155: This epoch took 347.487123 s

2022-06-29 16:55:10.483240: 
epoch:  302
2022-06-29 17:00:39.074993: train loss : -0.2950
2022-06-29 17:00:57.646739: validation loss: -0.2964
2022-06-29 17:00:57.650853: Average global foreground Dice: [0.8704, 0.9101, 0.9034, 0.6889, 0.6265, 0.9291, 0.8086, 0.9043, 0.8381, 0.834, 0.6947, 0.7075, 0.6488, 0.8558, 0.7649]
2022-06-29 17:00:57.653012: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:00:58.069636: Suus1 maybe_update_lr lr: 0.004325
2022-06-29 17:00:58.072232: This epoch took 347.586945 s

2022-06-29 17:00:58.074243: 
epoch:  303
2022-06-29 17:06:27.682701: train loss : -0.2693
2022-06-29 17:06:46.260289: validation loss: -0.3766
2022-06-29 17:06:46.264760: Average global foreground Dice: [0.9106, 0.9206, 0.9194, 0.7885, 0.6982, 0.9483, 0.8595, 0.894, 0.8529, 0.8015, 0.6991, 0.6999, 0.695, 0.8493, 0.6159]
2022-06-29 17:06:46.268120: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:06:46.696675: Suus1 maybe_update_lr lr: 0.004305
2022-06-29 17:06:46.698867: This epoch took 348.622640 s

2022-06-29 17:06:46.700767: 
epoch:  304
2022-06-29 17:12:16.448204: train loss : -0.2756
2022-06-29 17:12:35.050176: validation loss: -0.3384
2022-06-29 17:12:35.054065: Average global foreground Dice: [0.9061, 0.9078, 0.927, 0.8054, 0.6856, 0.9362, 0.8376, 0.9202, 0.8317, 0.7994, 0.6871, 0.6933, 0.6296, 0.8903, 0.7491]
2022-06-29 17:12:35.059949: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:12:35.473519: Suus1 maybe_update_lr lr: 0.004285
2022-06-29 17:12:35.475714: This epoch took 348.773000 s

2022-06-29 17:12:35.477668: 
epoch:  305
2022-06-29 17:18:05.719530: train loss : -0.2827
2022-06-29 17:18:24.335416: validation loss: -0.3444
2022-06-29 17:18:24.339409: Average global foreground Dice: [0.9137, 0.9106, 0.9269, 0.7741, 0.6273, 0.953, 0.8953, 0.9159, 0.8213, 0.7899, 0.6902, 0.6704, 0.6292, 0.8924, 0.8141]
2022-06-29 17:18:24.341398: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:18:24.754861: Suus1 maybe_update_lr lr: 0.004265
2022-06-29 17:18:24.757096: This epoch took 349.277536 s

2022-06-29 17:18:24.758955: 
epoch:  306
2022-06-29 17:23:55.096751: train loss : -0.2513
2022-06-29 17:24:13.692901: validation loss: -0.3828
2022-06-29 17:24:13.696324: Average global foreground Dice: [0.891, 0.945, 0.9388, 0.7516, 0.7432, 0.9515, 0.8508, 0.9025, 0.8537, 0.7856, 0.6981, 0.6806, 0.6752, 0.8831, 0.657]
2022-06-29 17:24:13.698304: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:24:14.111437: Suus1 maybe_update_lr lr: 0.004245
2022-06-29 17:24:14.113991: This epoch took 349.353176 s

2022-06-29 17:24:14.116253: 
epoch:  307
2022-06-29 17:29:44.339049: train loss : -0.2640
2022-06-29 17:30:02.948395: validation loss: -0.3611
2022-06-29 17:30:02.952062: Average global foreground Dice: [0.9365, 0.8844, 0.9162, 0.8403, 0.6609, 0.9423, 0.8529, 0.9129, 0.8393, 0.8149, 0.6386, 0.6769, 0.6753, 0.8447, 0.6609]
2022-06-29 17:30:02.954643: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:30:03.373302: Suus1 maybe_update_lr lr: 0.004226
2022-06-29 17:30:03.376634: This epoch took 349.257771 s

2022-06-29 17:30:03.379812: 
epoch:  308
2022-06-29 17:35:33.677366: train loss : -0.2595
2022-06-29 17:35:52.276406: validation loss: -0.2851
2022-06-29 17:35:52.281423: Average global foreground Dice: [0.886, 0.8014, 0.7954, 0.8371, 0.6396, 0.934, 0.7759, 0.8913, 0.8344, 0.7699, 0.6942, 0.6624, 0.6494, 0.9044, 0.6209]
2022-06-29 17:35:52.283836: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:35:52.697198: Suus1 maybe_update_lr lr: 0.004206
2022-06-29 17:35:52.699677: This epoch took 349.317797 s

2022-06-29 17:35:52.701864: 
epoch:  309
2022-06-29 17:41:22.849876: train loss : -0.2830
2022-06-29 17:41:41.447994: validation loss: -0.3151
2022-06-29 17:41:41.451315: Average global foreground Dice: [0.9003, 0.9004, 0.9005, 0.7118, 0.6817, 0.9421, 0.8065, 0.8959, 0.8479, 0.8111, 0.7197, 0.7027, 0.7032, 0.8837, 0.7517]
2022-06-29 17:41:41.453480: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:41:41.872966: Suus1 maybe_update_lr lr: 0.004186
2022-06-29 17:41:41.875340: This epoch took 349.171080 s

2022-06-29 17:41:41.877516: 
epoch:  310
2022-06-29 17:47:12.103275: train loss : -0.2973
2022-06-29 17:47:30.706079: validation loss: -0.3063
2022-06-29 17:47:30.709971: Average global foreground Dice: [0.9102, 0.9067, 0.903, 0.6836, 0.6271, 0.9365, 0.8564, 0.9178, 0.8195, 0.806, 0.6845, 0.6272, 0.6379, 0.8039, 0.7174]
2022-06-29 17:47:30.711966: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:47:31.128121: Suus1 maybe_update_lr lr: 0.004166
2022-06-29 17:47:31.132210: This epoch took 349.252545 s

2022-06-29 17:47:31.134675: 
epoch:  311
2022-06-29 17:53:01.344724: train loss : -0.2981
2022-06-29 17:53:19.940231: validation loss: -0.3482
2022-06-29 17:53:19.945372: Average global foreground Dice: [0.8932, 0.9186, 0.9256, 0.8276, 0.7136, 0.9393, 0.8357, 0.927, 0.8471, 0.8133, 0.7051, 0.6752, 0.7069, 0.8897, 0.7216]
2022-06-29 17:53:19.947665: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:53:20.362550: Suus1 maybe_update_lr lr: 0.004146
2022-06-29 17:53:20.364879: This epoch took 349.227734 s

2022-06-29 17:53:20.366896: 
epoch:  312
2022-06-29 17:58:50.559629: train loss : -0.2632
2022-06-29 17:59:09.153091: validation loss: -0.3293
2022-06-29 17:59:09.165024: Average global foreground Dice: [0.8691, 0.8706, 0.8576, 0.6605, 0.7174, 0.9448, 0.8042, 0.9233, 0.8556, 0.8005, 0.6975, 0.7147, 0.6718, 0.7916, 0.7077]
2022-06-29 17:59:09.166959: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 17:59:09.584027: Suus1 maybe_update_lr lr: 0.004127
2022-06-29 17:59:09.586498: This epoch took 349.217410 s

2022-06-29 17:59:09.588549: 
epoch:  313
2022-06-29 18:04:39.650576: train loss : -0.2510
2022-06-29 18:04:58.255280: validation loss: -0.3378
2022-06-29 18:04:58.260587: Average global foreground Dice: [0.9268, 0.8895, 0.8824, 0.832, 0.7068, 0.9563, 0.8451, 0.9189, 0.8454, 0.8129, 0.7179, 0.6639, 0.7383, 0.8534, 0.7434]
2022-06-29 18:04:58.262666: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:04:58.674532: Suus1 maybe_update_lr lr: 0.004107
2022-06-29 18:04:58.677538: This epoch took 349.086843 s

2022-06-29 18:04:58.679485: 
epoch:  314
2022-06-29 18:10:28.779036: train loss : -0.2543
2022-06-29 18:10:47.376407: validation loss: -0.3524
2022-06-29 18:10:47.380360: Average global foreground Dice: [0.889, 0.8882, 0.8821, 0.8025, 0.7545, 0.9554, 0.8389, 0.9201, 0.8574, 0.7982, 0.7281, 0.6931, 0.6793, 0.8945, 0.8721]
2022-06-29 18:10:47.382479: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:10:47.799341: Suus1 maybe_update_lr lr: 0.004087
2022-06-29 18:10:47.802508: This epoch took 349.120965 s

2022-06-29 18:10:47.804515: 
epoch:  315
2022-06-29 18:16:17.984419: train loss : -0.2808
2022-06-29 18:16:36.600612: validation loss: -0.3604
2022-06-29 18:16:36.606080: Average global foreground Dice: [0.9209, 0.92, 0.927, 0.7534, 0.6803, 0.9439, 0.8893, 0.8927, 0.8416, 0.8227, 0.685, 0.6823, 0.7492, 0.5171, 0.69]
2022-06-29 18:16:36.608373: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:16:37.022200: Suus1 maybe_update_lr lr: 0.004067
2022-06-29 18:16:37.024755: This epoch took 349.217966 s

2022-06-29 18:16:37.027136: 
epoch:  316
2022-06-29 18:22:07.192484: train loss : -0.2783
2022-06-29 18:22:25.798549: validation loss: -0.3234
2022-06-29 18:22:25.802245: Average global foreground Dice: [0.884, 0.9035, 0.8776, 0.7898, 0.6049, 0.9354, 0.8415, 0.9068, 0.8302, 0.8, 0.6733, 0.5942, 0.6581, 0.8166, 0.6922]
2022-06-29 18:22:25.804403: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:22:26.219788: Suus1 maybe_update_lr lr: 0.004047
2022-06-29 18:22:26.222025: This epoch took 349.192819 s

2022-06-29 18:22:26.223969: 
epoch:  317
2022-06-29 18:27:56.436542: train loss : -0.2746
2022-06-29 18:28:15.057596: validation loss: -0.3556
2022-06-29 18:28:15.061329: Average global foreground Dice: [0.9197, 0.9133, 0.8483, 0.8044, 0.7262, 0.9358, 0.8464, 0.9139, 0.8522, 0.7741, 0.6131, 0.6891, 0.6547, 0.8181, 0.8407]
2022-06-29 18:28:15.066311: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:28:15.485702: Suus1 maybe_update_lr lr: 0.004027
2022-06-29 18:28:15.491845: This epoch took 349.265844 s

2022-06-29 18:28:15.496043: 
epoch:  318
2022-06-29 18:33:45.668258: train loss : -0.2768
2022-06-29 18:34:04.291029: validation loss: -0.3800
2022-06-29 18:34:04.295611: Average global foreground Dice: [0.925, 0.903, 0.9229, 0.6808, 0.7037, 0.9529, 0.8586, 0.9259, 0.8569, 0.7987, 0.7371, 0.689, 0.6764, 0.902, 0.7153]
2022-06-29 18:34:04.297734: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:34:04.710627: Suus1 maybe_update_lr lr: 0.004007
2022-06-29 18:34:04.713055: This epoch took 349.211521 s

2022-06-29 18:34:04.715113: 
epoch:  319
2022-06-29 18:39:34.928792: train loss : -0.2885
2022-06-29 18:39:53.541069: validation loss: -0.3430
2022-06-29 18:39:53.545339: Average global foreground Dice: [0.9247, 0.9125, 0.7608, 0.7651, 0.6587, 0.957, 0.8129, 0.9188, 0.8481, 0.8069, 0.6741, 0.7067, 0.7175, 0.7578, 0.7956]
2022-06-29 18:39:53.547689: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:39:53.959732: Suus1 maybe_update_lr lr: 0.003987
2022-06-29 18:39:53.961950: This epoch took 349.244945 s

2022-06-29 18:39:53.964079: 
epoch:  320
2022-06-29 18:45:23.890545: train loss : -0.2815
2022-06-29 18:45:42.494388: validation loss: -0.3743
2022-06-29 18:45:42.498756: Average global foreground Dice: [0.9488, 0.9222, 0.9268, 0.8189, 0.6802, 0.9474, 0.8339, 0.8962, 0.8702, 0.8197, 0.7211, 0.66, 0.7108, 0.6582, 0.7113]
2022-06-29 18:45:42.501042: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:45:42.913300: Suus1 maybe_update_lr lr: 0.003967
2022-06-29 18:45:42.915647: This epoch took 348.949551 s

2022-06-29 18:45:42.917697: 
epoch:  321
2022-06-29 18:51:13.034325: train loss : -0.2693
2022-06-29 18:51:31.633496: validation loss: -0.3638
2022-06-29 18:51:31.638273: Average global foreground Dice: [0.9273, 0.9204, 0.9156, 0.7424, 0.6785, 0.9489, 0.8648, 0.9137, 0.8505, 0.7748, 0.6926, 0.6176, 0.6585, 0.843, 0.8015]
2022-06-29 18:51:31.640474: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:51:32.066754: Suus1 maybe_update_lr lr: 0.003947
2022-06-29 18:51:32.069281: This epoch took 349.149492 s

2022-06-29 18:51:32.071467: 
epoch:  322
2022-06-29 18:57:02.358723: train loss : -0.3083
2022-06-29 18:57:20.974214: validation loss: -0.3381
2022-06-29 18:57:20.977371: Average global foreground Dice: [0.9266, 0.9328, 0.8296, 0.7721, 0.7014, 0.9415, 0.773, 0.8981, 0.8594, 0.7843, 0.7129, 0.6835, 0.7031, 0.8899, 0.6798]
2022-06-29 18:57:20.979528: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 18:57:21.407929: Suus1 maybe_update_lr lr: 0.003927
2022-06-29 18:57:21.410653: This epoch took 349.337217 s

2022-06-29 18:57:21.412740: 
epoch:  323
2022-06-29 19:02:51.824286: train loss : -0.2537
2022-06-29 19:03:10.447550: validation loss: -0.3204
2022-06-29 19:03:10.453079: Average global foreground Dice: [0.8256, 0.927, 0.8034, 0.8134, 0.6782, 0.9347, 0.867, 0.8931, 0.8544, 0.8137, 0.7187, 0.6957, 0.6612, 0.8728, 0.7575]
2022-06-29 19:03:10.456585: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:03:10.874002: Suus1 maybe_update_lr lr: 0.003907
2022-06-29 19:03:10.877197: This epoch took 349.462315 s

2022-06-29 19:03:10.881078: 
epoch:  324
2022-06-29 19:08:41.259056: train loss : -0.2771
2022-06-29 19:08:59.881710: validation loss: -0.3587
2022-06-29 19:08:59.886767: Average global foreground Dice: [0.926, 0.9224, 0.9154, 0.783, 0.6441, 0.9531, 0.8723, 0.9158, 0.8254, 0.7679, 0.5968, 0.6925, 0.6668, 0.7525, 0.7527]
2022-06-29 19:08:59.889245: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:09:00.314243: Suus1 maybe_update_lr lr: 0.003887
2022-06-29 19:09:00.317510: This epoch took 349.434306 s

2022-06-29 19:09:00.320481: 
epoch:  325
2022-06-29 19:14:30.526186: train loss : -0.2920
2022-06-29 19:14:49.136311: validation loss: -0.3584
2022-06-29 19:14:49.141211: Average global foreground Dice: [0.9346, 0.9374, 0.9, 0.7545, 0.6956, 0.9499, 0.7889, 0.9034, 0.8186, 0.7959, 0.6955, 0.7081, 0.6268, 0.8672, 0.7359]
2022-06-29 19:14:49.143591: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:14:49.562395: Suus1 maybe_update_lr lr: 0.003867
2022-06-29 19:14:49.565358: This epoch took 349.242572 s

2022-06-29 19:14:49.569404: 
epoch:  326
2022-06-29 19:20:19.417675: train loss : -0.3010
2022-06-29 19:20:38.022433: validation loss: -0.3402
2022-06-29 19:20:38.026057: Average global foreground Dice: [0.8518, 0.8781, 0.8496, 0.6611, 0.7689, 0.9476, 0.8417, 0.9299, 0.8443, 0.8195, 0.7012, 0.7399, 0.6942, 0.8781, 0.7901]
2022-06-29 19:20:38.028128: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:20:38.448951: Suus1 maybe_update_lr lr: 0.003847
2022-06-29 19:20:38.451144: This epoch took 348.879821 s

2022-06-29 19:20:38.453080: 
epoch:  327
2022-06-29 19:26:08.133360: train loss : -0.3007
2022-06-29 19:26:26.733398: validation loss: -0.3296
2022-06-29 19:26:26.737820: Average global foreground Dice: [0.9175, 0.8758, 0.885, 0.787, 0.6943, 0.9582, 0.8606, 0.9257, 0.8305, 0.7648, 0.6807, 0.7111, 0.6714, 0.9273, 0.8556]
2022-06-29 19:26:26.740871: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:26:27.162113: Suus1 maybe_update_lr lr: 0.003827
2022-06-29 19:26:27.165190: This epoch took 348.710182 s

2022-06-29 19:26:27.170390: 
epoch:  328
2022-06-29 19:31:56.954537: train loss : -0.2740
2022-06-29 19:32:15.560280: validation loss: -0.3481
2022-06-29 19:32:15.564415: Average global foreground Dice: [0.9334, 0.9435, 0.9146, 0.7679, 0.6735, 0.9473, 0.7674, 0.9085, 0.8452, 0.8208, 0.6888, 0.6898, 0.6624, 0.9057, 0.7614]
2022-06-29 19:32:15.566573: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:32:15.981891: Suus1 maybe_update_lr lr: 0.003807
2022-06-29 19:32:15.984090: This epoch took 348.805506 s

2022-06-29 19:32:15.986161: 
epoch:  329
2022-06-29 19:37:45.835337: train loss : -0.2881
2022-06-29 19:38:04.436707: validation loss: -0.3032
2022-06-29 19:38:04.440020: Average global foreground Dice: [0.9368, 0.8575, 0.8094, 0.696, 0.708, 0.9438, 0.8246, 0.9025, 0.8587, 0.8013, 0.6848, 0.6399, 0.6437, 0.8612, 0.6416]
2022-06-29 19:38:04.442132: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:38:04.863002: Suus1 maybe_update_lr lr: 0.003787
2022-06-29 19:38:04.865470: This epoch took 348.875932 s

2022-06-29 19:38:04.867403: 
epoch:  330
2022-06-29 19:43:34.806941: train loss : -0.2842
2022-06-29 19:43:53.418622: validation loss: -0.3422
2022-06-29 19:43:53.427705: Average global foreground Dice: [0.9368, 0.9167, 0.9161, 0.7854, 0.6471, 0.954, 0.796, 0.9012, 0.8587, 0.7897, 0.6974, 0.6798, 0.6672, 0.8726, 0.7693]
2022-06-29 19:43:53.436022: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:43:53.860170: Suus1 maybe_update_lr lr: 0.003767
2022-06-29 19:43:53.862450: This epoch took 348.993056 s

2022-06-29 19:43:53.864471: 
epoch:  331
2022-06-29 19:49:24.070296: train loss : -0.2875
2022-06-29 19:49:42.695670: validation loss: -0.3191
2022-06-29 19:49:42.701051: Average global foreground Dice: [0.9248, 0.9251, 0.8371, 0.7078, 0.7397, 0.944, 0.8454, 0.9099, 0.8447, 0.7976, 0.678, 0.6594, 0.6934, 0.7582, 0.7294]
2022-06-29 19:49:42.703894: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:49:43.124934: Suus1 maybe_update_lr lr: 0.003747
2022-06-29 19:49:43.130639: This epoch took 349.264166 s

2022-06-29 19:49:43.132725: 
epoch:  332
2022-06-29 19:55:13.175200: train loss : -0.2772
2022-06-29 19:55:31.801864: validation loss: -0.3528
2022-06-29 19:55:31.805461: Average global foreground Dice: [0.8987, 0.9441, 0.7733, 0.8217, 0.6657, 0.9299, 0.8105, 0.927, 0.8688, 0.8151, 0.7356, 0.6763, 0.7132, 0.7366, 0.5832]
2022-06-29 19:55:31.808023: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 19:55:32.243316: Suus1 maybe_update_lr lr: 0.003727
2022-06-29 19:55:32.246700: This epoch took 349.112015 s

2022-06-29 19:55:32.248181: 
epoch:  333
2022-06-29 20:01:02.410848: train loss : -0.2722
2022-06-29 20:01:21.038158: validation loss: -0.3499
2022-06-29 20:01:21.041580: Average global foreground Dice: [0.9015, 0.9329, 0.9465, 0.79, 0.6878, 0.9446, 0.87, 0.931, 0.8703, 0.8331, 0.6969, 0.722, 0.6916, 0.9084, 0.7809]
2022-06-29 20:01:21.043449: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:01:21.461040: Suus1 maybe_update_lr lr: 0.003707
2022-06-29 20:01:21.463085: This epoch took 349.212215 s

2022-06-29 20:01:21.465037: 
epoch:  334
2022-06-29 20:06:51.553968: train loss : -0.2640
2022-06-29 20:07:10.177387: validation loss: -0.3643
2022-06-29 20:07:10.181739: Average global foreground Dice: [0.9405, 0.9282, 0.9215, 0.7804, 0.5945, 0.9572, 0.8023, 0.9149, 0.8681, 0.7965, 0.7484, 0.6859, 0.7265, 0.7575, 0.5552]
2022-06-29 20:07:10.183836: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:07:10.601932: Suus1 maybe_update_lr lr: 0.003687
2022-06-29 20:07:10.609338: This epoch took 349.142351 s

2022-06-29 20:07:10.611526: 
epoch:  335
2022-06-29 20:12:40.721505: train loss : -0.3004
2022-06-29 20:12:59.351673: validation loss: -0.3361
2022-06-29 20:12:59.356763: Average global foreground Dice: [0.8787, 0.8776, 0.8742, 0.753, 0.7044, 0.94, 0.856, 0.916, 0.8435, 0.8194, 0.6888, 0.6708, 0.6545, 0.9079, 0.8004]
2022-06-29 20:12:59.358977: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:12:59.781307: Suus1 maybe_update_lr lr: 0.003667
2022-06-29 20:12:59.784137: This epoch took 349.170721 s

2022-06-29 20:12:59.786098: 
epoch:  336
2022-06-29 20:18:29.962492: train loss : -0.2810
2022-06-29 20:18:48.595669: validation loss: -0.3684
2022-06-29 20:18:48.604971: Average global foreground Dice: [0.9291, 0.8856, 0.8757, 0.8325, 0.7386, 0.9584, 0.8453, 0.9242, 0.8493, 0.8154, 0.6924, 0.6772, 0.6841, 0.6856, 0.5395]
2022-06-29 20:18:48.607026: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:18:49.025290: Suus1 maybe_update_lr lr: 0.003647
2022-06-29 20:18:49.027538: This epoch took 349.239531 s

2022-06-29 20:18:49.029704: 
epoch:  337
2022-06-29 20:24:19.117156: train loss : -0.2847
2022-06-29 20:24:37.742856: validation loss: -0.3634
2022-06-29 20:24:37.746693: Average global foreground Dice: [0.8827, 0.9187, 0.9082, 0.6945, 0.7394, 0.9407, 0.8436, 0.9201, 0.8622, 0.7922, 0.7021, 0.704, 0.6575, 0.9047, 0.8382]
2022-06-29 20:24:37.748829: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:24:38.171746: Suus1 maybe_update_lr lr: 0.003627
2022-06-29 20:24:38.174589: This epoch took 349.138013 s

2022-06-29 20:24:38.176979: 
epoch:  338
2022-06-29 20:30:08.304083: train loss : -0.2903
2022-06-29 20:30:26.941909: validation loss: -0.3244
2022-06-29 20:30:26.945278: Average global foreground Dice: [0.8719, 0.8958, 0.8723, 0.7845, 0.6545, 0.9535, 0.864, 0.9259, 0.8518, 0.8343, 0.7089, 0.6682, 0.6756, 0.8833, 0.7939]
2022-06-29 20:30:26.947213: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:30:27.369250: Suus1 maybe_update_lr lr: 0.003606
2022-06-29 20:30:27.372183: This epoch took 349.193030 s

2022-06-29 20:30:27.376260: 
epoch:  339
2022-06-29 20:35:57.485302: train loss : -0.2752
2022-06-29 20:36:16.108408: validation loss: -0.3228
2022-06-29 20:36:16.165347: Average global foreground Dice: [0.908, 0.8946, 0.8575, 0.8256, 0.6378, 0.9278, 0.8013, 0.9316, 0.8407, 0.806, 0.6949, 0.7068, 0.6809, 0.8176, 0.7428]
2022-06-29 20:36:16.175949: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:36:16.816579: Suus1 maybe_update_lr lr: 0.003586
2022-06-29 20:36:16.818772: This epoch took 349.439221 s

2022-06-29 20:36:16.820754: 
epoch:  340
2022-06-29 20:41:47.104670: train loss : -0.2926
2022-06-29 20:42:05.740842: validation loss: -0.3467
2022-06-29 20:42:05.744843: Average global foreground Dice: [0.9392, 0.9361, 0.9432, 0.8527, 0.6945, 0.9422, 0.8588, 0.9159, 0.8459, 0.804, 0.7301, 0.6924, 0.7108, 0.8231, 0.7598]
2022-06-29 20:42:05.746763: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:42:06.164178: Suus1 maybe_update_lr lr: 0.003566
2022-06-29 20:42:06.166796: This epoch took 349.344103 s

2022-06-29 20:42:06.169319: 
epoch:  341
2022-06-29 20:47:36.548950: train loss : -0.2917
2022-06-29 20:47:55.182479: validation loss: -0.2991
2022-06-29 20:47:55.185888: Average global foreground Dice: [0.8539, 0.843, 0.7644, 0.7033, 0.6758, 0.9135, 0.7827, 0.8957, 0.824, 0.7936, 0.6646, 0.6474, 0.5938, 0.8226, 0.5953]
2022-06-29 20:47:55.187888: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:47:55.606111: Suus1 maybe_update_lr lr: 0.003546
2022-06-29 20:47:55.608456: This epoch took 349.436790 s

2022-06-29 20:47:55.610456: 
epoch:  342
2022-06-29 20:53:26.013929: train loss : -0.2679
2022-06-29 20:53:44.648376: validation loss: -0.3780
2022-06-29 20:53:44.652198: Average global foreground Dice: [0.9194, 0.9183, 0.8966, 0.8192, 0.7564, 0.963, 0.8781, 0.9238, 0.8584, 0.8231, 0.7387, 0.7339, 0.7, 0.8163, 0.7019]
2022-06-29 20:53:44.654464: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:53:45.073445: Suus1 maybe_update_lr lr: 0.003526
2022-06-29 20:53:45.075617: This epoch took 349.463095 s

2022-06-29 20:53:45.077552: 
epoch:  343
2022-06-29 20:59:15.447836: train loss : -0.2793
2022-06-29 20:59:34.084096: validation loss: -0.3697
2022-06-29 20:59:34.088170: Average global foreground Dice: [0.9164, 0.9259, 0.9332, 0.7683, 0.7309, 0.9499, 0.8659, 0.9285, 0.8416, 0.7934, 0.7158, 0.6406, 0.6827, 0.8101, 0.8337]
2022-06-29 20:59:34.106869: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 20:59:34.526641: Suus1 maybe_update_lr lr: 0.003505
2022-06-29 20:59:34.528882: This epoch took 349.449232 s

2022-06-29 20:59:34.530971: 
epoch:  344
2022-06-29 21:05:04.828833: train loss : -0.3014
2022-06-29 21:05:23.459178: validation loss: -0.3096
2022-06-29 21:05:23.462997: Average global foreground Dice: [0.9187, 0.8949, 0.7889, 0.812, 0.6562, 0.9228, 0.7495, 0.923, 0.831, 0.7536, 0.6651, 0.7082, 0.6322, 0.7476, 0.7256]
2022-06-29 21:05:23.465255: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:05:23.883999: Suus1 maybe_update_lr lr: 0.003485
2022-06-29 21:05:23.886007: This epoch took 349.353021 s

2022-06-29 21:05:23.888011: 
epoch:  345
2022-06-29 21:10:54.136033: train loss : -0.3002
2022-06-29 21:11:12.759560: validation loss: -0.3759
2022-06-29 21:11:12.763427: Average global foreground Dice: [0.9381, 0.9255, 0.9142, 0.723, 0.718, 0.9633, 0.895, 0.9254, 0.8527, 0.799, 0.7405, 0.6931, 0.7119, 0.8615, 0.7531]
2022-06-29 21:11:12.765585: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:11:13.183402: Suus1 maybe_update_lr lr: 0.003465
2022-06-29 21:11:13.185582: This epoch took 349.295651 s

2022-06-29 21:11:13.187528: 
epoch:  346
2022-06-29 21:16:43.391972: train loss : -0.2688
2022-06-29 21:17:02.015891: validation loss: -0.3682
2022-06-29 21:17:02.019327: Average global foreground Dice: [0.9375, 0.9452, 0.9331, 0.8108, 0.6891, 0.9613, 0.8646, 0.9249, 0.8396, 0.8112, 0.6922, 0.6718, 0.7168, 0.7039, 0.6084]
2022-06-29 21:17:02.021282: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:17:02.440431: Suus1 maybe_update_lr lr: 0.003445
2022-06-29 21:17:02.442586: This epoch took 349.252926 s

2022-06-29 21:17:02.444655: 
epoch:  347
2022-06-29 21:22:32.642691: train loss : -0.2933
2022-06-29 21:22:51.278589: validation loss: -0.3967
2022-06-29 21:22:51.281971: Average global foreground Dice: [0.9565, 0.9328, 0.9118, 0.817, 0.6611, 0.9558, 0.8899, 0.9241, 0.8629, 0.832, 0.6973, 0.7378, 0.7068, 0.8716, 0.6121]
2022-06-29 21:22:51.283925: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:22:51.699400: Suus1 maybe_update_lr lr: 0.003424
2022-06-29 21:22:51.701620: This epoch took 349.255031 s

2022-06-29 21:22:51.703470: 
epoch:  348
2022-06-29 21:28:21.936029: train loss : -0.2756
2022-06-29 21:28:40.553205: validation loss: -0.3363
2022-06-29 21:28:40.556613: Average global foreground Dice: [0.9327, 0.9143, 0.8601, 0.751, 0.7337, 0.9367, 0.8482, 0.9165, 0.8515, 0.7884, 0.7203, 0.7186, 0.707, 0.9173, 0.8597]
2022-06-29 21:28:40.558739: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:28:40.972953: Suus1 maybe_update_lr lr: 0.003404
2022-06-29 21:28:40.975389: This epoch took 349.269632 s

2022-06-29 21:28:40.977476: 
epoch:  349
2022-06-29 21:34:11.053650: train loss : -0.3183
2022-06-29 21:34:29.680631: validation loss: -0.3538
2022-06-29 21:34:29.684425: Average global foreground Dice: [0.9336, 0.9314, 0.9327, 0.73, 0.7293, 0.9483, 0.8561, 0.9155, 0.8435, 0.8014, 0.7376, 0.671, 0.6739, 0.8525, 0.7589]
2022-06-29 21:34:29.686440: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:34:30.102872: Suus1 maybe_update_lr lr: 0.003384
2022-06-29 21:34:30.105713: saving scheduled checkpoint file...
2022-06-29 21:34:30.169712: saving checkpoint...
2022-06-29 21:34:31.081692: done, saving took 0.97 seconds
2022-06-29 21:34:31.103066: done
2022-06-29 21:34:31.105859: This epoch took 350.126238 s

2022-06-29 21:34:31.108131: 
epoch:  350
2022-06-29 21:40:01.128958: train loss : -0.2967
2022-06-29 21:40:19.803654: validation loss: -0.3577
2022-06-29 21:40:19.807593: Average global foreground Dice: [0.875, 0.8983, 0.8965, 0.8251, 0.7468, 0.9405, 0.8168, 0.9263, 0.8511, 0.81, 0.7295, 0.6731, 0.6804, 0.87, 0.7373]
2022-06-29 21:40:19.809564: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:40:20.228198: Suus1 maybe_update_lr lr: 0.003364
2022-06-29 21:40:20.230546: This epoch took 349.120178 s

2022-06-29 21:40:20.232580: 
epoch:  351
2022-06-29 21:45:50.284246: train loss : -0.2952
2022-06-29 21:46:08.917136: validation loss: -0.3090
2022-06-29 21:46:08.920773: Average global foreground Dice: [0.946, 0.9375, 0.8918, 0.7068, 0.7408, 0.958, 0.8749, 0.922, 0.86, 0.7873, 0.7221, 0.6359, 0.6876, 0.8737, 0.7748]
2022-06-29 21:46:08.922690: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:46:09.341473: Suus1 maybe_update_lr lr: 0.003343
2022-06-29 21:46:09.343780: This epoch took 349.109111 s

2022-06-29 21:46:09.345759: 
epoch:  352
2022-06-29 21:51:39.600892: train loss : -0.2998
2022-06-29 21:51:58.225937: validation loss: -0.3829
2022-06-29 21:51:58.230482: Average global foreground Dice: [0.9386, 0.9337, 0.8262, 0.788, 0.7195, 0.9546, 0.8638, 0.9242, 0.8784, 0.8044, 0.7056, 0.704, 0.6774, 0.8828, 0.5842]
2022-06-29 21:51:58.232710: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:51:58.655000: Suus1 maybe_update_lr lr: 0.003323
2022-06-29 21:51:58.657396: This epoch took 349.309630 s

2022-06-29 21:51:58.659424: 
epoch:  353
2022-06-29 21:57:28.776203: train loss : -0.2852
2022-06-29 21:57:47.400502: validation loss: -0.3564
2022-06-29 21:57:47.404852: Average global foreground Dice: [0.9167, 0.8885, 0.8683, 0.7633, 0.7023, 0.9319, 0.7954, 0.9324, 0.8524, 0.8306, 0.7181, 0.6963, 0.7051, 0.9102, 0.8267]
2022-06-29 21:57:47.407191: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 21:57:47.827053: Suus1 maybe_update_lr lr: 0.003303
2022-06-29 21:57:47.829329: This epoch took 349.167919 s

2022-06-29 21:57:47.831380: 
epoch:  354
2022-06-29 22:03:18.013942: train loss : -0.2833
2022-06-29 22:03:36.644871: validation loss: -0.3354
2022-06-29 22:03:36.649284: Average global foreground Dice: [0.9105, 0.8638, 0.8558, 0.7557, 0.7154, 0.9137, 0.8329, 0.9179, 0.8306, 0.7725, 0.6736, 0.6564, 0.712, 0.836, 0.7508]
2022-06-29 22:03:36.651522: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:03:37.072991: Suus1 maybe_update_lr lr: 0.003282
2022-06-29 22:03:37.075346: This epoch took 349.241941 s

2022-06-29 22:03:37.077743: 
epoch:  355
2022-06-29 22:09:07.334459: train loss : -0.2761
2022-06-29 22:09:25.984971: validation loss: -0.2961
2022-06-29 22:09:26.022607: Average global foreground Dice: [0.944, 0.8893, 0.734, 0.6439, 0.7139, 0.9281, 0.861, 0.9173, 0.8536, 0.8261, 0.6995, 0.6994, 0.668, 0.6671, 0.81]
2022-06-29 22:09:26.024783: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:09:26.444181: Suus1 maybe_update_lr lr: 0.003262
2022-06-29 22:09:26.446761: This epoch took 349.366948 s

2022-06-29 22:09:26.448954: 
epoch:  356
2022-06-29 22:14:56.769064: train loss : -0.2554
2022-06-29 22:15:15.402743: validation loss: -0.3245
2022-06-29 22:15:15.406671: Average global foreground Dice: [0.9354, 0.9079, 0.9334, 0.6961, 0.6592, 0.9425, 0.8148, 0.9196, 0.8487, 0.8054, 0.6997, 0.6641, 0.6886, 0.8885, 0.7708]
2022-06-29 22:15:15.408749: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:15:15.825191: Suus1 maybe_update_lr lr: 0.003241
2022-06-29 22:15:15.827732: This epoch took 349.376654 s

2022-06-29 22:15:15.829944: 
epoch:  357
2022-06-29 22:20:46.153673: train loss : -0.2917
2022-06-29 22:21:04.786657: validation loss: -0.3102
2022-06-29 22:21:04.790823: Average global foreground Dice: [0.9284, 0.8991, 0.8158, 0.7931, 0.6326, 0.9279, 0.7575, 0.9117, 0.8587, 0.7809, 0.684, 0.6903, 0.658, 0.8481, 0.6314]
2022-06-29 22:21:04.793067: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:21:05.211534: Suus1 maybe_update_lr lr: 0.003221
2022-06-29 22:21:05.214065: This epoch took 349.382073 s

2022-06-29 22:21:05.216091: 
epoch:  358
2022-06-29 22:26:35.542823: train loss : -0.3047
2022-06-29 22:26:54.173196: validation loss: -0.3719
2022-06-29 22:26:54.177134: Average global foreground Dice: [0.9195, 0.915, 0.9201, 0.7908, 0.703, 0.9398, 0.8451, 0.9291, 0.8448, 0.8027, 0.6739, 0.6936, 0.6584, 0.7452, 0.7638]
2022-06-29 22:26:54.179400: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:26:54.597410: Suus1 maybe_update_lr lr: 0.003201
2022-06-29 22:26:54.599553: This epoch took 349.381087 s

2022-06-29 22:26:54.601717: 
epoch:  359
2022-06-29 22:32:24.952804: train loss : -0.3033
2022-06-29 22:32:43.586552: validation loss: -0.3906
2022-06-29 22:32:43.590449: Average global foreground Dice: [0.952, 0.9218, 0.9347, 0.8212, 0.7134, 0.9435, 0.8954, 0.92, 0.8534, 0.7968, 0.7216, 0.7272, 0.6697, 0.9267, 0.7927]
2022-06-29 22:32:43.592647: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:32:44.014889: Suus1 maybe_update_lr lr: 0.00318
2022-06-29 22:32:44.017171: This epoch took 349.413413 s

2022-06-29 22:32:44.019299: 
epoch:  360
2022-06-29 22:38:14.389419: train loss : -0.2957
2022-06-29 22:38:33.028318: validation loss: -0.3888
2022-06-29 22:38:33.032469: Average global foreground Dice: [0.8674, 0.8705, 0.8889, 0.7984, 0.6704, 0.9455, 0.8833, 0.9265, 0.8448, 0.8397, 0.6369, 0.6557, 0.7402, 0.9157, 0.8123]
2022-06-29 22:38:33.034535: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:38:33.458454: Suus1 maybe_update_lr lr: 0.00316
2022-06-29 22:38:33.460912: This epoch took 349.439447 s

2022-06-29 22:38:33.462915: 
epoch:  361
2022-06-29 22:44:03.760198: train loss : -0.3063
2022-06-29 22:44:22.399378: validation loss: -0.3542
2022-06-29 22:44:22.403051: Average global foreground Dice: [0.9422, 0.9152, 0.8814, 0.684, 0.7203, 0.951, 0.8606, 0.8854, 0.8561, 0.7797, 0.6958, 0.7595, 0.6715, 0.8319, 0.5082]
2022-06-29 22:44:22.405338: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:44:22.829064: Suus1 maybe_update_lr lr: 0.003139
2022-06-29 22:44:22.831537: This epoch took 349.366352 s

2022-06-29 22:44:22.833650: 
epoch:  362
2022-06-29 22:49:53.165264: train loss : -0.2940
2022-06-29 22:50:11.808367: validation loss: -0.3729
2022-06-29 22:50:11.812932: Average global foreground Dice: [0.9568, 0.8906, 0.9273, 0.8143, 0.7079, 0.9536, 0.8852, 0.9193, 0.8256, 0.8337, 0.6796, 0.713, 0.6815, 0.9319, 0.8657]
2022-06-29 22:50:11.815272: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:50:12.236542: Suus1 maybe_update_lr lr: 0.003119
2022-06-29 22:50:12.238918: This epoch took 349.403071 s

2022-06-29 22:50:12.240947: 
epoch:  363
2022-06-29 22:55:42.543366: train loss : -0.3031
2022-06-29 22:56:01.185104: validation loss: -0.3623
2022-06-29 22:56:01.210398: Average global foreground Dice: [0.8802, 0.8555, 0.8421, 0.8373, 0.7225, 0.9455, 0.8766, 0.9303, 0.847, 0.825, 0.7014, 0.7084, 0.7423, 0.8724, 0.6754]
2022-06-29 22:56:01.214543: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 22:56:01.657642: Suus1 maybe_update_lr lr: 0.003098
2022-06-29 22:56:01.666383: This epoch took 349.423588 s

2022-06-29 22:56:01.669748: 
epoch:  364
2022-06-29 23:01:31.970072: train loss : -0.3043
2022-06-29 23:01:50.611028: validation loss: -0.3588
2022-06-29 23:01:50.615324: Average global foreground Dice: [0.908, 0.926, 0.8367, 0.8267, 0.711, 0.9495, 0.7893, 0.9281, 0.8389, 0.7837, 0.7052, 0.6504, 0.6751, 0.9036, 0.8122]
2022-06-29 23:01:50.617652: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:01:51.032547: Suus1 maybe_update_lr lr: 0.003078
2022-06-29 23:01:51.034783: This epoch took 349.362839 s

2022-06-29 23:01:51.037030: 
epoch:  365
2022-06-29 23:07:21.386017: train loss : -0.3110
2022-06-29 23:07:40.029223: validation loss: -0.3507
2022-06-29 23:07:40.033540: Average global foreground Dice: [0.9466, 0.9136, 0.8029, 0.7679, 0.5926, 0.944, 0.8049, 0.8958, 0.8352, 0.7912, 0.6883, 0.7294, 0.6787, 0.9075, 0.7651]
2022-06-29 23:07:40.035783: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:07:40.452612: Suus1 maybe_update_lr lr: 0.003057
2022-06-29 23:07:40.455224: This epoch took 349.416025 s

2022-06-29 23:07:40.457334: 
epoch:  366
2022-06-29 23:13:10.738016: train loss : -0.2892
2022-06-29 23:13:29.377978: validation loss: -0.3568
2022-06-29 23:13:29.382197: Average global foreground Dice: [0.9228, 0.9188, 0.8747, 0.7623, 0.7321, 0.9396, 0.8523, 0.9135, 0.853, 0.8285, 0.7228, 0.7082, 0.6846, 0.7731, 0.7907]
2022-06-29 23:13:29.384388: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:13:29.799041: Suus1 maybe_update_lr lr: 0.003037
2022-06-29 23:13:29.801522: This epoch took 349.342191 s

2022-06-29 23:13:29.803568: 
epoch:  367
2022-06-29 23:19:00.173134: train loss : -0.3049
2022-06-29 23:19:18.799103: validation loss: -0.3370
2022-06-29 23:19:18.803414: Average global foreground Dice: [0.9292, 0.9164, 0.8232, 0.7807, 0.7218, 0.9521, 0.8248, 0.9341, 0.8201, 0.7775, 0.7172, 0.6455, 0.6438, 0.7473, 0.7609]
2022-06-29 23:19:18.805723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:19:19.228500: Suus1 maybe_update_lr lr: 0.003016
2022-06-29 23:19:19.230638: This epoch took 349.425110 s

2022-06-29 23:19:19.232683: 
epoch:  368
2022-06-29 23:24:49.555190: train loss : -0.2900
2022-06-29 23:25:08.179128: validation loss: -0.3814
2022-06-29 23:25:08.183541: Average global foreground Dice: [0.9417, 0.9252, 0.9063, 0.815, 0.6982, 0.9646, 0.8756, 0.917, 0.8523, 0.8123, 0.739, 0.7176, 0.6692, 0.8929, 0.7675]
2022-06-29 23:25:08.186357: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:25:08.607145: Suus1 maybe_update_lr lr: 0.002996
2022-06-29 23:25:08.609312: This epoch took 349.374591 s

2022-06-29 23:25:08.611944: 
epoch:  369
2022-06-29 23:30:38.933605: train loss : -0.3015
2022-06-29 23:30:57.563959: validation loss: -0.3693
2022-06-29 23:30:57.568126: Average global foreground Dice: [0.9486, 0.9256, 0.9105, 0.7974, 0.6508, 0.9511, 0.8125, 0.9272, 0.8628, 0.8167, 0.7063, 0.6917, 0.6979, 0.9217, 0.6591]
2022-06-29 23:30:57.570337: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:30:57.988612: Suus1 maybe_update_lr lr: 0.002975
2022-06-29 23:30:57.990925: This epoch took 349.376847 s

2022-06-29 23:30:57.992976: 
epoch:  370
2022-06-29 23:36:28.112104: train loss : -0.2750
2022-06-29 23:36:46.738440: validation loss: -0.3320
2022-06-29 23:36:46.743712: Average global foreground Dice: [0.924, 0.933, 0.9506, 0.7722, 0.608, 0.9517, 0.859, 0.9113, 0.853, 0.7967, 0.688, 0.6923, 0.6588, 0.9403, 0.7956]
2022-06-29 23:36:46.746068: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:36:47.169560: Suus1 maybe_update_lr lr: 0.002954
2022-06-29 23:36:47.171821: This epoch took 349.176804 s

2022-06-29 23:36:47.174081: 
epoch:  371
2022-06-29 23:42:17.398295: train loss : -0.3058
2022-06-29 23:42:36.027685: validation loss: -0.3422
2022-06-29 23:42:36.032022: Average global foreground Dice: [0.9498, 0.9266, 0.8156, 0.8005, 0.7307, 0.9433, 0.8241, 0.9332, 0.8372, 0.7903, 0.7003, 0.6408, 0.6494, 0.9104, 0.8667]
2022-06-29 23:42:36.034124: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:42:36.456769: Suus1 maybe_update_lr lr: 0.002934
2022-06-29 23:42:36.459121: This epoch took 349.282962 s

2022-06-29 23:42:36.461339: 
epoch:  372
2022-06-29 23:48:06.614448: train loss : -0.3038
2022-06-29 23:48:25.234417: validation loss: -0.3828
2022-06-29 23:48:25.239867: Average global foreground Dice: [0.9502, 0.9127, 0.9171, 0.8546, 0.7273, 0.9641, 0.8847, 0.9307, 0.8567, 0.7936, 0.7253, 0.7055, 0.7218, 0.9136, 0.7935]
2022-06-29 23:48:25.242867: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:48:25.661667: Suus1 maybe_update_lr lr: 0.002913
2022-06-29 23:48:25.664007: This epoch took 349.200599 s

2022-06-29 23:48:25.666516: 
epoch:  373
2022-06-29 23:53:56.011976: train loss : -0.2928
2022-06-29 23:54:14.659588: validation loss: -0.3753
2022-06-29 23:54:14.665784: Average global foreground Dice: [0.9322, 0.9058, 0.903, 0.8009, 0.715, 0.9426, 0.86, 0.9339, 0.8614, 0.7908, 0.7222, 0.6305, 0.6948, 0.8553, 0.7759]
2022-06-29 23:54:14.670821: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-29 23:54:15.092930: Suus1 maybe_update_lr lr: 0.002892
2022-06-29 23:54:15.095471: This epoch took 349.426702 s

2022-06-29 23:54:15.097783: 
epoch:  374
2022-06-29 23:59:45.480455: train loss : -0.3038
2022-06-30 00:00:04.118762: validation loss: -0.3747
2022-06-30 00:00:04.125288: Average global foreground Dice: [0.9174, 0.9024, 0.8694, 0.771, 0.6878, 0.9506, 0.867, 0.929, 0.8533, 0.8129, 0.6991, 0.678, 0.7026, 0.8743, 0.7986]
2022-06-30 00:00:04.127532: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:00:04.552297: Suus1 maybe_update_lr lr: 0.002872
2022-06-30 00:00:04.554865: This epoch took 349.454915 s

2022-06-30 00:00:04.557048: 
epoch:  375
2022-06-30 00:05:34.831046: train loss : -0.3223
2022-06-30 00:05:53.463231: validation loss: -0.3997
2022-06-30 00:05:53.467206: Average global foreground Dice: [0.9396, 0.9369, 0.9246, 0.8069, 0.7466, 0.9624, 0.8647, 0.9282, 0.858, 0.8396, 0.7344, 0.7575, 0.7097, 0.8848, 0.7491]
2022-06-30 00:05:53.469288: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:05:53.894771: Suus1 maybe_update_lr lr: 0.002851
2022-06-30 00:05:53.897221: This epoch took 349.337478 s

2022-06-30 00:05:53.899375: 
epoch:  376
2022-06-30 00:11:24.256050: train loss : -0.3023
2022-06-30 00:11:42.884806: validation loss: -0.3883
2022-06-30 00:11:42.888981: Average global foreground Dice: [0.9078, 0.9025, 0.8882, 0.8029, 0.7059, 0.9486, 0.8663, 0.9299, 0.8448, 0.805, 0.7104, 0.7078, 0.719, 0.8319, 0.6747]
2022-06-30 00:11:42.891493: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:11:43.313864: Suus1 maybe_update_lr lr: 0.00283
2022-06-30 00:11:43.316722: This epoch took 349.415110 s

2022-06-30 00:11:43.319203: 
epoch:  377
2022-06-30 00:17:13.665240: train loss : -0.2826
2022-06-30 00:17:32.300333: validation loss: -0.3497
2022-06-30 00:17:32.304399: Average global foreground Dice: [0.942, 0.9374, 0.9211, 0.7129, 0.7171, 0.9594, 0.8564, 0.9148, 0.8506, 0.8091, 0.736, 0.706, 0.6864, 0.898, 0.7789]
2022-06-30 00:17:32.305907: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:17:32.730346: Suus1 maybe_update_lr lr: 0.00281
2022-06-30 00:17:32.733177: This epoch took 349.411483 s

2022-06-30 00:17:32.737637: 
epoch:  378
2022-06-30 00:23:03.061733: train loss : -0.3320
2022-06-30 00:23:21.683787: validation loss: -0.3478
2022-06-30 00:23:21.687748: Average global foreground Dice: [0.9247, 0.9161, 0.7887, 0.8233, 0.7449, 0.9418, 0.8674, 0.9191, 0.8645, 0.8413, 0.7143, 0.7091, 0.7176, 0.7889, 0.8004]
2022-06-30 00:23:21.690913: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:23:22.113693: Suus1 maybe_update_lr lr: 0.002789
2022-06-30 00:23:22.116644: This epoch took 349.374337 s

2022-06-30 00:23:22.119323: 
epoch:  379
2022-06-30 00:28:52.454535: train loss : -0.3269
2022-06-30 00:29:11.083137: validation loss: -0.3753
2022-06-30 00:29:11.086755: Average global foreground Dice: [0.8946, 0.9314, 0.8687, 0.7429, 0.6764, 0.9466, 0.8262, 0.9319, 0.856, 0.8293, 0.693, 0.7159, 0.6938, 0.8716, 0.7731]
2022-06-30 00:29:11.088969: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:29:11.509840: Suus1 maybe_update_lr lr: 0.002768
2022-06-30 00:29:11.512490: This epoch took 349.390815 s

2022-06-30 00:29:11.514769: 
epoch:  380
2022-06-30 00:34:41.817016: train loss : -0.3031
2022-06-30 00:35:00.437160: validation loss: -0.3936
2022-06-30 00:35:00.440420: Average global foreground Dice: [0.9362, 0.8952, 0.8815, 0.8053, 0.7461, 0.955, 0.8738, 0.919, 0.8458, 0.8139, 0.7068, 0.6982, 0.7265, 0.8251, 0.5828]
2022-06-30 00:35:00.442723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:35:00.861292: Suus1 maybe_update_lr lr: 0.002747
2022-06-30 00:35:00.864492: This epoch took 349.347448 s

2022-06-30 00:35:00.867910: 
epoch:  381
2022-06-30 00:40:31.140251: train loss : -0.2953
2022-06-30 00:40:49.769028: validation loss: -0.3884
2022-06-30 00:40:49.773760: Average global foreground Dice: [0.9371, 0.9337, 0.9383, 0.8048, 0.7265, 0.9627, 0.8767, 0.9286, 0.8349, 0.839, 0.724, 0.7417, 0.6988, 0.9041, 0.8825]
2022-06-30 00:40:49.776199: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:40:50.196184: Suus1 maybe_update_lr lr: 0.002727
2022-06-30 00:40:50.198683: This epoch took 349.328098 s

2022-06-30 00:40:50.200907: 
epoch:  382
2022-06-30 00:46:20.316430: train loss : -0.3308
2022-06-30 00:46:38.939959: validation loss: -0.3853
2022-06-30 00:46:38.944054: Average global foreground Dice: [0.9558, 0.9405, 0.8246, 0.8043, 0.7319, 0.9469, 0.8618, 0.9309, 0.863, 0.8336, 0.7138, 0.6663, 0.6927, 0.8967, 0.8118]
2022-06-30 00:46:38.946354: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:46:39.379125: Suus1 maybe_update_lr lr: 0.002706
2022-06-30 00:46:39.382622: This epoch took 349.179601 s

2022-06-30 00:46:39.384970: 
epoch:  383
2022-06-30 00:52:09.524501: train loss : -0.3154
2022-06-30 00:52:28.139245: validation loss: -0.3442
2022-06-30 00:52:28.144414: Average global foreground Dice: [0.8802, 0.9091, 0.8766, 0.8589, 0.712, 0.9298, 0.8622, 0.9227, 0.8671, 0.8586, 0.7284, 0.6983, 0.7156, 0.9087, 0.7381]
2022-06-30 00:52:28.146808: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:52:28.574360: Suus1 maybe_update_lr lr: 0.002685
2022-06-30 00:52:28.577351: This epoch took 349.189538 s

2022-06-30 00:52:28.579552: 
epoch:  384
2022-06-30 00:57:58.719893: train loss : -0.3158
2022-06-30 00:58:17.340069: validation loss: -0.3630
2022-06-30 00:58:17.345695: Average global foreground Dice: [0.9037, 0.9225, 0.7945, 0.7676, 0.689, 0.9457, 0.8797, 0.9192, 0.8315, 0.8091, 0.6844, 0.6669, 0.7008, 0.8715, 0.8456]
2022-06-30 00:58:17.348119: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 00:58:17.768897: Suus1 maybe_update_lr lr: 0.002664
2022-06-30 00:58:17.771861: This epoch took 349.190052 s

2022-06-30 00:58:17.774213: 
epoch:  385
2022-06-30 01:03:47.905135: train loss : -0.3343
2022-06-30 01:04:06.534260: validation loss: -0.3731
2022-06-30 01:04:06.538323: Average global foreground Dice: [0.9385, 0.9415, 0.914, 0.7113, 0.6783, 0.9611, 0.8726, 0.9364, 0.8686, 0.835, 0.7206, 0.7137, 0.6896, 0.8566, 0.735]
2022-06-30 01:04:06.540882: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-06-30 01:04:06.964471: Suus1 maybe_update_lr lr: 0.002643
2022-06-30 01:04:06.973714: This epoch took 349.197132 s

2022-06-30 01:04:06.975873: 
epoch:  386
2022-06-30 01:09:37.055527: train loss : -0.3106
2022-06-30 01:09:55.674489: validation loss: -0.3677
2022-06-30 01:09:55.678474: Average global foreground Dice: [0.9095, 0.9255, 0.9288, 0.7658, 0.7058, 0.9335, 0.7765, 0.9194, 0.8359, 0.8306, 0.7012, 0.6952, 0.7211, 0.919, 0.8645]
2022-06-30 01:09:55.680797: 