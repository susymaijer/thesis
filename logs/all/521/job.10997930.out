Starting at Sun Jul 24 23:21:42 CEST 2022
Running on hosts: res-hpc-lkeb08
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 6.
Account: div2-lkeb
Job ID: 10997930
Job name: PancreasAll
Node running script: res-hpc-lkeb08
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Wed Jul 27 18:48:20 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A6000    On   | 00000000:D8:00.0 Off |                  Off |
| 30%   45C    P0    87W / 300W |      0MiB / 48685MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-0cvl7kmz/hiddenlayer_86dc036aecbc41e1bbc9e3afdf6e17f4
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Using legacy 'setup.py install' for hiddenlayer, since package 'wheel' is not installed.
Installing collected packages: hiddenlayer
    Running setup.py install for hiddenlayer: started
    Running setup.py install for hiddenlayer: finished with status 'done'
Successfully installed hiddenlayer-0.2
Start preprocessing..
Done preprocessing! Start training all the folds..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='521', fold='3', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-27 18:48:40.338579: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-27 18:48:40.419734: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-27 18:48:40.424251: The split file contains 5 splits.
2022-07-27 18:48:40.426106: Desired fold for training: 3
2022-07-27 18:48:40.427908: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-27 18:48:42.485987: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/model_latest.model train= True
SuusB run_training - zet learning rate als  
2022-07-27 18:49:12.532278: Suus1 maybe_update_lr lr: 2.3e-05
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-27 18:49:27.654635: Unable to plot network architecture:
2022-07-27 18:49:27.658053: local variable 'g' referenced before assignment
2022-07-27 18:49:27.660108: 
printing the network instead:

2022-07-27 18:49:27.662185: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-27 18:49:27.670983: 

2022-07-27 18:49:27.673090: 
epoch:  400
2022-07-27 18:50:47.809290: train loss : -0.8845
2022-07-27 18:50:53.521990: validation loss: -0.9067
2022-07-27 18:50:53.531361: Average global foreground Dice: [0.9517]
2022-07-27 18:50:53.553142: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 18:50:54.509677: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-27 18:50:54.512221: saving best epoch checkpoint...
2022-07-27 18:50:54.699775: saving checkpoint...
2022-07-27 18:50:59.512057: done, saving took 5.00 seconds
2022-07-27 18:50:59.520979: This epoch took 91.846021 s

2022-07-27 18:50:59.523417: 
epoch:  401
2022-07-27 18:52:08.089493: train loss : -0.8966
2022-07-27 18:52:13.670203: validation loss: -0.9061
2022-07-27 18:52:13.704158: Average global foreground Dice: [0.9439]
2022-07-27 18:52:13.735994: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 18:52:14.711776: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-27 18:52:14.714243: This epoch took 75.188356 s

2022-07-27 18:52:14.716204: 
epoch:  402
2022-07-27 18:53:24.495585: train loss : -0.8981
2022-07-27 18:53:30.719524: validation loss: -0.8787
2022-07-27 18:53:30.733646: Average global foreground Dice: [0.9216]
2022-07-27 18:53:30.759006: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 18:53:31.641426: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-27 18:53:31.644134: This epoch took 76.925744 s

2022-07-27 18:53:31.646187: 
epoch:  403
2022-07-27 18:54:42.262462: train loss : -0.8953
2022-07-27 18:54:48.580934: validation loss: -0.8831
2022-07-27 18:54:48.600696: Average global foreground Dice: [0.9267]
2022-07-27 18:54:48.611683: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 18:54:49.472736: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-27 18:54:49.475134: This epoch took 77.826779 s

2022-07-27 18:54:49.477412: 
epoch:  404
2022-07-27 18:55:59.585151: train loss : -0.8811
2022-07-27 18:56:06.136427: validation loss: -0.8794
2022-07-27 18:56:06.149670: Average global foreground Dice: [0.9237]
2022-07-27 18:56:06.171994: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 18:56:07.072749: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-27 18:56:07.075565: This epoch took 77.595768 s

2022-07-27 18:56:07.077916: 
epoch:  405
2022-07-27 18:57:17.477258: train loss : -0.8959
2022-07-27 18:57:23.596077: validation loss: -0.8379
2022-07-27 18:57:23.599923: Average global foreground Dice: [0.8719]
2022-07-27 18:57:23.602134: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 18:57:24.273452: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-27 18:57:24.275882: This epoch took 77.195382 s

2022-07-27 18:57:24.277985: 
epoch:  406
2022-07-27 18:58:34.033754: train loss : -0.8783
2022-07-27 18:58:39.645125: validation loss: -0.8920
2022-07-27 18:58:39.658968: Average global foreground Dice: [0.9372]
2022-07-27 18:58:39.671140: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 18:58:40.472066: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-27 18:58:40.474497: This epoch took 76.194556 s

2022-07-27 18:58:40.476606: 
epoch:  407
2022-07-27 18:59:50.565998: train loss : -0.8895
2022-07-27 18:59:55.981653: validation loss: -0.8707
2022-07-27 18:59:55.985934: Average global foreground Dice: [0.9302]
2022-07-27 18:59:55.988548: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 18:59:56.855976: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-27 18:59:56.874095: This epoch took 76.395399 s

2022-07-27 18:59:56.883415: 
epoch:  408
2022-07-27 19:01:05.986484: train loss : -0.8881
2022-07-27 19:01:11.766853: validation loss: -0.9053
2022-07-27 19:01:11.786485: Average global foreground Dice: [0.9476]
2022-07-27 19:01:11.788936: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:01:12.524843: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-27 19:01:12.527106: This epoch took 75.640375 s

2022-07-27 19:01:12.529144: 
epoch:  409
2022-07-27 19:02:23.287769: train loss : -0.8888
2022-07-27 19:02:29.312771: validation loss: -0.8956
2022-07-27 19:02:29.316986: Average global foreground Dice: [0.9411]
2022-07-27 19:02:29.319470: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:02:30.306635: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-27 19:02:30.309496: This epoch took 77.778217 s

2022-07-27 19:02:30.311964: 
epoch:  410
2022-07-27 19:03:40.610721: train loss : -0.8831
2022-07-27 19:03:46.609413: validation loss: -0.8870
2022-07-27 19:03:46.634444: Average global foreground Dice: [0.9341]
2022-07-27 19:03:46.655965: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:03:47.408114: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-27 19:03:47.410506: This epoch took 77.096210 s

2022-07-27 19:03:47.412769: 
epoch:  411
2022-07-27 19:04:56.173450: train loss : -0.8942
2022-07-27 19:05:01.995622: validation loss: -0.9052
2022-07-27 19:05:02.002367: Average global foreground Dice: [0.9461]
2022-07-27 19:05:02.030989: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:05:02.760785: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-27 19:05:02.763160: This epoch took 75.348261 s

2022-07-27 19:05:02.765304: 
epoch:  412
2022-07-27 19:06:11.697333: train loss : -0.9066
2022-07-27 19:06:17.340165: validation loss: -0.8897
2022-07-27 19:06:17.344026: Average global foreground Dice: [0.9377]
2022-07-27 19:06:17.346568: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:06:17.955174: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-27 19:06:17.957660: This epoch took 75.190376 s

2022-07-27 19:06:17.959667: 
epoch:  413
2022-07-27 19:07:27.957712: train loss : -0.8910
2022-07-27 19:07:33.801050: validation loss: -0.8954
2022-07-27 19:07:33.832719: Average global foreground Dice: [0.9369]
2022-07-27 19:07:33.852305: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:07:34.597136: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-27 19:07:34.599597: This epoch took 76.637940 s

2022-07-27 19:07:34.601830: 
epoch:  414
2022-07-27 19:08:43.790723: train loss : -0.9020
2022-07-27 19:08:49.216938: validation loss: -0.9094
2022-07-27 19:08:49.222313: Average global foreground Dice: [0.9484]
2022-07-27 19:08:49.233407: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:08:49.840829: Suus1 maybe_update_lr lr: 2e-05
2022-07-27 19:08:49.843064: This epoch took 75.238529 s

2022-07-27 19:08:49.845081: 
epoch:  415
2022-07-27 19:09:59.768403: train loss : -0.8783
2022-07-27 19:10:06.294998: validation loss: -0.8537
2022-07-27 19:10:06.319021: Average global foreground Dice: [0.8892]
2022-07-27 19:10:06.332805: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:10:07.219834: Suus1 maybe_update_lr lr: 2e-05
2022-07-27 19:10:07.222499: This epoch took 77.376120 s

2022-07-27 19:10:07.224600: 
epoch:  416
2022-07-27 19:11:16.675715: train loss : -0.8980
2022-07-27 19:11:22.228487: validation loss: -0.8966
2022-07-27 19:11:22.254439: Average global foreground Dice: [0.9298]
2022-07-27 19:11:22.256749: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:11:23.145645: Suus1 maybe_update_lr lr: 2e-05
2022-07-27 19:11:23.151351: This epoch took 75.924689 s

2022-07-27 19:11:23.153489: 
epoch:  417
2022-07-27 19:12:32.764557: train loss : -0.8928
2022-07-27 19:12:39.275898: validation loss: -0.8736
2022-07-27 19:12:39.291780: Average global foreground Dice: [0.9122]
2022-07-27 19:12:39.314020: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:12:40.234218: Suus1 maybe_update_lr lr: 2e-05
2022-07-27 19:12:40.236612: This epoch took 77.080996 s

2022-07-27 19:12:40.238725: 
epoch:  418
2022-07-27 19:13:50.859842: train loss : -0.8988
2022-07-27 19:13:56.721573: validation loss: -0.8723
2022-07-27 19:13:56.725266: Average global foreground Dice: [0.9132]
2022-07-27 19:13:56.729010: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:13:57.287485: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-27 19:13:57.289661: This epoch took 77.048888 s

2022-07-27 19:13:57.291623: 
epoch:  419
2022-07-27 19:15:06.782013: train loss : -0.8955
2022-07-27 19:15:12.500818: validation loss: -0.8777
2022-07-27 19:15:12.510039: Average global foreground Dice: [0.9221]
2022-07-27 19:15:12.546006: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:15:13.453925: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-27 19:15:13.456584: This epoch took 76.163020 s

2022-07-27 19:15:13.458980: 
epoch:  420
2022-07-27 19:16:22.897365: train loss : -0.8993
2022-07-27 19:16:28.621795: validation loss: -0.8435
2022-07-27 19:16:28.653585: Average global foreground Dice: [0.884]
2022-07-27 19:16:28.656404: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:16:29.342152: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-27 19:16:29.344454: This epoch took 75.883611 s

2022-07-27 19:16:29.346646: 
epoch:  421
2022-07-27 19:17:39.865206: train loss : -0.8752
2022-07-27 19:17:45.421786: validation loss: -0.8428
2022-07-27 19:17:45.462734: Average global foreground Dice: [0.8875]
2022-07-27 19:17:45.475824: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:17:46.457016: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-27 19:17:46.459876: This epoch took 77.111279 s

2022-07-27 19:17:46.462139: 
epoch:  422
2022-07-27 19:18:56.057163: train loss : -0.9020
2022-07-27 19:19:01.569687: validation loss: -0.8964
2022-07-27 19:19:01.573541: Average global foreground Dice: [0.9383]
2022-07-27 19:19:01.601073: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:19:02.561555: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-27 19:19:02.563976: This epoch took 76.099424 s

2022-07-27 19:19:02.566046: 
epoch:  423
2022-07-27 19:20:12.428658: train loss : -0.9071
2022-07-27 19:20:18.588423: validation loss: -0.8798
2022-07-27 19:20:18.629025: Average global foreground Dice: [0.934]
2022-07-27 19:20:18.659118: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:20:19.364977: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-27 19:20:19.367177: This epoch took 76.799147 s

2022-07-27 19:20:19.369322: 
epoch:  424
2022-07-27 19:21:29.064047: train loss : -0.9028
2022-07-27 19:21:34.556698: validation loss: -0.8803
2022-07-27 19:21:34.575831: Average global foreground Dice: [0.9217]
2022-07-27 19:21:34.589996: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:21:35.356250: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-27 19:21:35.358771: This epoch took 75.987214 s

2022-07-27 19:21:35.360963: 
epoch:  425
2022-07-27 19:22:47.033058: train loss : -0.9023
2022-07-27 19:22:52.670277: validation loss: -0.8991
2022-07-27 19:22:52.691309: Average global foreground Dice: [0.9441]
2022-07-27 19:22:52.693579: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:22:53.465389: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-27 19:22:53.468330: This epoch took 78.104895 s

2022-07-27 19:22:53.471204: 
epoch:  426
2022-07-27 19:24:04.214107: train loss : -0.8888
2022-07-27 19:24:10.214519: validation loss: -0.9027
2022-07-27 19:24:10.246709: Average global foreground Dice: [0.9465]
2022-07-27 19:24:10.249664: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:24:11.148688: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-27 19:24:11.151157: This epoch took 77.676477 s

2022-07-27 19:24:11.153673: 
epoch:  427
2022-07-27 19:25:21.142396: train loss : -0.9000
2022-07-27 19:25:26.860798: validation loss: -0.8692
2022-07-27 19:25:26.898324: Average global foreground Dice: [0.896]
2022-07-27 19:25:26.905018: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:25:27.837309: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-27 19:25:27.839762: This epoch took 76.683914 s

2022-07-27 19:25:27.842042: 
epoch:  428
2022-07-27 19:26:36.653727: train loss : -0.9001
2022-07-27 19:26:42.918347: validation loss: -0.8866
2022-07-27 19:26:42.967877: Average global foreground Dice: [0.915]
2022-07-27 19:26:42.998408: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:26:43.981761: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-27 19:26:43.983931: This epoch took 76.139374 s

2022-07-27 19:26:43.986305: 
epoch:  429
2022-07-27 19:27:54.058493: train loss : -0.9012
2022-07-27 19:27:59.521411: validation loss: -0.9063
2022-07-27 19:27:59.558769: Average global foreground Dice: [0.9494]
2022-07-27 19:27:59.592048: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:28:00.449628: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-27 19:28:00.452477: This epoch took 76.464302 s

2022-07-27 19:28:00.454867: 
epoch:  430
2022-07-27 19:29:09.734585: train loss : -0.8974
2022-07-27 19:29:15.451746: validation loss: -0.8448
2022-07-27 19:29:15.487839: Average global foreground Dice: [0.9029]
2022-07-27 19:29:15.509976: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:29:16.276010: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-27 19:29:16.278190: This epoch took 75.820838 s

2022-07-27 19:29:16.280085: 
epoch:  431
2022-07-27 19:30:26.018306: train loss : -0.8825
2022-07-27 19:30:31.972542: validation loss: -0.8615
2022-07-27 19:30:32.010082: Average global foreground Dice: [0.9106]
2022-07-27 19:30:32.029168: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:30:32.987539: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-27 19:30:32.989718: This epoch took 76.707609 s

2022-07-27 19:30:32.991693: 
epoch:  432
2022-07-27 19:31:41.688700: train loss : -0.9027
2022-07-27 19:31:47.272928: validation loss: -0.9033
2022-07-27 19:31:47.286368: Average global foreground Dice: [0.9496]
2022-07-27 19:31:47.288461: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:31:47.794539: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-27 19:31:47.796718: This epoch took 74.803106 s

2022-07-27 19:31:47.798735: 
epoch:  433
2022-07-27 19:32:57.354715: train loss : -0.8947
2022-07-27 19:33:03.346168: validation loss: -0.9047
2022-07-27 19:33:03.357824: Average global foreground Dice: [0.9478]
2022-07-27 19:33:03.381971: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:33:04.210106: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-27 19:33:04.212270: This epoch took 76.411695 s

2022-07-27 19:33:04.214105: 
epoch:  434
2022-07-27 19:34:13.137081: train loss : -0.8971
2022-07-27 19:34:18.861528: validation loss: -0.8764
2022-07-27 19:34:18.920393: Average global foreground Dice: [0.9048]
2022-07-27 19:34:18.952533: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:34:19.865491: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-27 19:34:19.868224: This epoch took 75.652288 s

2022-07-27 19:34:19.870179: 
epoch:  435
2022-07-27 19:35:29.146155: train loss : -0.9019
2022-07-27 19:35:34.926282: validation loss: -0.8893
2022-07-27 19:35:34.952282: Average global foreground Dice: [0.9346]
2022-07-27 19:35:34.964151: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:35:35.680130: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-27 19:35:35.683205: This epoch took 75.810757 s

2022-07-27 19:35:35.685348: 
epoch:  436
2022-07-27 19:36:45.121528: train loss : -0.8973
2022-07-27 19:36:51.234334: validation loss: -0.8910
2022-07-27 19:36:51.258747: Average global foreground Dice: [0.9215]
2022-07-27 19:36:51.271739: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:36:51.998475: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-27 19:36:52.000976: This epoch took 76.311927 s

2022-07-27 19:36:52.003156: 
epoch:  437
2022-07-27 19:38:03.828863: train loss : -0.9045
2022-07-27 19:38:09.508307: validation loss: -0.9169
2022-07-27 19:38:09.513534: Average global foreground Dice: [0.9546]
2022-07-27 19:38:09.539368: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:38:10.444769: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-27 19:38:10.447058: This epoch took 78.441250 s

2022-07-27 19:38:10.448585: 
epoch:  438
2022-07-27 19:39:19.632056: train loss : -0.8951
2022-07-27 19:39:25.367671: validation loss: -0.8673
2022-07-27 19:39:25.394185: Average global foreground Dice: [0.887]
2022-07-27 19:39:25.406091: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:39:26.267848: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-27 19:39:26.270001: This epoch took 75.818998 s

2022-07-27 19:39:26.271908: 
epoch:  439
2022-07-27 19:40:35.365806: train loss : -0.9037
2022-07-27 19:40:41.068223: validation loss: -0.8385
2022-07-27 19:40:41.085464: Average global foreground Dice: [0.8503]
2022-07-27 19:40:41.090246: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:40:41.938983: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-27 19:40:41.950665: This epoch took 75.676907 s

2022-07-27 19:40:41.954914: 
epoch:  440
2022-07-27 19:41:50.446355: train loss : -0.8992
2022-07-27 19:41:55.992391: validation loss: -0.9018
2022-07-27 19:41:56.015815: Average global foreground Dice: [0.9424]
2022-07-27 19:41:56.047982: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:41:56.676388: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-27 19:41:56.679030: This epoch took 74.715060 s

2022-07-27 19:41:56.681358: 
epoch:  441
2022-07-27 19:43:05.924035: train loss : -0.9034
2022-07-27 19:43:11.615640: validation loss: -0.9015
2022-07-27 19:43:11.663200: Average global foreground Dice: [0.9459]
2022-07-27 19:43:11.671872: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:43:12.572215: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-27 19:43:12.575142: This epoch took 75.891615 s

2022-07-27 19:43:12.577750: 
epoch:  442
2022-07-27 19:44:21.256300: train loss : -0.9000
2022-07-27 19:44:27.261173: validation loss: -0.9020
2022-07-27 19:44:27.270123: Average global foreground Dice: [0.9462]
2022-07-27 19:44:27.287832: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:44:28.154572: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-27 19:44:28.183917: This epoch took 75.603740 s

2022-07-27 19:44:28.186327: 
epoch:  443
2022-07-27 19:45:38.067637: train loss : -0.8989
2022-07-27 19:45:43.489910: validation loss: -0.9071
2022-07-27 19:45:43.517377: Average global foreground Dice: [0.9427]
2022-07-27 19:45:43.521819: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:45:44.379028: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-27 19:45:44.382226: This epoch took 76.193364 s

2022-07-27 19:45:44.384618: 
epoch:  444
2022-07-27 19:46:53.351863: train loss : -0.9044
2022-07-27 19:46:58.857259: validation loss: -0.8955
2022-07-27 19:46:58.885689: Average global foreground Dice: [0.9396]
2022-07-27 19:46:58.915981: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:46:59.825261: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-27 19:46:59.827692: This epoch took 75.440005 s

2022-07-27 19:46:59.830204: 
epoch:  445
2022-07-27 19:48:10.014970: train loss : -0.8915
2022-07-27 19:48:15.827213: validation loss: -0.8889
2022-07-27 19:48:15.860466: Average global foreground Dice: [0.9353]
2022-07-27 19:48:15.880038: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:48:16.841730: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-27 19:48:16.843921: This epoch took 77.006238 s

2022-07-27 19:48:16.845803: 
epoch:  446
2022-07-27 19:49:27.075181: train loss : -0.8936
2022-07-27 19:49:33.157230: validation loss: -0.8868
2022-07-27 19:49:33.200640: Average global foreground Dice: [0.9253]
2022-07-27 19:49:33.220286: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:49:34.177260: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-27 19:49:34.179912: This epoch took 77.332054 s

2022-07-27 19:49:34.182090: 
epoch:  447
2022-07-27 19:50:43.378805: train loss : -0.8933
2022-07-27 19:50:49.518829: validation loss: -0.9082
2022-07-27 19:50:49.543678: Average global foreground Dice: [0.9454]
2022-07-27 19:50:49.562804: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:50:50.419774: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-27 19:50:50.422628: This epoch took 76.238467 s

2022-07-27 19:50:50.425039: 
epoch:  448
2022-07-27 19:52:00.626057: train loss : -0.9118
2022-07-27 19:52:06.203939: validation loss: -0.8604
2022-07-27 19:52:06.215003: Average global foreground Dice: [0.8907]
2022-07-27 19:52:06.251034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:52:07.214228: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-27 19:52:07.216766: This epoch took 76.789527 s

2022-07-27 19:52:07.219087: 
epoch:  449
2022-07-27 19:53:16.957731: train loss : -0.9051
2022-07-27 19:53:22.783003: validation loss: -0.9121
2022-07-27 19:53:22.817467: Average global foreground Dice: [0.9508]
2022-07-27 19:53:22.837716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:53:23.680644: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-27 19:53:23.682869: saving scheduled checkpoint file...
2022-07-27 19:53:23.884246: saving checkpoint...
2022-07-27 19:53:28.913682: done, saving took 5.23 seconds
2022-07-27 19:53:28.931206: done
2022-07-27 19:53:28.934067: This epoch took 81.712836 s

2022-07-27 19:53:28.936415: 
epoch:  450
2022-07-27 19:54:41.233681: train loss : -0.9087
2022-07-27 19:54:47.237230: validation loss: -0.9054
2022-07-27 19:54:47.248445: Average global foreground Dice: [0.946]
2022-07-27 19:54:47.255644: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:54:48.073125: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-27 19:54:48.075648: This epoch took 79.137195 s

2022-07-27 19:54:48.077523: 
epoch:  451
2022-07-27 19:55:57.689935: train loss : -0.9058
2022-07-27 19:56:03.235574: validation loss: -0.8536
2022-07-27 19:56:03.271915: Average global foreground Dice: [0.8924]
2022-07-27 19:56:03.293090: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:56:04.266876: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-27 19:56:04.269251: This epoch took 76.190003 s

2022-07-27 19:56:04.272278: 
epoch:  452
2022-07-27 19:57:13.778133: train loss : -0.9018
2022-07-27 19:57:19.738622: validation loss: -0.8956
2022-07-27 19:57:19.772214: Average global foreground Dice: [0.9382]
2022-07-27 19:57:19.791658: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:57:20.628253: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-27 19:57:20.630443: This epoch took 76.355598 s

2022-07-27 19:57:20.632781: 
epoch:  453
2022-07-27 19:58:29.076598: train loss : -0.8970
2022-07-27 19:58:34.674196: validation loss: -0.8876
2022-07-27 19:58:34.704620: Average global foreground Dice: [0.9261]
2022-07-27 19:58:34.719040: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:58:36.019510: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-27 19:58:36.021683: This epoch took 75.386733 s

2022-07-27 19:58:36.023790: 
epoch:  454
2022-07-27 19:59:46.362324: train loss : -0.9017
2022-07-27 19:59:52.463711: validation loss: -0.9038
2022-07-27 19:59:52.469063: Average global foreground Dice: [0.94]
2022-07-27 19:59:52.471976: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 19:59:53.405756: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-27 19:59:53.408612: This epoch took 77.382729 s

2022-07-27 19:59:53.410955: 
epoch:  455
2022-07-27 20:01:03.231244: train loss : -0.9044
2022-07-27 20:01:09.205030: validation loss: -0.8860
2022-07-27 20:01:09.242129: Average global foreground Dice: [0.9325]
2022-07-27 20:01:09.266007: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:01:10.112770: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-27 20:01:10.115206: This epoch took 76.701805 s

2022-07-27 20:01:10.117183: 
epoch:  456
2022-07-27 20:02:20.524415: train loss : -0.9102
2022-07-27 20:02:26.523183: validation loss: -0.8797
2022-07-27 20:02:26.544496: Average global foreground Dice: [0.9083]
2022-07-27 20:02:26.579427: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:02:27.404666: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-27 20:02:27.406911: This epoch took 77.287660 s

2022-07-27 20:02:27.408893: 
epoch:  457
2022-07-27 20:03:36.651612: train loss : -0.9066
2022-07-27 20:03:42.680094: validation loss: -0.9025
2022-07-27 20:03:42.714449: Average global foreground Dice: [0.9465]
2022-07-27 20:03:42.729463: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:03:43.613219: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-27 20:03:43.615358: This epoch took 76.204506 s

2022-07-27 20:03:43.617398: 
epoch:  458
2022-07-27 20:04:52.788063: train loss : -0.9063
2022-07-27 20:04:58.605145: validation loss: -0.8424
2022-07-27 20:04:58.641813: Average global foreground Dice: [0.8928]
2022-07-27 20:04:58.649846: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:04:59.537714: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-27 20:04:59.540379: This epoch took 75.921106 s

2022-07-27 20:04:59.542512: 
epoch:  459
2022-07-27 20:06:09.468477: train loss : -0.9084
2022-07-27 20:06:15.324144: validation loss: -0.8817
2022-07-27 20:06:15.327623: Average global foreground Dice: [0.9276]
2022-07-27 20:06:15.329803: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:06:16.022496: Suus1 maybe_update_lr lr: 1e-05
2022-07-27 20:06:16.024821: This epoch took 76.480216 s

2022-07-27 20:06:16.027029: 
epoch:  460
2022-07-27 20:07:25.902245: train loss : -0.9054
2022-07-27 20:07:31.647968: validation loss: -0.9102
2022-07-27 20:07:31.681094: Average global foreground Dice: [0.9492]
2022-07-27 20:07:31.696990: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:07:32.437085: Suus1 maybe_update_lr lr: 1e-05
2022-07-27 20:07:32.439761: This epoch took 76.410510 s

2022-07-27 20:07:32.441902: 
epoch:  461
2022-07-27 20:08:42.822677: train loss : -0.9031
2022-07-27 20:08:48.628992: validation loss: -0.9072
2022-07-27 20:08:48.646364: Average global foreground Dice: [0.9473]
2022-07-27 20:08:48.659559: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:08:49.364061: Suus1 maybe_update_lr lr: 1e-05
2022-07-27 20:08:49.366399: This epoch took 76.922394 s

2022-07-27 20:08:49.368485: 
epoch:  462
2022-07-27 20:09:58.955518: train loss : -0.8987
2022-07-27 20:10:05.239503: validation loss: -0.9158
2022-07-27 20:10:05.262932: Average global foreground Dice: [0.9513]
2022-07-27 20:10:05.294986: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:10:06.398405: Suus1 maybe_update_lr lr: 1e-05
2022-07-27 20:10:06.400720: This epoch took 77.030242 s

2022-07-27 20:10:06.402759: 
epoch:  463
2022-07-27 20:11:16.696834: train loss : -0.9044
2022-07-27 20:11:22.293022: validation loss: -0.9097
2022-07-27 20:11:22.327408: Average global foreground Dice: [0.9501]
2022-07-27 20:11:22.330790: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:11:23.198733: Suus1 maybe_update_lr lr: 9e-06
2022-07-27 20:11:23.201089: This epoch took 76.796122 s

2022-07-27 20:11:23.203110: 
epoch:  464
2022-07-27 20:12:32.748817: train loss : -0.9048
2022-07-27 20:12:38.490882: validation loss: -0.8912
2022-07-27 20:12:38.507333: Average global foreground Dice: [0.9307]
2022-07-27 20:12:38.519011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:12:39.386171: Suus1 maybe_update_lr lr: 9e-06
2022-07-27 20:12:39.388760: This epoch took 76.182814 s

2022-07-27 20:12:39.390930: 
epoch:  465
2022-07-27 20:13:51.184779: train loss : -0.9068
2022-07-27 20:13:57.458622: validation loss: -0.8920
2022-07-27 20:13:57.508605: Average global foreground Dice: [0.9396]
2022-07-27 20:13:57.522031: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:13:58.424942: Suus1 maybe_update_lr lr: 9e-06
2022-07-27 20:13:58.427438: This epoch took 79.034209 s

2022-07-27 20:13:58.429670: 
epoch:  466
2022-07-27 20:15:08.863100: train loss : -0.9054
2022-07-27 20:15:14.960501: validation loss: -0.8608
2022-07-27 20:15:14.983039: Average global foreground Dice: [0.8942]
2022-07-27 20:15:15.001345: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:15:15.831615: Suus1 maybe_update_lr lr: 9e-06
2022-07-27 20:15:15.834130: This epoch took 77.402216 s

2022-07-27 20:15:15.836224: 
epoch:  467
2022-07-27 20:16:24.853486: train loss : -0.9088
2022-07-27 20:16:30.568331: validation loss: -0.8594
2022-07-27 20:16:30.580559: Average global foreground Dice: [0.8933]
2022-07-27 20:16:30.582950: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:16:31.483282: Suus1 maybe_update_lr lr: 8e-06
2022-07-27 20:16:31.486037: This epoch took 75.647745 s

2022-07-27 20:16:31.488447: 
epoch:  468
2022-07-27 20:17:41.605345: train loss : -0.9074
2022-07-27 20:17:47.191706: validation loss: -0.8805
2022-07-27 20:17:47.213538: Average global foreground Dice: [0.9294]
2022-07-27 20:17:47.225444: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:17:47.920193: Suus1 maybe_update_lr lr: 8e-06
2022-07-27 20:17:47.922716: This epoch took 76.431604 s

2022-07-27 20:17:47.925023: 
epoch:  469
2022-07-27 20:18:58.592601: train loss : -0.9038
2022-07-27 20:19:04.436229: validation loss: -0.8772
2022-07-27 20:19:04.450259: Average global foreground Dice: [0.9294]
2022-07-27 20:19:04.476823: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:19:05.377664: Suus1 maybe_update_lr lr: 8e-06
2022-07-27 20:19:05.380197: This epoch took 77.452948 s

2022-07-27 20:19:05.382247: 
epoch:  470
2022-07-27 20:20:15.766442: train loss : -0.9018
2022-07-27 20:20:21.560403: validation loss: -0.8770
2022-07-27 20:20:21.585457: Average global foreground Dice: [0.9095]
2022-07-27 20:20:21.588573: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:20:22.365627: Suus1 maybe_update_lr lr: 8e-06
2022-07-27 20:20:22.368091: This epoch took 76.984335 s

2022-07-27 20:20:22.370244: 
epoch:  471
2022-07-27 20:21:31.271544: train loss : -0.9058
2022-07-27 20:21:37.249464: validation loss: -0.8882
2022-07-27 20:21:37.261419: Average global foreground Dice: [0.9351]
2022-07-27 20:21:37.313179: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:21:38.168029: Suus1 maybe_update_lr lr: 7e-06
2022-07-27 20:21:38.170120: This epoch took 75.797821 s

2022-07-27 20:21:38.172343: 
epoch:  472
2022-07-27 20:22:49.267663: train loss : -0.8991
2022-07-27 20:22:54.936260: validation loss: -0.8870
2022-07-27 20:22:54.971086: Average global foreground Dice: [0.93]
2022-07-27 20:22:54.992168: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:22:55.736574: Suus1 maybe_update_lr lr: 7e-06
2022-07-27 20:22:55.739008: This epoch took 77.563332 s

2022-07-27 20:22:55.741227: 
epoch:  473
2022-07-27 20:24:05.953801: train loss : -0.9019
2022-07-27 20:24:11.915442: validation loss: -0.8793
2022-07-27 20:24:11.928773: Average global foreground Dice: [0.9247]
2022-07-27 20:24:11.933170: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:24:12.907574: Suus1 maybe_update_lr lr: 7e-06
2022-07-27 20:24:12.929090: This epoch took 77.185561 s

2022-07-27 20:24:12.931533: 
epoch:  474
2022-07-27 20:25:22.152233: train loss : -0.9140
2022-07-27 20:25:28.197175: validation loss: -0.8656
2022-07-27 20:25:28.227629: Average global foreground Dice: [0.9066]
2022-07-27 20:25:28.243210: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:25:29.121842: Suus1 maybe_update_lr lr: 7e-06
2022-07-27 20:25:29.124615: This epoch took 76.190481 s

2022-07-27 20:25:29.127233: 
epoch:  475
2022-07-27 20:26:39.634211: train loss : -0.9104
2022-07-27 20:26:45.162088: validation loss: -0.8797
2022-07-27 20:26:45.225455: Average global foreground Dice: [0.9185]
2022-07-27 20:26:45.242606: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:26:46.105620: Suus1 maybe_update_lr lr: 7e-06
2022-07-27 20:26:46.108346: This epoch took 76.978657 s

2022-07-27 20:26:46.111292: 
epoch:  476
2022-07-27 20:27:56.901949: train loss : -0.9100
2022-07-27 20:28:02.602789: validation loss: -0.8781
2022-07-27 20:28:02.606951: Average global foreground Dice: [0.9192]
2022-07-27 20:28:02.612733: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:28:03.588881: Suus1 maybe_update_lr lr: 6e-06
2022-07-27 20:28:03.592592: This epoch took 77.478889 s

2022-07-27 20:28:03.595435: 
epoch:  477
2022-07-27 20:29:13.139221: train loss : -0.8977
2022-07-27 20:29:18.831879: validation loss: -0.8489
2022-07-27 20:29:18.844206: Average global foreground Dice: [0.8723]
2022-07-27 20:29:18.848392: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:29:19.852630: Suus1 maybe_update_lr lr: 6e-06
2022-07-27 20:29:19.855905: This epoch took 76.257514 s

2022-07-27 20:29:19.858264: 
epoch:  478
2022-07-27 20:30:29.865379: train loss : -0.9100
2022-07-27 20:30:36.019284: validation loss: -0.8942
2022-07-27 20:30:36.036125: Average global foreground Dice: [0.9392]
2022-07-27 20:30:36.057378: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:30:36.885556: Suus1 maybe_update_lr lr: 6e-06
2022-07-27 20:30:36.888338: This epoch took 77.027629 s

2022-07-27 20:30:36.892042: 
epoch:  479
2022-07-27 20:31:46.845063: train loss : -0.9079
2022-07-27 20:31:52.734208: validation loss: -0.8773
2022-07-27 20:31:52.757415: Average global foreground Dice: [0.9045]
2022-07-27 20:31:52.768422: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:31:53.663036: Suus1 maybe_update_lr lr: 6e-06
2022-07-27 20:31:53.665516: This epoch took 76.771281 s

2022-07-27 20:31:53.667767: 
epoch:  480
2022-07-27 20:33:03.220170: train loss : -0.8959
2022-07-27 20:33:08.979431: validation loss: -0.8583
2022-07-27 20:33:08.985200: Average global foreground Dice: [0.8957]
2022-07-27 20:33:08.995970: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:33:09.838817: Suus1 maybe_update_lr lr: 5e-06
2022-07-27 20:33:09.841923: This epoch took 76.171936 s

2022-07-27 20:33:09.844028: 
epoch:  481
2022-07-27 20:34:19.097698: train loss : -0.9091
2022-07-27 20:34:24.848187: validation loss: -0.8964
2022-07-27 20:34:24.882540: Average global foreground Dice: [0.9444]
2022-07-27 20:34:24.902007: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:34:25.889604: Suus1 maybe_update_lr lr: 5e-06
2022-07-27 20:34:25.916148: This epoch took 76.069958 s

2022-07-27 20:34:25.938009: 
epoch:  482
2022-07-27 20:35:37.621557: train loss : -0.9109
2022-07-27 20:35:43.477413: validation loss: -0.8725
2022-07-27 20:35:43.509044: Average global foreground Dice: [0.9128]
2022-07-27 20:35:43.528992: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:35:44.492459: Suus1 maybe_update_lr lr: 5e-06
2022-07-27 20:35:44.512499: This epoch took 78.555489 s

2022-07-27 20:35:44.526234: 
epoch:  483
2022-07-27 20:36:54.421061: train loss : -0.9082
2022-07-27 20:37:00.333582: validation loss: -0.8773
2022-07-27 20:37:00.361830: Average global foreground Dice: [0.9202]
2022-07-27 20:37:00.364644: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:37:01.056664: Suus1 maybe_update_lr lr: 5e-06
2022-07-27 20:37:01.059596: This epoch took 76.507560 s

2022-07-27 20:37:01.062281: 
epoch:  484
2022-07-27 20:38:09.814184: train loss : -0.9047
2022-07-27 20:38:15.388521: validation loss: -0.9020
2022-07-27 20:38:15.404117: Average global foreground Dice: [0.9434]
2022-07-27 20:38:15.414455: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:38:16.398237: Suus1 maybe_update_lr lr: 4e-06
2022-07-27 20:38:16.400688: This epoch took 75.335680 s

2022-07-27 20:38:16.403937: 
epoch:  485
2022-07-27 20:39:27.301033: train loss : -0.9129
2022-07-27 20:39:33.367321: validation loss: -0.8830
2022-07-27 20:39:33.384710: Average global foreground Dice: [0.9333]
2022-07-27 20:39:33.411784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:39:34.430038: Suus1 maybe_update_lr lr: 4e-06
2022-07-27 20:39:34.432684: This epoch took 78.026479 s

2022-07-27 20:39:34.435159: 
epoch:  486
2022-07-27 20:40:44.010367: train loss : -0.9090
2022-07-27 20:40:49.896637: validation loss: -0.8523
2022-07-27 20:40:49.903229: Average global foreground Dice: [0.8896]
2022-07-27 20:40:49.917016: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:40:50.920614: Suus1 maybe_update_lr lr: 4e-06
2022-07-27 20:40:50.923345: This epoch took 76.485317 s

2022-07-27 20:40:50.925842: 
epoch:  487
2022-07-27 20:42:00.303562: train loss : -0.9129
2022-07-27 20:42:05.798877: validation loss: -0.8777
2022-07-27 20:42:05.835629: Average global foreground Dice: [0.9041]
2022-07-27 20:42:05.867973: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:42:06.620165: Suus1 maybe_update_lr lr: 3e-06
2022-07-27 20:42:06.622428: This epoch took 75.694224 s

2022-07-27 20:42:06.624979: 
epoch:  488
2022-07-27 20:43:16.995343: train loss : -0.9020
2022-07-27 20:43:22.980093: validation loss: -0.8705
2022-07-27 20:43:23.022030: Average global foreground Dice: [0.9141]
2022-07-27 20:43:23.041368: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:43:24.017144: Suus1 maybe_update_lr lr: 3e-06
2022-07-27 20:43:24.019740: This epoch took 77.392225 s

2022-07-27 20:43:24.022688: 
epoch:  489
2022-07-27 20:44:32.691799: train loss : -0.9186
2022-07-27 20:44:38.240207: validation loss: -0.9006
2022-07-27 20:44:38.247739: Average global foreground Dice: [0.9394]
2022-07-27 20:44:38.272991: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:44:39.179550: Suus1 maybe_update_lr lr: 3e-06
2022-07-27 20:44:39.182572: This epoch took 75.157446 s

2022-07-27 20:44:39.185194: 
epoch:  490
2022-07-27 20:45:48.610059: train loss : -0.9087
2022-07-27 20:45:54.309245: validation loss: -0.8699
2022-07-27 20:45:54.325530: Average global foreground Dice: [0.9055]
2022-07-27 20:45:54.338307: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:45:55.161444: Suus1 maybe_update_lr lr: 3e-06
2022-07-27 20:45:55.163880: This epoch took 75.976092 s

2022-07-27 20:45:55.166389: 
epoch:  491
2022-07-27 20:47:04.341959: train loss : -0.9012
2022-07-27 20:47:09.707841: validation loss: -0.8731
2022-07-27 20:47:09.712104: Average global foreground Dice: [0.896]
2022-07-27 20:47:09.714505: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:47:10.537591: Suus1 maybe_update_lr lr: 2e-06
2022-07-27 20:47:10.540191: This epoch took 75.371592 s

2022-07-27 20:47:10.542500: 
epoch:  492
2022-07-27 20:48:20.345772: train loss : -0.9036
2022-07-27 20:48:26.086237: validation loss: -0.8763
2022-07-27 20:48:26.116385: Average global foreground Dice: [0.8943]
2022-07-27 20:48:26.120862: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:48:26.994221: Suus1 maybe_update_lr lr: 2e-06
2022-07-27 20:48:26.996785: This epoch took 76.451680 s

2022-07-27 20:48:26.999100: 
epoch:  493
2022-07-27 20:49:37.986535: train loss : -0.9052
2022-07-27 20:49:43.639342: validation loss: -0.9070
2022-07-27 20:49:43.671421: Average global foreground Dice: [0.9477]
2022-07-27 20:49:43.677522: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:49:44.780921: Suus1 maybe_update_lr lr: 2e-06
2022-07-27 20:49:44.783387: This epoch took 77.782032 s

2022-07-27 20:49:44.785682: 
epoch:  494
2022-07-27 20:50:53.796167: train loss : -0.9116
2022-07-27 20:50:59.447777: validation loss: -0.9144
2022-07-27 20:50:59.478481: Average global foreground Dice: [0.9535]
2022-07-27 20:50:59.492980: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:51:00.530892: Suus1 maybe_update_lr lr: 2e-06
2022-07-27 20:51:00.533495: This epoch took 75.745489 s

2022-07-27 20:51:00.535899: 
epoch:  495
2022-07-27 20:52:09.988051: train loss : -0.9039
2022-07-27 20:52:16.057320: validation loss: -0.8932
2022-07-27 20:52:16.090711: Average global foreground Dice: [0.933]
2022-07-27 20:52:16.123011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:52:17.072625: Suus1 maybe_update_lr lr: 1e-06
2022-07-27 20:52:17.075373: This epoch took 76.537313 s

2022-07-27 20:52:17.077703: 
epoch:  496
2022-07-27 20:53:27.255627: train loss : -0.9217
2022-07-27 20:53:33.375566: validation loss: -0.9045
2022-07-27 20:53:33.400771: Average global foreground Dice: [0.9465]
2022-07-27 20:53:33.422013: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:53:34.226924: Suus1 maybe_update_lr lr: 1e-06
2022-07-27 20:53:34.229711: This epoch took 77.149209 s

2022-07-27 20:53:34.232076: 
epoch:  497
2022-07-27 20:54:44.681799: train loss : -0.9065
2022-07-27 20:54:50.609349: validation loss: -0.8896
2022-07-27 20:54:50.646864: Average global foreground Dice: [0.9155]
2022-07-27 20:54:50.654408: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:54:51.506668: Suus1 maybe_update_lr lr: 1e-06
2022-07-27 20:54:51.509070: This epoch took 77.274565 s

2022-07-27 20:54:51.511374: 
epoch:  498
2022-07-27 20:56:00.435286: train loss : -0.9149
2022-07-27 20:56:05.888089: validation loss: -0.8878
2022-07-27 20:56:05.892682: Average global foreground Dice: [0.9327]
2022-07-27 20:56:05.898662: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:56:06.632094: Suus1 maybe_update_lr lr: 0.0
2022-07-27 20:56:06.634354: This epoch took 75.118997 s

2022-07-27 20:56:06.636481: 
epoch:  499
2022-07-27 20:57:15.583009: train loss : -0.9064
2022-07-27 20:57:21.119988: validation loss: -0.9084
2022-07-27 20:57:21.146474: Average global foreground Dice: [0.9465]
2022-07-27 20:57:21.167115: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 20:57:22.002057: Suus1 maybe_update_lr lr: 0.0
2022-07-27 20:57:22.004329: saving scheduled checkpoint file...
2022-07-27 20:57:22.150220: saving checkpoint...
2022-07-27 20:57:27.610939: done, saving took 5.60 seconds
2022-07-27 20:57:27.624181: done
2022-07-27 20:57:27.626659: This epoch took 80.988758 s

2022-07-27 20:57:27.769055: saving checkpoint...
2022-07-27 20:57:32.822911: done, saving took 5.19 seconds
panc_0026 (2, 218, 520, 520)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
computing Gaussian
done
prediction done
suus panc_0026 transposed
suus panc_0026 not saving softmax
suus panc_0026 voeg toe aan pred_gt tuples voor later
panc_0029 (2, 100, 584, 584)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
suus panc_0029 transposed
suus panc_0029 not saving softmax
suus panc_0029 voeg toe aan pred_gt tuples voor later
panc_0036 (2, 184, 498, 498)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0036 transposed
suus panc_0036 not saving softmax
suus panc_0036 voeg toe aan pred_gt tuples voor later
panc_0038 (2, 167, 651, 651)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0038 transposed
suus panc_0038 not saving softmax
suus panc_0038 voeg toe aan pred_gt tuples voor later
panc_0039 (2, 150, 651, 651)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
suus panc_0039 transposed
suus panc_0039 not saving softmax
suus panc_0039 voeg toe aan pred_gt tuples voor later
panc_0040 (2, 195, 495, 495)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
suus panc_0040 transposed
suus panc_0040 not saving softmax
suus panc_0040 voeg toe aan pred_gt tuples voor later
2022-07-27 21:04:43.688701: finished prediction
2022-07-27 21:04:43.692144: evaluation of raw predictions
2022-07-27 21:04:51.877249: determining postprocessing
Foreground vs background
before: 0.16341412412339784
after:  0.16252686261679486
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0026.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0029.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0036.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0038.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0039.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0040.nii.gz
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='521', fold='4', validation_only=False, continue_training=False, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-27 21:05:22.369378: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-27 21:05:22.449080: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-27 21:05:22.454033: The split file contains 5 splits.
2022-07-27 21:05:22.456327: Desired fold for training: 4
2022-07-27 21:05:22.458567: This split has 24 training and 5 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusB run_training - zet learning rate als  
2022-07-27 21:05:24.599205: Suus1 maybe_update_lr lr: 0.0001
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-27 21:05:35.097867: Unable to plot network architecture:
2022-07-27 21:05:35.160429: local variable 'g' referenced before assignment
2022-07-27 21:05:35.212765: 
printing the network instead:

2022-07-27 21:05:35.248001: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-27 21:05:35.280688: 

2022-07-27 21:05:35.329902: 
epoch:  0
2022-07-27 21:06:56.193183: train loss : 0.0732
2022-07-27 21:07:02.632259: validation loss: -0.0430
2022-07-27 21:07:02.635646: Average global foreground Dice: [0.3915]
2022-07-27 21:07:02.653024: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:07:03.654930: Suus1 maybe_update_lr lr: 0.0001
2022-07-27 21:07:03.658707: This epoch took 88.296980 s

2022-07-27 21:07:03.661718: 
epoch:  1
2022-07-27 21:08:13.772541: train loss : -0.1594
2022-07-27 21:08:19.543897: validation loss: -0.2387
2022-07-27 21:08:19.552767: Average global foreground Dice: [0.55]
2022-07-27 21:08:19.555244: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:08:20.309788: Suus1 maybe_update_lr lr: 0.0001
2022-07-27 21:08:20.312997: saving best epoch checkpoint...
2022-07-27 21:08:20.633401: saving checkpoint...
2022-07-27 21:08:25.913031: done, saving took 5.60 seconds
2022-07-27 21:08:25.922492: This epoch took 82.258415 s

2022-07-27 21:08:25.925430: 
epoch:  2
2022-07-27 21:09:34.976083: train loss : -0.2875
2022-07-27 21:09:40.958436: validation loss: -0.2954
2022-07-27 21:09:40.982738: Average global foreground Dice: [0.5847]
2022-07-27 21:09:41.015212: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:09:41.810109: Suus1 maybe_update_lr lr: 9.9e-05
2022-07-27 21:09:41.812697: saving best epoch checkpoint...
2022-07-27 21:09:41.940704: saving checkpoint...
2022-07-27 21:09:47.048918: done, saving took 5.23 seconds
2022-07-27 21:09:47.057891: This epoch took 81.129930 s

2022-07-27 21:09:47.060147: 
epoch:  3
2022-07-27 21:10:55.421364: train loss : -0.3585
2022-07-27 21:11:01.011400: validation loss: -0.1454
2022-07-27 21:11:01.015012: Average global foreground Dice: [0.395]
2022-07-27 21:11:01.037743: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:11:01.771188: Suus1 maybe_update_lr lr: 9.9e-05
2022-07-27 21:11:01.773620: This epoch took 74.711115 s

2022-07-27 21:11:01.775668: 
epoch:  4
2022-07-27 21:12:10.876670: train loss : -0.4113
2022-07-27 21:12:16.601245: validation loss: 0.0400
2022-07-27 21:12:16.612142: Average global foreground Dice: [0.2479]
2022-07-27 21:12:16.635090: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:12:17.319065: Suus1 maybe_update_lr lr: 9.9e-05
2022-07-27 21:12:17.321368: This epoch took 75.543600 s

2022-07-27 21:12:17.323585: 
epoch:  5
2022-07-27 21:13:27.243657: train loss : -0.3777
2022-07-27 21:13:33.162585: validation loss: -0.2633
2022-07-27 21:13:33.206624: Average global foreground Dice: [0.5763]
2022-07-27 21:13:33.215293: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:13:34.203882: Suus1 maybe_update_lr lr: 9.9e-05
2022-07-27 21:13:34.206892: This epoch took 76.881300 s

2022-07-27 21:13:34.209587: 
epoch:  6
2022-07-27 21:14:42.844993: train loss : -0.5073
2022-07-27 21:14:48.621846: validation loss: -0.2219
2022-07-27 21:14:48.641613: Average global foreground Dice: [0.4474]
2022-07-27 21:14:48.652020: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:14:49.324692: Suus1 maybe_update_lr lr: 9.9e-05
2022-07-27 21:14:49.326900: This epoch took 75.114497 s

2022-07-27 21:14:49.328908: 
epoch:  7
2022-07-27 21:15:59.642492: train loss : -0.4974
2022-07-27 21:16:05.190662: validation loss: -0.3350
2022-07-27 21:16:05.200011: Average global foreground Dice: [0.542]
2022-07-27 21:16:05.219256: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:16:05.970347: Suus1 maybe_update_lr lr: 9.9e-05
2022-07-27 21:16:05.973364: saving best epoch checkpoint...
2022-07-27 21:16:06.122478: saving checkpoint...
2022-07-27 21:16:11.661692: done, saving took 5.69 seconds
2022-07-27 21:16:11.669507: This epoch took 82.338406 s

2022-07-27 21:16:11.671638: 
epoch:  8
2022-07-27 21:17:20.866405: train loss : -0.4916
2022-07-27 21:17:26.808139: validation loss: -0.2829
2022-07-27 21:17:26.828501: Average global foreground Dice: [0.5189]
2022-07-27 21:17:26.866999: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:17:27.732868: Suus1 maybe_update_lr lr: 9.8e-05
2022-07-27 21:17:27.735669: saving best epoch checkpoint...
2022-07-27 21:17:27.925149: saving checkpoint...
2022-07-27 21:17:33.112167: done, saving took 5.37 seconds
2022-07-27 21:17:33.125138: This epoch took 81.451078 s

2022-07-27 21:17:33.127147: 
epoch:  9
2022-07-27 21:18:42.033606: train loss : -0.5070
2022-07-27 21:18:48.010963: validation loss: -0.3512
2022-07-27 21:18:48.026389: Average global foreground Dice: [0.6179]
2022-07-27 21:18:48.033111: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:18:49.211787: Suus1 maybe_update_lr lr: 9.8e-05
2022-07-27 21:18:49.214147: saving best epoch checkpoint...
2022-07-27 21:18:49.323375: saving checkpoint...
2022-07-27 21:18:54.566269: done, saving took 5.35 seconds
2022-07-27 21:18:54.574445: This epoch took 81.445090 s

2022-07-27 21:18:54.576988: 
epoch:  10
2022-07-27 21:20:03.623164: train loss : -0.5073
2022-07-27 21:20:09.514799: validation loss: -0.2636
2022-07-27 21:20:09.555681: Average global foreground Dice: [0.5456]
2022-07-27 21:20:09.568033: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:20:10.328284: Suus1 maybe_update_lr lr: 9.8e-05
2022-07-27 21:20:10.330806: saving best epoch checkpoint...
2022-07-27 21:20:10.466728: saving checkpoint...
2022-07-27 21:20:16.350092: done, saving took 6.02 seconds
2022-07-27 21:20:16.357065: This epoch took 81.777503 s

2022-07-27 21:20:16.359788: 
epoch:  11
2022-07-27 21:21:25.843203: train loss : -0.5515
2022-07-27 21:21:31.670752: validation loss: -0.3315
2022-07-27 21:21:31.679878: Average global foreground Dice: [0.5301]
2022-07-27 21:21:31.686815: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:21:32.444630: Suus1 maybe_update_lr lr: 9.8e-05
2022-07-27 21:21:32.448056: saving best epoch checkpoint...
2022-07-27 21:21:32.698932: saving checkpoint...
2022-07-27 21:21:38.404547: done, saving took 5.95 seconds
2022-07-27 21:21:38.419278: This epoch took 82.057372 s

2022-07-27 21:21:38.421592: 
epoch:  12
2022-07-27 21:22:46.919477: train loss : -0.5115
2022-07-27 21:22:52.708257: validation loss: -0.4722
2022-07-27 21:22:52.721091: Average global foreground Dice: [0.641]
2022-07-27 21:22:52.760453: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:22:53.526791: Suus1 maybe_update_lr lr: 9.8e-05
2022-07-27 21:22:53.529091: saving best epoch checkpoint...
2022-07-27 21:22:53.661552: saving checkpoint...
2022-07-27 21:22:58.509652: done, saving took 4.98 seconds
2022-07-27 21:22:58.519944: This epoch took 80.096082 s

2022-07-27 21:22:58.522134: 
epoch:  13
2022-07-27 21:24:08.068836: train loss : -0.5404
2022-07-27 21:24:13.726668: validation loss: -0.3520
2022-07-27 21:24:13.730315: Average global foreground Dice: [0.5582]
2022-07-27 21:24:13.752633: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:24:14.579019: Suus1 maybe_update_lr lr: 9.7e-05
2022-07-27 21:24:14.581397: saving best epoch checkpoint...
2022-07-27 21:24:14.737494: saving checkpoint...
2022-07-27 21:24:20.203516: done, saving took 5.62 seconds
2022-07-27 21:24:20.214645: This epoch took 81.690326 s

2022-07-27 21:24:20.216875: 
epoch:  14
2022-07-27 21:25:29.399624: train loss : -0.5707
2022-07-27 21:25:35.168140: validation loss: -0.3357
2022-07-27 21:25:35.178558: Average global foreground Dice: [0.641]
2022-07-27 21:25:35.198072: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:25:36.139633: Suus1 maybe_update_lr lr: 9.7e-05
2022-07-27 21:25:36.172004: saving best epoch checkpoint...
2022-07-27 21:25:36.347197: saving checkpoint...
2022-07-27 21:25:42.792881: done, saving took 6.60 seconds
2022-07-27 21:25:42.805837: This epoch took 82.586437 s

2022-07-27 21:25:42.808601: 
epoch:  15
2022-07-27 21:26:52.292077: train loss : -0.5570
2022-07-27 21:26:58.526098: validation loss: -0.3638
2022-07-27 21:26:58.550719: Average global foreground Dice: [0.664]
2022-07-27 21:26:58.553600: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:26:59.287662: Suus1 maybe_update_lr lr: 9.7e-05
2022-07-27 21:26:59.290331: saving best epoch checkpoint...
2022-07-27 21:26:59.436709: saving checkpoint...
2022-07-27 21:27:05.052375: done, saving took 5.76 seconds
2022-07-27 21:27:05.065429: This epoch took 82.254473 s

2022-07-27 21:27:05.068015: 
epoch:  16
2022-07-27 21:28:14.046105: train loss : -0.5568
2022-07-27 21:28:20.017749: validation loss: -0.4338
2022-07-27 21:28:20.053432: Average global foreground Dice: [0.7345]
2022-07-27 21:28:20.066015: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:28:20.854989: Suus1 maybe_update_lr lr: 9.7e-05
2022-07-27 21:28:20.857625: saving best epoch checkpoint...
2022-07-27 21:28:21.042949: saving checkpoint...
2022-07-27 21:28:25.976411: done, saving took 5.12 seconds
2022-07-27 21:28:25.984164: This epoch took 80.913895 s

2022-07-27 21:28:25.986437: 
epoch:  17
2022-07-27 21:29:35.693364: train loss : -0.5763
2022-07-27 21:29:41.921226: validation loss: -0.2920
2022-07-27 21:29:41.966551: Average global foreground Dice: [0.6641]
2022-07-27 21:29:41.988439: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:29:42.910773: Suus1 maybe_update_lr lr: 9.7e-05
2022-07-27 21:29:42.913256: saving best epoch checkpoint...
2022-07-27 21:29:43.070448: saving checkpoint...
2022-07-27 21:29:48.251501: done, saving took 5.34 seconds
2022-07-27 21:29:48.261349: This epoch took 82.272806 s

2022-07-27 21:29:48.263559: 
epoch:  18
2022-07-27 21:30:56.950403: train loss : -0.5878
2022-07-27 21:31:03.139453: validation loss: -0.3667
2022-07-27 21:31:03.179554: Average global foreground Dice: [0.6083]
2022-07-27 21:31:03.194028: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:31:04.268252: Suus1 maybe_update_lr lr: 9.7e-05
2022-07-27 21:31:04.271091: saving best epoch checkpoint...
2022-07-27 21:31:04.556465: saving checkpoint...
2022-07-27 21:31:09.550827: done, saving took 5.28 seconds
2022-07-27 21:31:09.563129: This epoch took 81.297133 s

2022-07-27 21:31:09.565446: 
epoch:  19
2022-07-27 21:32:18.671804: train loss : -0.6035
2022-07-27 21:32:24.246513: validation loss: -0.3857
2022-07-27 21:32:24.263205: Average global foreground Dice: [0.6272]
2022-07-27 21:32:24.272017: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:32:25.339211: Suus1 maybe_update_lr lr: 9.6e-05
2022-07-27 21:32:25.341382: saving best epoch checkpoint...
2022-07-27 21:32:25.465166: saving checkpoint...
2022-07-27 21:32:30.586780: done, saving took 5.24 seconds
2022-07-27 21:32:30.594999: This epoch took 81.027492 s

2022-07-27 21:32:30.597157: 
epoch:  20
2022-07-27 21:33:40.025590: train loss : -0.6319
2022-07-27 21:33:45.850239: validation loss: -0.3969
2022-07-27 21:33:45.875840: Average global foreground Dice: [0.6189]
2022-07-27 21:33:45.903461: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:33:46.599747: Suus1 maybe_update_lr lr: 9.6e-05
2022-07-27 21:33:46.602307: saving best epoch checkpoint...
2022-07-27 21:33:46.757792: saving checkpoint...
2022-07-27 21:33:51.783619: done, saving took 5.18 seconds
2022-07-27 21:33:51.791999: This epoch took 81.192551 s

2022-07-27 21:33:51.794106: 
epoch:  21
2022-07-27 21:35:01.487339: train loss : -0.6167
2022-07-27 21:35:07.120622: validation loss: -0.1674
2022-07-27 21:35:07.133755: Average global foreground Dice: [0.3873]
2022-07-27 21:35:07.136106: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:35:07.903125: Suus1 maybe_update_lr lr: 9.6e-05
2022-07-27 21:35:07.908676: This epoch took 76.112363 s

2022-07-27 21:35:07.910867: 
epoch:  22
2022-07-27 21:36:18.230638: train loss : -0.6333
2022-07-27 21:36:24.110903: validation loss: -0.3395
2022-07-27 21:36:24.116500: Average global foreground Dice: [0.5975]
2022-07-27 21:36:24.136940: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:36:25.111630: Suus1 maybe_update_lr lr: 9.6e-05
2022-07-27 21:36:25.114068: This epoch took 77.200697 s

2022-07-27 21:36:25.116185: 
epoch:  23
2022-07-27 21:37:34.096496: train loss : -0.6501
2022-07-27 21:37:39.809696: validation loss: -0.3517
2022-07-27 21:37:39.813569: Average global foreground Dice: [0.62]
2022-07-27 21:37:39.815943: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:37:40.510958: Suus1 maybe_update_lr lr: 9.6e-05
2022-07-27 21:37:40.513294: This epoch took 75.395036 s

2022-07-27 21:37:40.515532: 
epoch:  24
2022-07-27 21:38:49.151056: train loss : -0.5811
2022-07-27 21:38:54.774905: validation loss: -0.1704
2022-07-27 21:38:54.801873: Average global foreground Dice: [0.4213]
2022-07-27 21:38:54.817020: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:38:55.515308: Suus1 maybe_update_lr lr: 9.5e-05
2022-07-27 21:38:55.517311: This epoch took 74.999780 s

2022-07-27 21:38:55.520442: 
epoch:  25
2022-07-27 21:40:05.428088: train loss : -0.6350
2022-07-27 21:40:11.111503: validation loss: -0.3332
2022-07-27 21:40:11.135590: Average global foreground Dice: [0.556]
2022-07-27 21:40:11.139021: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:40:11.957914: Suus1 maybe_update_lr lr: 9.5e-05
2022-07-27 21:40:11.960816: This epoch took 76.437927 s

2022-07-27 21:40:11.962968: 
epoch:  26
2022-07-27 21:41:21.307396: train loss : -0.6196
2022-07-27 21:41:26.901526: validation loss: -0.3494
2022-07-27 21:41:26.922223: Average global foreground Dice: [0.617]
2022-07-27 21:41:26.952468: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:41:27.834306: Suus1 maybe_update_lr lr: 9.5e-05
2022-07-27 21:41:27.836964: This epoch took 75.871523 s

2022-07-27 21:41:27.839367: 
epoch:  27
2022-07-27 21:42:36.925561: train loss : -0.6636
2022-07-27 21:42:42.537583: validation loss: -0.3137
2022-07-27 21:42:42.603066: Average global foreground Dice: [0.5633]
2022-07-27 21:42:42.622366: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:42:43.280785: Suus1 maybe_update_lr lr: 9.5e-05
2022-07-27 21:42:43.282801: This epoch took 75.440946 s

2022-07-27 21:42:43.285458: 
epoch:  28
2022-07-27 21:43:52.252656: train loss : -0.6392
2022-07-27 21:43:58.219692: validation loss: -0.3691
2022-07-27 21:43:58.255759: Average global foreground Dice: [0.6411]
2022-07-27 21:43:58.275865: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:43:59.077370: Suus1 maybe_update_lr lr: 9.5e-05
2022-07-27 21:43:59.079823: This epoch took 75.792279 s

2022-07-27 21:43:59.081911: 
epoch:  29
2022-07-27 21:45:09.552973: train loss : -0.6310
2022-07-27 21:45:15.546669: validation loss: -0.3918
2022-07-27 21:45:15.566581: Average global foreground Dice: [0.7178]
2022-07-27 21:45:15.587984: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:45:16.431952: Suus1 maybe_update_lr lr: 9.5e-05
2022-07-27 21:45:16.434464: saving best epoch checkpoint...
2022-07-27 21:45:16.604252: saving checkpoint...
2022-07-27 21:45:21.609732: done, saving took 5.17 seconds
2022-07-27 21:45:21.620643: This epoch took 82.536677 s

2022-07-27 21:45:21.623224: 
epoch:  30
2022-07-27 21:46:30.612799: train loss : -0.6670
2022-07-27 21:46:36.342112: validation loss: -0.3693
2022-07-27 21:46:36.350027: Average global foreground Dice: [0.5961]
2022-07-27 21:46:36.352807: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:46:37.377454: Suus1 maybe_update_lr lr: 9.4e-05
2022-07-27 21:46:37.388354: saving best epoch checkpoint...
2022-07-27 21:46:37.627505: saving checkpoint...
2022-07-27 21:46:42.496305: done, saving took 5.10 seconds
2022-07-27 21:46:42.503520: This epoch took 80.878320 s

2022-07-27 21:46:42.505645: 
epoch:  31
2022-07-27 21:47:51.857064: train loss : -0.6044
2022-07-27 21:47:57.306443: validation loss: -0.0989
2022-07-27 21:47:57.337274: Average global foreground Dice: [0.2509]
2022-07-27 21:47:57.351102: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:47:58.241187: Suus1 maybe_update_lr lr: 9.4e-05
2022-07-27 21:47:58.243824: This epoch took 75.736218 s

2022-07-27 21:47:58.246204: 
epoch:  32
2022-07-27 21:49:07.041929: train loss : -0.6467
2022-07-27 21:49:12.260097: validation loss: -0.3612
2022-07-27 21:49:12.263344: Average global foreground Dice: [0.6749]
2022-07-27 21:49:12.265614: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:49:12.904907: Suus1 maybe_update_lr lr: 9.4e-05
2022-07-27 21:49:12.907295: This epoch took 74.658994 s

2022-07-27 21:49:12.909176: 
epoch:  33
2022-07-27 21:50:22.043027: train loss : -0.6544
2022-07-27 21:50:28.475874: validation loss: -0.2648
2022-07-27 21:50:28.507680: Average global foreground Dice: [0.5305]
2022-07-27 21:50:28.519974: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:50:29.364922: Suus1 maybe_update_lr lr: 9.4e-05
2022-07-27 21:50:29.367308: This epoch took 76.456270 s

2022-07-27 21:50:29.369574: 
epoch:  34
2022-07-27 21:51:37.788591: train loss : -0.7110
2022-07-27 21:51:43.171303: validation loss: -0.5142
2022-07-27 21:51:43.187984: Average global foreground Dice: [0.7581]
2022-07-27 21:51:43.207982: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:51:43.885777: Suus1 maybe_update_lr lr: 9.4e-05
2022-07-27 21:51:43.887740: This epoch took 74.515815 s

2022-07-27 21:51:43.889598: 
epoch:  35
2022-07-27 21:52:52.625463: train loss : -0.6337
2022-07-27 21:52:58.438758: validation loss: -0.2823
2022-07-27 21:52:58.467013: Average global foreground Dice: [0.4575]
2022-07-27 21:52:58.469990: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:52:59.327502: Suus1 maybe_update_lr lr: 9.3e-05
2022-07-27 21:52:59.329711: This epoch took 75.438328 s

2022-07-27 21:52:59.331730: 
epoch:  36
2022-07-27 21:54:08.357182: train loss : -0.6828
2022-07-27 21:54:14.367088: validation loss: -0.4358
2022-07-27 21:54:14.372986: Average global foreground Dice: [0.7117]
2022-07-27 21:54:14.395636: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:54:15.368349: Suus1 maybe_update_lr lr: 9.3e-05
2022-07-27 21:54:15.370640: This epoch took 76.036953 s

2022-07-27 21:54:15.373031: 
epoch:  37
2022-07-27 21:55:24.215723: train loss : -0.6634
2022-07-27 21:55:30.121998: validation loss: -0.3458
2022-07-27 21:55:30.133508: Average global foreground Dice: [0.6404]
2022-07-27 21:55:30.161993: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:55:30.866655: Suus1 maybe_update_lr lr: 9.3e-05
2022-07-27 21:55:30.868785: saving best epoch checkpoint...
2022-07-27 21:55:31.016719: saving checkpoint...
2022-07-27 21:55:35.708108: done, saving took 4.84 seconds
2022-07-27 21:55:35.719881: This epoch took 80.344718 s

2022-07-27 21:55:35.721932: 
epoch:  38
2022-07-27 21:56:44.491622: train loss : -0.6868
2022-07-27 21:56:49.941778: validation loss: -0.2945
2022-07-27 21:56:49.966180: Average global foreground Dice: [0.6128]
2022-07-27 21:56:49.993207: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:56:50.904238: Suus1 maybe_update_lr lr: 9.3e-05
2022-07-27 21:56:50.906684: saving best epoch checkpoint...
2022-07-27 21:56:51.108569: saving checkpoint...
2022-07-27 21:56:55.998641: done, saving took 5.09 seconds
2022-07-27 21:56:56.010531: This epoch took 80.286674 s

2022-07-27 21:56:56.012551: 
epoch:  39
2022-07-27 21:58:04.916728: train loss : -0.6665
2022-07-27 21:58:10.832522: validation loss: -0.5642
2022-07-27 21:58:10.871583: Average global foreground Dice: [0.8107]
2022-07-27 21:58:10.885580: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:58:11.840028: Suus1 maybe_update_lr lr: 9.3e-05
2022-07-27 21:58:11.842158: saving best epoch checkpoint...
2022-07-27 21:58:11.957102: saving checkpoint...
2022-07-27 21:58:16.881076: done, saving took 5.04 seconds
2022-07-27 21:58:16.888652: This epoch took 80.874144 s

2022-07-27 21:58:16.890730: 
epoch:  40
2022-07-27 21:59:26.190919: train loss : -0.6789
2022-07-27 21:59:31.826621: validation loss: -0.3604
2022-07-27 21:59:31.849164: Average global foreground Dice: [0.7192]
2022-07-27 21:59:31.851669: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 21:59:32.511704: Suus1 maybe_update_lr lr: 9.3e-05
2022-07-27 21:59:32.514118: saving best epoch checkpoint...
2022-07-27 21:59:32.658210: saving checkpoint...
2022-07-27 21:59:38.603802: done, saving took 6.09 seconds
2022-07-27 21:59:38.611082: This epoch took 81.718466 s

2022-07-27 21:59:38.613120: 
epoch:  41
2022-07-27 22:00:47.949089: train loss : -0.6689
2022-07-27 22:00:53.915143: validation loss: -0.4531
2022-07-27 22:00:53.938443: Average global foreground Dice: [0.7782]
2022-07-27 22:00:53.945460: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:00:54.773421: Suus1 maybe_update_lr lr: 9.2e-05
2022-07-27 22:00:54.775661: saving best epoch checkpoint...
2022-07-27 22:00:54.888403: saving checkpoint...
2022-07-27 22:00:59.895655: done, saving took 5.12 seconds
2022-07-27 22:00:59.905863: This epoch took 81.290806 s

2022-07-27 22:00:59.908048: 
epoch:  42
2022-07-27 22:02:10.121596: train loss : -0.6790
2022-07-27 22:02:16.220045: validation loss: -0.4365
2022-07-27 22:02:16.224690: Average global foreground Dice: [0.6791]
2022-07-27 22:02:16.228979: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:02:16.953204: Suus1 maybe_update_lr lr: 9.2e-05
2022-07-27 22:02:16.955366: saving best epoch checkpoint...
2022-07-27 22:02:17.091490: saving checkpoint...
2022-07-27 22:02:22.026800: done, saving took 5.07 seconds
2022-07-27 22:02:22.036260: This epoch took 82.126167 s

2022-07-27 22:02:22.038768: 
epoch:  43
2022-07-27 22:03:30.825623: train loss : -0.6991
2022-07-27 22:03:36.573471: validation loss: -0.4237
2022-07-27 22:03:36.607235: Average global foreground Dice: [0.7356]
2022-07-27 22:03:36.615819: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:03:37.269752: Suus1 maybe_update_lr lr: 9.2e-05
2022-07-27 22:03:37.271985: saving best epoch checkpoint...
2022-07-27 22:03:37.400430: saving checkpoint...
2022-07-27 22:03:42.524286: done, saving took 5.25 seconds
2022-07-27 22:03:42.534973: This epoch took 80.494224 s

2022-07-27 22:03:42.537180: 
epoch:  44
2022-07-27 22:04:52.149351: train loss : -0.6598
2022-07-27 22:04:57.898338: validation loss: -0.4699
2022-07-27 22:04:57.907681: Average global foreground Dice: [0.72]
2022-07-27 22:04:57.918397: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:04:58.799302: Suus1 maybe_update_lr lr: 9.2e-05
2022-07-27 22:04:58.827030: saving best epoch checkpoint...
2022-07-27 22:04:59.059696: saving checkpoint...
2022-07-27 22:05:04.374141: done, saving took 5.54 seconds
2022-07-27 22:05:04.382103: This epoch took 81.842770 s

2022-07-27 22:05:04.384214: 
epoch:  45
2022-07-27 22:06:13.739569: train loss : -0.6749
2022-07-27 22:06:19.489039: validation loss: -0.3015
2022-07-27 22:06:19.492270: Average global foreground Dice: [0.613]
2022-07-27 22:06:19.504942: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:06:20.297786: Suus1 maybe_update_lr lr: 9.2e-05
2022-07-27 22:06:20.300321: This epoch took 75.914011 s

2022-07-27 22:06:20.302570: 
epoch:  46
2022-07-27 22:07:28.898310: train loss : -0.6983
2022-07-27 22:07:34.428635: validation loss: -0.2926
2022-07-27 22:07:34.450506: Average global foreground Dice: [0.5519]
2022-07-27 22:07:34.485986: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:07:35.251678: Suus1 maybe_update_lr lr: 9.1e-05
2022-07-27 22:07:35.254249: This epoch took 74.949321 s

2022-07-27 22:07:35.256514: 
epoch:  47
2022-07-27 22:08:45.915495: train loss : -0.6865
2022-07-27 22:08:51.686140: validation loss: -0.5034
2022-07-27 22:08:51.689332: Average global foreground Dice: [0.7458]
2022-07-27 22:08:51.708972: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:08:52.572033: Suus1 maybe_update_lr lr: 9.1e-05
2022-07-27 22:08:52.574458: This epoch took 77.315589 s

2022-07-27 22:08:52.576508: 
epoch:  48
2022-07-27 22:10:01.688851: train loss : -0.7337
2022-07-27 22:10:07.343301: validation loss: -0.3571
2022-07-27 22:10:07.346647: Average global foreground Dice: [0.6833]
2022-07-27 22:10:07.349223: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:10:08.098044: Suus1 maybe_update_lr lr: 9.1e-05
2022-07-27 22:10:08.100457: This epoch took 75.521750 s

2022-07-27 22:10:08.102539: 
epoch:  49
2022-07-27 22:11:16.720150: train loss : -0.7300
2022-07-27 22:11:22.709523: validation loss: -0.2630
2022-07-27 22:11:22.730412: Average global foreground Dice: [0.5685]
2022-07-27 22:11:22.757386: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:11:23.606276: Suus1 maybe_update_lr lr: 9.1e-05
2022-07-27 22:11:23.609030: saving scheduled checkpoint file...
2022-07-27 22:11:23.838614: saving checkpoint...
2022-07-27 22:11:29.045366: done, saving took 5.43 seconds
2022-07-27 22:11:29.057208: done
2022-07-27 22:11:29.059987: This epoch took 80.955381 s

2022-07-27 22:11:29.062257: 
epoch:  50
2022-07-27 22:12:38.674574: train loss : -0.7193
2022-07-27 22:12:44.480660: validation loss: -0.2322
2022-07-27 22:12:44.501946: Average global foreground Dice: [0.5583]
2022-07-27 22:12:44.514378: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:12:45.378019: Suus1 maybe_update_lr lr: 9.1e-05
2022-07-27 22:12:45.380914: This epoch took 76.316082 s

2022-07-27 22:12:45.383353: 
epoch:  51
2022-07-27 22:13:55.658809: train loss : -0.7020
2022-07-27 22:14:01.564567: validation loss: -0.3645
2022-07-27 22:14:01.568831: Average global foreground Dice: [0.7325]
2022-07-27 22:14:01.579587: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:14:02.484774: Suus1 maybe_update_lr lr: 9.1e-05
2022-07-27 22:14:02.487610: This epoch took 77.102038 s

2022-07-27 22:14:02.490202: 
epoch:  52
2022-07-27 22:15:11.276659: train loss : -0.7331
2022-07-27 22:15:17.332678: validation loss: -0.4152
2022-07-27 22:15:17.368577: Average global foreground Dice: [0.7509]
2022-07-27 22:15:17.389090: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:15:18.194440: Suus1 maybe_update_lr lr: 9e-05
2022-07-27 22:15:18.197088: saving best epoch checkpoint...
2022-07-27 22:15:18.360115: saving checkpoint...
2022-07-27 22:15:23.860575: done, saving took 5.66 seconds
2022-07-27 22:15:23.870577: This epoch took 81.378196 s

2022-07-27 22:15:23.872758: 
epoch:  53
2022-07-27 22:16:33.693995: train loss : -0.7048
2022-07-27 22:16:39.564349: validation loss: -0.3502
2022-07-27 22:16:39.568198: Average global foreground Dice: [0.6053]
2022-07-27 22:16:39.583745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:16:40.520295: Suus1 maybe_update_lr lr: 9e-05
2022-07-27 22:16:40.524290: This epoch took 76.649314 s

2022-07-27 22:16:40.526880: 
epoch:  54
2022-07-27 22:17:49.250527: train loss : -0.6943
2022-07-27 22:17:55.012420: validation loss: -0.2731
2022-07-27 22:17:55.050191: Average global foreground Dice: [0.6635]
2022-07-27 22:17:55.080998: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:17:55.978897: Suus1 maybe_update_lr lr: 9e-05
2022-07-27 22:17:55.981403: This epoch took 75.451478 s

2022-07-27 22:17:55.983558: 
epoch:  55
2022-07-27 22:19:05.302523: train loss : -0.7144
2022-07-27 22:19:11.050616: validation loss: -0.3042
2022-07-27 22:19:11.062791: Average global foreground Dice: [0.5752]
2022-07-27 22:19:11.078915: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:19:11.821783: Suus1 maybe_update_lr lr: 9e-05
2022-07-27 22:19:11.824010: This epoch took 75.838286 s

2022-07-27 22:19:11.825993: 
epoch:  56
2022-07-27 22:20:20.088653: train loss : -0.6954
2022-07-27 22:20:25.648964: validation loss: -0.4129
2022-07-27 22:20:25.682062: Average global foreground Dice: [0.7098]
2022-07-27 22:20:25.712066: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:20:26.558519: Suus1 maybe_update_lr lr: 9e-05
2022-07-27 22:20:26.561026: This epoch took 74.733115 s

2022-07-27 22:20:26.563242: 
epoch:  57
2022-07-27 22:21:35.901789: train loss : -0.6879
2022-07-27 22:21:41.479013: validation loss: -0.4536
2022-07-27 22:21:41.484271: Average global foreground Dice: [0.7239]
2022-07-27 22:21:41.502989: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:21:42.115639: Suus1 maybe_update_lr lr: 8.9e-05
2022-07-27 22:21:42.117870: saving best epoch checkpoint...
2022-07-27 22:21:42.269383: saving checkpoint...
2022-07-27 22:21:47.531527: done, saving took 5.41 seconds
2022-07-27 22:21:47.547900: This epoch took 80.982328 s

2022-07-27 22:21:47.549999: 
epoch:  58
2022-07-27 22:22:56.120863: train loss : -0.7105
2022-07-27 22:23:02.156035: validation loss: -0.4683
2022-07-27 22:23:02.162800: Average global foreground Dice: [0.7537]
2022-07-27 22:23:02.165491: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:23:02.925574: Suus1 maybe_update_lr lr: 8.9e-05
2022-07-27 22:23:02.927988: saving best epoch checkpoint...
2022-07-27 22:23:03.136134: saving checkpoint...
2022-07-27 22:23:08.797211: done, saving took 5.87 seconds
2022-07-27 22:23:08.810000: This epoch took 81.257642 s

2022-07-27 22:23:08.812580: 
epoch:  59
2022-07-27 22:24:18.483016: train loss : -0.7458
2022-07-27 22:24:24.199864: validation loss: -0.5199
2022-07-27 22:24:24.224237: Average global foreground Dice: [0.7805]
2022-07-27 22:24:24.244070: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:24:25.271115: Suus1 maybe_update_lr lr: 8.9e-05
2022-07-27 22:24:25.273532: saving best epoch checkpoint...
2022-07-27 22:24:25.384264: saving checkpoint...
2022-07-27 22:24:30.861308: done, saving took 5.59 seconds
2022-07-27 22:24:30.869651: This epoch took 82.055001 s

2022-07-27 22:24:30.871739: 
epoch:  60
2022-07-27 22:25:39.588585: train loss : -0.7262
2022-07-27 22:25:45.824580: validation loss: -0.3923
2022-07-27 22:25:45.852455: Average global foreground Dice: [0.7353]
2022-07-27 22:25:45.855571: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:25:46.601304: Suus1 maybe_update_lr lr: 8.9e-05
2022-07-27 22:25:46.603672: saving best epoch checkpoint...
2022-07-27 22:25:46.720791: saving checkpoint...
2022-07-27 22:25:52.292693: done, saving took 5.69 seconds
2022-07-27 22:25:52.301934: This epoch took 81.428069 s

2022-07-27 22:25:52.304304: 
epoch:  61
2022-07-27 22:27:01.225740: train loss : -0.7363
2022-07-27 22:27:06.798905: validation loss: -0.3302
2022-07-27 22:27:06.831529: Average global foreground Dice: [0.7035]
2022-07-27 22:27:06.845946: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:27:07.681283: Suus1 maybe_update_lr lr: 8.9e-05
2022-07-27 22:27:07.683855: saving best epoch checkpoint...
2022-07-27 22:27:07.811030: saving checkpoint...
2022-07-27 22:27:13.762917: done, saving took 6.08 seconds
2022-07-27 22:27:13.771053: This epoch took 81.464506 s

2022-07-27 22:27:13.773242: 
epoch:  62
2022-07-27 22:28:22.201442: train loss : -0.7390
2022-07-27 22:28:27.937949: validation loss: -0.2828
2022-07-27 22:28:27.985551: Average global foreground Dice: [0.676]
2022-07-27 22:28:27.995509: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:28:28.679845: Suus1 maybe_update_lr lr: 8.9e-05
2022-07-27 22:28:28.682166: This epoch took 74.906637 s

2022-07-27 22:28:28.684288: 
epoch:  63
2022-07-27 22:29:39.179215: train loss : -0.6870
2022-07-27 22:29:44.884956: validation loss: -0.3112
2022-07-27 22:29:44.910656: Average global foreground Dice: [0.6673]
2022-07-27 22:29:44.913531: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:29:45.807325: Suus1 maybe_update_lr lr: 8.8e-05
2022-07-27 22:29:45.810429: This epoch took 77.124033 s

2022-07-27 22:29:45.813446: 
epoch:  64
2022-07-27 22:30:54.839662: train loss : -0.7096
2022-07-27 22:31:00.580564: validation loss: -0.4180
2022-07-27 22:31:00.607669: Average global foreground Dice: [0.7855]
2022-07-27 22:31:00.628983: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:31:01.258741: Suus1 maybe_update_lr lr: 8.8e-05
2022-07-27 22:31:01.261077: saving best epoch checkpoint...
2022-07-27 22:31:01.404648: saving checkpoint...
2022-07-27 22:31:06.688061: done, saving took 5.42 seconds
2022-07-27 22:31:06.696368: This epoch took 80.880218 s

2022-07-27 22:31:06.698778: 
epoch:  65
2022-07-27 22:32:14.962605: train loss : -0.7435
2022-07-27 22:32:20.202243: validation loss: -0.3423
2022-07-27 22:32:20.205513: Average global foreground Dice: [0.7159]
2022-07-27 22:32:20.222234: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:32:21.007695: Suus1 maybe_update_lr lr: 8.8e-05
2022-07-27 22:32:21.010345: saving best epoch checkpoint...
2022-07-27 22:32:21.160039: saving checkpoint...
2022-07-27 22:32:26.536585: done, saving took 5.52 seconds
2022-07-27 22:32:26.548467: This epoch took 79.847264 s

2022-07-27 22:32:26.550745: 
epoch:  66
2022-07-27 22:33:36.253173: train loss : -0.7332
2022-07-27 22:33:42.195019: validation loss: -0.4112
2022-07-27 22:33:42.219508: Average global foreground Dice: [0.7157]
2022-07-27 22:33:42.247970: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:33:43.035203: Suus1 maybe_update_lr lr: 8.8e-05
2022-07-27 22:33:43.037723: saving best epoch checkpoint...
2022-07-27 22:33:43.194159: saving checkpoint...
2022-07-27 22:33:48.652503: done, saving took 5.61 seconds
2022-07-27 22:33:48.664726: This epoch took 82.111644 s

2022-07-27 22:33:48.666719: 
epoch:  67
2022-07-27 22:34:56.949833: train loss : -0.7205
2022-07-27 22:35:03.114388: validation loss: -0.4094
2022-07-27 22:35:03.150591: Average global foreground Dice: [0.7744]
2022-07-27 22:35:03.153547: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:35:03.982049: Suus1 maybe_update_lr lr: 8.8e-05
2022-07-27 22:35:03.984421: saving best epoch checkpoint...
2022-07-27 22:35:04.128839: saving checkpoint...
2022-07-27 22:35:09.498775: done, saving took 5.51 seconds
2022-07-27 22:35:09.510571: This epoch took 80.841949 s

2022-07-27 22:35:09.513014: 
epoch:  68
2022-07-27 22:36:18.628475: train loss : -0.7200
2022-07-27 22:36:24.717921: validation loss: -0.3360
2022-07-27 22:36:24.745211: Average global foreground Dice: [0.7102]
2022-07-27 22:36:24.771735: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:36:25.621055: Suus1 maybe_update_lr lr: 8.7e-05
2022-07-27 22:36:25.623565: saving best epoch checkpoint...
2022-07-27 22:36:25.779855: saving checkpoint...
2022-07-27 22:36:31.149272: done, saving took 5.52 seconds
2022-07-27 22:36:31.162448: This epoch took 81.647357 s

2022-07-27 22:36:31.165102: 
epoch:  69
2022-07-27 22:37:40.912441: train loss : -0.7402
2022-07-27 22:37:46.637746: validation loss: -0.3508
2022-07-27 22:37:46.686003: Average global foreground Dice: [0.6701]
2022-07-27 22:37:46.704457: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:37:47.598041: Suus1 maybe_update_lr lr: 8.7e-05
2022-07-27 22:37:47.600693: This epoch took 76.433140 s

2022-07-27 22:37:47.602909: 
epoch:  70
2022-07-27 22:38:55.919449: train loss : -0.7167
2022-07-27 22:39:01.710239: validation loss: -0.5196
2022-07-27 22:39:01.742687: Average global foreground Dice: [0.7873]
2022-07-27 22:39:01.763991: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:39:02.565714: Suus1 maybe_update_lr lr: 8.7e-05
2022-07-27 22:39:02.568005: saving best epoch checkpoint...
2022-07-27 22:39:02.683859: saving checkpoint...
2022-07-27 22:39:07.942857: done, saving took 5.37 seconds
2022-07-27 22:39:07.951807: This epoch took 80.346812 s

2022-07-27 22:39:07.953871: 
epoch:  71
2022-07-27 22:40:18.012432: train loss : -0.7409
2022-07-27 22:40:23.614649: validation loss: -0.4146
2022-07-27 22:40:23.617821: Average global foreground Dice: [0.7723]
2022-07-27 22:40:23.640010: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:40:24.326039: Suus1 maybe_update_lr lr: 8.7e-05
2022-07-27 22:40:24.328782: saving best epoch checkpoint...
2022-07-27 22:40:24.450227: saving checkpoint...
2022-07-27 22:40:29.649368: done, saving took 5.32 seconds
2022-07-27 22:40:29.659999: This epoch took 81.703932 s

2022-07-27 22:40:29.662450: 
epoch:  72
2022-07-27 22:41:40.397104: train loss : -0.7237
2022-07-27 22:41:45.762945: validation loss: -0.3072
2022-07-27 22:41:45.773005: Average global foreground Dice: [0.692]
2022-07-27 22:41:45.775409: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:41:46.328916: Suus1 maybe_update_lr lr: 8.7e-05
2022-07-27 22:41:46.331348: This epoch took 76.666552 s

2022-07-27 22:41:46.334479: 
epoch:  73
2022-07-27 22:42:54.996210: train loss : -0.7266
2022-07-27 22:43:00.388301: validation loss: -0.3057
2022-07-27 22:43:00.397549: Average global foreground Dice: [0.7154]
2022-07-27 22:43:00.412520: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:43:01.303537: Suus1 maybe_update_lr lr: 8.7e-05
2022-07-27 22:43:01.306122: This epoch took 74.969339 s

2022-07-27 22:43:01.308302: 
epoch:  74
2022-07-27 22:44:10.522024: train loss : -0.7425
2022-07-27 22:44:16.390025: validation loss: -0.4021
2022-07-27 22:44:16.396483: Average global foreground Dice: [0.7544]
2022-07-27 22:44:16.418487: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:44:17.077392: Suus1 maybe_update_lr lr: 8.6e-05
2022-07-27 22:44:17.079913: saving best epoch checkpoint...
2022-07-27 22:44:17.248915: saving checkpoint...
2022-07-27 22:44:22.449136: done, saving took 5.37 seconds
2022-07-27 22:44:22.457995: This epoch took 81.147520 s

2022-07-27 22:44:22.460721: 
epoch:  75
2022-07-27 22:45:30.875247: train loss : -0.7540
2022-07-27 22:45:36.445323: validation loss: -0.3199
2022-07-27 22:45:36.453304: Average global foreground Dice: [0.5825]
2022-07-27 22:45:36.473590: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:45:37.352718: Suus1 maybe_update_lr lr: 8.6e-05
2022-07-27 22:45:37.355845: This epoch took 74.892375 s

2022-07-27 22:45:37.358679: 
epoch:  76
2022-07-27 22:46:47.277947: train loss : -0.7442
2022-07-27 22:46:52.961845: validation loss: -0.3677
2022-07-27 22:46:52.968688: Average global foreground Dice: [0.719]
2022-07-27 22:46:52.971237: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:46:53.950861: Suus1 maybe_update_lr lr: 8.6e-05
2022-07-27 22:46:53.953601: This epoch took 76.592342 s

2022-07-27 22:46:53.956115: 
epoch:  77
2022-07-27 22:48:02.817439: train loss : -0.7622
2022-07-27 22:48:08.473841: validation loss: -0.3365
2022-07-27 22:48:08.487032: Average global foreground Dice: [0.6705]
2022-07-27 22:48:08.495136: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:48:09.612227: Suus1 maybe_update_lr lr: 8.6e-05
2022-07-27 22:48:09.614686: This epoch took 75.655948 s

2022-07-27 22:48:09.616990: 
epoch:  78
2022-07-27 22:49:18.918430: train loss : -0.7533
2022-07-27 22:49:24.512381: validation loss: -0.4770
2022-07-27 22:49:24.528368: Average global foreground Dice: [0.7473]
2022-07-27 22:49:24.558600: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:49:25.473891: Suus1 maybe_update_lr lr: 8.6e-05
2022-07-27 22:49:25.476843: This epoch took 75.857647 s

2022-07-27 22:49:25.479067: 
epoch:  79
2022-07-27 22:50:35.825726: train loss : -0.7574
2022-07-27 22:50:41.460371: validation loss: -0.3916
2022-07-27 22:50:41.476147: Average global foreground Dice: [0.7554]
2022-07-27 22:50:41.492464: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:50:42.132947: Suus1 maybe_update_lr lr: 8.5e-05
2022-07-27 22:50:42.135659: This epoch took 76.654852 s

2022-07-27 22:50:42.137826: 
epoch:  80
2022-07-27 22:51:52.066092: train loss : -0.7544
2022-07-27 22:51:57.793763: validation loss: -0.1021
2022-07-27 22:51:57.824060: Average global foreground Dice: [0.3843]
2022-07-27 22:51:57.834004: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:51:58.649581: Suus1 maybe_update_lr lr: 8.5e-05
2022-07-27 22:51:58.652403: This epoch took 76.512558 s

2022-07-27 22:51:58.654444: 
epoch:  81
2022-07-27 22:53:08.470305: train loss : -0.7320
2022-07-27 22:53:14.449863: validation loss: -0.4153
2022-07-27 22:53:14.469493: Average global foreground Dice: [0.7103]
2022-07-27 22:53:14.480008: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:53:15.450378: Suus1 maybe_update_lr lr: 8.5e-05
2022-07-27 22:53:15.453089: This epoch took 76.796624 s

2022-07-27 22:53:15.455629: 
epoch:  82
2022-07-27 22:54:24.210844: train loss : -0.7415
2022-07-27 22:54:30.203814: validation loss: -0.1866
2022-07-27 22:54:30.244336: Average global foreground Dice: [0.5211]
2022-07-27 22:54:30.256003: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:54:31.360041: Suus1 maybe_update_lr lr: 8.5e-05
2022-07-27 22:54:31.362899: This epoch took 75.904836 s

2022-07-27 22:54:31.366290: 
epoch:  83
2022-07-27 22:55:41.001920: train loss : -0.7568
2022-07-27 22:55:46.878313: validation loss: -0.4549
2022-07-27 22:55:46.882019: Average global foreground Dice: [0.7879]
2022-07-27 22:55:46.884483: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:55:47.711807: Suus1 maybe_update_lr lr: 8.5e-05
2022-07-27 22:55:47.714144: This epoch took 76.344705 s

2022-07-27 22:55:47.716288: 
epoch:  84
2022-07-27 22:56:56.011373: train loss : -0.7578
2022-07-27 22:57:01.574473: validation loss: -0.4047
2022-07-27 22:57:01.593789: Average global foreground Dice: [0.7447]
2022-07-27 22:57:01.607006: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:57:02.200800: Suus1 maybe_update_lr lr: 8.5e-05
2022-07-27 22:57:02.202986: This epoch took 74.484402 s

2022-07-27 22:57:02.205213: 
epoch:  85
2022-07-27 22:58:13.871210: train loss : -0.7481
2022-07-27 22:58:19.659039: validation loss: -0.4683
2022-07-27 22:58:19.676672: Average global foreground Dice: [0.6571]
2022-07-27 22:58:19.690295: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:58:20.405913: Suus1 maybe_update_lr lr: 8.4e-05
2022-07-27 22:58:20.408466: This epoch took 78.201247 s

2022-07-27 22:58:20.410440: 
epoch:  86
2022-07-27 22:59:30.118753: train loss : -0.7268
2022-07-27 22:59:35.957794: validation loss: -0.4176
2022-07-27 22:59:35.988378: Average global foreground Dice: [0.7358]
2022-07-27 22:59:36.000950: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 22:59:36.919267: Suus1 maybe_update_lr lr: 8.4e-05
2022-07-27 22:59:36.922485: This epoch took 76.509946 s

2022-07-27 22:59:36.926074: 
epoch:  87
2022-07-27 23:00:46.121627: train loss : -0.7201
2022-07-27 23:00:51.769742: validation loss: -0.4499
2022-07-27 23:00:51.788708: Average global foreground Dice: [0.7935]
2022-07-27 23:00:51.796666: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:00:52.703470: Suus1 maybe_update_lr lr: 8.4e-05
2022-07-27 23:00:52.706038: This epoch took 75.775941 s

2022-07-27 23:00:52.708272: 
epoch:  88
2022-07-27 23:02:01.870202: train loss : -0.7579
2022-07-27 23:02:07.438287: validation loss: -0.4696
2022-07-27 23:02:07.466088: Average global foreground Dice: [0.7489]
2022-07-27 23:02:07.485180: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:02:08.210556: Suus1 maybe_update_lr lr: 8.4e-05
2022-07-27 23:02:08.213341: This epoch took 75.503044 s

2022-07-27 23:02:08.215441: 
epoch:  89
2022-07-27 23:03:17.486807: train loss : -0.7546
2022-07-27 23:03:23.600897: validation loss: -0.3131
2022-07-27 23:03:23.615962: Average global foreground Dice: [0.6383]
2022-07-27 23:03:23.619025: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:03:24.486642: Suus1 maybe_update_lr lr: 8.4e-05
2022-07-27 23:03:24.489136: This epoch took 76.271345 s

2022-07-27 23:03:24.491226: 
epoch:  90
2022-07-27 23:04:34.262556: train loss : -0.7698
2022-07-27 23:04:40.469813: validation loss: -0.3245
2022-07-27 23:04:40.500911: Average global foreground Dice: [0.6411]
2022-07-27 23:04:40.511241: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:04:41.381146: Suus1 maybe_update_lr lr: 8.3e-05
2022-07-27 23:04:41.383446: This epoch took 76.890323 s

2022-07-27 23:04:41.385476: 
epoch:  91
2022-07-27 23:05:52.021666: train loss : -0.7824
2022-07-27 23:05:57.893535: validation loss: -0.3265
2022-07-27 23:05:57.915916: Average global foreground Dice: [0.7338]
2022-07-27 23:05:57.938042: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:05:58.756743: Suus1 maybe_update_lr lr: 8.3e-05
2022-07-27 23:05:58.759195: This epoch took 77.371571 s

2022-07-27 23:05:58.761678: 
epoch:  92
2022-07-27 23:07:07.887868: train loss : -0.7730
2022-07-27 23:07:13.842237: validation loss: -0.3928
2022-07-27 23:07:13.882523: Average global foreground Dice: [0.6721]
2022-07-27 23:07:13.908973: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:07:14.673831: Suus1 maybe_update_lr lr: 8.3e-05
2022-07-27 23:07:14.679754: This epoch took 75.915738 s

2022-07-27 23:07:14.681872: 
epoch:  93
2022-07-27 23:08:23.524336: train loss : -0.7323
2022-07-27 23:08:29.437692: validation loss: -0.3012
2022-07-27 23:08:29.464934: Average global foreground Dice: [0.5981]
2022-07-27 23:08:29.485979: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:08:30.167821: Suus1 maybe_update_lr lr: 8.3e-05
2022-07-27 23:08:30.170299: This epoch took 75.486341 s

2022-07-27 23:08:30.172413: 
epoch:  94
2022-07-27 23:09:39.196417: train loss : -0.7687
2022-07-27 23:09:44.894044: validation loss: -0.3474
2022-07-27 23:09:44.918237: Average global foreground Dice: [0.6471]
2022-07-27 23:09:44.922530: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:09:45.550206: Suus1 maybe_update_lr lr: 8.3e-05
2022-07-27 23:09:45.552822: This epoch took 75.378296 s

2022-07-27 23:09:45.554262: 
epoch:  95
2022-07-27 23:10:55.177427: train loss : -0.7564
2022-07-27 23:11:00.894389: validation loss: -0.4524
2022-07-27 23:11:00.927797: Average global foreground Dice: [0.7695]
2022-07-27 23:11:00.945330: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:11:01.822596: Suus1 maybe_update_lr lr: 8.3e-05
2022-07-27 23:11:01.825709: This epoch took 76.268932 s

2022-07-27 23:11:01.828308: 
epoch:  96
2022-07-27 23:12:10.494011: train loss : -0.7631
2022-07-27 23:12:16.323867: validation loss: -0.3796
2022-07-27 23:12:16.345320: Average global foreground Dice: [0.7237]
2022-07-27 23:12:16.356899: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:12:17.271378: Suus1 maybe_update_lr lr: 8.2e-05
2022-07-27 23:12:17.273948: This epoch took 75.443252 s

2022-07-27 23:12:17.276101: 
epoch:  97
2022-07-27 23:13:26.486907: train loss : -0.7924
2022-07-27 23:13:32.122477: validation loss: -0.4276
2022-07-27 23:13:32.134663: Average global foreground Dice: [0.7346]
2022-07-27 23:13:32.143127: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:13:33.088481: Suus1 maybe_update_lr lr: 8.2e-05
2022-07-27 23:13:33.091393: This epoch took 75.813174 s

2022-07-27 23:13:33.095609: 
epoch:  98
2022-07-27 23:14:42.194947: train loss : -0.7925
2022-07-27 23:14:47.782950: validation loss: -0.3681
2022-07-27 23:14:47.802971: Average global foreground Dice: [0.7411]
2022-07-27 23:14:47.815033: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:14:48.726198: Suus1 maybe_update_lr lr: 8.2e-05
2022-07-27 23:14:48.728879: This epoch took 75.630700 s

2022-07-27 23:14:48.731632: 
epoch:  99
2022-07-27 23:15:57.952843: train loss : -0.7873
2022-07-27 23:16:03.504064: validation loss: -0.2694
2022-07-27 23:16:03.527511: Average global foreground Dice: [0.6214]
2022-07-27 23:16:03.548985: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:16:04.182029: Suus1 maybe_update_lr lr: 8.2e-05
2022-07-27 23:16:04.184281: saving scheduled checkpoint file...
2022-07-27 23:16:04.376836: saving checkpoint...
2022-07-27 23:16:10.713380: done, saving took 6.53 seconds
2022-07-27 23:16:10.726881: done
2022-07-27 23:16:10.729838: This epoch took 81.995874 s

2022-07-27 23:16:10.732403: 
epoch:  100
2022-07-27 23:17:20.836121: train loss : -0.7937
2022-07-27 23:17:27.242334: validation loss: -0.6305
2022-07-27 23:17:27.252517: Average global foreground Dice: [0.8303]
2022-07-27 23:17:27.262187: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:17:28.153678: Suus1 maybe_update_lr lr: 8.2e-05
2022-07-27 23:17:28.156121: This epoch took 77.420879 s

2022-07-27 23:17:28.158352: 
epoch:  101
2022-07-27 23:18:37.081477: train loss : -0.7547
2022-07-27 23:18:42.925967: validation loss: -0.4043
2022-07-27 23:18:42.954440: Average global foreground Dice: [0.7675]
2022-07-27 23:18:42.976978: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:18:43.947054: Suus1 maybe_update_lr lr: 8.1e-05
2022-07-27 23:18:43.949596: This epoch took 75.788989 s

2022-07-27 23:18:43.951999: 
epoch:  102
2022-07-27 23:19:53.711560: train loss : -0.7703
2022-07-27 23:19:59.119340: validation loss: -0.4855
2022-07-27 23:19:59.123240: Average global foreground Dice: [0.7635]
2022-07-27 23:19:59.125794: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:19:59.744134: Suus1 maybe_update_lr lr: 8.1e-05
2022-07-27 23:19:59.746400: This epoch took 75.792092 s

2022-07-27 23:19:59.748401: 
epoch:  103
2022-07-27 23:21:09.921023: train loss : -0.7561
2022-07-27 23:21:15.625363: validation loss: -0.3478
2022-07-27 23:21:15.642143: Average global foreground Dice: [0.6196]
2022-07-27 23:21:15.647848: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:21:16.274298: Suus1 maybe_update_lr lr: 8.1e-05
2022-07-27 23:21:16.276948: This epoch took 76.526631 s

2022-07-27 23:21:16.279224: 
epoch:  104
2022-07-27 23:22:25.867250: train loss : -0.7518
2022-07-27 23:22:31.446986: validation loss: -0.4405
2022-07-27 23:22:31.462318: Average global foreground Dice: [0.7541]
2022-07-27 23:22:31.467895: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:22:32.269109: Suus1 maybe_update_lr lr: 8.1e-05
2022-07-27 23:22:32.271463: This epoch took 75.990033 s

2022-07-27 23:22:32.273595: 
epoch:  105
2022-07-27 23:23:41.161713: train loss : -0.7615
2022-07-27 23:23:47.230015: validation loss: -0.2475
2022-07-27 23:23:47.249184: Average global foreground Dice: [0.617]
2022-07-27 23:23:47.251540: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:23:48.149857: Suus1 maybe_update_lr lr: 8.1e-05
2022-07-27 23:23:48.152578: This epoch took 75.877010 s

2022-07-27 23:23:48.154788: 
epoch:  106
2022-07-27 23:24:58.049537: train loss : -0.7435
2022-07-27 23:25:03.680575: validation loss: -0.4041
2022-07-27 23:25:03.706660: Average global foreground Dice: [0.7772]
2022-07-27 23:25:03.727117: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:25:04.357202: Suus1 maybe_update_lr lr: 8.1e-05
2022-07-27 23:25:04.360117: This epoch took 76.203218 s

2022-07-27 23:25:04.362180: 
epoch:  107
2022-07-27 23:26:13.951360: train loss : -0.8114
2022-07-27 23:26:19.904788: validation loss: -0.4956
2022-07-27 23:26:19.941657: Average global foreground Dice: [0.7508]
2022-07-27 23:26:19.953993: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:26:20.747722: Suus1 maybe_update_lr lr: 8e-05
2022-07-27 23:26:20.750479: This epoch took 76.385902 s

2022-07-27 23:26:20.752863: 
epoch:  108
2022-07-27 23:27:30.517402: train loss : -0.7707
2022-07-27 23:27:36.702027: validation loss: -0.4327
2022-07-27 23:27:36.728495: Average global foreground Dice: [0.7559]
2022-07-27 23:27:36.730649: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:27:37.667982: Suus1 maybe_update_lr lr: 8e-05
2022-07-27 23:27:37.670564: This epoch took 76.915367 s

2022-07-27 23:27:37.674377: 
epoch:  109
2022-07-27 23:28:47.658194: train loss : -0.7493
2022-07-27 23:28:53.323249: validation loss: -0.3544
2022-07-27 23:28:53.356688: Average global foreground Dice: [0.6855]
2022-07-27 23:28:53.367215: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:28:54.416691: Suus1 maybe_update_lr lr: 8e-05
2022-07-27 23:28:54.420081: This epoch took 76.743340 s

2022-07-27 23:28:54.431303: 
epoch:  110
2022-07-27 23:30:05.715120: train loss : -0.7595
2022-07-27 23:30:11.737235: validation loss: -0.3153
2022-07-27 23:30:11.772366: Average global foreground Dice: [0.6572]
2022-07-27 23:30:11.802468: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:30:12.779523: Suus1 maybe_update_lr lr: 8e-05
2022-07-27 23:30:12.782231: This epoch took 78.333019 s

2022-07-27 23:30:12.784494: 
epoch:  111
2022-07-27 23:31:23.180587: train loss : -0.7624
2022-07-27 23:31:28.750810: validation loss: -0.4310
2022-07-27 23:31:28.770600: Average global foreground Dice: [0.7454]
2022-07-27 23:31:28.794039: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:31:29.549714: Suus1 maybe_update_lr lr: 8e-05
2022-07-27 23:31:29.551945: This epoch took 76.765214 s

2022-07-27 23:31:29.554228: 
epoch:  112
2022-07-27 23:32:38.998702: train loss : -0.7831
2022-07-27 23:32:44.994500: validation loss: -0.3378
2022-07-27 23:32:45.017612: Average global foreground Dice: [0.6993]
2022-07-27 23:32:45.020259: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:32:45.805986: Suus1 maybe_update_lr lr: 7.9e-05
2022-07-27 23:32:45.808213: This epoch took 76.251963 s

2022-07-27 23:32:45.810107: 
epoch:  113
2022-07-27 23:33:54.528109: train loss : -0.7818
2022-07-27 23:34:00.519273: validation loss: -0.3864
2022-07-27 23:34:00.528390: Average global foreground Dice: [0.6544]
2022-07-27 23:34:00.530865: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:34:01.418987: Suus1 maybe_update_lr lr: 7.9e-05
2022-07-27 23:34:01.437378: This epoch took 75.625253 s

2022-07-27 23:34:01.458966: 
epoch:  114
2022-07-27 23:35:11.950124: train loss : -0.7571
2022-07-27 23:35:17.607497: validation loss: -0.4011
2022-07-27 23:35:17.622013: Average global foreground Dice: [0.7417]
2022-07-27 23:35:17.624922: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:35:18.572757: Suus1 maybe_update_lr lr: 7.9e-05
2022-07-27 23:35:18.575858: This epoch took 77.090355 s

2022-07-27 23:35:18.578541: 
epoch:  115
2022-07-27 23:36:27.608780: train loss : -0.7929
2022-07-27 23:36:33.400973: validation loss: -0.4381
2022-07-27 23:36:33.415069: Average global foreground Dice: [0.7809]
2022-07-27 23:36:33.435193: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:36:34.365018: Suus1 maybe_update_lr lr: 7.9e-05
2022-07-27 23:36:34.367980: This epoch took 75.787045 s

2022-07-27 23:36:34.371098: 
epoch:  116
2022-07-27 23:37:44.197512: train loss : -0.8011
2022-07-27 23:37:49.924803: validation loss: -0.4245
2022-07-27 23:37:49.961638: Average global foreground Dice: [0.7675]
2022-07-27 23:37:49.975074: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:37:50.978097: Suus1 maybe_update_lr lr: 7.9e-05
2022-07-27 23:37:50.980997: saving best epoch checkpoint...
2022-07-27 23:37:51.135454: saving checkpoint...
2022-07-27 23:37:57.756962: done, saving took 6.77 seconds
2022-07-27 23:37:57.765231: This epoch took 83.390911 s

2022-07-27 23:37:57.768190: 
epoch:  117
2022-07-27 23:39:07.055106: train loss : -0.8092
2022-07-27 23:39:12.715675: validation loss: -0.5207
2022-07-27 23:39:12.747999: Average global foreground Dice: [0.8217]
2022-07-27 23:39:12.782001: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:39:13.689830: Suus1 maybe_update_lr lr: 7.8e-05
2022-07-27 23:39:13.692422: saving best epoch checkpoint...
2022-07-27 23:39:13.884247: saving checkpoint...
2022-07-27 23:39:20.693079: done, saving took 7.00 seconds
2022-07-27 23:39:20.705763: This epoch took 82.935215 s

2022-07-27 23:39:20.708034: 
epoch:  118
2022-07-27 23:40:29.672172: train loss : -0.7915
2022-07-27 23:40:35.377405: validation loss: -0.5064
2022-07-27 23:40:35.409772: Average global foreground Dice: [0.7798]
2022-07-27 23:40:35.431138: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:40:36.339299: Suus1 maybe_update_lr lr: 7.8e-05
2022-07-27 23:40:36.367026: saving best epoch checkpoint...
2022-07-27 23:40:36.634816: saving checkpoint...
2022-07-27 23:40:43.337927: done, saving took 6.94 seconds
2022-07-27 23:40:43.351201: This epoch took 82.640571 s

2022-07-27 23:40:43.353594: 
epoch:  119
2022-07-27 23:41:52.240827: train loss : -0.7898
2022-07-27 23:41:58.047454: validation loss: -0.2895
2022-07-27 23:41:58.073659: Average global foreground Dice: [0.5855]
2022-07-27 23:41:58.088341: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:41:58.909076: Suus1 maybe_update_lr lr: 7.8e-05
2022-07-27 23:41:58.912864: This epoch took 75.556841 s

2022-07-27 23:41:58.917084: 
epoch:  120
2022-07-27 23:43:08.752429: train loss : -0.8174
2022-07-27 23:43:14.513716: validation loss: -0.3759
2022-07-27 23:43:14.558509: Average global foreground Dice: [0.7739]
2022-07-27 23:43:14.589006: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:43:15.444509: Suus1 maybe_update_lr lr: 7.8e-05
2022-07-27 23:43:15.447662: This epoch took 76.528003 s

2022-07-27 23:43:15.450096: 
epoch:  121
2022-07-27 23:44:24.266982: train loss : -0.7761
2022-07-27 23:44:30.436688: validation loss: -0.3619
2022-07-27 23:44:30.451114: Average global foreground Dice: [0.704]
2022-07-27 23:44:30.468002: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:44:31.344767: Suus1 maybe_update_lr lr: 7.8e-05
2022-07-27 23:44:31.347685: This epoch took 75.894798 s

2022-07-27 23:44:31.350426: 
epoch:  122
2022-07-27 23:45:40.633328: train loss : -0.8182
2022-07-27 23:45:46.303148: validation loss: -0.4690
2022-07-27 23:45:46.330823: Average global foreground Dice: [0.7569]
2022-07-27 23:45:46.339476: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:45:47.226767: Suus1 maybe_update_lr lr: 7.8e-05
2022-07-27 23:45:47.229718: This epoch took 75.876754 s

2022-07-27 23:45:47.232530: 
epoch:  123
2022-07-27 23:46:57.059160: train loss : -0.8053
2022-07-27 23:47:03.346405: validation loss: -0.4116
2022-07-27 23:47:03.353394: Average global foreground Dice: [0.7923]
2022-07-27 23:47:03.360725: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:47:04.187258: Suus1 maybe_update_lr lr: 7.7e-05
2022-07-27 23:47:04.190801: This epoch took 76.955485 s

2022-07-27 23:47:04.193480: 
epoch:  124
2022-07-27 23:48:13.763057: train loss : -0.7841
2022-07-27 23:48:19.956788: validation loss: -0.5037
2022-07-27 23:48:19.969792: Average global foreground Dice: [0.7989]
2022-07-27 23:48:19.978146: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:48:20.775047: Suus1 maybe_update_lr lr: 7.7e-05
2022-07-27 23:48:20.778380: saving best epoch checkpoint...
2022-07-27 23:48:20.981539: saving checkpoint...
2022-07-27 23:48:27.571047: done, saving took 6.79 seconds
2022-07-27 23:48:27.580340: This epoch took 83.384119 s

2022-07-27 23:48:27.582732: 
epoch:  125
2022-07-27 23:49:39.199269: train loss : -0.8221
2022-07-27 23:49:44.986344: validation loss: -0.4876
2022-07-27 23:49:45.016513: Average global foreground Dice: [0.7686]
2022-07-27 23:49:45.030455: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:49:45.755867: Suus1 maybe_update_lr lr: 7.7e-05
2022-07-27 23:49:45.758704: saving best epoch checkpoint...
2022-07-27 23:49:45.889025: saving checkpoint...
2022-07-27 23:49:52.935273: done, saving took 7.17 seconds
2022-07-27 23:49:52.949271: This epoch took 85.364389 s

2022-07-27 23:49:52.951648: 
epoch:  126
2022-07-27 23:51:01.520867: train loss : -0.8419
2022-07-27 23:51:07.525923: validation loss: -0.3439
2022-07-27 23:51:07.543355: Average global foreground Dice: [0.7209]
2022-07-27 23:51:07.547223: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:51:08.149719: Suus1 maybe_update_lr lr: 7.7e-05
2022-07-27 23:51:08.152452: This epoch took 75.198280 s

2022-07-27 23:51:08.155097: 
epoch:  127
2022-07-27 23:52:17.151963: train loss : -0.7888
2022-07-27 23:52:22.731215: validation loss: -0.3733
2022-07-27 23:52:22.761344: Average global foreground Dice: [0.6821]
2022-07-27 23:52:22.787034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:52:23.324867: Suus1 maybe_update_lr lr: 7.7e-05
2022-07-27 23:52:23.327987: This epoch took 75.170642 s

2022-07-27 23:52:23.330767: 
epoch:  128
2022-07-27 23:53:33.802699: train loss : -0.7875
2022-07-27 23:53:39.492846: validation loss: -0.4702
2022-07-27 23:53:39.532181: Average global foreground Dice: [0.798]
2022-07-27 23:53:39.552951: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:53:40.423239: Suus1 maybe_update_lr lr: 7.6e-05
2022-07-27 23:53:40.426403: This epoch took 77.093065 s

2022-07-27 23:53:40.429098: 
epoch:  129
2022-07-27 23:54:49.186798: train loss : -0.8273
2022-07-27 23:54:55.103404: validation loss: -0.4411
2022-07-27 23:54:55.119135: Average global foreground Dice: [0.7568]
2022-07-27 23:54:55.135394: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:54:56.001269: Suus1 maybe_update_lr lr: 7.6e-05
2022-07-27 23:54:56.003813: This epoch took 75.571893 s

2022-07-27 23:54:56.006336: 
epoch:  130
2022-07-27 23:56:06.422422: train loss : -0.8353
2022-07-27 23:56:12.280921: validation loss: -0.4571
2022-07-27 23:56:12.285539: Average global foreground Dice: [0.7591]
2022-07-27 23:56:12.297985: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:56:13.253141: Suus1 maybe_update_lr lr: 7.6e-05
2022-07-27 23:56:13.256233: saving best epoch checkpoint...
2022-07-27 23:56:13.430416: saving checkpoint...
2022-07-27 23:56:20.527092: done, saving took 7.27 seconds
2022-07-27 23:56:20.536098: This epoch took 84.527051 s

2022-07-27 23:56:20.539263: 
epoch:  131
2022-07-27 23:57:30.794811: train loss : -0.8019
2022-07-27 23:57:36.595364: validation loss: -0.4897
2022-07-27 23:57:36.628033: Average global foreground Dice: [0.8074]
2022-07-27 23:57:36.637642: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:57:37.627263: Suus1 maybe_update_lr lr: 7.6e-05
2022-07-27 23:57:37.629928: saving best epoch checkpoint...
2022-07-27 23:57:37.740921: saving checkpoint...
2022-07-27 23:57:44.647518: done, saving took 7.02 seconds
2022-07-27 23:57:44.659148: This epoch took 84.117232 s

2022-07-27 23:57:44.661648: 
epoch:  132
2022-07-27 23:58:53.182928: train loss : -0.7862
2022-07-27 23:58:58.797698: validation loss: -0.4294
2022-07-27 23:58:58.818759: Average global foreground Dice: [0.697]
2022-07-27 23:58:58.836961: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-27 23:58:59.782955: Suus1 maybe_update_lr lr: 7.6e-05
2022-07-27 23:58:59.785490: This epoch took 75.121134 s

2022-07-27 23:58:59.788321: 
epoch:  133
2022-07-28 00:00:08.282579: train loss : -0.8230
2022-07-28 00:00:13.941272: validation loss: -0.4576
2022-07-28 00:00:13.974516: Average global foreground Dice: [0.7541]
2022-07-28 00:00:13.989028: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:00:14.848070: Suus1 maybe_update_lr lr: 7.6e-05
2022-07-28 00:00:14.850613: This epoch took 75.059718 s

2022-07-28 00:00:14.852808: 
epoch:  134
2022-07-28 00:01:23.800155: train loss : -0.8057
2022-07-28 00:01:29.181811: validation loss: -0.4343
2022-07-28 00:01:29.202901: Average global foreground Dice: [0.7077]
2022-07-28 00:01:29.216858: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:01:29.952418: Suus1 maybe_update_lr lr: 7.5e-05
2022-07-28 00:01:29.954944: This epoch took 75.099938 s

2022-07-28 00:01:29.957147: 
epoch:  135
2022-07-28 00:02:39.753287: train loss : -0.8145
2022-07-28 00:02:45.389614: validation loss: -0.4437
2022-07-28 00:02:45.450336: Average global foreground Dice: [0.8032]
2022-07-28 00:02:45.453797: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:02:46.185070: Suus1 maybe_update_lr lr: 7.5e-05
2022-07-28 00:02:46.189790: This epoch took 76.230284 s

2022-07-28 00:02:46.193548: 
epoch:  136
2022-07-28 00:03:55.272393: train loss : -0.8119
2022-07-28 00:04:01.209197: validation loss: -0.4187
2022-07-28 00:04:01.232368: Average global foreground Dice: [0.7648]
2022-07-28 00:04:01.250469: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:04:02.025374: Suus1 maybe_update_lr lr: 7.5e-05
2022-07-28 00:04:02.027809: This epoch took 75.828591 s

2022-07-28 00:04:02.029887: 
epoch:  137
2022-07-28 00:05:11.282472: train loss : -0.7961
2022-07-28 00:05:17.017876: validation loss: -0.4803
2022-07-28 00:05:17.040950: Average global foreground Dice: [0.746]
2022-07-28 00:05:17.070037: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:05:17.944678: Suus1 maybe_update_lr lr: 7.5e-05
2022-07-28 00:05:17.946818: This epoch took 75.914819 s

2022-07-28 00:05:17.949158: 
epoch:  138
2022-07-28 00:06:27.264585: train loss : -0.8156
2022-07-28 00:06:33.440502: validation loss: -0.5428
2022-07-28 00:06:33.454405: Average global foreground Dice: [0.7711]
2022-07-28 00:06:33.457042: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:06:34.244792: Suus1 maybe_update_lr lr: 7.5e-05
2022-07-28 00:06:34.247842: saving best epoch checkpoint...
2022-07-28 00:06:34.456430: saving checkpoint...
2022-07-28 00:06:39.851922: done, saving took 5.60 seconds
2022-07-28 00:06:39.865099: This epoch took 81.913962 s

2022-07-28 00:06:39.867285: 
epoch:  139
2022-07-28 00:07:48.935251: train loss : -0.8144
2022-07-28 00:07:54.712853: validation loss: -0.2895
2022-07-28 00:07:54.729584: Average global foreground Dice: [0.5651]
2022-07-28 00:07:54.763033: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:07:55.710620: Suus1 maybe_update_lr lr: 7.4e-05
2022-07-28 00:07:55.713838: This epoch took 75.844539 s

2022-07-28 00:07:55.716254: 
epoch:  140
2022-07-28 00:09:06.292279: train loss : -0.8183
2022-07-28 00:09:12.343578: validation loss: -0.4014
2022-07-28 00:09:12.347200: Average global foreground Dice: [0.7925]
2022-07-28 00:09:12.349703: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:09:13.176877: Suus1 maybe_update_lr lr: 7.4e-05
2022-07-28 00:09:13.179461: This epoch took 77.460864 s

2022-07-28 00:09:13.181903: 
epoch:  141
2022-07-28 00:10:21.576820: train loss : -0.8194
2022-07-28 00:10:27.068257: validation loss: -0.6302
2022-07-28 00:10:27.091527: Average global foreground Dice: [0.8517]
2022-07-28 00:10:27.119573: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:10:28.086914: Suus1 maybe_update_lr lr: 7.4e-05
2022-07-28 00:10:28.090568: This epoch took 74.906308 s

2022-07-28 00:10:28.111994: 
epoch:  142
2022-07-28 00:11:36.775385: train loss : -0.8282
2022-07-28 00:11:42.764346: validation loss: -0.4840
2022-07-28 00:11:42.781339: Average global foreground Dice: [0.8185]
2022-07-28 00:11:42.802042: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:11:43.564779: Suus1 maybe_update_lr lr: 7.4e-05
2022-07-28 00:11:43.566978: saving best epoch checkpoint...
2022-07-28 00:11:43.690823: saving checkpoint...
2022-07-28 00:11:48.679563: done, saving took 5.11 seconds
2022-07-28 00:11:48.687086: This epoch took 80.554988 s

2022-07-28 00:11:48.689169: 
epoch:  143
2022-07-28 00:12:57.164929: train loss : -0.8357
2022-07-28 00:13:02.988519: validation loss: -0.1926
2022-07-28 00:13:03.002954: Average global foreground Dice: [0.6137]
2022-07-28 00:13:03.005711: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:13:03.896458: Suus1 maybe_update_lr lr: 7.4e-05
2022-07-28 00:13:03.899220: This epoch took 75.208040 s

2022-07-28 00:13:03.901509: 
epoch:  144
2022-07-28 00:14:13.490863: train loss : -0.8336
2022-07-28 00:14:19.261523: validation loss: -0.3919
2022-07-28 00:14:19.284538: Average global foreground Dice: [0.699]
2022-07-28 00:14:19.303941: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:14:20.286123: Suus1 maybe_update_lr lr: 7.3e-05
2022-07-28 00:14:20.288445: This epoch took 76.384696 s

2022-07-28 00:14:20.290559: 
epoch:  145
2022-07-28 00:15:29.556621: train loss : -0.8524
2022-07-28 00:15:35.172340: validation loss: -0.4263
2022-07-28 00:15:35.189191: Average global foreground Dice: [0.8138]
2022-07-28 00:15:35.191550: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:15:36.087745: Suus1 maybe_update_lr lr: 7.3e-05
2022-07-28 00:15:36.089959: This epoch took 75.797261 s

2022-07-28 00:15:36.091889: 
epoch:  146
2022-07-28 00:16:44.691065: train loss : -0.8389
2022-07-28 00:16:50.270056: validation loss: -0.3805
2022-07-28 00:16:50.296702: Average global foreground Dice: [0.6912]
2022-07-28 00:16:50.314033: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:16:51.224614: Suus1 maybe_update_lr lr: 7.3e-05
2022-07-28 00:16:51.227219: This epoch took 75.133233 s

2022-07-28 00:16:51.229413: 
epoch:  147
2022-07-28 00:18:00.607720: train loss : -0.8287
2022-07-28 00:18:06.508234: validation loss: -0.3369
2022-07-28 00:18:06.522332: Average global foreground Dice: [0.7273]
2022-07-28 00:18:06.528951: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:18:07.133999: Suus1 maybe_update_lr lr: 7.3e-05
2022-07-28 00:18:07.136410: This epoch took 75.904420 s

2022-07-28 00:18:07.138964: 
epoch:  148
2022-07-28 00:19:16.188129: train loss : -0.8237
2022-07-28 00:19:21.730469: validation loss: -0.4331
2022-07-28 00:19:21.744453: Average global foreground Dice: [0.7428]
2022-07-28 00:19:21.756374: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:19:22.382870: Suus1 maybe_update_lr lr: 7.3e-05
2022-07-28 00:19:22.385220: This epoch took 75.243833 s

2022-07-28 00:19:22.387110: 
epoch:  149
2022-07-28 00:20:31.133188: train loss : -0.8211
2022-07-28 00:20:37.227242: validation loss: -0.4110
2022-07-28 00:20:37.241001: Average global foreground Dice: [0.7924]
2022-07-28 00:20:37.264443: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:20:38.166539: Suus1 maybe_update_lr lr: 7.3e-05
2022-07-28 00:20:38.169959: saving scheduled checkpoint file...
2022-07-28 00:20:38.395488: saving checkpoint...
2022-07-28 00:20:43.771108: done, saving took 5.60 seconds
2022-07-28 00:20:43.785689: done
2022-07-28 00:20:43.788170: This epoch took 81.398858 s

2022-07-28 00:20:43.790454: 
epoch:  150
2022-07-28 00:21:53.345112: train loss : -0.8242
2022-07-28 00:21:58.639040: validation loss: -0.3173
2022-07-28 00:21:58.651512: Average global foreground Dice: [0.8]
2022-07-28 00:21:58.678572: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:21:59.303777: Suus1 maybe_update_lr lr: 7.2e-05
2022-07-28 00:21:59.306295: This epoch took 75.513648 s

2022-07-28 00:21:59.308528: 
epoch:  151
2022-07-28 00:23:09.554412: train loss : -0.8190
2022-07-28 00:23:15.058942: validation loss: -0.3164
2022-07-28 00:23:15.077905: Average global foreground Dice: [0.6995]
2022-07-28 00:23:15.080254: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:23:16.060768: Suus1 maybe_update_lr lr: 7.2e-05
2022-07-28 00:23:16.063089: This epoch took 76.752322 s

2022-07-28 00:23:16.065260: 
epoch:  152
2022-07-28 00:24:25.385160: train loss : -0.8191
2022-07-28 00:24:32.016508: validation loss: -0.3654
2022-07-28 00:24:32.041044: Average global foreground Dice: [0.7627]
2022-07-28 00:24:32.069033: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:24:32.938374: Suus1 maybe_update_lr lr: 7.2e-05
2022-07-28 00:24:32.940704: This epoch took 76.873295 s

2022-07-28 00:24:32.942814: 
epoch:  153
2022-07-28 00:25:42.980819: train loss : -0.7992
2022-07-28 00:25:49.524009: validation loss: -0.4679
2022-07-28 00:25:49.553807: Average global foreground Dice: [0.7442]
2022-07-28 00:25:49.572429: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:25:50.146302: Suus1 maybe_update_lr lr: 7.2e-05
2022-07-28 00:25:50.148963: This epoch took 77.203903 s

2022-07-28 00:25:50.152047: 
epoch:  154
2022-07-28 00:27:00.019346: train loss : -0.8340
2022-07-28 00:27:05.731593: validation loss: -0.4634
2022-07-28 00:27:05.756346: Average global foreground Dice: [0.8281]
2022-07-28 00:27:05.772329: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:27:06.750920: Suus1 maybe_update_lr lr: 7.2e-05
2022-07-28 00:27:06.754021: This epoch took 76.598520 s

2022-07-28 00:27:06.756202: 
epoch:  155
2022-07-28 00:28:15.510141: train loss : -0.8302
2022-07-28 00:28:20.933934: validation loss: -0.3202
2022-07-28 00:28:20.948075: Average global foreground Dice: [0.7181]
2022-07-28 00:28:20.963525: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:28:21.640172: Suus1 maybe_update_lr lr: 7.1e-05
2022-07-28 00:28:21.642482: This epoch took 74.883849 s

2022-07-28 00:28:21.644520: 
epoch:  156
2022-07-28 00:29:30.899845: train loss : -0.8064
2022-07-28 00:29:36.373052: validation loss: -0.5233
2022-07-28 00:29:36.397619: Average global foreground Dice: [0.7911]
2022-07-28 00:29:36.400121: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:29:37.260447: Suus1 maybe_update_lr lr: 7.1e-05
2022-07-28 00:29:37.263228: This epoch took 75.616647 s

2022-07-28 00:29:37.265568: 
epoch:  157
2022-07-28 00:30:45.815979: train loss : -0.8223
2022-07-28 00:30:51.502216: validation loss: -0.4580
2022-07-28 00:30:51.522644: Average global foreground Dice: [0.7705]
2022-07-28 00:30:51.526976: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:30:52.504621: Suus1 maybe_update_lr lr: 7.1e-05
2022-07-28 00:30:52.507293: This epoch took 75.239245 s

2022-07-28 00:30:52.509820: 
epoch:  158
2022-07-28 00:32:01.669571: train loss : -0.7832
2022-07-28 00:32:07.308102: validation loss: -0.3680
2022-07-28 00:32:07.318011: Average global foreground Dice: [0.7018]
2022-07-28 00:32:07.337068: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:32:08.262309: Suus1 maybe_update_lr lr: 7.1e-05
2022-07-28 00:32:08.264913: This epoch took 75.752700 s

2022-07-28 00:32:08.267195: 
epoch:  159
2022-07-28 00:33:16.962341: train loss : -0.8353
2022-07-28 00:33:22.980865: validation loss: -0.2830
2022-07-28 00:33:23.013875: Average global foreground Dice: [0.6172]
2022-07-28 00:33:23.025347: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:33:23.808268: Suus1 maybe_update_lr lr: 7.1e-05
2022-07-28 00:33:23.810785: This epoch took 75.541013 s

2022-07-28 00:33:23.813035: 
epoch:  160
2022-07-28 00:34:33.832529: train loss : -0.8488
2022-07-28 00:34:39.630600: validation loss: -0.4036
2022-07-28 00:34:39.640445: Average global foreground Dice: [0.7963]
2022-07-28 00:34:39.643273: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:34:40.447088: Suus1 maybe_update_lr lr: 7e-05
2022-07-28 00:34:40.449593: This epoch took 76.634152 s

2022-07-28 00:34:40.451929: 
epoch:  161
2022-07-28 00:35:50.035913: train loss : -0.8591
2022-07-28 00:35:55.674536: validation loss: -0.3731
2022-07-28 00:35:55.708896: Average global foreground Dice: [0.792]
2022-07-28 00:35:55.749797: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:35:56.541321: Suus1 maybe_update_lr lr: 7e-05
2022-07-28 00:35:56.544548: This epoch took 76.090470 s

2022-07-28 00:35:56.547013: 
epoch:  162
2022-07-28 00:37:07.044857: train loss : -0.8525
2022-07-28 00:37:12.773969: validation loss: -0.2325
2022-07-28 00:37:12.777574: Average global foreground Dice: [0.5978]
2022-07-28 00:37:12.796998: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:37:13.755868: Suus1 maybe_update_lr lr: 7e-05
2022-07-28 00:37:13.760736: This epoch took 77.211242 s

2022-07-28 00:37:13.763134: 
epoch:  163
2022-07-28 00:38:22.905268: train loss : -0.8420
2022-07-28 00:38:29.113193: validation loss: -0.3729
2022-07-28 00:38:29.116460: Average global foreground Dice: [0.7978]
2022-07-28 00:38:29.119010: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:38:29.717738: Suus1 maybe_update_lr lr: 7e-05
2022-07-28 00:38:29.720401: This epoch took 75.954739 s

2022-07-28 00:38:29.722649: 
epoch:  164
2022-07-28 00:39:39.019843: train loss : -0.8507
2022-07-28 00:39:44.520256: validation loss: -0.5815
2022-07-28 00:39:44.551744: Average global foreground Dice: [0.8099]
2022-07-28 00:39:44.554543: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:39:45.324341: Suus1 maybe_update_lr lr: 7e-05
2022-07-28 00:39:45.326855: This epoch took 75.601928 s

2022-07-28 00:39:45.329045: 
epoch:  165
2022-07-28 00:40:55.122274: train loss : -0.8113
2022-07-28 00:41:01.172439: validation loss: -0.4019
2022-07-28 00:41:01.195675: Average global foreground Dice: [0.8047]
2022-07-28 00:41:01.219573: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:41:02.235572: Suus1 maybe_update_lr lr: 7e-05
2022-07-28 00:41:02.238794: This epoch took 76.907401 s

2022-07-28 00:41:02.241421: 
epoch:  166
2022-07-28 00:42:12.161061: train loss : -0.8469
2022-07-28 00:42:18.286205: validation loss: -0.2706
2022-07-28 00:42:18.290020: Average global foreground Dice: [0.6427]
2022-07-28 00:42:18.308991: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:42:19.029633: Suus1 maybe_update_lr lr: 6.9e-05
2022-07-28 00:42:19.031932: This epoch took 76.788077 s

2022-07-28 00:42:19.033988: 
epoch:  167
2022-07-28 00:43:27.981605: train loss : -0.8063
2022-07-28 00:43:33.493786: validation loss: -0.3514
2022-07-28 00:43:33.528711: Average global foreground Dice: [0.71]
2022-07-28 00:43:33.541083: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:43:34.382383: Suus1 maybe_update_lr lr: 6.9e-05
2022-07-28 00:43:34.384785: This epoch took 75.348532 s

2022-07-28 00:43:34.386790: 
epoch:  168
2022-07-28 00:44:42.165118: train loss : -0.8372
2022-07-28 00:44:48.061172: validation loss: -0.2969
2022-07-28 00:44:48.074787: Average global foreground Dice: [0.6032]
2022-07-28 00:44:48.087278: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:44:48.950390: Suus1 maybe_update_lr lr: 6.9e-05
2022-07-28 00:44:48.952735: This epoch took 74.563817 s

2022-07-28 00:44:48.954761: 
epoch:  169
2022-07-28 00:45:59.452706: train loss : -0.8230
2022-07-28 00:46:05.015753: validation loss: -0.3516
2022-07-28 00:46:05.023396: Average global foreground Dice: [0.7241]
2022-07-28 00:46:05.030344: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:46:05.712713: Suus1 maybe_update_lr lr: 6.9e-05
2022-07-28 00:46:05.715071: This epoch took 76.758327 s

2022-07-28 00:46:05.717078: 
epoch:  170
2022-07-28 00:47:15.214763: train loss : -0.8494
2022-07-28 00:47:20.960327: validation loss: -0.4632
2022-07-28 00:47:20.964830: Average global foreground Dice: [0.8413]
2022-07-28 00:47:20.970955: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:47:21.893896: Suus1 maybe_update_lr lr: 6.9e-05
2022-07-28 00:47:21.896490: This epoch took 76.177158 s

2022-07-28 00:47:21.899517: 
epoch:  171
2022-07-28 00:48:31.837309: train loss : -0.8331
2022-07-28 00:48:37.917272: validation loss: -0.4249
2022-07-28 00:48:37.962614: Average global foreground Dice: [0.762]
2022-07-28 00:48:37.973693: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:48:38.841624: Suus1 maybe_update_lr lr: 6.8e-05
2022-07-28 00:48:38.844531: This epoch took 76.942768 s

2022-07-28 00:48:38.847820: 
epoch:  172
2022-07-28 00:49:47.440463: train loss : -0.8193
2022-07-28 00:49:52.926660: validation loss: -0.3003
2022-07-28 00:49:52.931192: Average global foreground Dice: [0.6826]
2022-07-28 00:49:52.947031: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:49:53.767055: Suus1 maybe_update_lr lr: 6.8e-05
2022-07-28 00:49:53.770084: This epoch took 74.920224 s

2022-07-28 00:49:53.772785: 
epoch:  173
2022-07-28 00:51:04.184038: train loss : -0.8142
2022-07-28 00:51:09.876354: validation loss: -0.3990
2022-07-28 00:51:09.910034: Average global foreground Dice: [0.7766]
2022-07-28 00:51:09.927006: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:51:10.787437: Suus1 maybe_update_lr lr: 6.8e-05
2022-07-28 00:51:10.790070: This epoch took 77.014670 s

2022-07-28 00:51:10.792413: 
epoch:  174
2022-07-28 00:52:18.981194: train loss : -0.8291
2022-07-28 00:52:24.757755: validation loss: -0.5917
2022-07-28 00:52:24.781546: Average global foreground Dice: [0.825]
2022-07-28 00:52:24.784104: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:52:25.534078: Suus1 maybe_update_lr lr: 6.8e-05
2022-07-28 00:52:25.536413: This epoch took 74.741493 s

2022-07-28 00:52:25.538413: 
epoch:  175
2022-07-28 00:53:33.819495: train loss : -0.8511
2022-07-28 00:53:39.224364: validation loss: -0.4124
2022-07-28 00:53:39.235590: Average global foreground Dice: [0.7873]
2022-07-28 00:53:39.238549: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:53:40.040991: Suus1 maybe_update_lr lr: 6.8e-05
2022-07-28 00:53:40.043326: This epoch took 74.503023 s

2022-07-28 00:53:40.045664: 
epoch:  176
2022-07-28 00:54:49.330971: train loss : -0.8510
2022-07-28 00:54:55.317000: validation loss: -0.4957
2022-07-28 00:54:55.320891: Average global foreground Dice: [0.8345]
2022-07-28 00:54:55.323716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:54:56.439168: Suus1 maybe_update_lr lr: 6.7e-05
2022-07-28 00:54:56.441996: saving best epoch checkpoint...
2022-07-28 00:54:56.722036: saving checkpoint...
2022-07-28 00:55:02.068257: done, saving took 5.62 seconds
2022-07-28 00:55:02.076803: This epoch took 82.028770 s

2022-07-28 00:55:02.079101: 
epoch:  177
2022-07-28 00:56:10.990419: train loss : -0.8302
2022-07-28 00:56:16.775998: validation loss: -0.2768
2022-07-28 00:56:16.798547: Average global foreground Dice: [0.6922]
2022-07-28 00:56:16.824310: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:56:17.600168: Suus1 maybe_update_lr lr: 6.7e-05
2022-07-28 00:56:17.602725: This epoch took 75.521518 s

2022-07-28 00:56:17.604903: 
epoch:  178
2022-07-28 00:57:26.124294: train loss : -0.8458
2022-07-28 00:57:31.449962: validation loss: -0.4001
2022-07-28 00:57:31.474867: Average global foreground Dice: [0.8051]
2022-07-28 00:57:31.478065: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:57:32.144807: Suus1 maybe_update_lr lr: 6.7e-05
2022-07-28 00:57:32.147249: This epoch took 74.540135 s

2022-07-28 00:57:32.149489: 
epoch:  179
2022-07-28 00:58:40.538076: train loss : -0.8245
2022-07-28 00:58:46.235549: validation loss: -0.4691
2022-07-28 00:58:46.257128: Average global foreground Dice: [0.827]
2022-07-28 00:58:46.287101: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 00:58:47.185613: Suus1 maybe_update_lr lr: 6.7e-05
2022-07-28 00:58:47.188375: saving best epoch checkpoint...
2022-07-28 00:58:47.399899: saving checkpoint...
2022-07-28 00:58:52.768456: done, saving took 5.58 seconds
2022-07-28 00:58:52.780230: This epoch took 80.628607 s

2022-07-28 00:58:52.782398: 
epoch:  180
2022-07-28 01:00:02.713202: train loss : -0.8691
2022-07-28 01:00:08.316214: validation loss: -0.4490
2022-07-28 01:00:08.339023: Average global foreground Dice: [0.816]
2022-07-28 01:00:08.360622: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:00:09.380330: Suus1 maybe_update_lr lr: 6.7e-05
2022-07-28 01:00:09.382981: saving best epoch checkpoint...
2022-07-28 01:00:09.571653: saving checkpoint...
2022-07-28 01:00:14.706135: done, saving took 5.32 seconds
2022-07-28 01:00:14.717399: This epoch took 81.932697 s

2022-07-28 01:00:14.719864: 
epoch:  181
2022-07-28 01:01:24.456071: train loss : -0.8333
2022-07-28 01:01:30.683743: validation loss: -0.2985
2022-07-28 01:01:30.717746: Average global foreground Dice: [0.6722]
2022-07-28 01:01:30.725953: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:01:31.431721: Suus1 maybe_update_lr lr: 6.7e-05
2022-07-28 01:01:31.433942: This epoch took 76.711622 s

2022-07-28 01:01:31.436081: 
epoch:  182
2022-07-28 01:02:42.071571: train loss : -0.8195
2022-07-28 01:02:48.194914: validation loss: -0.4301
2022-07-28 01:02:48.210947: Average global foreground Dice: [0.7894]
2022-07-28 01:02:48.213378: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:02:49.065608: Suus1 maybe_update_lr lr: 6.6e-05
2022-07-28 01:02:49.068306: This epoch took 77.630005 s

2022-07-28 01:02:49.070842: 
epoch:  183
2022-07-28 01:03:58.343098: train loss : -0.8414
2022-07-28 01:04:04.057385: validation loss: -0.3542
2022-07-28 01:04:04.080689: Average global foreground Dice: [0.7642]
2022-07-28 01:04:04.090898: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:04:04.926271: Suus1 maybe_update_lr lr: 6.6e-05
2022-07-28 01:04:04.928863: This epoch took 75.855374 s

2022-07-28 01:04:04.931176: 
epoch:  184
2022-07-28 01:05:14.665018: train loss : -0.8626
2022-07-28 01:05:20.462656: validation loss: -0.5002
2022-07-28 01:05:20.488630: Average global foreground Dice: [0.8393]
2022-07-28 01:05:20.509960: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:05:21.465190: Suus1 maybe_update_lr lr: 6.6e-05
2022-07-28 01:05:21.467663: saving best epoch checkpoint...
2022-07-28 01:05:21.779485: saving checkpoint...
2022-07-28 01:05:27.274292: done, saving took 5.80 seconds
2022-07-28 01:05:27.286424: This epoch took 82.353204 s

2022-07-28 01:05:27.288676: 
epoch:  185
2022-07-28 01:06:36.562651: train loss : -0.8309
2022-07-28 01:06:42.322440: validation loss: -0.4749
2022-07-28 01:06:42.326373: Average global foreground Dice: [0.836]
2022-07-28 01:06:42.328937: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:06:43.441464: Suus1 maybe_update_lr lr: 6.6e-05
2022-07-28 01:06:43.461361: saving best epoch checkpoint...
2022-07-28 01:06:43.717103: saving checkpoint...
2022-07-28 01:06:48.991175: done, saving took 5.50 seconds
2022-07-28 01:06:49.002213: This epoch took 81.711421 s

2022-07-28 01:06:49.004254: 
epoch:  186
2022-07-28 01:07:58.138405: train loss : -0.8425
2022-07-28 01:08:03.797322: validation loss: -0.3705
2022-07-28 01:08:03.838476: Average global foreground Dice: [0.8099]
2022-07-28 01:08:03.858021: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:08:04.886798: Suus1 maybe_update_lr lr: 6.6e-05
2022-07-28 01:08:04.889452: saving best epoch checkpoint...
2022-07-28 01:08:05.045237: saving checkpoint...
2022-07-28 01:08:10.165755: done, saving took 5.27 seconds
2022-07-28 01:08:10.174709: This epoch took 81.168408 s

2022-07-28 01:08:10.178133: 
epoch:  187
2022-07-28 01:09:19.173218: train loss : -0.8473
2022-07-28 01:09:24.617332: validation loss: -0.4585
2022-07-28 01:09:24.631700: Average global foreground Dice: [0.791]
2022-07-28 01:09:24.657007: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:09:25.611842: Suus1 maybe_update_lr lr: 6.5e-05
2022-07-28 01:09:25.614595: saving best epoch checkpoint...
2022-07-28 01:09:25.768309: saving checkpoint...
2022-07-28 01:09:30.988117: done, saving took 5.37 seconds
2022-07-28 01:09:30.996947: This epoch took 80.816365 s

2022-07-28 01:09:30.999159: 
epoch:  188
2022-07-28 01:10:40.780108: train loss : -0.8342
2022-07-28 01:10:46.444809: validation loss: -0.3759
2022-07-28 01:10:46.467412: Average global foreground Dice: [0.7455]
2022-07-28 01:10:46.493235: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:10:47.683422: Suus1 maybe_update_lr lr: 6.5e-05
2022-07-28 01:10:47.685750: This epoch took 76.684413 s

2022-07-28 01:10:47.687714: 
epoch:  189
2022-07-28 01:11:55.479084: train loss : -0.8179
2022-07-28 01:12:01.664255: validation loss: -0.2036
2022-07-28 01:12:01.679936: Average global foreground Dice: [0.4558]
2022-07-28 01:12:01.693289: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:12:02.543335: Suus1 maybe_update_lr lr: 6.5e-05
2022-07-28 01:12:02.547125: This epoch took 74.857115 s

2022-07-28 01:12:02.550939: 
epoch:  190
2022-07-28 01:13:10.942726: train loss : -0.8249
2022-07-28 01:13:16.658301: validation loss: -0.4632
2022-07-28 01:13:16.670048: Average global foreground Dice: [0.8064]
2022-07-28 01:13:16.673923: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:13:17.242864: Suus1 maybe_update_lr lr: 6.5e-05
2022-07-28 01:13:17.245296: This epoch took 74.691975 s

2022-07-28 01:13:17.247419: 
epoch:  191
2022-07-28 01:14:25.893048: train loss : -0.8629
2022-07-28 01:14:31.505336: validation loss: -0.4515
2022-07-28 01:14:31.513095: Average global foreground Dice: [0.7999]
2022-07-28 01:14:31.541004: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:14:32.686183: Suus1 maybe_update_lr lr: 6.5e-05
2022-07-28 01:14:32.688621: This epoch took 75.439123 s

2022-07-28 01:14:32.690779: 
epoch:  192
2022-07-28 01:15:42.441831: train loss : -0.8372
2022-07-28 01:15:48.462225: validation loss: -0.3031
2022-07-28 01:15:48.477152: Average global foreground Dice: [0.7174]
2022-07-28 01:15:48.481893: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:15:49.379088: Suus1 maybe_update_lr lr: 6.4e-05
2022-07-28 01:15:49.381477: This epoch took 76.688304 s

2022-07-28 01:15:49.383681: 
epoch:  193
2022-07-28 01:16:58.584891: train loss : -0.8380
2022-07-28 01:17:04.376456: validation loss: -0.3860
2022-07-28 01:17:04.382200: Average global foreground Dice: [0.7165]
2022-07-28 01:17:04.385655: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:17:05.246079: Suus1 maybe_update_lr lr: 6.4e-05
2022-07-28 01:17:05.248906: This epoch took 75.863095 s

2022-07-28 01:17:05.250960: 
epoch:  194
2022-07-28 01:18:14.738537: train loss : -0.8221
2022-07-28 01:18:21.227877: validation loss: -0.2778
2022-07-28 01:18:21.259093: Average global foreground Dice: [0.6969]
2022-07-28 01:18:21.273008: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:18:22.142751: Suus1 maybe_update_lr lr: 6.4e-05
2022-07-28 01:18:22.145361: This epoch took 76.892375 s

2022-07-28 01:18:22.147579: 
epoch:  195
2022-07-28 01:19:32.676143: train loss : -0.8442
2022-07-28 01:19:38.558161: validation loss: -0.4816
2022-07-28 01:19:38.564283: Average global foreground Dice: [0.8057]
2022-07-28 01:19:38.566875: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:19:39.342955: Suus1 maybe_update_lr lr: 6.4e-05
2022-07-28 01:19:39.345747: This epoch took 77.195034 s

2022-07-28 01:19:39.348161: 
epoch:  196
2022-07-28 01:20:47.856625: train loss : -0.8595
2022-07-28 01:20:53.704313: validation loss: -0.3412
2022-07-28 01:20:53.714373: Average global foreground Dice: [0.7679]
2022-07-28 01:20:53.731223: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:20:54.555121: Suus1 maybe_update_lr lr: 6.4e-05
2022-07-28 01:20:54.557623: This epoch took 75.207175 s

2022-07-28 01:20:54.559743: 
epoch:  197
2022-07-28 01:22:03.764687: train loss : -0.8554
2022-07-28 01:22:09.819418: validation loss: -0.2740
2022-07-28 01:22:09.825595: Average global foreground Dice: [0.6352]
2022-07-28 01:22:09.828085: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:22:10.855496: Suus1 maybe_update_lr lr: 6.4e-05
2022-07-28 01:22:10.857842: This epoch took 76.296012 s

2022-07-28 01:22:10.860036: 
epoch:  198
2022-07-28 01:23:20.645374: train loss : -0.8690
2022-07-28 01:23:26.323185: validation loss: -0.4911
2022-07-28 01:23:26.339079: Average global foreground Dice: [0.7776]
2022-07-28 01:23:26.360016: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:23:27.379599: Suus1 maybe_update_lr lr: 6.3e-05
2022-07-28 01:23:27.382283: This epoch took 76.520123 s

2022-07-28 01:23:27.385310: 
epoch:  199
2022-07-28 01:24:35.944419: train loss : -0.8596
2022-07-28 01:24:41.874253: validation loss: -0.4268
2022-07-28 01:24:41.905504: Average global foreground Dice: [0.8121]
2022-07-28 01:24:41.927978: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:24:42.847939: Suus1 maybe_update_lr lr: 6.3e-05
2022-07-28 01:24:42.850565: saving scheduled checkpoint file...
2022-07-28 01:24:43.023505: saving checkpoint...
2022-07-28 01:24:48.250540: done, saving took 5.40 seconds
2022-07-28 01:24:48.264418: done
2022-07-28 01:24:48.267153: This epoch took 80.879613 s

2022-07-28 01:24:48.269287: 
epoch:  200
2022-07-28 01:25:57.353336: train loss : -0.8537
2022-07-28 01:26:03.178384: validation loss: -0.3847
2022-07-28 01:26:03.220661: Average global foreground Dice: [0.7223]
2022-07-28 01:26:03.244252: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:26:04.202705: Suus1 maybe_update_lr lr: 6.3e-05
2022-07-28 01:26:04.205999: This epoch took 75.934603 s

2022-07-28 01:26:04.208987: 
epoch:  201
2022-07-28 01:27:12.607575: train loss : -0.8623
2022-07-28 01:27:18.654684: validation loss: -0.2931
2022-07-28 01:27:18.704469: Average global foreground Dice: [0.7059]
2022-07-28 01:27:18.707492: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:27:19.386278: Suus1 maybe_update_lr lr: 6.3e-05
2022-07-28 01:27:19.388448: This epoch took 75.176925 s

2022-07-28 01:27:19.390365: 
epoch:  202
2022-07-28 01:28:28.474540: train loss : -0.8315
2022-07-28 01:28:34.330244: validation loss: -0.5028
2022-07-28 01:28:34.341258: Average global foreground Dice: [0.8353]
2022-07-28 01:28:34.351207: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:28:35.183505: Suus1 maybe_update_lr lr: 6.3e-05
2022-07-28 01:28:35.185970: This epoch took 75.793558 s

2022-07-28 01:28:35.188077: 
epoch:  203
2022-07-28 01:29:44.463362: train loss : -0.8619
2022-07-28 01:29:50.223295: validation loss: -0.4829
2022-07-28 01:29:50.247705: Average global foreground Dice: [0.8452]
2022-07-28 01:29:50.250137: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:29:51.233283: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 01:29:51.235593: This epoch took 76.045581 s

2022-07-28 01:29:51.237610: 
epoch:  204
2022-07-28 01:31:01.871563: train loss : -0.8451
2022-07-28 01:31:07.600946: validation loss: -0.3833
2022-07-28 01:31:07.616070: Average global foreground Dice: [0.7461]
2022-07-28 01:31:07.630028: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:31:08.718537: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 01:31:08.721193: This epoch took 77.481360 s

2022-07-28 01:31:08.723674: 
epoch:  205
2022-07-28 01:32:17.095102: train loss : -0.8627
2022-07-28 01:32:22.835834: validation loss: -0.2904
2022-07-28 01:32:22.855693: Average global foreground Dice: [0.6657]
2022-07-28 01:32:22.865464: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:32:23.695988: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 01:32:23.698541: This epoch took 74.972550 s

2022-07-28 01:32:23.700583: 
epoch:  206
2022-07-28 01:33:32.801123: train loss : -0.8375
2022-07-28 01:33:38.793939: validation loss: -0.2855
2022-07-28 01:33:38.826570: Average global foreground Dice: [0.6598]
2022-07-28 01:33:38.829989: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:33:39.696652: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 01:33:39.699371: This epoch took 75.996775 s

2022-07-28 01:33:39.701509: 
epoch:  207
2022-07-28 01:34:48.533152: train loss : -0.8622
2022-07-28 01:34:54.319057: validation loss: -0.4018
2022-07-28 01:34:54.334069: Average global foreground Dice: [0.8145]
2022-07-28 01:34:54.359854: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:34:55.237709: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 01:34:55.240174: This epoch took 75.536279 s

2022-07-28 01:34:55.242078: 
epoch:  208
2022-07-28 01:36:05.646352: train loss : -0.8573
2022-07-28 01:36:11.760693: validation loss: -0.2731
2022-07-28 01:36:11.795233: Average global foreground Dice: [0.6983]
2022-07-28 01:36:11.811023: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:36:12.822293: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 01:36:12.824953: This epoch took 77.580733 s

2022-07-28 01:36:12.827115: 
epoch:  209
2022-07-28 01:37:23.014605: train loss : -0.8403
2022-07-28 01:37:28.734432: validation loss: -0.3016
2022-07-28 01:37:28.745275: Average global foreground Dice: [0.6887]
2022-07-28 01:37:28.758348: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:37:29.623626: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 01:37:29.625828: This epoch took 76.796580 s

2022-07-28 01:37:29.627767: 
epoch:  210
2022-07-28 01:38:38.560464: train loss : -0.8630
2022-07-28 01:38:44.232648: validation loss: -0.2595
2022-07-28 01:38:44.237824: Average global foreground Dice: [0.6719]
2022-07-28 01:38:44.240993: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:38:45.034947: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 01:38:45.037314: This epoch took 75.407640 s

2022-07-28 01:38:45.039575: 
epoch:  211
2022-07-28 01:39:54.114759: train loss : -0.8802
2022-07-28 01:39:59.966811: validation loss: -0.3850
2022-07-28 01:39:59.994758: Average global foreground Dice: [0.7675]
2022-07-28 01:40:00.003713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:40:00.937390: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 01:40:00.939626: This epoch took 75.897856 s

2022-07-28 01:40:00.941691: 
epoch:  212
2022-07-28 01:41:09.901213: train loss : -0.8386
2022-07-28 01:41:16.087582: validation loss: -0.3242
2022-07-28 01:41:16.099267: Average global foreground Dice: [0.7646]
2022-07-28 01:41:16.105180: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:41:16.886571: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 01:41:16.888916: This epoch took 75.945195 s

2022-07-28 01:41:16.890991: 
epoch:  213
2022-07-28 01:42:26.016304: train loss : -0.8660
2022-07-28 01:42:31.977497: validation loss: -0.3898
2022-07-28 01:42:31.990543: Average global foreground Dice: [0.7404]
2022-07-28 01:42:32.000287: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:42:32.991437: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 01:42:32.993735: This epoch took 76.100811 s

2022-07-28 01:42:32.995981: 
epoch:  214
2022-07-28 01:43:41.560463: train loss : -0.8619
2022-07-28 01:43:47.303989: validation loss: -0.4723
2022-07-28 01:43:47.319505: Average global foreground Dice: [0.8107]
2022-07-28 01:43:47.344279: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:43:48.156308: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 01:43:48.158785: This epoch took 75.160716 s

2022-07-28 01:43:48.160840: 
epoch:  215
2022-07-28 01:44:57.819648: train loss : -0.8598
2022-07-28 01:45:03.278799: validation loss: -0.4180
2022-07-28 01:45:03.291713: Average global foreground Dice: [0.7736]
2022-07-28 01:45:03.294773: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:45:04.049549: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 01:45:04.051933: This epoch took 75.888889 s

2022-07-28 01:45:04.054022: 
epoch:  216
2022-07-28 01:46:13.111645: train loss : -0.8754
2022-07-28 01:46:19.157897: validation loss: -0.3220
2022-07-28 01:46:19.161042: Average global foreground Dice: [0.7149]
2022-07-28 01:46:19.163453: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:46:19.693945: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 01:46:19.696231: This epoch took 75.640028 s

2022-07-28 01:46:19.698328: 
epoch:  217
2022-07-28 01:47:28.842072: train loss : -0.8541
2022-07-28 01:47:34.470365: validation loss: -0.3620
2022-07-28 01:47:34.495398: Average global foreground Dice: [0.8246]
2022-07-28 01:47:34.498198: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:47:35.334862: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 01:47:35.337298: This epoch took 75.636998 s

2022-07-28 01:47:35.339364: 
epoch:  218
2022-07-28 01:48:44.781109: train loss : -0.8517
2022-07-28 01:48:50.059124: validation loss: -0.2578
2022-07-28 01:48:50.062362: Average global foreground Dice: [0.7181]
2022-07-28 01:48:50.065441: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:48:50.742483: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 01:48:50.744763: This epoch took 75.403438 s

2022-07-28 01:48:50.746811: 
epoch:  219
2022-07-28 01:49:59.196705: train loss : -0.8565
2022-07-28 01:50:05.006150: validation loss: -0.4752
2022-07-28 01:50:05.023167: Average global foreground Dice: [0.8485]
2022-07-28 01:50:05.055044: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:50:05.871615: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 01:50:05.874258: This epoch took 75.125407 s

2022-07-28 01:50:05.876585: 
epoch:  220
2022-07-28 01:51:15.145827: train loss : -0.8488
2022-07-28 01:51:21.037349: validation loss: -0.4154
2022-07-28 01:51:21.042303: Average global foreground Dice: [0.7756]
2022-07-28 01:51:21.044643: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:51:21.624312: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 01:51:21.626577: This epoch took 75.746675 s

2022-07-28 01:51:21.628546: 
epoch:  221
2022-07-28 01:52:30.471780: train loss : -0.8855
2022-07-28 01:52:36.464016: validation loss: -0.3493
2022-07-28 01:52:36.485503: Average global foreground Dice: [0.7313]
2022-07-28 01:52:36.499825: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:52:37.432412: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 01:52:37.434891: This epoch took 75.804457 s

2022-07-28 01:52:37.436809: 
epoch:  222
2022-07-28 01:53:46.625236: train loss : -0.8683
2022-07-28 01:53:52.464917: validation loss: -0.4034
2022-07-28 01:53:52.472396: Average global foreground Dice: [0.8201]
2022-07-28 01:53:52.474895: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:53:53.315210: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 01:53:53.317573: This epoch took 75.878818 s

2022-07-28 01:53:53.319658: 
epoch:  223
2022-07-28 01:55:02.436962: train loss : -0.8495
2022-07-28 01:55:08.559967: validation loss: -0.2555
2022-07-28 01:55:08.563568: Average global foreground Dice: [0.6414]
2022-07-28 01:55:08.576253: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:55:09.328664: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 01:55:09.330917: This epoch took 76.009255 s

2022-07-28 01:55:09.332898: 
epoch:  224
2022-07-28 01:56:18.054657: train loss : -0.8383
2022-07-28 01:56:23.461166: validation loss: -0.3822
2022-07-28 01:56:23.467483: Average global foreground Dice: [0.7567]
2022-07-28 01:56:23.469951: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:56:24.011050: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 01:56:24.013261: This epoch took 74.678518 s

2022-07-28 01:56:24.015300: 
epoch:  225
2022-07-28 01:57:33.921683: train loss : -0.8666
2022-07-28 01:57:39.531359: validation loss: -0.5330
2022-07-28 01:57:39.536735: Average global foreground Dice: [0.8506]
2022-07-28 01:57:39.560432: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:57:40.351362: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 01:57:40.353982: This epoch took 76.336678 s

2022-07-28 01:57:40.356516: 
epoch:  226
2022-07-28 01:58:49.221293: train loss : -0.8519
2022-07-28 01:58:54.692137: validation loss: -0.4962
2022-07-28 01:58:54.696051: Average global foreground Dice: [0.856]
2022-07-28 01:58:54.711794: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 01:58:55.451773: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 01:58:55.454082: This epoch took 75.095106 s

2022-07-28 01:58:55.456144: 
epoch:  227
2022-07-28 02:00:05.552319: train loss : -0.8789
2022-07-28 02:00:10.771803: validation loss: -0.2074
2022-07-28 02:00:10.776653: Average global foreground Dice: [0.6557]
2022-07-28 02:00:10.779931: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:00:11.389506: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 02:00:11.392171: This epoch took 75.933950 s

2022-07-28 02:00:11.394349: 
epoch:  228
2022-07-28 02:01:20.583428: train loss : -0.8638
2022-07-28 02:01:26.183969: validation loss: -0.1935
2022-07-28 02:01:26.217016: Average global foreground Dice: [0.5197]
2022-07-28 02:01:26.246998: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:01:27.153803: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 02:01:27.156221: This epoch took 75.759895 s

2022-07-28 02:01:27.158417: 
epoch:  229
2022-07-28 02:02:36.473999: train loss : -0.8459
2022-07-28 02:02:42.100228: validation loss: -0.3954
2022-07-28 02:02:42.120742: Average global foreground Dice: [0.8033]
2022-07-28 02:02:42.145997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:02:43.019201: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 02:02:43.021440: This epoch took 75.861522 s

2022-07-28 02:02:43.023439: 
epoch:  230
2022-07-28 02:03:51.300172: train loss : -0.8833
2022-07-28 02:03:57.193616: validation loss: -0.3098
2022-07-28 02:03:57.214454: Average global foreground Dice: [0.6891]
2022-07-28 02:03:57.243992: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:03:58.044692: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 02:03:58.047184: This epoch took 75.021806 s

2022-07-28 02:03:58.049546: 
epoch:  231
2022-07-28 02:05:08.771474: train loss : -0.8930
2022-07-28 02:05:14.758787: validation loss: -0.4131
2022-07-28 02:05:14.764323: Average global foreground Dice: [0.735]
2022-07-28 02:05:14.775414: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:05:15.589659: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 02:05:15.592392: This epoch took 77.540612 s

2022-07-28 02:05:15.594481: 
epoch:  232
2022-07-28 02:06:25.571733: train loss : -0.8838
2022-07-28 02:06:31.219180: validation loss: -0.3270
2022-07-28 02:06:31.225034: Average global foreground Dice: [0.7253]
2022-07-28 02:06:31.227890: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:06:32.010180: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 02:06:32.012604: This epoch took 76.415914 s

2022-07-28 02:06:32.014697: 
epoch:  233
2022-07-28 02:07:40.777825: train loss : -0.8688
2022-07-28 02:07:46.453199: validation loss: -0.3726
2022-07-28 02:07:46.484937: Average global foreground Dice: [0.8302]
2022-07-28 02:07:46.499049: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:07:47.372729: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 02:07:47.375031: This epoch took 75.358318 s

2022-07-28 02:07:47.377136: 
epoch:  234
2022-07-28 02:08:56.327783: train loss : -0.8506
2022-07-28 02:09:02.088626: validation loss: -0.5989
2022-07-28 02:09:02.096020: Average global foreground Dice: [0.8531]
2022-07-28 02:09:02.098330: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:09:02.898201: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 02:09:02.900920: This epoch took 75.521656 s

2022-07-28 02:09:02.903257: 
epoch:  235
2022-07-28 02:10:11.685134: train loss : -0.8966
2022-07-28 02:10:17.715791: validation loss: -0.3796
2022-07-28 02:10:17.747909: Average global foreground Dice: [0.7785]
2022-07-28 02:10:17.762815: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:10:18.528368: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 02:10:18.530692: This epoch took 75.625140 s

2022-07-28 02:10:18.532699: 
epoch:  236
2022-07-28 02:11:28.548198: train loss : -0.8655
2022-07-28 02:11:34.747699: validation loss: -0.4340
2022-07-28 02:11:34.775127: Average global foreground Dice: [0.794]
2022-07-28 02:11:34.802030: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:11:35.731289: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 02:11:35.733519: This epoch took 77.198783 s

2022-07-28 02:11:35.735631: 
epoch:  237
2022-07-28 02:12:44.304940: train loss : -0.8587
2022-07-28 02:12:50.197278: validation loss: -0.3589
2022-07-28 02:12:50.213421: Average global foreground Dice: [0.743]
2022-07-28 02:12:50.247145: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:12:51.082025: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 02:12:51.084218: This epoch took 75.346419 s

2022-07-28 02:12:51.086338: 
epoch:  238
2022-07-28 02:14:01.126769: train loss : -0.8561
2022-07-28 02:14:06.450202: validation loss: -0.3922
2022-07-28 02:14:06.468765: Average global foreground Dice: [0.7867]
2022-07-28 02:14:06.478116: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:14:07.277989: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 02:14:07.280807: This epoch took 76.192279 s

2022-07-28 02:14:07.283003: 
epoch:  239
2022-07-28 02:15:15.201361: train loss : -0.8761
2022-07-28 02:15:21.012725: validation loss: -0.3468
2022-07-28 02:15:21.017272: Average global foreground Dice: [0.7859]
2022-07-28 02:15:21.024137: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:15:21.986403: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 02:15:21.988996: This epoch took 74.703737 s

2022-07-28 02:15:21.991273: 
epoch:  240
2022-07-28 02:16:31.080836: train loss : -0.8919
2022-07-28 02:16:36.892622: validation loss: -0.3560
2022-07-28 02:16:36.898670: Average global foreground Dice: [0.7918]
2022-07-28 02:16:36.902200: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:16:37.792562: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 02:16:37.795029: This epoch took 75.801477 s

2022-07-28 02:16:37.797743: 
epoch:  241
2022-07-28 02:17:46.830859: train loss : -0.8765
2022-07-28 02:17:52.591992: validation loss: -0.4396
2022-07-28 02:17:52.624709: Average global foreground Dice: [0.7993]
2022-07-28 02:17:52.636985: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:17:53.560961: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 02:17:53.563729: This epoch took 75.763736 s

2022-07-28 02:17:53.565963: 
epoch:  242
2022-07-28 02:19:03.025966: train loss : -0.8604
2022-07-28 02:19:08.418692: validation loss: -0.2139
2022-07-28 02:19:08.452458: Average global foreground Dice: [0.6955]
2022-07-28 02:19:08.474022: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:19:09.233099: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 02:19:09.235650: This epoch took 75.665826 s

2022-07-28 02:19:09.237744: 
epoch:  243
2022-07-28 02:20:19.253495: train loss : -0.8648
2022-07-28 02:20:25.277348: validation loss: -0.3687
2022-07-28 02:20:25.309598: Average global foreground Dice: [0.7665]
2022-07-28 02:20:25.330962: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:20:26.072800: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 02:20:26.075026: This epoch took 76.835316 s

2022-07-28 02:20:26.077327: 
epoch:  244
2022-07-28 02:21:35.464663: train loss : -0.8764
2022-07-28 02:21:41.691255: validation loss: -0.3790
2022-07-28 02:21:41.715584: Average global foreground Dice: [0.8026]
2022-07-28 02:21:41.731976: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:21:42.665756: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 02:21:42.669579: This epoch took 76.590200 s

2022-07-28 02:21:42.674250: 
epoch:  245
2022-07-28 02:22:51.490167: train loss : -0.8703
2022-07-28 02:22:57.501523: validation loss: -0.4689
2022-07-28 02:22:57.508076: Average global foreground Dice: [0.8254]
2022-07-28 02:22:57.510547: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:22:58.246672: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 02:22:58.249130: This epoch took 75.572235 s

2022-07-28 02:22:58.251207: 
epoch:  246
2022-07-28 02:24:08.032305: train loss : -0.8522
2022-07-28 02:24:13.771773: validation loss: -0.3526
2022-07-28 02:24:13.790847: Average global foreground Dice: [0.7509]
2022-07-28 02:24:13.800839: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:24:14.775754: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 02:24:14.778313: This epoch took 76.524964 s

2022-07-28 02:24:14.780865: 
epoch:  247
2022-07-28 02:25:23.722677: train loss : -0.8603
2022-07-28 02:25:29.779888: validation loss: -0.3736
2022-07-28 02:25:29.799235: Average global foreground Dice: [0.7677]
2022-07-28 02:25:29.818608: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:25:30.633978: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 02:25:30.636474: This epoch took 75.853211 s

2022-07-28 02:25:30.638810: 
epoch:  248
2022-07-28 02:26:39.741185: train loss : -0.8813
2022-07-28 02:26:45.815993: validation loss: -0.4609
2022-07-28 02:26:45.826439: Average global foreground Dice: [0.8213]
2022-07-28 02:26:45.848574: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:26:46.578287: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 02:26:46.580648: This epoch took 75.939630 s

2022-07-28 02:26:46.582754: 
epoch:  249
2022-07-28 02:27:55.578718: train loss : -0.8432
2022-07-28 02:28:01.270906: validation loss: -0.3351
2022-07-28 02:28:01.286724: Average global foreground Dice: [0.7406]
2022-07-28 02:28:01.301968: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:28:02.193671: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 02:28:02.195999: saving scheduled checkpoint file...
2022-07-28 02:28:02.354689: saving checkpoint...
2022-07-28 02:28:07.397187: done, saving took 5.20 seconds
2022-07-28 02:28:07.406303: done
2022-07-28 02:28:07.408325: This epoch took 80.823488 s

2022-07-28 02:28:07.410420: 
epoch:  250
2022-07-28 02:29:18.241341: train loss : -0.8864
2022-07-28 02:29:23.622851: validation loss: -0.3933
2022-07-28 02:29:23.652166: Average global foreground Dice: [0.7532]
2022-07-28 02:29:23.673096: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:29:24.214940: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 02:29:24.218035: This epoch took 76.805313 s

2022-07-28 02:29:24.221866: 
epoch:  251
2022-07-28 02:30:42.374629: train loss : -0.8515
2022-07-28 02:30:50.013913: validation loss: -0.3601
2022-07-28 02:30:50.051026: Average global foreground Dice: [0.7011]
2022-07-28 02:30:50.068738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:30:51.194621: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 02:30:51.221121: This epoch took 86.995221 s

2022-07-28 02:30:51.242177: 
epoch:  252
2022-07-28 02:32:10.459142: train loss : -0.8940
2022-07-28 02:32:17.346519: validation loss: -0.3432
2022-07-28 02:32:17.375877: Average global foreground Dice: [0.6865]
2022-07-28 02:32:17.410011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:32:18.602493: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 02:32:18.621819: This epoch took 87.340226 s

2022-07-28 02:32:18.635998: 
epoch:  253
2022-07-28 02:33:37.884115: train loss : -0.8770
2022-07-28 02:33:44.566519: validation loss: -0.2976
2022-07-28 02:33:44.594851: Average global foreground Dice: [0.6767]
2022-07-28 02:33:44.628011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:33:45.752532: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 02:33:45.781108: This epoch took 87.125654 s

2022-07-28 02:33:45.794596: 
epoch:  254
2022-07-28 02:35:03.216731: train loss : -0.8661
2022-07-28 02:35:10.061433: validation loss: -0.4464
2022-07-28 02:35:10.086304: Average global foreground Dice: [0.7839]
2022-07-28 02:35:10.102283: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:35:11.339828: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 02:35:11.364098: This epoch took 85.555836 s

2022-07-28 02:35:11.385996: 
epoch:  255
2022-07-28 02:36:27.074004: train loss : -0.8724
2022-07-28 02:36:34.126645: validation loss: -0.3634
2022-07-28 02:36:34.148496: Average global foreground Dice: [0.7418]
2022-07-28 02:36:34.178257: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:36:35.297118: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 02:36:35.322728: This epoch took 83.915114 s

2022-07-28 02:36:35.336565: 
epoch:  256
2022-07-28 02:37:54.526818: train loss : -0.8557
2022-07-28 02:38:01.440459: validation loss: -0.3746
2022-07-28 02:38:01.459366: Average global foreground Dice: [0.8075]
2022-07-28 02:38:01.473529: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:38:02.398714: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 02:38:02.453974: This epoch took 87.114959 s

2022-07-28 02:38:02.493987: 
epoch:  257
2022-07-28 02:39:22.450376: train loss : -0.8868
2022-07-28 02:39:29.503235: validation loss: -0.3722
2022-07-28 02:39:29.520123: Average global foreground Dice: [0.7805]
2022-07-28 02:39:29.524956: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:39:30.794173: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 02:39:30.819143: This epoch took 88.280869 s

2022-07-28 02:39:30.846974: 
epoch:  258
2022-07-28 02:40:51.169235: train loss : -0.8843
2022-07-28 02:40:58.112861: validation loss: -0.4268
2022-07-28 02:40:58.132156: Average global foreground Dice: [0.8084]
2022-07-28 02:40:58.159459: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:40:59.352765: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 02:40:59.356895: This epoch took 88.454789 s

2022-07-28 02:40:59.377008: 
epoch:  259
2022-07-28 02:42:18.816265: train loss : -0.8711
2022-07-28 02:42:25.845982: validation loss: -0.5553
2022-07-28 02:42:25.941628: Average global foreground Dice: [0.8775]
2022-07-28 02:42:25.955030: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:42:27.239332: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 02:42:27.241853: This epoch took 87.850445 s

2022-07-28 02:42:27.244050: 
epoch:  260
2022-07-28 02:43:45.022431: train loss : -0.8710
2022-07-28 02:43:51.583374: validation loss: -0.4364
2022-07-28 02:43:51.615865: Average global foreground Dice: [0.7634]
2022-07-28 02:43:51.628017: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:43:52.870393: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 02:43:52.873519: This epoch took 85.627163 s

2022-07-28 02:43:52.875938: 
epoch:  261
2022-07-28 02:45:14.358333: train loss : -0.8692
2022-07-28 02:45:20.331563: validation loss: -0.3365
2022-07-28 02:45:20.367195: Average global foreground Dice: [0.7262]
2022-07-28 02:45:20.376730: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:45:21.735422: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 02:45:21.755406: This epoch took 88.876865 s

2022-07-28 02:45:21.769980: 
epoch:  262
2022-07-28 02:46:41.890348: train loss : -0.8739
2022-07-28 02:46:48.499199: validation loss: -0.2489
2022-07-28 02:46:48.520576: Average global foreground Dice: [0.6932]
2022-07-28 02:46:48.529765: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:46:49.659710: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 02:46:49.677340: This epoch took 87.878315 s

2022-07-28 02:46:49.705341: 
epoch:  263
2022-07-28 02:48:06.919171: train loss : -0.8598
2022-07-28 02:48:14.109178: validation loss: -0.3729
2022-07-28 02:48:14.143411: Average global foreground Dice: [0.7996]
2022-07-28 02:48:14.179430: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:48:15.249679: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 02:48:15.252686: This epoch took 85.516848 s

2022-07-28 02:48:15.254958: 
epoch:  264
2022-07-28 02:49:31.245279: train loss : -0.8811
2022-07-28 02:49:38.394701: validation loss: -0.4214
2022-07-28 02:49:38.411265: Average global foreground Dice: [0.8168]
2022-07-28 02:49:38.450063: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:49:39.559450: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 02:49:39.623241: This epoch took 84.366001 s

2022-07-28 02:49:39.684274: 
epoch:  265
2022-07-28 02:50:59.728303: train loss : -0.8761
2022-07-28 02:51:07.132085: validation loss: -0.4651
2022-07-28 02:51:07.137331: Average global foreground Dice: [0.8396]
2022-07-28 02:51:07.165371: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:51:08.379915: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 02:51:08.415155: This epoch took 88.658690 s

2022-07-28 02:51:08.438987: 
epoch:  266
2022-07-28 02:52:27.910582: train loss : -0.8811
2022-07-28 02:52:34.720301: validation loss: -0.4264
2022-07-28 02:52:34.745660: Average global foreground Dice: [0.8388]
2022-07-28 02:52:34.749288: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:52:35.924820: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 02:52:35.934142: saving best epoch checkpoint...
2022-07-28 02:52:36.319767: saving checkpoint...
2022-07-28 02:52:42.962173: done, saving took 6.99 seconds
2022-07-28 02:52:42.972307: This epoch took 94.505266 s

2022-07-28 02:52:42.974122: 
epoch:  267
2022-07-28 02:54:00.150701: train loss : -0.8892
2022-07-28 02:54:07.313660: validation loss: -0.3828
2022-07-28 02:54:07.389457: Average global foreground Dice: [0.738]
2022-07-28 02:54:07.450335: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:54:08.632641: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 02:54:08.643641: This epoch took 85.666924 s

2022-07-28 02:54:08.656030: 
epoch:  268
2022-07-28 02:55:29.423357: train loss : -0.8856
2022-07-28 02:55:36.116512: validation loss: -0.3745
2022-07-28 02:55:36.155654: Average global foreground Dice: [0.733]
2022-07-28 02:55:36.175081: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:55:37.259697: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 02:55:37.285082: This epoch took 88.602859 s

2022-07-28 02:55:37.311219: 
epoch:  269
2022-07-28 02:56:56.543700: train loss : -0.8903
2022-07-28 02:57:02.916138: validation loss: -0.5350
2022-07-28 02:57:02.920645: Average global foreground Dice: [0.8627]
2022-07-28 02:57:02.928667: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:57:03.966259: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 02:57:03.973031: This epoch took 86.641636 s

2022-07-28 02:57:03.976217: 
epoch:  270
2022-07-28 02:58:20.035740: train loss : -0.9015
2022-07-28 02:58:26.652020: validation loss: -0.4580
2022-07-28 02:58:26.699707: Average global foreground Dice: [0.8428]
2022-07-28 02:58:26.742023: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:58:27.700028: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 02:58:27.738431: saving best epoch checkpoint...
2022-07-28 02:58:27.990923: saving checkpoint...
2022-07-28 02:58:33.424957: done, saving took 5.63 seconds
2022-07-28 02:58:33.436675: This epoch took 89.433671 s

2022-07-28 02:58:33.438938: 
epoch:  271
2022-07-28 02:59:52.403969: train loss : -0.8917
2022-07-28 02:59:59.344932: validation loss: -0.4219
2022-07-28 02:59:59.393574: Average global foreground Dice: [0.7926]
2022-07-28 02:59:59.445841: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:00:00.391265: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 03:00:00.436032: saving best epoch checkpoint...
2022-07-28 03:00:00.783104: saving checkpoint...
2022-07-28 03:00:06.282700: done, saving took 5.81 seconds
2022-07-28 03:00:06.296381: This epoch took 92.855407 s

2022-07-28 03:00:06.298775: 
epoch:  272
2022-07-28 03:01:22.630466: train loss : -0.8831
2022-07-28 03:01:30.795354: validation loss: -0.4003
2022-07-28 03:01:30.813747: Average global foreground Dice: [0.7788]
2022-07-28 03:01:30.843405: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:01:32.101793: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 03:01:32.105854: This epoch took 85.805040 s

2022-07-28 03:01:32.130362: 
epoch:  273
2022-07-28 03:02:51.676273: train loss : -0.8995
2022-07-28 03:03:00.436191: validation loss: -0.4517
2022-07-28 03:03:00.459882: Average global foreground Dice: [0.8426]
2022-07-28 03:03:00.478314: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:03:01.482010: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 03:03:01.484929: saving best epoch checkpoint...
2022-07-28 03:03:01.672513: saving checkpoint...
2022-07-28 03:03:07.552694: done, saving took 6.06 seconds
2022-07-28 03:03:07.563269: This epoch took 95.400289 s

2022-07-28 03:03:07.565141: 
epoch:  274
2022-07-28 03:04:26.624741: train loss : -0.8456
2022-07-28 03:04:35.056360: validation loss: -0.3214
2022-07-28 03:04:35.084978: Average global foreground Dice: [0.698]
2022-07-28 03:04:35.090611: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:04:36.281170: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 03:04:36.304186: This epoch took 88.736961 s

2022-07-28 03:04:36.333050: 
epoch:  275
2022-07-28 03:05:54.577936: train loss : -0.8777
2022-07-28 03:06:01.561104: validation loss: -0.2871
2022-07-28 03:06:01.583443: Average global foreground Dice: [0.689]
2022-07-28 03:06:01.611293: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:06:02.715643: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 03:06:02.756852: This epoch took 86.398769 s

2022-07-28 03:06:02.784580: 
epoch:  276
2022-07-28 03:07:20.397572: train loss : -0.8778
2022-07-28 03:07:27.456279: validation loss: -0.2819
2022-07-28 03:07:27.476761: Average global foreground Dice: [0.675]
2022-07-28 03:07:27.551740: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:07:28.961402: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 03:07:28.983350: This epoch took 86.167280 s

2022-07-28 03:07:28.991348: 
epoch:  277
2022-07-28 03:08:49.113099: train loss : -0.8859
2022-07-28 03:08:55.990070: validation loss: -0.2734
2022-07-28 03:08:56.003951: Average global foreground Dice: [0.7178]
2022-07-28 03:08:56.024831: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:08:57.174616: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 03:08:57.200092: This epoch took 88.180480 s

2022-07-28 03:08:57.222015: 
epoch:  278
2022-07-28 03:10:14.331023: train loss : -0.8837
2022-07-28 03:10:22.553549: validation loss: -0.3472
2022-07-28 03:10:22.582842: Average global foreground Dice: [0.7724]
2022-07-28 03:10:22.602110: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:10:23.974633: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 03:10:24.018260: This epoch took 86.776230 s

2022-07-28 03:10:24.030106: 
epoch:  279
2022-07-28 03:11:42.732171: train loss : -0.8890
2022-07-28 03:11:51.405270: validation loss: -0.3611
2022-07-28 03:11:51.491499: Average global foreground Dice: [0.7788]
2022-07-28 03:11:51.596057: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:11:52.800646: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 03:11:52.831192: This epoch took 88.781454 s

2022-07-28 03:11:52.859834: 
epoch:  280
2022-07-28 03:13:08.637491: train loss : -0.9032
2022-07-28 03:13:17.313283: validation loss: -0.4240
2022-07-28 03:13:17.371195: Average global foreground Dice: [0.784]
2022-07-28 03:13:17.387602: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:13:18.648843: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 03:13:18.659831: This epoch took 85.789728 s

2022-07-28 03:13:18.662621: 
epoch:  281
2022-07-28 03:14:36.666912: train loss : -0.8877
2022-07-28 03:14:44.858224: validation loss: -0.3860
2022-07-28 03:14:44.890840: Average global foreground Dice: [0.7439]
2022-07-28 03:14:44.909628: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:14:46.177195: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 03:14:46.192126: This epoch took 87.526904 s

2022-07-28 03:14:46.212974: 
epoch:  282
2022-07-28 03:16:03.495334: train loss : -0.8959
2022-07-28 03:16:09.984615: validation loss: -0.2706
2022-07-28 03:16:10.001179: Average global foreground Dice: [0.6467]
2022-07-28 03:16:10.039456: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:16:11.261954: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 03:16:11.302133: This epoch took 85.070859 s

2022-07-28 03:16:11.334048: 
epoch:  283
2022-07-28 03:17:27.983181: train loss : -0.8735
2022-07-28 03:17:35.760083: validation loss: -0.4000
2022-07-28 03:17:35.789741: Average global foreground Dice: [0.7942]
2022-07-28 03:17:35.811713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:17:37.002593: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 03:17:37.006017: This epoch took 85.646023 s

2022-07-28 03:17:37.009584: 
epoch:  284
2022-07-28 03:18:57.595250: train loss : -0.8897
2022-07-28 03:19:03.905230: validation loss: -0.3759
2022-07-28 03:19:03.941898: Average global foreground Dice: [0.8085]
2022-07-28 03:19:04.001079: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:19:05.157077: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 03:19:05.179073: This epoch took 88.166711 s

2022-07-28 03:19:05.191035: 
epoch:  285
2022-07-28 03:20:23.474178: train loss : -0.8808
2022-07-28 03:20:30.306568: validation loss: -0.5113
2022-07-28 03:20:30.337922: Average global foreground Dice: [0.8399]
2022-07-28 03:20:30.370421: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:20:31.499467: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 03:20:31.540222: This epoch took 86.345741 s

2022-07-28 03:20:31.562074: 
epoch:  286
2022-07-28 03:21:51.857712: train loss : -0.8885
2022-07-28 03:21:59.349669: validation loss: -0.3326
2022-07-28 03:21:59.353761: Average global foreground Dice: [0.6853]
2022-07-28 03:21:59.356393: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:22:00.400931: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 03:22:00.422967: This epoch took 88.837596 s

2022-07-28 03:22:00.455942: 
epoch:  287
2022-07-28 03:23:19.861785: train loss : -0.8704
2022-07-28 03:23:27.738961: validation loss: -0.3374
2022-07-28 03:23:27.804004: Average global foreground Dice: [0.7585]
2022-07-28 03:23:27.832072: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:23:29.007269: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 03:23:29.054068: This epoch took 88.593336 s

2022-07-28 03:23:29.105258: 
epoch:  288
2022-07-28 03:24:49.127559: train loss : -0.8711
2022-07-28 03:24:56.732599: validation loss: -0.3989
2022-07-28 03:24:56.762831: Average global foreground Dice: [0.7803]
2022-07-28 03:24:56.773690: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:24:57.970864: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 03:24:57.987742: This epoch took 88.848598 s

2022-07-28 03:24:58.000979: 
epoch:  289
2022-07-28 03:26:16.139387: train loss : -0.8970
2022-07-28 03:26:23.442427: validation loss: -0.3988
2022-07-28 03:26:23.463190: Average global foreground Dice: [0.7773]
2022-07-28 03:26:23.483083: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:26:24.605268: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 03:26:24.642346: This epoch took 86.620137 s

2022-07-28 03:26:24.654653: 
epoch:  290
2022-07-28 03:27:42.437670: train loss : -0.8787
2022-07-28 03:27:48.774279: validation loss: -0.4155
2022-07-28 03:27:48.795429: Average global foreground Dice: [0.8175]
2022-07-28 03:27:48.816725: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:27:49.865303: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 03:27:49.872104: This epoch took 85.214074 s

2022-07-28 03:27:49.905441: 
epoch:  291
2022-07-28 03:29:07.144245: train loss : -0.8766
2022-07-28 03:29:14.442672: validation loss: -0.2767
2022-07-28 03:29:14.449811: Average global foreground Dice: [0.7422]
2022-07-28 03:29:14.480075: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:29:15.716642: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 03:29:15.748125: This epoch took 85.838125 s

2022-07-28 03:29:15.777402: 
epoch:  292
2022-07-28 03:30:36.949105: train loss : -0.8904
2022-07-28 03:30:44.178408: validation loss: -0.3390
2022-07-28 03:30:44.209517: Average global foreground Dice: [0.7683]
2022-07-28 03:30:44.229977: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:30:45.328717: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 03:30:45.372028: This epoch took 89.562037 s

2022-07-28 03:30:45.409155: 
epoch:  293
2022-07-28 03:32:03.152500: train loss : -0.8905
2022-07-28 03:32:10.665051: validation loss: -0.3597
2022-07-28 03:32:10.680246: Average global foreground Dice: [0.7758]
2022-07-28 03:32:10.708014: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:32:11.822634: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 03:32:11.838035: This epoch took 86.413897 s

2022-07-28 03:32:11.860018: 
epoch:  294
2022-07-28 03:33:33.301105: train loss : -0.8832
2022-07-28 03:33:40.365739: validation loss: -0.4587
2022-07-28 03:33:40.404775: Average global foreground Dice: [0.8291]
2022-07-28 03:33:40.440406: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:33:41.698765: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 03:33:41.731105: This epoch took 89.838395 s

2022-07-28 03:33:41.764040: 
epoch:  295
2022-07-28 03:34:59.954071: train loss : -0.8982
2022-07-28 03:35:06.958987: validation loss: -0.5378
2022-07-28 03:35:06.990794: Average global foreground Dice: [0.8746]
2022-07-28 03:35:07.015434: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:35:08.312325: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 03:35:08.331730: This epoch took 86.534678 s

2022-07-28 03:35:08.344265: 
epoch:  296
2022-07-28 03:36:28.097059: train loss : -0.8866
2022-07-28 03:36:35.754332: validation loss: -0.3855
2022-07-28 03:36:35.769817: Average global foreground Dice: [0.7648]
2022-07-28 03:36:35.789104: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:36:36.887744: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 03:36:36.931101: This epoch took 88.581992 s

2022-07-28 03:36:36.974986: 
epoch:  297
2022-07-28 03:37:52.760916: train loss : -0.8607
2022-07-28 03:37:59.204274: validation loss: -0.2966
2022-07-28 03:37:59.227546: Average global foreground Dice: [0.6954]
2022-07-28 03:37:59.249339: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:38:00.367030: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 03:38:00.400075: This epoch took 83.403067 s

2022-07-28 03:38:00.423003: 
epoch:  298
2022-07-28 03:39:18.742620: train loss : -0.8982
2022-07-28 03:39:26.155279: validation loss: -0.4272
2022-07-28 03:39:26.199313: Average global foreground Dice: [0.8124]
2022-07-28 03:39:26.225185: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:39:27.460127: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 03:39:27.515006: This epoch took 87.073992 s

2022-07-28 03:39:27.544170: 
epoch:  299
2022-07-28 03:40:46.921250: train loss : -0.8958
2022-07-28 03:40:53.400743: validation loss: -0.3951
2022-07-28 03:40:53.432453: Average global foreground Dice: [0.784]
2022-07-28 03:40:53.438889: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:40:54.663794: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 03:40:54.676069: saving scheduled checkpoint file...
2022-07-28 03:40:55.085226: saving checkpoint...
2022-07-28 03:41:01.620671: done, saving took 6.92 seconds
2022-07-28 03:41:01.634670: done
2022-07-28 03:41:01.636446: This epoch took 94.085040 s

2022-07-28 03:41:01.639182: 
epoch:  300
2022-07-28 03:42:20.047470: train loss : -0.8761
2022-07-28 03:42:26.381557: validation loss: -0.4131
2022-07-28 03:42:26.391701: Average global foreground Dice: [0.8275]
2022-07-28 03:42:26.408006: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:42:27.679578: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 03:42:27.691130: This epoch took 86.050030 s

2022-07-28 03:42:27.708265: 
epoch:  301
2022-07-28 03:43:46.317768: train loss : -0.8863
2022-07-28 03:43:53.492067: validation loss: -0.3907
2022-07-28 03:43:53.528997: Average global foreground Dice: [0.7793]
2022-07-28 03:43:53.585138: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:43:55.093853: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 03:43:55.122614: This epoch took 87.393152 s

2022-07-28 03:43:55.156023: 
epoch:  302
2022-07-28 03:45:14.292337: train loss : -0.8764
2022-07-28 03:45:21.212916: validation loss: -0.5315
2022-07-28 03:45:21.260808: Average global foreground Dice: [0.8515]
2022-07-28 03:45:21.272844: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:45:23.369601: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 03:45:23.427440: This epoch took 88.236940 s

2022-07-28 03:45:23.449985: 
epoch:  303
2022-07-28 03:46:41.257780: train loss : -0.8906
2022-07-28 03:46:48.054395: validation loss: -0.5127
2022-07-28 03:46:48.085488: Average global foreground Dice: [0.8528]
2022-07-28 03:46:48.100031: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:46:49.409756: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 03:46:49.425501: saving best epoch checkpoint...
2022-07-28 03:46:49.764742: saving checkpoint...
2022-07-28 03:46:56.097624: done, saving took 6.65 seconds
2022-07-28 03:46:56.105821: This epoch took 92.634669 s

2022-07-28 03:46:56.107948: 
epoch:  304
2022-07-28 03:48:12.021970: train loss : -0.8798
2022-07-28 03:48:19.834505: validation loss: -0.3118
2022-07-28 03:48:19.872095: Average global foreground Dice: [0.7047]
2022-07-28 03:48:19.884484: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:48:21.066433: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 03:48:21.090878: This epoch took 84.980935 s

2022-07-28 03:48:21.124038: 
epoch:  305
2022-07-28 03:49:41.805845: train loss : -0.8874
2022-07-28 03:49:48.591047: validation loss: -0.4951
2022-07-28 03:49:48.594965: Average global foreground Dice: [0.8247]
2022-07-28 03:49:48.597753: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:49:49.714067: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 03:49:49.740029: This epoch took 88.575040 s

2022-07-28 03:49:49.766565: 
epoch:  306
2022-07-28 03:51:06.197682: train loss : -0.8959
2022-07-28 03:51:12.669862: validation loss: -0.3448
2022-07-28 03:51:12.715630: Average global foreground Dice: [0.751]
2022-07-28 03:51:12.737630: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:51:13.769040: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 03:51:13.782690: This epoch took 84.002704 s

2022-07-28 03:51:13.785460: 
epoch:  307
2022-07-28 03:52:33.245912: train loss : -0.8752
2022-07-28 03:52:40.567871: validation loss: -0.3163
2022-07-28 03:52:40.588678: Average global foreground Dice: [0.7242]
2022-07-28 03:52:40.612697: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:52:41.660278: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 03:52:41.718072: This epoch took 87.930210 s

2022-07-28 03:52:41.784030: 
epoch:  308
2022-07-28 03:53:58.242606: train loss : -0.8842
2022-07-28 03:54:03.992071: validation loss: -0.3644
2022-07-28 03:54:04.026091: Average global foreground Dice: [0.739]
2022-07-28 03:54:04.052161: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:54:05.170909: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 03:54:05.209240: This epoch took 83.381208 s

2022-07-28 03:54:05.252978: 
epoch:  309
2022-07-28 03:55:24.941688: train loss : -0.8887
2022-07-28 03:55:31.577750: validation loss: -0.4905
2022-07-28 03:55:31.586809: Average global foreground Dice: [0.8121]
2022-07-28 03:55:31.618563: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:55:32.864140: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 03:55:32.909207: This epoch took 87.612147 s

2022-07-28 03:55:32.932772: 
epoch:  310
2022-07-28 03:56:51.828402: train loss : -0.8642
2022-07-28 03:56:58.704325: validation loss: -0.2324
2022-07-28 03:56:58.758757: Average global foreground Dice: [0.6998]
2022-07-28 03:56:58.802014: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:56:59.898504: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 03:56:59.931164: This epoch took 86.985712 s

2022-07-28 03:57:00.006275: 
epoch:  311
2022-07-28 03:58:20.339497: train loss : -0.8910
2022-07-28 03:58:27.545202: validation loss: -0.3588
2022-07-28 03:58:27.631263: Average global foreground Dice: [0.7656]
2022-07-28 03:58:27.674111: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:58:28.894027: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 03:58:28.907058: This epoch took 88.845923 s

2022-07-28 03:58:28.912134: 
epoch:  312
2022-07-28 03:59:47.104188: train loss : -0.8681
2022-07-28 03:59:55.229391: validation loss: -0.3306
2022-07-28 03:59:55.271855: Average global foreground Dice: [0.7756]
2022-07-28 03:59:55.294331: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:59:56.532076: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 03:59:56.557736: This epoch took 87.642045 s

2022-07-28 03:59:56.583269: 
epoch:  313
2022-07-28 04:01:14.137799: train loss : -0.8942
2022-07-28 04:01:20.845614: validation loss: -0.3136
2022-07-28 04:01:20.902744: Average global foreground Dice: [0.76]
2022-07-28 04:01:20.946034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:01:21.977736: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 04:01:22.021338: This epoch took 85.417726 s

2022-07-28 04:01:22.063559: 
epoch:  314
2022-07-28 04:02:38.243480: train loss : -0.8950
2022-07-28 04:02:46.337045: validation loss: -0.3429
2022-07-28 04:02:46.391402: Average global foreground Dice: [0.7745]
2022-07-28 04:02:46.431041: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:02:47.831203: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 04:02:47.861180: This epoch took 85.756602 s

2022-07-28 04:02:47.906036: 
epoch:  315
2022-07-28 04:04:06.262395: train loss : -0.8823
2022-07-28 04:04:15.962600: validation loss: -0.3488
2022-07-28 04:04:15.994989: Average global foreground Dice: [0.7659]
2022-07-28 04:04:16.026099: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:04:17.158524: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 04:04:17.162305: This epoch took 89.206286 s

2022-07-28 04:04:17.199978: 
epoch:  316
2022-07-28 04:05:37.468800: train loss : -0.8960
2022-07-28 04:05:45.587298: validation loss: -0.4246
2022-07-28 04:05:45.615057: Average global foreground Dice: [0.8156]
2022-07-28 04:05:45.639073: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:05:46.815811: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 04:05:46.862146: This epoch took 89.635591 s

2022-07-28 04:05:46.905994: 
epoch:  317
2022-07-28 04:07:07.196395: train loss : -0.8936
2022-07-28 04:07:16.575460: validation loss: -0.3959
2022-07-28 04:07:16.617083: Average global foreground Dice: [0.7633]
2022-07-28 04:07:16.631538: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:07:17.892101: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 04:07:17.937101: This epoch took 90.998097 s

2022-07-28 04:07:17.970005: 
epoch:  318
2022-07-28 04:08:39.135723: train loss : -0.8948
2022-07-28 04:08:45.119665: validation loss: -0.2991
2022-07-28 04:08:45.137730: Average global foreground Dice: [0.7944]
2022-07-28 04:08:45.159080: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:08:46.420916: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 04:08:46.423610: This epoch took 88.437237 s

2022-07-28 04:08:46.431007: 
epoch:  319
2022-07-28 04:10:06.298160: train loss : -0.8918
2022-07-28 04:10:13.189112: validation loss: -0.4181
2022-07-28 04:10:13.214617: Average global foreground Dice: [0.7973]
2022-07-28 04:10:13.227461: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:10:14.453171: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 04:10:14.509825: This epoch took 88.063812 s

2022-07-28 04:10:14.538902: 
epoch:  320
2022-07-28 04:11:31.679099: train loss : -0.9076
2022-07-28 04:11:40.038127: validation loss: -0.3037
2022-07-28 04:11:40.106018: Average global foreground Dice: [0.7735]
2022-07-28 04:11:40.150392: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:11:41.490299: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 04:11:41.519165: This epoch took 86.966041 s

2022-07-28 04:11:41.553453: 
epoch:  321
2022-07-28 04:13:00.636474: train loss : -0.8871
2022-07-28 04:13:07.459809: validation loss: -0.4877
2022-07-28 04:13:07.485892: Average global foreground Dice: [0.8581]
2022-07-28 04:13:07.539565: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:13:08.628005: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 04:13:08.671152: This epoch took 87.085122 s

2022-07-28 04:13:08.704060: 
epoch:  322
2022-07-28 04:14:30.053574: train loss : -0.9077
2022-07-28 04:14:37.195129: validation loss: -0.4576
2022-07-28 04:14:37.227716: Average global foreground Dice: [0.8679]
2022-07-28 04:14:37.247026: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:14:38.387732: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 04:14:38.390711: This epoch took 89.654666 s

2022-07-28 04:14:38.411966: 
epoch:  323
2022-07-28 04:15:57.620481: train loss : -0.9064
2022-07-28 04:16:03.917690: validation loss: -0.4849
2022-07-28 04:16:03.962960: Average global foreground Dice: [0.8583]
2022-07-28 04:16:03.981172: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:16:05.399016: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 04:16:05.416439: saving best epoch checkpoint...
2022-07-28 04:16:05.693260: saving checkpoint...
2022-07-28 04:16:11.956943: done, saving took 6.51 seconds
2022-07-28 04:16:11.980306: This epoch took 93.548110 s

2022-07-28 04:16:11.982490: 
epoch:  324
2022-07-28 04:17:29.431028: train loss : -0.9101
2022-07-28 04:17:36.391486: validation loss: -0.3418
2022-07-28 04:17:36.422207: Average global foreground Dice: [0.7526]
2022-07-28 04:17:36.425565: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:17:37.551419: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 04:17:37.564215: This epoch took 85.579183 s

2022-07-28 04:17:37.579013: 
epoch:  325
2022-07-28 04:18:58.578816: train loss : -0.8971
2022-07-28 04:19:05.689046: validation loss: -0.4651
2022-07-28 04:19:05.712897: Average global foreground Dice: [0.8201]
2022-07-28 04:19:05.728088: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:19:07.043964: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 04:19:07.091221: This epoch took 89.497220 s

2022-07-28 04:19:07.136022: 
epoch:  326
2022-07-28 04:20:27.339154: train loss : -0.8914
2022-07-28 04:20:35.736785: validation loss: -0.5356
2022-07-28 04:20:35.777321: Average global foreground Dice: [0.8534]
2022-07-28 04:20:35.793132: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:20:36.881928: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 04:20:36.914468: saving best epoch checkpoint...
2022-07-28 04:20:37.181146: saving checkpoint...
2022-07-28 04:20:43.345946: done, saving took 6.42 seconds
2022-07-28 04:20:43.354129: This epoch took 96.184037 s

2022-07-28 04:20:43.356701: 
epoch:  327
2022-07-28 04:22:00.411613: train loss : -0.8830
2022-07-28 04:22:06.728551: validation loss: -0.3802
2022-07-28 04:22:06.766821: Average global foreground Dice: [0.7206]
2022-07-28 04:22:06.797976: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:22:07.885225: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 04:22:07.915131: This epoch took 84.556437 s

2022-07-28 04:22:07.936986: 
epoch:  328
2022-07-28 04:23:27.880212: train loss : -0.8867
2022-07-28 04:23:34.703057: validation loss: -0.5036
2022-07-28 04:23:34.724972: Average global foreground Dice: [0.8195]
2022-07-28 04:23:34.750764: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:23:36.018520: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 04:23:36.071224: This epoch took 88.128200 s

2022-07-28 04:23:36.103985: 
epoch:  329
2022-07-28 04:24:55.672700: train loss : -0.8778
2022-07-28 04:25:03.193314: validation loss: -0.5027
2022-07-28 04:25:03.212865: Average global foreground Dice: [0.8558]
2022-07-28 04:25:03.232843: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:25:04.288669: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 04:25:04.324056: This epoch took 88.169966 s

2022-07-28 04:25:04.347929: 
epoch:  330
2022-07-28 04:26:21.297213: train loss : -0.9003
2022-07-28 04:26:28.535210: validation loss: -0.5193
2022-07-28 04:26:28.579980: Average global foreground Dice: [0.8566]
2022-07-28 04:26:28.593988: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:26:29.580116: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 04:26:29.583026: saving best epoch checkpoint...
2022-07-28 04:26:29.984480: saving checkpoint...
2022-07-28 04:26:35.347217: done, saving took 5.76 seconds
2022-07-28 04:26:35.355764: This epoch took 90.992690 s

2022-07-28 04:26:35.357798: 
epoch:  331
2022-07-28 04:27:53.087505: train loss : -0.9072
2022-07-28 04:28:00.035119: validation loss: -0.4611
2022-07-28 04:28:00.078917: Average global foreground Dice: [0.8237]
2022-07-28 04:28:00.099171: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:28:01.068772: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 04:28:01.093451: saving best epoch checkpoint...
2022-07-28 04:28:01.445776: saving checkpoint...
2022-07-28 04:28:08.094059: done, saving took 6.98 seconds
2022-07-28 04:28:08.106622: This epoch took 92.746967 s

2022-07-28 04:28:08.108727: 
epoch:  332
2022-07-28 04:29:26.740756: train loss : -0.8818
2022-07-28 04:29:34.009676: validation loss: -0.3735
2022-07-28 04:29:34.023703: Average global foreground Dice: [0.8051]
2022-07-28 04:29:34.039043: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:29:35.408043: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 04:29:35.412128: This epoch took 87.301224 s

2022-07-28 04:29:35.437696: 
epoch:  333
2022-07-28 04:30:53.506574: train loss : -0.8767
2022-07-28 04:30:59.782197: validation loss: -0.4401
2022-07-28 04:30:59.832299: Average global foreground Dice: [0.8312]
2022-07-28 04:30:59.870980: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:31:01.045055: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 04:31:01.061047: saving best epoch checkpoint...
2022-07-28 04:31:01.309976: saving checkpoint...
2022-07-28 04:31:06.944166: done, saving took 5.87 seconds
2022-07-28 04:31:06.954141: This epoch took 91.498745 s

2022-07-28 04:31:06.956457: 
epoch:  334
2022-07-28 04:32:23.805202: train loss : -0.8939
2022-07-28 04:32:30.491786: validation loss: -0.4495
2022-07-28 04:32:30.528198: Average global foreground Dice: [0.8569]
2022-07-28 04:32:30.548050: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:32:31.759484: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 04:32:31.784058: saving best epoch checkpoint...
2022-07-28 04:32:32.116552: saving checkpoint...
2022-07-28 04:32:37.678195: done, saving took 5.86 seconds
2022-07-28 04:32:37.688012: This epoch took 90.729423 s

2022-07-28 04:32:37.690186: 
epoch:  335
2022-07-28 04:33:51.855093: train loss : -0.9026
2022-07-28 04:33:59.940506: validation loss: -0.3961
2022-07-28 04:33:59.958726: Average global foreground Dice: [0.7716]
2022-07-28 04:33:59.989006: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:34:01.114756: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 04:34:01.142971: This epoch took 83.450815 s

2022-07-28 04:34:01.147929: 
epoch:  336
2022-07-28 04:35:21.210840: train loss : -0.9052
2022-07-28 04:35:28.668054: validation loss: -0.4210
2022-07-28 04:35:28.734804: Average global foreground Dice: [0.8153]
2022-07-28 04:35:28.766465: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:35:29.907768: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 04:35:29.934082: This epoch took 88.750475 s

2022-07-28 04:35:30.000283: 
epoch:  337
2022-07-28 04:36:48.624875: train loss : -0.8796
2022-07-28 04:36:55.497716: validation loss: -0.3283
2022-07-28 04:36:55.572864: Average global foreground Dice: [0.775]
2022-07-28 04:36:55.578644: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:36:56.670287: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 04:36:56.702098: This epoch took 86.689029 s

2022-07-28 04:36:56.735185: 
epoch:  338
2022-07-28 04:38:18.179307: train loss : -0.9068
2022-07-28 04:38:24.957112: validation loss: -0.2959
2022-07-28 04:38:24.960616: Average global foreground Dice: [0.6598]
2022-07-28 04:38:24.998908: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:38:26.005146: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 04:38:26.031297: This epoch took 89.260281 s

2022-07-28 04:38:26.071313: 
epoch:  339
2022-07-28 04:39:47.361279: train loss : -0.9056
2022-07-28 04:39:54.490304: validation loss: -0.5534
2022-07-28 04:39:54.514143: Average global foreground Dice: [0.8518]
2022-07-28 04:39:54.535174: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:39:55.692583: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 04:39:55.714039: This epoch took 89.614236 s

2022-07-28 04:39:55.719598: 
epoch:  340
2022-07-28 04:41:16.611274: train loss : -0.8801
2022-07-28 04:41:23.462976: validation loss: -0.3214
2022-07-28 04:41:23.522183: Average global foreground Dice: [0.7465]
2022-07-28 04:41:23.568061: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:41:24.743761: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 04:41:24.761216: This epoch took 89.037310 s

2022-07-28 04:41:24.783062: 
epoch:  341
2022-07-28 04:42:43.700969: train loss : -0.8946
2022-07-28 04:42:50.720212: validation loss: -0.4024
2022-07-28 04:42:50.746485: Average global foreground Dice: [0.7627]
2022-07-28 04:42:50.767444: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:42:51.751020: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 04:42:51.766162: This epoch took 86.960168 s

2022-07-28 04:42:51.787989: 
epoch:  342
2022-07-28 04:44:11.440497: train loss : -0.9078
2022-07-28 04:44:18.288356: validation loss: -0.3732
2022-07-28 04:44:18.317755: Average global foreground Dice: [0.8131]
2022-07-28 04:44:18.346121: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:44:19.452614: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 04:44:19.476900: This epoch took 87.666864 s

2022-07-28 04:44:19.480464: 
epoch:  343
2022-07-28 04:45:37.968077: train loss : -0.8872
2022-07-28 04:45:45.356309: validation loss: -0.3416
2022-07-28 04:45:45.395982: Average global foreground Dice: [0.7892]
2022-07-28 04:45:45.429021: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:45:46.410053: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 04:45:46.438475: This epoch took 86.937602 s

2022-07-28 04:45:46.460167: 
epoch:  344
2022-07-28 04:47:04.050061: train loss : -0.9026
2022-07-28 04:47:10.840928: validation loss: -0.4306
2022-07-28 04:47:10.865574: Average global foreground Dice: [0.834]
2022-07-28 04:47:10.885264: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:47:11.889060: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 04:47:11.915058: This epoch took 85.422030 s

2022-07-28 04:47:11.947993: 
epoch:  345
2022-07-28 04:48:32.114708: train loss : -0.9140
2022-07-28 04:48:39.118058: validation loss: -0.4409
2022-07-28 04:48:39.136444: Average global foreground Dice: [0.8104]
2022-07-28 04:48:39.145475: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:48:40.204618: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 04:48:40.226079: This epoch took 88.245905 s

2022-07-28 04:48:40.253822: 
epoch:  346
2022-07-28 04:50:00.806377: train loss : -0.8996
2022-07-28 04:50:07.715165: validation loss: -0.4239
2022-07-28 04:50:07.742632: Average global foreground Dice: [0.7994]
2022-07-28 04:50:07.785119: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:50:08.938816: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 04:50:08.969090: This epoch took 88.702098 s

2022-07-28 04:50:08.973780: 
epoch:  347
2022-07-28 04:51:27.824945: train loss : -0.8978
2022-07-28 04:51:35.315966: validation loss: -0.4644
2022-07-28 04:51:35.319706: Average global foreground Dice: [0.8438]
2022-07-28 04:51:35.322182: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:51:36.235839: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 04:51:36.267541: This epoch took 87.274926 s

2022-07-28 04:51:36.294649: 
epoch:  348
2022-07-28 04:52:52.440736: train loss : -0.8932
2022-07-28 04:52:58.906983: validation loss: -0.4800
2022-07-28 04:52:58.933006: Average global foreground Dice: [0.8323]
2022-07-28 04:52:58.976194: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:53:00.243901: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 04:53:00.264821: This epoch took 83.945368 s

2022-07-28 04:53:00.295058: 
epoch:  349
2022-07-28 04:54:19.674408: train loss : -0.8804
2022-07-28 04:54:26.315097: validation loss: -0.4219
2022-07-28 04:54:26.359214: Average global foreground Dice: [0.7985]
2022-07-28 04:54:26.405385: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:54:27.581383: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 04:54:27.603858: saving scheduled checkpoint file...
2022-07-28 04:54:28.031141: saving checkpoint...
2022-07-28 04:54:34.008078: done, saving took 6.37 seconds
2022-07-28 04:54:34.023221: done
2022-07-28 04:54:34.025568: This epoch took 93.693253 s

2022-07-28 04:54:34.027776: 
epoch:  350
2022-07-28 04:55:51.958013: train loss : -0.9019
2022-07-28 04:55:59.145898: validation loss: -0.4009
2022-07-28 04:55:59.168125: Average global foreground Dice: [0.8504]
2022-07-28 04:55:59.180409: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:56:00.349796: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 04:56:00.361820: This epoch took 86.331762 s

2022-07-28 04:56:00.381969: 
epoch:  351
2022-07-28 04:57:18.357900: train loss : -0.8824
2022-07-28 04:57:25.053421: validation loss: -0.3911
2022-07-28 04:57:25.060121: Average global foreground Dice: [0.781]
2022-07-28 04:57:25.063124: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:57:26.174972: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 04:57:26.206011: This epoch took 85.794528 s

2022-07-28 04:57:26.251473: 
epoch:  352
2022-07-28 04:58:46.644042: train loss : -0.9048
2022-07-28 04:58:54.301312: validation loss: -0.5248
2022-07-28 04:58:54.329923: Average global foreground Dice: [0.8505]
2022-07-28 04:58:54.351790: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:58:55.676715: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 04:58:55.700131: This epoch took 89.432342 s

2022-07-28 04:58:55.719704: 
epoch:  353
2022-07-28 05:00:14.238119: train loss : -0.8920
2022-07-28 05:00:19.902942: validation loss: -0.4134
2022-07-28 05:00:19.962225: Average global foreground Dice: [0.8077]
2022-07-28 05:00:19.999054: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:00:21.052066: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 05:00:21.083836: This epoch took 85.344646 s

2022-07-28 05:00:21.124056: 
epoch:  354
2022-07-28 05:01:40.794784: train loss : -0.8876
2022-07-28 05:01:47.868677: validation loss: -0.2866
2022-07-28 05:01:47.885321: Average global foreground Dice: [0.7537]
2022-07-28 05:01:47.902017: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:01:49.216396: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 05:01:49.240426: This epoch took 88.094362 s

2022-07-28 05:01:49.261999: 
epoch:  355
2022-07-28 05:03:08.681568: train loss : -0.8935
2022-07-28 05:03:15.443846: validation loss: -0.3675
2022-07-28 05:03:15.485631: Average global foreground Dice: [0.7983]
2022-07-28 05:03:15.527474: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:03:16.581420: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 05:03:16.613086: This epoch took 87.329070 s

2022-07-28 05:03:16.656990: 
epoch:  356
2022-07-28 05:04:34.306950: train loss : -0.9121
2022-07-28 05:04:42.245840: validation loss: -0.3890
2022-07-28 05:04:42.275135: Average global foreground Dice: [0.8118]
2022-07-28 05:04:42.296034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:04:43.428957: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 05:04:43.431952: This epoch took 86.721877 s

2022-07-28 05:04:43.434636: 
epoch:  357
2022-07-28 05:06:01.489304: train loss : -0.9064
2022-07-28 05:06:08.270034: validation loss: -0.3051
2022-07-28 05:06:08.277852: Average global foreground Dice: [0.7544]
2022-07-28 05:06:08.296068: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:06:09.556775: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 05:06:09.613533: This epoch took 86.176182 s

2022-07-28 05:06:09.634469: 
epoch:  358
2022-07-28 05:07:30.283156: train loss : -0.8970
2022-07-28 05:07:36.812121: validation loss: -0.3088
2022-07-28 05:07:36.817245: Average global foreground Dice: [0.7607]
2022-07-28 05:07:36.820163: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:07:37.971117: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 05:07:37.988139: This epoch took 88.328991 s

2022-07-28 05:07:38.010201: 
epoch:  359
2022-07-28 05:08:58.563579: train loss : -0.9036
2022-07-28 05:09:05.840156: validation loss: -0.3336
2022-07-28 05:09:05.872527: Average global foreground Dice: [0.7826]
2022-07-28 05:09:05.896997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:09:07.021297: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 05:09:07.101126: This epoch took 89.052184 s

2022-07-28 05:09:07.167014: 
epoch:  360
2022-07-28 05:10:21.674865: train loss : -0.9065
2022-07-28 05:10:30.348053: validation loss: -0.3577
2022-07-28 05:10:30.369564: Average global foreground Dice: [0.7648]
2022-07-28 05:10:30.387003: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:10:31.440064: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 05:10:31.476141: This epoch took 84.276124 s

2022-07-28 05:10:31.505331: 
epoch:  361
2022-07-28 05:11:51.185735: train loss : -0.9119
2022-07-28 05:11:59.227427: validation loss: -0.4384
2022-07-28 05:11:59.242347: Average global foreground Dice: [0.8465]
2022-07-28 05:11:59.255129: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:12:00.272830: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 05:12:00.309178: This epoch took 88.776762 s

2022-07-28 05:12:00.363999: 
epoch:  362
2022-07-28 05:13:17.819197: train loss : -0.9070
2022-07-28 05:13:26.507570: validation loss: -0.3806
2022-07-28 05:13:26.516237: Average global foreground Dice: [0.7982]
2022-07-28 05:13:26.541009: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:13:27.829531: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 05:13:27.846565: This epoch took 87.449522 s

2022-07-28 05:13:27.881058: 
epoch:  363
2022-07-28 05:14:45.394737: train loss : -0.8997
2022-07-28 05:14:54.224801: validation loss: -0.4462
2022-07-28 05:14:54.256260: Average global foreground Dice: [0.8051]
2022-07-28 05:14:54.270038: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:14:55.499142: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 05:14:55.524161: This epoch took 87.597085 s

2022-07-28 05:14:55.566513: 
epoch:  364
2022-07-28 05:16:11.314266: train loss : -0.9019
2022-07-28 05:16:17.981263: validation loss: -0.4317
2022-07-28 05:16:17.996828: Average global foreground Dice: [0.8428]
2022-07-28 05:16:18.011322: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:16:19.077879: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 05:16:19.081689: This epoch took 83.487719 s

2022-07-28 05:16:19.100533: 
epoch:  365
2022-07-28 05:17:36.810842: train loss : -0.9062
2022-07-28 05:17:43.975228: validation loss: -0.3701
2022-07-28 05:17:44.019149: Average global foreground Dice: [0.7882]
2022-07-28 05:17:44.058306: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:17:45.426213: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 05:17:45.467211: This epoch took 86.362881 s

2022-07-28 05:17:45.503945: 
epoch:  366
2022-07-28 05:19:05.212574: train loss : -0.9037
2022-07-28 05:19:12.559488: validation loss: -0.4382
2022-07-28 05:19:12.613704: Average global foreground Dice: [0.8101]
2022-07-28 05:19:12.678226: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:19:13.891106: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 05:19:13.923118: This epoch took 88.397097 s

2022-07-28 05:19:13.962001: 
epoch:  367
2022-07-28 05:20:31.055453: train loss : -0.8929
2022-07-28 05:20:37.613780: validation loss: -0.3264
2022-07-28 05:20:37.648628: Average global foreground Dice: [0.6691]
2022-07-28 05:20:37.674066: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:20:38.909413: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 05:20:38.912541: This epoch took 84.929505 s

2022-07-28 05:20:38.939360: 
epoch:  368
2022-07-28 05:21:56.093744: train loss : -0.9051
2022-07-28 05:22:02.650995: validation loss: -0.4005
2022-07-28 05:22:02.658389: Average global foreground Dice: [0.8233]
2022-07-28 05:22:02.661257: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:22:03.983107: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 05:22:03.993477: This epoch took 85.023218 s

2022-07-28 05:22:04.016396: 
epoch:  369
2022-07-28 05:23:23.034218: train loss : -0.8837
2022-07-28 05:23:30.159271: validation loss: -0.4272
2022-07-28 05:23:30.201139: Average global foreground Dice: [0.7877]
2022-07-28 05:23:30.262043: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:23:31.355436: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 05:23:31.419618: This epoch took 87.383392 s

2022-07-28 05:23:31.458116: 
epoch:  370
2022-07-28 05:24:51.715171: train loss : -0.9047
2022-07-28 05:24:58.350443: validation loss: -0.4303
2022-07-28 05:24:58.354199: Average global foreground Dice: [0.8412]
2022-07-28 05:24:58.400092: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:24:59.537043: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 05:24:59.556697: This epoch took 88.060664 s

2022-07-28 05:24:59.560944: 
epoch:  371
2022-07-28 05:26:17.765192: train loss : -0.8935
2022-07-28 05:26:24.359115: validation loss: -0.3089
2022-07-28 05:26:24.391824: Average global foreground Dice: [0.769]
2022-07-28 05:26:24.400928: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:26:25.452047: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 05:26:25.481732: This epoch took 85.900906 s

2022-07-28 05:26:25.511408: 
epoch:  372
2022-07-28 05:27:44.895684: train loss : -0.9183
2022-07-28 05:27:51.971264: validation loss: -0.3418
2022-07-28 05:27:52.014827: Average global foreground Dice: [0.784]
2022-07-28 05:27:52.036044: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:27:53.053627: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 05:27:53.058602: This epoch took 87.499594 s

2022-07-28 05:27:53.061142: 
epoch:  373
2022-07-28 05:29:13.870092: train loss : -0.8898
2022-07-28 05:29:21.383992: validation loss: -0.2501
2022-07-28 05:29:21.399690: Average global foreground Dice: [0.713]
2022-07-28 05:29:21.406642: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:29:22.558650: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 05:29:22.591815: This epoch took 89.528236 s

2022-07-28 05:29:22.613009: 
epoch:  374
2022-07-28 05:30:39.990787: train loss : -0.9082
2022-07-28 05:30:47.106661: validation loss: -0.3976
2022-07-28 05:30:47.116294: Average global foreground Dice: [0.8034]
2022-07-28 05:30:47.132051: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:30:48.161185: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 05:30:48.185786: This epoch took 85.526711 s

2022-07-28 05:30:48.217045: 
epoch:  375
2022-07-28 05:32:07.297164: train loss : -0.8923
2022-07-28 05:32:13.878832: validation loss: -0.5066
2022-07-28 05:32:13.907552: Average global foreground Dice: [0.8258]
2022-07-28 05:32:13.933017: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:32:15.119701: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 05:32:15.141097: This epoch took 86.895916 s

2022-07-28 05:32:15.161517: 
epoch:  376
2022-07-28 05:33:37.378655: train loss : -0.8955
2022-07-28 05:33:44.022025: validation loss: -0.4598
2022-07-28 05:33:44.128817: Average global foreground Dice: [0.818]
2022-07-28 05:33:44.139899: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:33:45.230083: Suus1 maybe_update_lr lr: 2.8e-05
2022-07-28 05:33:45.237777: This epoch took 90.063772 s

2022-07-28 05:33:45.259845: 
epoch:  377
2022-07-28 05:35:05.320496: train loss : -0.8890
2022-07-28 05:35:13.476107: validation loss: -0.4199
2022-07-28 05:35:13.533121: Average global foreground Dice: [0.7779]
2022-07-28 05:35:13.585110: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:35:14.760065: Suus1 maybe_update_lr lr: 2.8e-05
2022-07-28 05:35:14.776152: This epoch took 89.490098 s

2022-07-28 05:35:14.800977: 
epoch:  378
2022-07-28 05:36:32.838381: train loss : -0.9085
2022-07-28 05:36:39.433303: validation loss: -0.3239
2022-07-28 05:36:39.479698: Average global foreground Dice: [0.7704]
2022-07-28 05:36:39.488000: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:36:40.477032: Suus1 maybe_update_lr lr: 2.8e-05
2022-07-28 05:36:40.524135: This epoch took 85.692927 s

2022-07-28 05:36:40.569035: 
epoch:  379
2022-07-28 05:37:58.682493: train loss : -0.9170
2022-07-28 05:38:05.636044: validation loss: -0.3120
2022-07-28 05:38:05.701361: Average global foreground Dice: [0.7243]
2022-07-28 05:38:05.753302: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:38:06.927309: Suus1 maybe_update_lr lr: 2.8e-05
2022-07-28 05:38:06.961400: This epoch took 86.343362 s

2022-07-28 05:38:06.976310: 
epoch:  380
2022-07-28 05:39:25.671944: train loss : -0.9060
2022-07-28 05:39:32.303298: validation loss: -0.3919
2022-07-28 05:39:32.359159: Average global foreground Dice: [0.7767]
2022-07-28 05:39:32.398368: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:39:33.444331: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 05:39:33.466174: This epoch took 86.465132 s

2022-07-28 05:39:33.475653: 
epoch:  381
2022-07-28 05:40:51.864499: train loss : -0.9051
2022-07-28 05:40:58.405138: validation loss: -0.3633
2022-07-28 05:40:58.425417: Average global foreground Dice: [0.7778]
2022-07-28 05:40:58.444854: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:41:00.031048: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 05:41:00.037198: This epoch took 86.530147 s

2022-07-28 05:41:00.044058: 
epoch:  382
2022-07-28 05:42:16.062245: train loss : -0.9075
2022-07-28 05:42:22.799048: validation loss: -0.5300
2022-07-28 05:42:22.874964: Average global foreground Dice: [0.8453]
2022-07-28 05:42:22.918068: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:42:24.181980: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 05:42:24.243193: This epoch took 84.196834 s

2022-07-28 05:42:24.263815: 
epoch:  383
2022-07-28 05:43:42.287484: train loss : -0.8908
2022-07-28 05:43:49.116442: validation loss: -0.5708
2022-07-28 05:43:49.150732: Average global foreground Dice: [0.8625]
2022-07-28 05:43:49.192014: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:43:50.218604: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 05:43:50.243865: This epoch took 85.972556 s

2022-07-28 05:43:50.264881: 
epoch:  384
2022-07-28 05:45:09.982458: train loss : -0.9071
2022-07-28 05:45:17.272729: validation loss: -0.3410
2022-07-28 05:45:17.310331: Average global foreground Dice: [0.7327]
2022-07-28 05:45:17.325502: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:45:18.427191: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 05:45:18.431581: This epoch took 88.158480 s

2022-07-28 05:45:18.447562: 
epoch:  385
2022-07-28 05:46:34.742904: train loss : -0.8940
2022-07-28 05:46:41.223815: validation loss: -0.3520
2022-07-28 05:46:41.255435: Average global foreground Dice: [0.786]
2022-07-28 05:46:41.263948: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:46:42.431722: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 05:46:42.474313: This epoch took 84.011059 s

2022-07-28 05:46:42.508006: 
epoch:  386
2022-07-28 05:48:02.112227: train loss : -0.8995
2022-07-28 05:48:08.280702: validation loss: -0.3437
2022-07-28 05:48:08.316792: Average global foreground Dice: [0.7967]
2022-07-28 05:48:08.347050: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:48:09.753781: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 05:48:09.787182: This epoch took 87.271693 s

2022-07-28 05:48:09.826522: 
epoch:  387
2022-07-28 05:49:30.966550: train loss : -0.9129
2022-07-28 05:49:37.869044: validation loss: -0.4326
2022-07-28 05:49:37.900232: Average global foreground Dice: [0.844]
2022-07-28 05:49:37.917038: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:49:39.410434: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 05:49:39.443145: This epoch took 89.584089 s

2022-07-28 05:49:39.478280: 
epoch:  388
2022-07-28 05:50:59.674203: train loss : -0.9105
2022-07-28 05:51:06.666931: validation loss: -0.4882
2022-07-28 05:51:06.675932: Average global foreground Dice: [0.8413]
2022-07-28 05:51:06.677938: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:51:07.675856: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 05:51:07.687115: This epoch took 88.191362 s

2022-07-28 05:51:07.757223: 
epoch:  389
2022-07-28 05:52:29.160172: train loss : -0.9184
2022-07-28 05:52:35.688790: validation loss: -0.4510
2022-07-28 05:52:35.708573: Average global foreground Dice: [0.8311]
2022-07-28 05:52:35.722280: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:52:36.954782: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 05:52:37.000667: This epoch took 89.188646 s

2022-07-28 05:52:37.016420: 
epoch:  390
2022-07-28 05:53:56.219520: train loss : -0.9090
2022-07-28 05:54:03.191881: validation loss: -0.4426
2022-07-28 05:54:03.221424: Average global foreground Dice: [0.8266]
2022-07-28 05:54:03.244022: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:54:04.468759: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 05:54:04.492308: This epoch took 87.459938 s

2022-07-28 05:54:04.525030: 
epoch:  391
2022-07-28 05:55:22.651870: train loss : -0.9135
2022-07-28 05:55:28.822025: validation loss: -0.4432
2022-07-28 05:55:28.835378: Average global foreground Dice: [0.806]
2022-07-28 05:55:28.863425: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:55:30.033684: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 05:55:30.058896: This epoch took 85.500652 s

2022-07-28 05:55:30.071109: 
epoch:  392
2022-07-28 05:56:49.521955: train loss : -0.9163
2022-07-28 05:56:57.239712: validation loss: -0.3834
2022-07-28 05:56:57.263096: Average global foreground Dice: [0.7806]
2022-07-28 05:56:57.279028: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:56:58.414066: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 05:56:58.427417: This epoch took 88.306679 s

2022-07-28 05:56:58.444978: 
epoch:  393
2022-07-28 05:58:16.032634: train loss : -0.8962
2022-07-28 05:58:22.884529: validation loss: -0.4469
2022-07-28 05:58:22.912804: Average global foreground Dice: [0.8114]
2022-07-28 05:58:22.931423: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:58:24.245934: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 05:58:24.269515: This epoch took 85.817753 s

2022-07-28 05:58:24.293400: 
epoch:  394
2022-07-28 05:59:40.915049: train loss : -0.9038
2022-07-28 05:59:48.032661: validation loss: -0.3933
2022-07-28 05:59:48.072831: Average global foreground Dice: [0.8391]
2022-07-28 05:59:48.105054: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:59:49.343393: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 05:59:49.366102: This epoch took 85.069222 s

2022-07-28 05:59:49.379362: 
epoch:  395
2022-07-28 06:01:08.074965: train loss : -0.9176
2022-07-28 06:01:15.137072: validation loss: -0.3252
2022-07-28 06:01:15.169600: Average global foreground Dice: [0.7414]
2022-07-28 06:01:15.191035: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:01:16.349548: Suus1 maybe_update_lr lr: 2.4e-05
2022-07-28 06:01:16.381065: This epoch took 86.981135 s

2022-07-28 06:01:16.414001: 
epoch:  396
2022-07-28 06:02:35.458607: train loss : -0.9129
2022-07-28 06:02:42.326260: validation loss: -0.3580
2022-07-28 06:02:42.383876: Average global foreground Dice: [0.7704]
2022-07-28 06:02:42.415995: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:02:43.737141: Suus1 maybe_update_lr lr: 2.4e-05
2022-07-28 06:02:43.791119: This epoch took 87.322251 s

2022-07-28 06:02:43.824011: 
epoch:  397
2022-07-28 06:04:03.993744: train loss : -0.9234
2022-07-28 06:04:10.834664: validation loss: -0.4404
2022-07-28 06:04:10.879721: Average global foreground Dice: [0.8083]
2022-07-28 06:04:10.905114: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:04:12.178789: Suus1 maybe_update_lr lr: 2.4e-05
2022-07-28 06:04:12.213094: This epoch took 88.364862 s

2022-07-28 06:04:12.287011: 
epoch:  398
2022-07-28 06:05:31.518035: train loss : -0.9099
2022-07-28 06:05:39.150812: validation loss: -0.5531
2022-07-28 06:05:39.184237: Average global foreground Dice: [0.8592]
2022-07-28 06:05:39.198371: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:05:40.229722: Suus1 maybe_update_lr lr: 2.4e-05
2022-07-28 06:05:40.238040: This epoch took 87.912919 s

2022-07-28 06:05:40.256435: 
epoch:  399
2022-07-28 06:06:59.634615: train loss : -0.8995
2022-07-28 06:07:07.835294: validation loss: -0.4277
2022-07-28 06:07:07.873618: Average global foreground Dice: [0.818]
2022-07-28 06:07:07.888082: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:07:09.348683: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 06:07:09.382782: saving scheduled checkpoint file...
2022-07-28 06:07:09.779197: saving checkpoint...
2022-07-28 06:07:14.829684: done, saving took 5.40 seconds
2022-07-28 06:07:14.851642: done
2022-07-28 06:07:14.853682: This epoch took 94.578980 s

2022-07-28 06:07:14.855607: 
epoch:  400
2022-07-28 06:08:32.912961: train loss : -0.9161
2022-07-28 06:08:40.277429: validation loss: -0.4743
2022-07-28 06:08:40.322378: Average global foreground Dice: [0.813]
2022-07-28 06:08:40.339947: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:08:41.409259: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 06:08:41.486814: This epoch took 86.629298 s

2022-07-28 06:08:41.499982: 
epoch:  401
2022-07-28 06:10:04.109605: train loss : -0.9262
2022-07-28 06:10:10.984249: validation loss: -0.3645
2022-07-28 06:10:11.011904: Average global foreground Dice: [0.7687]
2022-07-28 06:10:11.036997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:10:12.212894: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 06:10:12.237132: This epoch took 90.723957 s

2022-07-28 06:10:12.262901: 
epoch:  402
2022-07-28 06:11:31.804808: train loss : -0.9145
2022-07-28 06:11:38.450751: validation loss: -0.4483
2022-07-28 06:11:38.482571: Average global foreground Dice: [0.8274]
2022-07-28 06:11:38.532005: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:11:39.991761: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 06:11:40.017948: This epoch took 87.734889 s

2022-07-28 06:11:40.043408: 
epoch:  403
2022-07-28 06:12:56.979331: train loss : -0.9054
2022-07-28 06:13:05.019744: validation loss: -0.3988
2022-07-28 06:13:05.074244: Average global foreground Dice: [0.7852]
2022-07-28 06:13:05.082699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:13:06.145986: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 06:13:06.171940: This epoch took 86.109508 s

2022-07-28 06:13:06.175958: 
epoch:  404
2022-07-28 06:14:24.226612: train loss : -0.9099
2022-07-28 06:14:32.238364: validation loss: -0.5136
2022-07-28 06:14:32.260191: Average global foreground Dice: [0.8317]
2022-07-28 06:14:32.264812: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:14:33.326605: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 06:14:33.359072: This epoch took 87.142958 s

2022-07-28 06:14:33.382978: 
epoch:  405
2022-07-28 06:15:51.877783: train loss : -0.9181
2022-07-28 06:15:59.125772: validation loss: -0.4239
2022-07-28 06:15:59.130410: Average global foreground Dice: [0.8252]
2022-07-28 06:15:59.161333: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:16:00.164965: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 06:16:00.195259: This epoch took 86.809767 s

2022-07-28 06:16:00.246533: 
epoch:  406
2022-07-28 06:17:16.889910: train loss : -0.9145
2022-07-28 06:17:24.004881: validation loss: -0.3802
2022-07-28 06:17:24.045037: Average global foreground Dice: [0.8369]
2022-07-28 06:17:24.058625: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:17:25.282283: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 06:17:25.297039: This epoch took 85.013983 s

2022-07-28 06:17:25.327004: 
epoch:  407
2022-07-28 06:18:42.320507: train loss : -0.9125
2022-07-28 06:18:48.453533: validation loss: -0.3712
2022-07-28 06:18:48.480121: Average global foreground Dice: [0.7879]
2022-07-28 06:18:48.504089: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:18:49.644337: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 06:18:49.657093: This epoch took 84.319272 s

2022-07-28 06:18:49.659322: 
epoch:  408
2022-07-28 06:20:09.054265: train loss : -0.9124
2022-07-28 06:20:15.880827: validation loss: -0.4337
2022-07-28 06:20:15.904112: Average global foreground Dice: [0.8381]
2022-07-28 06:20:15.930983: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:20:17.191232: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 06:20:17.211594: This epoch took 87.528144 s

2022-07-28 06:20:17.234615: 
epoch:  409
2022-07-28 06:21:36.824474: train loss : -0.9202
2022-07-28 06:21:44.315623: validation loss: -0.3313
2022-07-28 06:21:44.322637: Average global foreground Dice: [0.7839]
2022-07-28 06:21:44.325358: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:21:45.660587: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 06:21:45.675595: This epoch took 88.432729 s

2022-07-28 06:21:45.689449: 
epoch:  410
2022-07-28 06:23:05.253186: train loss : -0.9113
2022-07-28 06:23:13.213670: validation loss: -0.5733
2022-07-28 06:23:13.256076: Average global foreground Dice: [0.8447]
2022-07-28 06:23:13.300077: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:23:14.632632: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 06:23:14.642030: This epoch took 88.928986 s

2022-07-28 06:23:14.664970: 
epoch:  411
2022-07-28 06:24:35.706692: train loss : -0.9111
2022-07-28 06:24:42.968584: validation loss: -0.4463
2022-07-28 06:24:43.014509: Average global foreground Dice: [0.8293]
2022-07-28 06:24:43.068089: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:24:44.239468: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 06:24:44.265111: This epoch took 89.578118 s

2022-07-28 06:24:44.293997: 
epoch:  412
2022-07-28 06:26:02.974349: train loss : -0.9252
2022-07-28 06:26:10.512894: validation loss: -0.3664
2022-07-28 06:26:10.580439: Average global foreground Dice: [0.8103]
2022-07-28 06:26:10.634083: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:26:11.788038: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 06:26:11.850174: This epoch took 87.512200 s

2022-07-28 06:26:11.878050: 
epoch:  413
2022-07-28 06:27:28.619447: train loss : -0.9016
2022-07-28 06:27:37.058403: validation loss: -0.4325
2022-07-28 06:27:37.077006: Average global foreground Dice: [0.7836]
2022-07-28 06:27:37.083700: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:27:38.346494: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 06:27:38.349437: This epoch took 86.439398 s

2022-07-28 06:27:38.351710: 
epoch:  414
2022-07-28 06:28:54.725530: train loss : -0.9098
2022-07-28 06:29:03.623869: validation loss: -0.3881
2022-07-28 06:29:03.630705: Average global foreground Dice: [0.8207]
2022-07-28 06:29:03.660019: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:29:04.858529: Suus1 maybe_update_lr lr: 2e-05
2022-07-28 06:29:04.912073: This epoch took 86.557940 s

2022-07-28 06:29:04.976972: 
epoch:  415
2022-07-28 06:30:23.627214: train loss : -0.9196
2022-07-28 06:30:31.043878: validation loss: -0.3588
2022-07-28 06:30:31.096007: Average global foreground Dice: [0.7964]
2022-07-28 06:30:31.105012: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:30:32.213909: Suus1 maybe_update_lr lr: 2e-05
2022-07-28 06:30:32.222259: This epoch took 87.216321 s

2022-07-28 06:30:32.250219: 
epoch:  416
2022-07-28 06:31:50.670833: train loss : -0.9215
2022-07-28 06:31:58.912905: validation loss: -0.3462
2022-07-28 06:31:58.973004: Average global foreground Dice: [0.7569]
2022-07-28 06:31:59.034003: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:32:00.145420: Suus1 maybe_update_lr lr: 2e-05
2022-07-28 06:32:00.174461: This epoch took 87.873328 s

2022-07-28 06:32:00.180064: 
epoch:  417
2022-07-28 06:33:20.354162: train loss : -0.9102
2022-07-28 06:33:29.049390: validation loss: -0.3069
2022-07-28 06:33:29.074902: Average global foreground Dice: [0.7448]
2022-07-28 06:33:29.096009: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:33:30.454076: Suus1 maybe_update_lr lr: 2e-05
2022-07-28 06:33:30.476156: This epoch took 90.250093 s

2022-07-28 06:33:30.505924: 
epoch:  418
2022-07-28 06:34:48.356351: train loss : -0.9149
2022-07-28 06:34:55.641298: validation loss: -0.4179
2022-07-28 06:34:55.703585: Average global foreground Dice: [0.8129]
2022-07-28 06:34:55.766624: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:34:56.982882: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 06:34:57.041193: This epoch took 86.504057 s

2022-07-28 06:34:57.099999: 
epoch:  419
2022-07-28 06:36:18.506051: train loss : -0.9111
2022-07-28 06:36:26.253297: validation loss: -0.3660
2022-07-28 06:36:26.279530: Average global foreground Dice: [0.7299]
2022-07-28 06:36:26.299999: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:36:27.701198: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 06:36:27.733721: This epoch took 90.626728 s

2022-07-28 06:36:27.748991: 
epoch:  420
2022-07-28 06:37:44.283272: train loss : -0.9092
2022-07-28 06:37:52.175156: validation loss: -0.4390
2022-07-28 06:37:52.192714: Average global foreground Dice: [0.8308]
2022-07-28 06:37:52.206034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:37:53.551135: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 06:37:53.586115: This epoch took 85.816092 s

2022-07-28 06:37:53.632063: 
epoch:  421
2022-07-28 06:39:11.758651: train loss : -0.9134
2022-07-28 06:39:18.891115: validation loss: -0.4443
2022-07-28 06:39:18.922401: Average global foreground Dice: [0.8015]
2022-07-28 06:39:18.950794: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:39:20.142968: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 06:39:20.148062: This epoch took 86.489036 s

2022-07-28 06:39:20.176011: 
epoch:  422
2022-07-28 06:40:42.995171: train loss : -0.9090
2022-07-28 06:40:50.007780: validation loss: -0.4420
2022-07-28 06:40:50.039541: Average global foreground Dice: [0.8249]
2022-07-28 06:40:50.043574: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:40:51.311078: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 06:40:51.325152: This epoch took 91.128647 s

2022-07-28 06:40:51.361656: 
epoch:  423
2022-07-28 06:42:09.784743: train loss : -0.9047
2022-07-28 06:42:17.937501: validation loss: -0.3792
2022-07-28 06:42:17.969747: Average global foreground Dice: [0.7581]
2022-07-28 06:42:18.003033: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:42:19.076145: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-28 06:42:19.117118: This epoch took 87.740160 s

2022-07-28 06:42:19.142006: 
epoch:  424
2022-07-28 06:43:37.954768: train loss : -0.9199
2022-07-28 06:43:44.302569: validation loss: -0.2815
2022-07-28 06:43:44.306668: Average global foreground Dice: [0.7072]
2022-07-28 06:43:44.340042: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:43:45.649775: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-28 06:43:45.665093: This epoch took 86.486081 s

2022-07-28 06:43:45.685071: 
epoch:  425
2022-07-28 06:45:02.228695: train loss : -0.8960
2022-07-28 06:45:09.055182: validation loss: -0.4111
2022-07-28 06:45:09.090821: Average global foreground Dice: [0.7932]
2022-07-28 06:45:09.112457: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:45:10.201708: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-28 06:45:10.255543: This epoch took 84.526431 s

2022-07-28 06:45:10.274721: 
epoch:  426
2022-07-28 06:46:27.738337: train loss : -0.9144
2022-07-28 06:46:34.526510: validation loss: -0.3980
2022-07-28 06:46:34.539132: Average global foreground Dice: [0.8206]
2022-07-28 06:46:34.572903: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:46:35.696093: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-28 06:46:35.717685: This epoch took 85.424292 s

2022-07-28 06:46:35.729239: 
epoch:  427
2022-07-28 06:47:55.710001: train loss : -0.9043
2022-07-28 06:48:03.036242: validation loss: -0.3898
2022-07-28 06:48:03.067816: Average global foreground Dice: [0.8017]
2022-07-28 06:48:03.080631: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:48:04.320055: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 06:48:04.374171: This epoch took 88.625952 s

2022-07-28 06:48:04.421033: 
epoch:  428
2022-07-28 06:49:25.898729: train loss : -0.9225
2022-07-28 06:49:31.751293: validation loss: -0.4270
2022-07-28 06:49:31.788953: Average global foreground Dice: [0.8193]
2022-07-28 06:49:31.806991: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:49:33.039572: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 06:49:33.063279: This epoch took 88.612843 s

2022-07-28 06:49:33.132131: 
epoch:  429
2022-07-28 06:50:51.486393: train loss : -0.9170
2022-07-28 06:50:58.093357: validation loss: -0.5373
2022-07-28 06:50:58.131644: Average global foreground Dice: [0.8619]
2022-07-28 06:50:58.156712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:50:59.248315: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 06:50:59.275239: This epoch took 86.112180 s

2022-07-28 06:50:59.321043: 
epoch:  430
2022-07-28 06:52:19.251291: train loss : -0.9181
2022-07-28 06:52:27.605735: validation loss: -0.3666
2022-07-28 06:52:27.641987: Average global foreground Dice: [0.7663]
2022-07-28 06:52:27.652008: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:52:28.814695: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 06:52:28.834136: This epoch took 89.474989 s

2022-07-28 06:52:28.846370: 
epoch:  431
2022-07-28 06:53:48.533312: train loss : -0.9093
2022-07-28 06:53:55.717744: validation loss: -0.3501
2022-07-28 06:53:55.745856: Average global foreground Dice: [0.7774]
2022-07-28 06:53:55.784093: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:53:56.925690: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 06:53:56.985132: This epoch took 88.125445 s

2022-07-28 06:53:57.019003: 
epoch:  432
2022-07-28 06:55:17.623928: train loss : -0.9105
2022-07-28 06:55:24.154163: validation loss: -0.5063
2022-07-28 06:55:24.191836: Average global foreground Dice: [0.846]
2022-07-28 06:55:24.195856: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:55:25.394906: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 06:55:25.435137: This epoch took 88.354492 s

2022-07-28 06:55:25.474099: 
epoch:  433
2022-07-28 06:56:43.042086: train loss : -0.9129
2022-07-28 06:56:49.853330: validation loss: -0.3646
2022-07-28 06:56:49.888052: Average global foreground Dice: [0.7869]
2022-07-28 06:56:49.924644: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:56:51.069647: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 06:56:51.099130: This epoch took 85.598081 s

2022-07-28 06:56:51.125018: 
epoch:  434
2022-07-28 06:58:08.938471: train loss : -0.9061
2022-07-28 06:58:16.203678: validation loss: -0.3766
2022-07-28 06:58:16.309704: Average global foreground Dice: [0.7589]
2022-07-28 06:58:16.354035: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:58:17.552442: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 06:58:17.570405: This epoch took 86.423345 s

2022-07-28 06:58:17.628422: 
epoch:  435
2022-07-28 06:59:39.084143: train loss : -0.9188
2022-07-28 06:59:47.148365: validation loss: -0.4699
2022-07-28 06:59:47.161251: Average global foreground Dice: [0.8062]
2022-07-28 06:59:47.188931: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:59:48.231822: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 06:59:48.252351: This epoch took 90.588645 s

2022-07-28 06:59:48.267864: 
epoch:  436
2022-07-28 07:01:06.626787: train loss : -0.9220
2022-07-28 07:01:13.561999: validation loss: -0.4449
2022-07-28 07:01:13.602147: Average global foreground Dice: [0.8342]
2022-07-28 07:01:13.622563: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:01:14.597545: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 07:01:14.629279: This epoch took 86.353384 s

2022-07-28 07:01:14.643000: 
epoch:  437
2022-07-28 07:02:31.521851: train loss : -0.9199
2022-07-28 07:02:38.721657: validation loss: -0.3498
2022-07-28 07:02:38.755538: Average global foreground Dice: [0.7593]
2022-07-28 07:02:38.777771: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:02:39.888009: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-28 07:02:39.890768: This epoch took 85.226650 s

2022-07-28 07:02:39.892878: 
epoch:  438
2022-07-28 07:03:57.157259: train loss : -0.9213
2022-07-28 07:04:03.939579: validation loss: -0.3834
2022-07-28 07:04:03.971729: Average global foreground Dice: [0.8026]
2022-07-28 07:04:03.995025: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:04:05.050042: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-28 07:04:05.076165: This epoch took 85.181074 s

2022-07-28 07:04:05.078647: 
epoch:  439
2022-07-28 07:05:23.844274: train loss : -0.9247
2022-07-28 07:05:30.943163: validation loss: -0.4913
2022-07-28 07:05:30.986553: Average global foreground Dice: [0.8621]
2022-07-28 07:05:31.018011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:05:32.283836: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-28 07:05:32.336022: This epoch took 87.249819 s

2022-07-28 07:05:32.359948: 
epoch:  440
2022-07-28 07:06:50.035450: train loss : -0.9255
2022-07-28 07:06:57.448630: validation loss: -0.3284
2022-07-28 07:06:57.509725: Average global foreground Dice: [0.7559]
2022-07-28 07:06:57.535503: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:06:58.675472: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-28 07:06:58.713665: This epoch took 86.333668 s

2022-07-28 07:06:58.748113: 
epoch:  441
2022-07-28 07:08:16.453772: train loss : -0.9201
2022-07-28 07:08:24.165187: validation loss: -0.4366
2022-07-28 07:08:24.199392: Average global foreground Dice: [0.8198]
2022-07-28 07:08:24.220555: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:08:25.454663: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-28 07:08:25.487114: This epoch took 86.714029 s

2022-07-28 07:08:25.503014: 
epoch:  442
2022-07-28 07:09:43.958820: train loss : -0.9209
2022-07-28 07:09:51.393795: validation loss: -0.3271
2022-07-28 07:09:51.399261: Average global foreground Dice: [0.7545]
2022-07-28 07:09:51.418189: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:09:52.593168: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-28 07:09:52.595624: This epoch took 87.070554 s

2022-07-28 07:09:52.597743: 
epoch:  443
2022-07-28 07:11:09.896009: train loss : -0.9151
2022-07-28 07:11:17.999248: validation loss: -0.4826
2022-07-28 07:11:18.037844: Average global foreground Dice: [0.8448]
2022-07-28 07:11:18.074151: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:11:19.257810: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-28 07:11:19.316732: This epoch took 86.716299 s

2022-07-28 07:11:19.356529: 
epoch:  444
2022-07-28 07:12:38.451724: train loss : -0.9199
2022-07-28 07:12:46.073114: validation loss: -0.5161
2022-07-28 07:12:46.098484: Average global foreground Dice: [0.8775]
2022-07-28 07:12:46.129649: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:12:47.242027: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-28 07:12:47.300213: This epoch took 87.893201 s

2022-07-28 07:12:47.333990: 
epoch:  445
2022-07-28 07:14:07.376331: train loss : -0.9140
2022-07-28 07:14:14.346265: validation loss: -0.4128
2022-07-28 07:14:14.376187: Average global foreground Dice: [0.836]
2022-07-28 07:14:14.385775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:14:15.440280: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 07:14:15.444195: This epoch took 88.083184 s

2022-07-28 07:14:15.469257: 
epoch:  446
2022-07-28 07:15:34.869849: train loss : -0.9236
2022-07-28 07:15:41.679212: validation loss: -0.4243
2022-07-28 07:15:41.747591: Average global foreground Dice: [0.8344]
2022-07-28 07:15:41.802072: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:15:43.333353: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 07:15:43.356336: This epoch took 87.846918 s

2022-07-28 07:15:43.368967: 
epoch:  447
2022-07-28 07:17:02.497742: train loss : -0.9182
2022-07-28 07:17:09.123430: validation loss: -0.4380
2022-07-28 07:17:09.151283: Average global foreground Dice: [0.822]
2022-07-28 07:17:09.170167: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:17:10.561391: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 07:17:10.575036: This epoch took 87.174014 s

2022-07-28 07:17:10.588653: 
epoch:  448
2022-07-28 07:18:28.312622: train loss : -0.9221
2022-07-28 07:18:34.529226: validation loss: -0.4053
2022-07-28 07:18:34.554681: Average global foreground Dice: [0.7524]
2022-07-28 07:18:34.577023: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:18:35.648970: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 07:18:35.662210: This epoch took 85.060467 s

2022-07-28 07:18:35.664758: 
epoch:  449
2022-07-28 07:19:54.810114: train loss : -0.9176
2022-07-28 07:20:02.091344: validation loss: -0.4507
2022-07-28 07:20:02.097992: Average global foreground Dice: [0.8161]
2022-07-28 07:20:02.141016: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:20:03.306752: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 07:20:03.372210: saving scheduled checkpoint file...
2022-07-28 07:20:03.730679: saving checkpoint...
2022-07-28 07:20:09.171077: done, saving took 5.77 seconds
2022-07-28 07:20:09.202129: done
2022-07-28 07:20:09.204179: This epoch took 93.537141 s

2022-07-28 07:20:09.206117: 
epoch:  450
2022-07-28 07:21:25.525735: train loss : -0.9152
2022-07-28 07:21:31.627652: validation loss: -0.3953
2022-07-28 07:21:31.688983: Average global foreground Dice: [0.7945]
2022-07-28 07:21:31.713146: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:21:33.120551: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-28 07:21:33.147954: This epoch took 83.939782 s

2022-07-28 07:21:33.170997: 
epoch:  451
2022-07-28 07:22:52.961499: train loss : -0.9116
2022-07-28 07:23:00.323016: validation loss: -0.4216
2022-07-28 07:23:00.330795: Average global foreground Dice: [0.8334]
2022-07-28 07:23:00.344037: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:23:01.494361: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-28 07:23:01.535061: This epoch took 88.335719 s

2022-07-28 07:23:01.567963: 
epoch:  452
2022-07-28 07:24:19.171363: train loss : -0.9154
2022-07-28 07:24:25.985849: validation loss: -0.3637
2022-07-28 07:24:25.989727: Average global foreground Dice: [0.8098]
2022-07-28 07:24:25.992241: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:24:27.016756: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-28 07:24:27.034022: This epoch took 85.436036 s

2022-07-28 07:24:27.055977: 
epoch:  453
2022-07-28 07:25:47.848690: train loss : -0.9248
2022-07-28 07:25:53.792728: validation loss: -0.4979
2022-07-28 07:25:53.800368: Average global foreground Dice: [0.8486]
2022-07-28 07:25:53.811900: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:25:54.938983: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-28 07:25:54.980038: This epoch took 87.892309 s

2022-07-28 07:25:55.021377: 
epoch:  454
2022-07-28 07:27:15.601757: train loss : -0.9242
2022-07-28 07:27:22.414413: validation loss: -0.3956
2022-07-28 07:27:22.447824: Average global foreground Dice: [0.8182]
2022-07-28 07:27:22.472126: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:27:23.657617: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 07:27:23.713088: This epoch took 88.675499 s

2022-07-28 07:27:23.780163: 
epoch:  455
2022-07-28 07:28:43.672863: train loss : -0.9287
2022-07-28 07:28:51.750098: validation loss: -0.4059
2022-07-28 07:28:51.783167: Average global foreground Dice: [0.8163]
2022-07-28 07:28:51.814573: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:28:53.009295: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 07:28:53.019865: This epoch took 89.215150 s

2022-07-28 07:28:53.027600: 
epoch:  456
2022-07-28 07:30:12.610143: train loss : -0.9186
2022-07-28 07:30:19.905485: validation loss: -0.4227
2022-07-28 07:30:19.946245: Average global foreground Dice: [0.8276]
2022-07-28 07:30:19.953359: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:30:21.371407: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 07:30:21.382818: This epoch took 88.330561 s

2022-07-28 07:30:21.385179: 
epoch:  457
2022-07-28 07:31:41.698138: train loss : -0.9134
2022-07-28 07:31:48.613526: validation loss: -0.3629
2022-07-28 07:31:48.646411: Average global foreground Dice: [0.7599]
2022-07-28 07:31:48.673754: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:31:50.082811: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 07:31:50.155263: This epoch took 88.767869 s

2022-07-28 07:31:50.198016: 
epoch:  458
2022-07-28 07:33:09.504039: train loss : -0.9126
2022-07-28 07:33:15.966670: validation loss: -0.4976
2022-07-28 07:33:15.992663: Average global foreground Dice: [0.8682]
2022-07-28 07:33:15.998662: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:33:17.087421: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 07:33:17.112914: This epoch took 86.863846 s

2022-07-28 07:33:17.135248: 
epoch:  459
2022-07-28 07:34:35.366369: train loss : -0.9135
2022-07-28 07:34:42.137053: validation loss: -0.3681
2022-07-28 07:34:42.152736: Average global foreground Dice: [0.7638]
2022-07-28 07:34:42.162387: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:34:43.261816: Suus1 maybe_update_lr lr: 1e-05
2022-07-28 07:34:43.285784: This epoch took 86.115700 s

2022-07-28 07:34:43.308195: 
epoch:  460
2022-07-28 07:36:01.600679: train loss : -0.9191
2022-07-28 07:36:08.207192: validation loss: -0.4295
2022-07-28 07:36:08.247829: Average global foreground Dice: [0.8028]
2022-07-28 07:36:08.278542: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:36:09.297365: Suus1 maybe_update_lr lr: 1e-05
2022-07-28 07:36:09.319727: This epoch took 85.971580 s

2022-07-28 07:36:09.328294: 
epoch:  461
2022-07-28 07:37:28.915000: train loss : -0.9197
2022-07-28 07:37:35.653519: validation loss: -0.5377
2022-07-28 07:37:35.697865: Average global foreground Dice: [0.8408]
2022-07-28 07:37:35.740161: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:37:36.803471: Suus1 maybe_update_lr lr: 1e-05
2022-07-28 07:37:36.822819: This epoch took 87.488739 s

2022-07-28 07:37:36.842029: 
epoch:  462
2022-07-28 07:38:54.381670: train loss : -0.9131
2022-07-28 07:39:01.357335: validation loss: -0.4493
2022-07-28 07:39:01.371959: Average global foreground Dice: [0.8319]
2022-07-28 07:39:01.400761: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:39:02.696356: Suus1 maybe_update_lr lr: 1e-05
2022-07-28 07:39:02.728106: This epoch took 85.863750 s

2022-07-28 07:39:02.760068: 
epoch:  463
2022-07-28 07:40:20.634662: train loss : -0.9163
2022-07-28 07:40:27.831388: validation loss: -0.4308
2022-07-28 07:40:27.837834: Average global foreground Dice: [0.8341]
2022-07-28 07:40:27.851551: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:40:29.186535: Suus1 maybe_update_lr lr: 9e-06
2022-07-28 07:40:29.237296: saving best epoch checkpoint...
2022-07-28 07:40:29.845120: saving checkpoint...
2022-07-28 07:40:35.568737: done, saving took 6.30 seconds
2022-07-28 07:40:35.596642: This epoch took 92.803378 s

2022-07-28 07:40:35.598661: 
epoch:  464
2022-07-28 07:41:55.697993: train loss : -0.9263
2022-07-28 07:42:02.674109: validation loss: -0.4801
2022-07-28 07:42:02.696747: Average global foreground Dice: [0.8438]
2022-07-28 07:42:02.726166: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:42:03.874274: Suus1 maybe_update_lr lr: 9e-06
2022-07-28 07:42:03.913168: saving best epoch checkpoint...
2022-07-28 07:42:04.247916: saving checkpoint...
2022-07-28 07:42:10.501655: done, saving took 6.53 seconds
2022-07-28 07:42:10.509274: This epoch took 94.908571 s

2022-07-28 07:42:10.511123: 
epoch:  465
2022-07-28 07:43:27.401000: train loss : -0.9264
2022-07-28 07:43:35.154016: validation loss: -0.2815
2022-07-28 07:43:35.237782: Average global foreground Dice: [0.7176]
2022-07-28 07:43:35.281077: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:43:36.328235: Suus1 maybe_update_lr lr: 9e-06
2022-07-28 07:43:36.358057: This epoch took 85.840819 s

2022-07-28 07:43:36.390958: 
epoch:  466
2022-07-28 07:44:54.878152: train loss : -0.9233
2022-07-28 07:45:02.157186: validation loss: -0.4775
2022-07-28 07:45:02.175019: Average global foreground Dice: [0.843]
2022-07-28 07:45:02.200800: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:45:03.342870: Suus1 maybe_update_lr lr: 9e-06
2022-07-28 07:45:03.356628: This epoch took 86.931635 s

2022-07-28 07:45:03.367288: 
epoch:  467
2022-07-28 07:46:25.086149: train loss : -0.9192
2022-07-28 07:46:32.031650: validation loss: -0.3300
2022-07-28 07:46:32.069983: Average global foreground Dice: [0.7585]
2022-07-28 07:46:32.090226: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:46:33.508740: Suus1 maybe_update_lr lr: 8e-06
2022-07-28 07:46:33.544463: This epoch took 90.156446 s

2022-07-28 07:46:33.572989: 
epoch:  468
2022-07-28 07:47:52.224618: train loss : -0.9199
2022-07-28 07:47:59.707721: validation loss: -0.4893
2022-07-28 07:47:59.747926: Average global foreground Dice: [0.8574]
2022-07-28 07:47:59.771041: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:48:00.796583: Suus1 maybe_update_lr lr: 8e-06
2022-07-28 07:48:00.816005: This epoch took 87.199009 s

2022-07-28 07:48:00.837034: 
epoch:  469
2022-07-28 07:49:21.598257: train loss : -0.9255
2022-07-28 07:49:27.836927: validation loss: -0.4254
2022-07-28 07:49:27.865760: Average global foreground Dice: [0.8404]
2022-07-28 07:49:27.886114: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:49:29.018181: Suus1 maybe_update_lr lr: 8e-06
2022-07-28 07:49:29.070084: This epoch took 88.200274 s

2022-07-28 07:49:29.093006: 
epoch:  470
2022-07-28 07:50:47.170740: train loss : -0.9233
2022-07-28 07:50:55.472346: validation loss: -0.3685
2022-07-28 07:50:55.495975: Average global foreground Dice: [0.7864]
2022-07-28 07:50:55.511264: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:50:56.932163: Suus1 maybe_update_lr lr: 8e-06
2022-07-28 07:50:56.941442: This epoch took 87.827449 s

2022-07-28 07:50:56.955171: 
epoch:  471
2022-07-28 07:52:16.117856: train loss : -0.9200
2022-07-28 07:52:23.112053: validation loss: -0.3653
2022-07-28 07:52:23.151218: Average global foreground Dice: [0.7812]
2022-07-28 07:52:23.157135: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:52:24.315166: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 07:52:24.338555: This epoch took 87.357647 s

2022-07-28 07:52:24.363453: 
epoch:  472
2022-07-28 07:53:40.949546: train loss : -0.9255
2022-07-28 07:53:47.490762: validation loss: -0.3590
2022-07-28 07:53:47.510743: Average global foreground Dice: [0.79]
2022-07-28 07:53:47.533433: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:53:48.664380: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 07:53:48.668181: This epoch took 84.280149 s

2022-07-28 07:53:48.682049: 
epoch:  473
2022-07-28 07:55:09.372225: train loss : -0.9202
2022-07-28 07:55:15.788471: validation loss: -0.3721
2022-07-28 07:55:15.792244: Average global foreground Dice: [0.7964]
2022-07-28 07:55:15.794656: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:55:17.069081: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 07:55:17.103935: This epoch took 88.394814 s

2022-07-28 07:55:17.148058: 
epoch:  474
2022-07-28 07:56:34.399951: train loss : -0.9238
2022-07-28 07:56:41.103504: validation loss: -0.4219
2022-07-28 07:56:41.126848: Average global foreground Dice: [0.7958]
2022-07-28 07:56:41.130597: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:56:42.127351: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 07:56:42.140604: This epoch took 84.962437 s

2022-07-28 07:56:42.147799: 
epoch:  475
2022-07-28 07:58:00.567395: train loss : -0.9219
2022-07-28 07:58:07.459481: validation loss: -0.4981
2022-07-28 07:58:07.491542: Average global foreground Dice: [0.8717]
2022-07-28 07:58:07.506055: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:58:08.606014: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 07:58:08.631159: This epoch took 86.461091 s

2022-07-28 07:58:08.652126: 
epoch:  476
2022-07-28 07:59:27.552218: train loss : -0.9273
2022-07-28 07:59:35.461136: validation loss: -0.4961
2022-07-28 07:59:35.524763: Average global foreground Dice: [0.8585]
2022-07-28 07:59:35.568705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:59:36.841233: Suus1 maybe_update_lr lr: 6e-06
2022-07-28 07:59:36.861498: This epoch took 88.182457 s

2022-07-28 07:59:36.872698: 
epoch:  477
2022-07-28 08:00:56.861673: train loss : -0.9207
2022-07-28 08:01:04.053900: validation loss: -0.3772
2022-07-28 08:01:04.058027: Average global foreground Dice: [0.7927]
2022-07-28 08:01:04.084060: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:01:05.309240: Suus1 maybe_update_lr lr: 6e-06
2022-07-28 08:01:05.333267: This epoch took 88.452345 s

2022-07-28 08:01:05.348042: 
epoch:  478
2022-07-28 08:02:23.972782: train loss : -0.9243
2022-07-28 08:02:31.677832: validation loss: -0.3380
2022-07-28 08:02:31.719037: Average global foreground Dice: [0.7846]
2022-07-28 08:02:31.747093: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:02:32.889918: Suus1 maybe_update_lr lr: 6e-06
2022-07-28 08:02:32.922051: This epoch took 87.557229 s

2022-07-28 08:02:32.941180: 
epoch:  479
2022-07-28 08:03:50.067907: train loss : -0.9184
2022-07-28 08:03:56.841585: validation loss: -0.3880
2022-07-28 08:03:56.862650: Average global foreground Dice: [0.803]
2022-07-28 08:03:56.877034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:03:57.917947: Suus1 maybe_update_lr lr: 6e-06
2022-07-28 08:03:57.938545: This epoch took 84.982831 s

2022-07-28 08:03:57.950899: 
epoch:  480
2022-07-28 08:05:14.189557: train loss : -0.9246
2022-07-28 08:05:20.661446: validation loss: -0.2981
2022-07-28 08:05:20.673461: Average global foreground Dice: [0.7604]
2022-07-28 08:05:20.692986: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:05:22.115402: Suus1 maybe_update_lr lr: 5e-06
2022-07-28 08:05:22.146348: This epoch took 84.182367 s

2022-07-28 08:05:22.151190: 
epoch:  481
2022-07-28 08:06:41.537185: train loss : -0.9255
2022-07-28 08:06:49.393788: validation loss: -0.3892
2022-07-28 08:06:49.431871: Average global foreground Dice: [0.7997]
2022-07-28 08:06:49.464027: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:06:50.821505: Suus1 maybe_update_lr lr: 5e-06
2022-07-28 08:06:50.847640: This epoch took 88.664672 s

2022-07-28 08:06:50.850228: 
epoch:  482
2022-07-28 08:08:08.728437: train loss : -0.9301
2022-07-28 08:08:16.300519: validation loss: -0.5288
2022-07-28 08:08:16.328667: Average global foreground Dice: [0.8556]
2022-07-28 08:08:16.346468: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:08:17.438269: Suus1 maybe_update_lr lr: 5e-06
2022-07-28 08:08:17.461165: This epoch took 86.608788 s

2022-07-28 08:08:17.511996: 
epoch:  483
2022-07-28 08:09:33.982368: train loss : -0.9223
2022-07-28 08:09:41.081112: validation loss: -0.3908
2022-07-28 08:09:41.117745: Average global foreground Dice: [0.8027]
2022-07-28 08:09:41.151504: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:09:42.355559: Suus1 maybe_update_lr lr: 5e-06
2022-07-28 08:09:42.376757: This epoch took 84.788540 s

2022-07-28 08:09:42.379114: 
epoch:  484
2022-07-28 08:11:01.001789: train loss : -0.9251
2022-07-28 08:11:07.080608: validation loss: -0.4203
2022-07-28 08:11:07.084520: Average global foreground Dice: [0.8119]
2022-07-28 08:11:07.087004: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:11:08.111077: Suus1 maybe_update_lr lr: 4e-06
2022-07-28 08:11:08.146425: This epoch took 85.753343 s

2022-07-28 08:11:08.167998: 
epoch:  485
2022-07-28 08:12:27.237951: train loss : -0.9259
2022-07-28 08:12:34.426583: validation loss: -0.4326
2022-07-28 08:12:34.458349: Average global foreground Dice: [0.8024]
2022-07-28 08:12:34.484765: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:12:35.960189: Suus1 maybe_update_lr lr: 4e-06
2022-07-28 08:12:36.006337: This epoch took 87.808259 s

2022-07-28 08:12:36.054770: 
epoch:  486
2022-07-28 08:13:52.297225: train loss : -0.9260
2022-07-28 08:14:00.295629: validation loss: -0.3250
2022-07-28 08:14:00.310459: Average global foreground Dice: [0.7135]
2022-07-28 08:14:00.313154: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:14:01.702247: Suus1 maybe_update_lr lr: 4e-06
2022-07-28 08:14:01.705715: This epoch took 85.606151 s

2022-07-28 08:14:01.708059: 
epoch:  487
2022-07-28 08:15:20.352386: train loss : -0.9215
2022-07-28 08:15:28.429954: validation loss: -0.5051
2022-07-28 08:15:28.483868: Average global foreground Dice: [0.8607]
2022-07-28 08:15:28.508966: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:15:29.742619: Suus1 maybe_update_lr lr: 3e-06
2022-07-28 08:15:29.755315: This epoch took 88.045084 s

2022-07-28 08:15:29.771321: 
epoch:  488
2022-07-28 08:16:46.060991: train loss : -0.9299
2022-07-28 08:16:53.906666: validation loss: -0.3794
2022-07-28 08:16:53.940822: Average global foreground Dice: [0.8058]
2022-07-28 08:16:53.973018: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:16:55.197553: Suus1 maybe_update_lr lr: 3e-06
2022-07-28 08:16:55.201267: This epoch took 85.400723 s

2022-07-28 08:16:55.206251: 
epoch:  489
2022-07-28 08:18:14.787876: train loss : -0.9241
2022-07-28 08:18:23.265861: validation loss: -0.3605
2022-07-28 08:18:23.305382: Average global foreground Dice: [0.7885]
2022-07-28 08:18:23.331828: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:18:24.701023: Suus1 maybe_update_lr lr: 3e-06
2022-07-28 08:18:24.739226: This epoch took 89.529076 s

2022-07-28 08:18:24.771988: 
epoch:  490
2022-07-28 08:19:42.313930: train loss : -0.9271
2022-07-28 08:19:49.634916: validation loss: -0.4513
2022-07-28 08:19:49.672760: Average global foreground Dice: [0.8363]
2022-07-28 08:19:49.693048: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:19:51.037778: Suus1 maybe_update_lr lr: 3e-06
2022-07-28 08:19:51.059068: This epoch took 86.244090 s

2022-07-28 08:19:51.092053: 
epoch:  491
2022-07-28 08:21:10.375719: train loss : -0.9291
2022-07-28 08:21:19.441721: validation loss: -0.4603
2022-07-28 08:21:19.499723: Average global foreground Dice: [0.8309]
2022-07-28 08:21:19.532647: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:21:20.662002: Suus1 maybe_update_lr lr: 2e-06
2022-07-28 08:21:20.683915: This epoch took 89.525928 s

2022-07-28 08:21:20.702511: 
epoch:  492
2022-07-28 08:22:37.567976: train loss : -0.9232
2022-07-28 08:22:46.314998: validation loss: -0.3699
2022-07-28 08:22:46.335737: Average global foreground Dice: [0.8132]
2022-07-28 08:22:46.357363: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:22:47.561032: Suus1 maybe_update_lr lr: 2e-06
2022-07-28 08:22:47.587106: This epoch took 86.863242 s

2022-07-28 08:22:47.594771: 
epoch:  493
2022-07-28 08:24:06.670103: train loss : -0.9206
2022-07-28 08:24:14.684213: validation loss: -0.4211
2022-07-28 08:24:14.715804: Average global foreground Dice: [0.7923]
2022-07-28 08:24:14.735369: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:24:15.842308: Suus1 maybe_update_lr lr: 2e-06
2022-07-28 08:24:15.871196: This epoch took 88.248205 s

2022-07-28 08:24:15.915066: 
epoch:  494
2022-07-28 08:25:30.629764: train loss : -0.9240
2022-07-28 08:25:37.629048: validation loss: -0.5297
2022-07-28 08:25:37.669084: Average global foreground Dice: [0.851]
2022-07-28 08:25:37.688120: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:25:38.991358: Suus1 maybe_update_lr lr: 2e-06
2022-07-28 08:25:39.043898: This epoch took 83.095896 s

2022-07-28 08:25:39.099159: 
epoch:  495
2022-07-28 08:26:57.818821: train loss : -0.9227
2022-07-28 08:27:05.273444: validation loss: -0.3433
2022-07-28 08:27:05.278822: Average global foreground Dice: [0.7728]
2022-07-28 08:27:05.301016: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:27:06.514877: Suus1 maybe_update_lr lr: 1e-06
2022-07-28 08:27:06.526055: This epoch took 87.365514 s

2022-07-28 08:27:06.528194: 
epoch:  496
2022-07-28 08:28:27.291526: train loss : -0.9301
2022-07-28 08:28:33.463492: validation loss: -0.3462
2022-07-28 08:28:33.467306: Average global foreground Dice: [0.7817]
2022-07-28 08:28:33.469153: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:28:34.688506: Suus1 maybe_update_lr lr: 1e-06
2022-07-28 08:28:34.748168: This epoch took 88.217778 s

2022-07-28 08:28:34.794002: 
epoch:  497
2022-07-28 08:29:54.953830: train loss : -0.9228
2022-07-28 08:30:01.846731: validation loss: -0.5072
2022-07-28 08:30:01.892348: Average global foreground Dice: [0.8665]
2022-07-28 08:30:01.939014: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:30:03.369175: Suus1 maybe_update_lr lr: 1e-06
2022-07-28 08:30:03.395211: This epoch took 88.521211 s

2022-07-28 08:30:03.452125: 
epoch:  498
2022-07-28 08:31:25.417422: train loss : -0.9229
2022-07-28 08:31:32.769205: validation loss: -0.2394
2022-07-28 08:31:32.785228: Average global foreground Dice: [0.7289]
2022-07-28 08:31:32.816337: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:31:34.209588: Suus1 maybe_update_lr lr: 0.0
2022-07-28 08:31:34.224834: This epoch took 90.736994 s

2022-07-28 08:31:34.276423: 
epoch:  499
2022-07-28 08:32:55.244950: train loss : -0.9235
2022-07-28 08:33:03.066196: validation loss: -0.5040
2022-07-28 08:33:03.089656: Average global foreground Dice: [0.8623]
2022-07-28 08:33:03.104546: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:33:04.408649: Suus1 maybe_update_lr lr: 0.0
2022-07-28 08:33:04.443970: saving scheduled checkpoint file...
2022-07-28 08:33:04.819149: saving checkpoint...
2022-07-28 08:33:10.796590: done, saving took 6.33 seconds
2022-07-28 08:33:10.809042: done
2022-07-28 08:33:10.811539: This epoch took 96.506524 s

2022-07-28 08:33:10.948553: saving checkpoint...
2022-07-28 08:33:15.651489: done, saving took 4.84 seconds
panc_0002 (2, 139, 481, 481)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
computing Gaussian
done
prediction done
suus panc_0002 transposed
suus panc_0002 not saving softmax
suus panc_0002 voeg toe aan pred_gt tuples voor later
panc_0003 (2, 198, 599, 599)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
suus panc_0003 transposed
suus panc_0003 not saving softmax
suus panc_0003 voeg toe aan pred_gt tuples voor later
panc_0005 (2, 117, 604, 604)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
suus panc_0005 transposed
suus panc_0005 not saving softmax
suus panc_0005 voeg toe aan pred_gt tuples voor later
panc_0006 (2, 131, 468, 468)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
suus panc_0006 transposed
suus panc_0006 not saving softmax
suus panc_0006 voeg toe aan pred_gt tuples voor later
panc_0021 (2, 143, 456, 456)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
suus panc_0021 transposed
suus panc_0021 not saving softmax
suus panc_0021 voeg toe aan pred_gt tuples voor later
2022-07-28 08:37:47.487404: finished prediction
2022-07-28 08:37:47.490720: evaluation of raw predictions
2022-07-28 08:37:55.693525: determining postprocessing
Foreground vs background
before: 0.014068928103196252
after:  0.0
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0002.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0003.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0005.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0006.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0021.nii.gz
done
Done training all the folds! Now start the same command but with continue option, to generate log files


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='521', fold='0', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 08:38:19.769817: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 08:38:19.860051: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 08:38:19.864029: The split file contains 5 splits.
2022-07-28 08:38:19.865908: Desired fold for training: 0
2022-07-28 08:38:19.867720: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 08:38:22.016010: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 08:38:53.191701: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 08:39:12.697268: Unable to plot network architecture:
2022-07-28 08:39:12.700698: local variable 'g' referenced before assignment
2022-07-28 08:39:12.702646: 
printing the network instead:

2022-07-28 08:39:12.704550: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 08:39:12.717811: 

2022-07-28 08:39:13.136658: saving checkpoint...
2022-07-28 08:39:18.043403: done, saving took 5.32 seconds
suus panc_0001 voeg toe aan pred_gt tuples voor later
suus panc_0004 voeg toe aan pred_gt tuples voor later
suus panc_0024 voeg toe aan pred_gt tuples voor later
suus panc_0027 voeg toe aan pred_gt tuples voor later
suus panc_0031 voeg toe aan pred_gt tuples voor later
suus panc_0037 voeg toe aan pred_gt tuples voor later
2022-07-28 08:39:18.416910: finished prediction
2022-07-28 08:39:18.419369: evaluation of raw predictions
2022-07-28 08:39:21.967923: determining postprocessing
Foreground vs background
before: 0.0206187385097928
after:  3.4117175439045404e-05
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='521', fold='1', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 08:39:43.769737: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 08:39:43.829458: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 08:39:43.832962: The split file contains 5 splits.
2022-07-28 08:39:43.834854: Desired fold for training: 1
2022-07-28 08:39:43.836854: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 08:39:46.134447: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 08:40:21.523896: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 08:40:39.916795: Unable to plot network architecture:
2022-07-28 08:40:39.920793: local variable 'g' referenced before assignment
2022-07-28 08:40:39.922968: 
printing the network instead:

2022-07-28 08:40:39.924916: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 08:40:39.932779: 

2022-07-28 08:40:40.309847: saving checkpoint...
2022-07-28 08:40:45.119455: done, saving took 5.18 seconds
suus panc_0007 voeg toe aan pred_gt tuples voor later
suus panc_0009 voeg toe aan pred_gt tuples voor later
suus panc_0010 voeg toe aan pred_gt tuples voor later
suus panc_0032 voeg toe aan pred_gt tuples voor later
suus panc_0034 voeg toe aan pred_gt tuples voor later
suus panc_0035 voeg toe aan pred_gt tuples voor later
2022-07-28 08:40:45.500982: finished prediction
2022-07-28 08:40:45.502917: evaluation of raw predictions
2022-07-28 08:40:49.354597: determining postprocessing
Foreground vs background
before: 0.15969355973523705
after:  0.16053420550558736
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='521', fold='2', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 08:41:10.831814: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 08:41:10.881027: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 08:41:10.884376: The split file contains 5 splits.
2022-07-28 08:41:10.886203: Desired fold for training: 2
2022-07-28 08:41:10.888028: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 08:41:12.973756: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 08:41:46.181892: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 08:42:06.879976: Unable to plot network architecture:
2022-07-28 08:42:06.883734: local variable 'g' referenced before assignment
2022-07-28 08:42:06.885717: 
printing the network instead:

2022-07-28 08:42:06.887549: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 08:42:06.893463: 

2022-07-28 08:42:07.289671: saving checkpoint...
2022-07-28 08:42:13.548217: done, saving took 6.65 seconds
suus panc_0022 voeg toe aan pred_gt tuples voor later
suus panc_0023 voeg toe aan pred_gt tuples voor later
suus panc_0025 voeg toe aan pred_gt tuples voor later
suus panc_0028 voeg toe aan pred_gt tuples voor later
suus panc_0030 voeg toe aan pred_gt tuples voor later
suus panc_0033 voeg toe aan pred_gt tuples voor later
2022-07-28 08:42:13.939412: finished prediction
2022-07-28 08:42:13.941296: evaluation of raw predictions
2022-07-28 08:42:17.009115: determining postprocessing
Foreground vs background
before: 1.9715310910453057e-05
after:  2.955781508630882e-05
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='521', fold='3', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 08:42:35.449279: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 08:42:35.499757: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 08:42:35.503187: The split file contains 5 splits.
2022-07-28 08:42:35.504963: Desired fold for training: 3
2022-07-28 08:42:35.506715: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 08:42:37.629621: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 08:42:38.782881: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 08:42:55.498392: Unable to plot network architecture:
2022-07-28 08:42:55.557021: local variable 'g' referenced before assignment
2022-07-28 08:42:55.612965: 
printing the network instead:

2022-07-28 08:42:55.669245: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 08:42:55.729647: 

2022-07-28 08:42:56.312943: saving checkpoint...
2022-07-28 08:43:01.033461: done, saving took 5.25 seconds
suus panc_0026 voeg toe aan pred_gt tuples voor later
suus panc_0029 voeg toe aan pred_gt tuples voor later
suus panc_0036 voeg toe aan pred_gt tuples voor later
suus panc_0038 voeg toe aan pred_gt tuples voor later
suus panc_0039 voeg toe aan pred_gt tuples voor later
suus panc_0040 voeg toe aan pred_gt tuples voor later
2022-07-28 08:43:01.375593: finished prediction
2022-07-28 08:43:01.378057: evaluation of raw predictions
2022-07-28 08:43:04.042276: determining postprocessing
Foreground vs background
before: 0.16341412412339784
after:  0.16252686261679486
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='521', fold='4', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 08:43:31.087342: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 08:43:31.135774: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 08:43:31.139175: The split file contains 5 splits.
2022-07-28 08:43:31.141007: Desired fold for training: 4
2022-07-28 08:43:31.142952: This split has 24 training and 5 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 08:43:33.301117: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 08:43:34.281888: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 08:43:46.158687: Unable to plot network architecture:
2022-07-28 08:43:46.208023: local variable 'g' referenced before assignment
2022-07-28 08:43:46.230958: 
printing the network instead:

2022-07-28 08:43:46.257937: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 08:43:46.293917: 

2022-07-28 08:43:48.182072: saving checkpoint...
2022-07-28 08:43:57.040868: done, saving took 10.73 seconds
suus panc_0002 voeg toe aan pred_gt tuples voor later
suus panc_0003 voeg toe aan pred_gt tuples voor later
suus panc_0005 voeg toe aan pred_gt tuples voor later
suus panc_0006 voeg toe aan pred_gt tuples voor later
suus panc_0021 voeg toe aan pred_gt tuples voor later
2022-07-28 08:43:57.378865: finished prediction
2022-07-28 08:43:57.380522: evaluation of raw predictions
2022-07-28 08:44:00.179783: determining postprocessing
Foreground vs background
before: 0.014068928103196252
after:  0.0
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done
Start postprocessing..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Foreground vs background
before: 0.07354556781351759
after:  0.06685339512680846
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done
Done postprocessing! Now start inferencing its own train and test files.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

using model stored in  /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1
This model expects 1 input modalities for each image
Found 26 unique case ids, here are some examples: ['panc_0002' 'panc_0002' 'panc_0023' 'panc_0002' 'panc_0030' 'panc_0005'
 'panc_0002' 'panc_0004' 'panc_0009' 'panc_0037']
If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc
number of cases: 26
number of cases that still need to be predicted: 26
emptying cuda cache
loading parameters for folds, None
folds is None so we will automatically look for output folders (not using 'all'!)
found the following folds:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4']
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus5 - zet de plans properties
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 08:45:02.835545: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
using the following model files:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']
starting preprocessing generator
starting prediction...
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0004.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 140, 512, 512) after crop: (1, 140, 512, 512) spacing: [3.      0.59375 0.59375] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.     , 0.59375, 0.59375]), 'spacing_transposed': array([3.     , 0.59375, 0.59375]), 'data.shape (data is transposed)': (1, 140, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 140, 396, 396)} 

(1, 140, 396, 396)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0022.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 89, 512, 512) after crop: (1, 89, 512, 512) spacing: [5.         0.76757801 0.76757801] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.76757801, 0.76757801]), 'spacing_transposed': array([5.        , 0.76757801, 0.76757801]), 'data.shape (data is transposed)': (1, 89, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 148, 512, 512)} 

(1, 148, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0028.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 89, 512, 512) after crop: (1, 89, 512, 512) spacing: [5.       0.796875 0.796875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.      , 0.796875, 0.796875]), 'spacing_transposed': array([5.      , 0.796875, 0.796875]), 'data.shape (data is transposed)': (1, 89, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 148, 532, 532)} 

(1, 148, 532, 532)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0034.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 98, 512, 512) after crop: (1, 98, 512, 512) spacing: [5.       0.671875 0.671875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.      , 0.671875, 0.671875]), 'spacing_transposed': array([5.      , 0.671875, 0.671875]), 'data.shape (data is transposed)': (1, 98, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 163, 448, 448)} 

(1, 163, 448, 448)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0005.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 117, 512, 512) after crop: (1, 117, 512, 512) spacing: [3.      0.90625 0.90625] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.     , 0.90625, 0.90625]), 'spacing_transposed': array([3.     , 0.90625, 0.90625]), 'data.shape (data is transposed)': (1, 117, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 117, 604, 604)} 

(1, 117, 604, 604)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0023.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 96, 512, 512) after crop: (1, 96, 512, 512) spacing: [5.         0.70703125 0.70703125] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.70703125, 0.70703125]), 'spacing_transposed': array([5.        , 0.70703125, 0.70703125]), 'data.shape (data is transposed)': (1, 96, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 160, 472, 472)} 

(1, 160, 472, 472)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0029.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 100, 512, 512) after crop: (1, 100, 512, 512) spacing: [3.    0.875 0.875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.   , 0.875, 0.875]), 'spacing_transposed': array([3.   , 0.875, 0.875]), 'data.shape (data is transposed)': (1, 100, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 100, 584, 584)} 

(1, 100, 584, 584)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0036.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 184, 512, 512) after crop: (1, 184, 512, 512) spacing: [3.         0.74609375 0.74609375] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.74609375, 0.74609375]), 'spacing_transposed': array([3.        , 0.74609375, 0.74609375]), 'data.shape (data is transposed)': (1, 184, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 184, 498, 498)} 

(1, 184, 498, 498)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0007.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 163, 512, 512) after crop: (1, 163, 512, 512) spacing: [3.         0.74804688 0.74804688] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.74804688, 0.74804688]), 'spacing_transposed': array([3.        , 0.74804688, 0.74804688]), 'data.shape (data is transposed)': (1, 163, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 163, 499, 499)} 

(1, 163, 499, 499)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0024.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 124, 512, 512) after crop: (1, 124, 512, 512) spacing: [3.         0.68554688 0.68554688] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.68554688, 0.68554688]), 'spacing_transposed': array([3.        , 0.68554688, 0.68554688]), 'data.shape (data is transposed)': (1, 124, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 124, 457, 457)} 

(1, 124, 457, 457)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0030.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 153, 512, 512) after crop: (1, 153, 512, 512) spacing: [3.        0.7421875 0.7421875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.       , 0.7421875, 0.7421875]), 'spacing_transposed': array([3.       , 0.7421875, 0.7421875]), 'data.shape (data is transposed)': (1, 153, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 153, 495, 495)} 

(1, 153, 495, 495)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0037.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 99, 512, 512) after crop: (1, 99, 512, 512) spacing: [5.       0.703125 0.703125] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.      , 0.703125, 0.703125]), 'spacing_transposed': array([5.      , 0.703125, 0.703125]), 'data.shape (data is transposed)': (1, 99, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 165, 469, 469)} 

(1, 165, 469, 469)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0003.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 198, 512, 512) after crop: (1, 198, 512, 512) spacing: [3.        0.8984375 0.8984375] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([3.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 198, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 198, 599, 599)} 

(1, 198, 599, 599)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0021.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 143, 512, 512) after crop: (1, 143, 512, 512) spacing: [3.         0.68359375 0.68359375] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.68359375, 0.68359375]), 'spacing_transposed': array([3.        , 0.68359375, 0.68359375]), 'data.shape (data is transposed)': (1, 143, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 143, 456, 456)} 

(1, 143, 456, 456)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0027.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 88, 512, 512) after crop: (1, 88, 512, 512) spacing: [5.         0.77539098 0.77539098] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.77539098, 0.77539098]), 'spacing_transposed': array([5.        , 0.77539098, 0.77539098]), 'data.shape (data is transposed)': (1, 88, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 147, 517, 517)} 

(1, 147, 517, 517)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0033.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 104, 512, 512) after crop: (1, 104, 512, 512) spacing: [5.         0.81445301 0.81445301] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.81445301, 0.81445301]), 'spacing_transposed': array([5.        , 0.81445301, 0.81445301]), 'data.shape (data is transposed)': (1, 104, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 173, 543, 543)} 

(1, 173, 543, 543)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0002.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 139, 512, 512) after crop: (1, 139, 512, 512) spacing: [3.         0.72070312 0.72070312] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.72070312, 0.72070312]), 'spacing_transposed': array([3.        , 0.72070312, 0.72070312]), 'data.shape (data is transposed)': (1, 139, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 139, 481, 481)} 

(1, 139, 481, 481)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0010.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 148, 512, 512) after crop: (1, 148, 512, 512) spacing: [3.      0.78125 0.78125] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.     , 0.78125, 0.78125]), 'spacing_transposed': array([3.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 148, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 148, 521, 521)} 

(1, 148, 521, 521)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0026.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 131, 512, 512) after crop: (1, 131, 512, 512) spacing: [5.         0.77929699 0.77929699] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.77929699, 0.77929699]), 'spacing_transposed': array([5.        , 0.77929699, 0.77929699]), 'data.shape (data is transposed)': (1, 131, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 218, 520, 520)} 

(1, 218, 520, 520)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0032.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 144, 512, 512) after crop: (1, 144, 512, 512) spacing: [3.         0.74023438 0.74023438] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.74023438, 0.74023438]), 'spacing_transposed': array([3.        , 0.74023438, 0.74023438]), 'data.shape (data is transposed)': (1, 144, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 144, 494, 494)} 

(1, 144, 494, 494)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0040.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 195, 512, 512) after crop: (1, 195, 512, 512) spacing: [3.        0.7421875 0.7421875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.       , 0.7421875, 0.7421875]), 'spacing_transposed': array([3.       , 0.7421875, 0.7421875]), 'data.shape (data is transposed)': (1, 195, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 195, 495, 495)} 

(1, 195, 495, 495)
This worker has ended successfully, no errors to report
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0005.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0001.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0003.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0023.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0009.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0021.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0029.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0025.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0027.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0036.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0031.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0033.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0038.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0004.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0002.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0007.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0022.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0010.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0024.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0028.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0026.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0030.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0034.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0032.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0037.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0040.nii.gz
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0004.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
computing Gaussian
done
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0005.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0002.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0001.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0007.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0003.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0022.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0023.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0010.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0009.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0024.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0021.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0028.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0029.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0026.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0025.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0030.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0027.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0034.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0036.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0032.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0031.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0037.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0033.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0040.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTr/panc_0038.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
inference done. Now waiting for the segmentation export to finish...
postprocessing...


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

using model stored in  /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1
This model expects 1 input modalities for each image
Found 4 unique case ids, here are some examples: ['panc_0035' 'panc_0008' 'panc_0035' 'panc_0035']
If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc
number of cases: 4
number of cases that still need to be predicted: 4
emptying cuda cache
loading parameters for folds, None
folds is None so we will automatically look for output folders (not using 'all'!)
found the following folds:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4']
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus5 - zet de plans properties
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 10:36:28.697252: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
using the following model files:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']
starting preprocessing generator
starting prediction...
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0006.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 131, 512, 512) after crop: (1, 131, 512, 512) spacing: [3.         0.70117188 0.70117188] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.70117188, 0.70117188]), 'spacing_transposed': array([3.        , 0.70117188, 0.70117188]), 'data.shape (data is transposed)': (1, 131, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 131, 468, 468)} 

(1, 131, 468, 468)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0008.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 148, 512, 512) after crop: (1, 148, 512, 512) spacing: [3.         0.72851562 0.72851562] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.72851562, 0.72851562]), 'spacing_transposed': array([3.        , 0.72851562, 0.72851562]), 'data.shape (data is transposed)': (1, 148, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 148, 486, 486)} 

(1, 148, 486, 486)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0039.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 90, 512, 512) after crop: (1, 90, 512, 512) spacing: [5.         0.97600001 0.97600001] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.97600001, 0.97600001]), 'spacing_transposed': array([5.        , 0.97600001, 0.97600001]), 'data.shape (data is transposed)': (1, 90, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 150, 651, 651)} 

(1, 150, 651, 651)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0035.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 94, 512, 512) after crop: (1, 94, 512, 512) spacing: [5.         0.91992199 0.91992199] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.91992199, 0.91992199]), 'spacing_transposed': array([5.        , 0.91992199, 0.91992199]), 'data.shape (data is transposed)': (1, 94, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 157, 614, 614)} 

(1, 157, 614, 614)
This worker has ended successfully, no errors to report
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0008.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0035.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0006.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0039.nii.gz
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0006.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
computing Gaussian
done
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0008.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0039.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2/521/imagesTs/panc_0035.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
inference done. Now waiting for the segmentation export to finish...
postprocessing...
Done inferencing! Now start the evaluation.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet



Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Program finished with exit code 0 at: Sun Jul 24 23:21:42 CEST 2022
