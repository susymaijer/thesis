Starting at Sun Jul 24 23:22:11 CEST 2022
Running on hosts: res-hpc-lkeb08
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 6.
Account: div2-lkeb
Job ID: 10997932
Job name: PancreasAll
Node running script: res-hpc-lkeb08
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Thu Jul 28 02:29:05 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A6000    On   | 00000000:3B:00.0 Off |                  Off |
| 30%   30C    P8     8W / 300W |      0MiB / 48685MiB |     10%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-ptxr5evt/hiddenlayer_5a2e9e8c79f44b60bc8033a6578179eb
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Using legacy 'setup.py install' for hiddenlayer, since package 'wheel' is not installed.
Installing collected packages: hiddenlayer
    Running setup.py install for hiddenlayer: started
    Running setup.py install for hiddenlayer: finished with status 'done'
Successfully installed hiddenlayer-0.2
Start preprocessing..
Done preprocessing! Start training all the folds..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='521', fold='4', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 02:29:26.262651: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 02:29:26.374795: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 02:29:26.379330: The split file contains 5 splits.
2022-07-28 02:29:26.381446: Desired fold for training: 4
2022-07-28 02:29:26.383395: This split has 24 training and 5 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 02:29:28.625150: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/model_latest.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 02:29:29.679637: Suus1 maybe_update_lr lr: 6.3e-05
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 02:29:49.948143: Unable to plot network architecture:
2022-07-28 02:29:49.951354: local variable 'g' referenced before assignment
2022-07-28 02:29:49.953614: 
printing the network instead:

2022-07-28 02:29:49.955930: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 02:29:49.964551: 

2022-07-28 02:29:49.967201: 
epoch:  200
2022-07-28 02:31:18.366160: train loss : -0.8428
2022-07-28 02:31:25.220304: validation loss: -0.3326
2022-07-28 02:31:25.245031: Average global foreground Dice: [0.706]
2022-07-28 02:31:25.282205: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:31:27.227105: Suus1 maybe_update_lr lr: 6.3e-05
2022-07-28 02:31:27.239190: This epoch took 97.269777 s

2022-07-28 02:31:27.257626: 
epoch:  201
2022-07-28 02:32:48.251859: train loss : -0.8751
2022-07-28 02:32:55.301152: validation loss: -0.4419
2022-07-28 02:32:55.338598: Average global foreground Dice: [0.7937]
2022-07-28 02:32:55.380992: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:32:56.637382: Suus1 maybe_update_lr lr: 6.3e-05
2022-07-28 02:32:56.649331: This epoch took 89.374180 s

2022-07-28 02:32:56.662994: 
epoch:  202
2022-07-28 02:34:20.893466: train loss : -0.8602
2022-07-28 02:34:28.386513: validation loss: -0.4243
2022-07-28 02:34:28.391593: Average global foreground Dice: [0.8005]
2022-07-28 02:34:28.396662: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:34:29.439503: Suus1 maybe_update_lr lr: 6.3e-05
2022-07-28 02:34:29.456358: This epoch took 92.773041 s

2022-07-28 02:34:29.479353: 
epoch:  203
2022-07-28 02:35:50.695148: train loss : -0.8386
2022-07-28 02:35:58.594652: validation loss: -0.4987
2022-07-28 02:35:58.600363: Average global foreground Dice: [0.8102]
2022-07-28 02:35:58.615965: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:35:59.636894: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 02:35:59.655147: This epoch took 90.123061 s

2022-07-28 02:35:59.661558: 
epoch:  204
2022-07-28 02:37:18.347739: train loss : -0.8314
2022-07-28 02:37:26.138039: validation loss: -0.2647
2022-07-28 02:37:26.150281: Average global foreground Dice: [0.5464]
2022-07-28 02:37:26.178043: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:37:27.247009: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 02:37:27.313154: This epoch took 87.630557 s

2022-07-28 02:37:27.346070: 
epoch:  205
2022-07-28 02:38:51.256773: train loss : -0.8281
2022-07-28 02:38:57.283918: validation loss: -0.4563
2022-07-28 02:38:57.321168: Average global foreground Dice: [0.8315]
2022-07-28 02:38:57.336723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:38:58.468512: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 02:38:58.500490: This epoch took 91.121451 s

2022-07-28 02:38:58.522918: 
epoch:  206
2022-07-28 02:40:18.747952: train loss : -0.8547
2022-07-28 02:40:26.868900: validation loss: -0.4240
2022-07-28 02:40:26.897717: Average global foreground Dice: [0.7987]
2022-07-28 02:40:26.912784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:40:27.999980: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 02:40:28.002783: This epoch took 89.459678 s

2022-07-28 02:40:28.039987: 
epoch:  207
2022-07-28 02:41:50.076801: train loss : -0.8437
2022-07-28 02:41:56.931718: validation loss: -0.5130
2022-07-28 02:41:56.961088: Average global foreground Dice: [0.8061]
2022-07-28 02:41:56.965701: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:41:58.117696: Suus1 maybe_update_lr lr: 6.2e-05
2022-07-28 02:41:58.129937: This epoch took 90.062101 s

2022-07-28 02:41:58.151777: 
epoch:  208
2022-07-28 02:43:18.344980: train loss : -0.8515
2022-07-28 02:43:25.310403: validation loss: -0.4553
2022-07-28 02:43:25.318251: Average global foreground Dice: [0.778]
2022-07-28 02:43:25.334030: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:43:26.683365: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 02:43:26.688822: This epoch took 88.533395 s

2022-07-28 02:43:26.711104: 
epoch:  209
2022-07-28 02:44:44.421515: train loss : -0.8590
2022-07-28 02:44:51.269506: validation loss: -0.2848
2022-07-28 02:44:51.336279: Average global foreground Dice: [0.7117]
2022-07-28 02:44:51.394544: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:44:52.446702: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 02:44:52.476190: This epoch took 85.758803 s

2022-07-28 02:44:52.503016: 
epoch:  210
2022-07-28 02:46:14.845377: train loss : -0.8528
2022-07-28 02:46:22.888997: validation loss: -0.5508
2022-07-28 02:46:22.950099: Average global foreground Dice: [0.8506]
2022-07-28 02:46:22.977025: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:46:24.132833: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 02:46:24.159169: This epoch took 91.635146 s

2022-07-28 02:46:24.189217: 
epoch:  211
2022-07-28 02:47:42.086332: train loss : -0.8556
2022-07-28 02:47:49.688836: validation loss: -0.3149
2022-07-28 02:47:49.727144: Average global foreground Dice: [0.6637]
2022-07-28 02:47:49.741776: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:47:50.867405: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 02:47:50.941145: This epoch took 86.720954 s

2022-07-28 02:47:50.979361: 
epoch:  212
2022-07-28 02:49:11.873446: train loss : -0.8499
2022-07-28 02:49:18.596549: validation loss: -0.4656
2022-07-28 02:49:18.600771: Average global foreground Dice: [0.8075]
2022-07-28 02:49:18.619045: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:49:19.539542: Suus1 maybe_update_lr lr: 6.1e-05
2022-07-28 02:49:19.582842: This epoch took 88.563216 s

2022-07-28 02:49:19.606261: 
epoch:  213
2022-07-28 02:50:40.122605: train loss : -0.8560
2022-07-28 02:50:46.655904: validation loss: -0.4810
2022-07-28 02:50:46.672662: Average global foreground Dice: [0.8359]
2022-07-28 02:50:46.695236: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:50:47.813117: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 02:50:47.824530: This epoch took 88.188507 s

2022-07-28 02:50:47.829076: 
epoch:  214
2022-07-28 02:52:06.311775: train loss : -0.8423
2022-07-28 02:52:12.918116: validation loss: -0.4341
2022-07-28 02:52:12.946667: Average global foreground Dice: [0.7856]
2022-07-28 02:52:12.953822: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:52:14.087543: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 02:52:14.145500: This epoch took 86.295411 s

2022-07-28 02:52:14.201017: 
epoch:  215
2022-07-28 02:53:32.124677: train loss : -0.8500
2022-07-28 02:53:39.286227: validation loss: -0.3896
2022-07-28 02:53:39.296122: Average global foreground Dice: [0.7947]
2022-07-28 02:53:39.304860: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:53:40.242878: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 02:53:40.252304: saving best epoch checkpoint...
2022-07-28 02:53:40.568670: saving checkpoint...
2022-07-28 02:53:46.751734: done, saving took 6.49 seconds
2022-07-28 02:53:46.777504: This epoch took 92.542370 s

2022-07-28 02:53:46.779837: 
epoch:  216
2022-07-28 02:55:05.924187: train loss : -0.8427
2022-07-28 02:55:13.366917: validation loss: -0.5142
2022-07-28 02:55:13.399810: Average global foreground Dice: [0.8666]
2022-07-28 02:55:13.421019: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:55:14.563103: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 02:55:14.591067: saving best epoch checkpoint...
2022-07-28 02:55:15.008416: saving checkpoint...
2022-07-28 02:55:20.712860: done, saving took 6.09 seconds
2022-07-28 02:55:20.727689: This epoch took 93.945628 s

2022-07-28 02:55:20.729824: 
epoch:  217
2022-07-28 02:56:40.522720: train loss : -0.8507
2022-07-28 02:56:48.112531: validation loss: -0.4957
2022-07-28 02:56:48.146367: Average global foreground Dice: [0.8006]
2022-07-28 02:56:48.163764: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:56:49.257338: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 02:56:49.266288: saving best epoch checkpoint...
2022-07-28 02:56:49.585170: saving checkpoint...
2022-07-28 02:56:55.379425: done, saving took 6.09 seconds
2022-07-28 02:56:55.395111: This epoch took 94.663139 s

2022-07-28 02:56:55.397225: 
epoch:  218
2022-07-28 02:58:12.905359: train loss : -0.8773
2022-07-28 02:58:19.995681: validation loss: -0.4401
2022-07-28 02:58:20.016853: Average global foreground Dice: [0.8194]
2022-07-28 02:58:20.021345: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:58:21.134228: Suus1 maybe_update_lr lr: 6e-05
2022-07-28 02:58:21.151108: saving best epoch checkpoint...
2022-07-28 02:58:21.701919: saving checkpoint...
2022-07-28 02:58:28.262999: done, saving took 7.08 seconds
2022-07-28 02:58:28.271995: This epoch took 92.872696 s

2022-07-28 02:58:28.274101: 
epoch:  219
2022-07-28 02:59:44.817641: train loss : -0.8625
2022-07-28 02:59:53.029780: validation loss: -0.4725
2022-07-28 02:59:53.075402: Average global foreground Dice: [0.8037]
2022-07-28 02:59:53.120080: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 02:59:54.316287: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 02:59:54.318917: saving best epoch checkpoint...
2022-07-28 02:59:54.757824: saving checkpoint...
2022-07-28 03:00:01.111226: done, saving took 6.78 seconds
2022-07-28 03:00:01.129749: This epoch took 92.853540 s

2022-07-28 03:00:01.132092: 
epoch:  220
2022-07-28 03:01:15.975604: train loss : -0.8788
2022-07-28 03:01:23.059073: validation loss: -0.4510
2022-07-28 03:01:23.118196: Average global foreground Dice: [0.7535]
2022-07-28 03:01:23.146437: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:01:25.191627: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 03:01:25.254275: This epoch took 84.119768 s

2022-07-28 03:01:25.284159: 
epoch:  221
2022-07-28 03:02:45.154786: train loss : -0.8569
2022-07-28 03:02:53.247323: validation loss: -0.3827
2022-07-28 03:02:53.289153: Average global foreground Dice: [0.7323]
2022-07-28 03:02:53.312442: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:02:54.519297: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 03:02:54.522920: This epoch took 89.213667 s

2022-07-28 03:02:54.537108: 
epoch:  222
2022-07-28 03:04:13.666771: train loss : -0.8364
2022-07-28 03:04:21.142366: validation loss: -0.4008
2022-07-28 03:04:21.168343: Average global foreground Dice: [0.7966]
2022-07-28 03:04:21.176437: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:04:22.231123: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 03:04:22.271523: This epoch took 87.731412 s

2022-07-28 03:04:22.326044: 
epoch:  223
2022-07-28 03:05:41.468319: train loss : -0.8725
2022-07-28 03:05:48.034908: validation loss: -0.2607
2022-07-28 03:05:48.069199: Average global foreground Dice: [0.608]
2022-07-28 03:05:48.104002: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:05:49.360950: Suus1 maybe_update_lr lr: 5.9e-05
2022-07-28 03:05:49.421296: This epoch took 87.088599 s

2022-07-28 03:05:49.463643: 
epoch:  224
2022-07-28 03:07:09.370725: train loss : -0.8695
2022-07-28 03:07:16.357410: validation loss: -0.3932
2022-07-28 03:07:16.392462: Average global foreground Dice: [0.7608]
2022-07-28 03:07:16.418238: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:07:17.728467: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 03:07:17.748461: This epoch took 88.260375 s

2022-07-28 03:07:17.767977: 
epoch:  225
2022-07-28 03:08:38.250868: train loss : -0.8670
2022-07-28 03:08:46.557312: validation loss: -0.4412
2022-07-28 03:08:46.592980: Average global foreground Dice: [0.8112]
2022-07-28 03:08:46.624296: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:08:47.676805: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 03:08:47.715098: This epoch took 89.934723 s

2022-07-28 03:08:47.749013: 
epoch:  226
2022-07-28 03:10:07.907608: train loss : -0.8554
2022-07-28 03:10:14.224239: validation loss: -0.3577
2022-07-28 03:10:14.258954: Average global foreground Dice: [0.763]
2022-07-28 03:10:14.279064: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:10:15.375726: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 03:10:15.383970: This epoch took 87.569498 s

2022-07-28 03:10:15.395222: 
epoch:  227
2022-07-28 03:11:36.589873: train loss : -0.8727
2022-07-28 03:11:43.776939: validation loss: -0.3943
2022-07-28 03:11:43.781392: Average global foreground Dice: [0.7802]
2022-07-28 03:11:43.783917: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:11:44.929029: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 03:11:44.953317: This epoch took 89.537915 s

2022-07-28 03:11:44.972078: 
epoch:  228
2022-07-28 03:13:06.979530: train loss : -0.8693
2022-07-28 03:13:13.946936: validation loss: -0.3104
2022-07-28 03:13:13.972794: Average global foreground Dice: [0.6607]
2022-07-28 03:13:13.997706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:13:15.164309: Suus1 maybe_update_lr lr: 5.8e-05
2022-07-28 03:13:15.192577: This epoch took 90.199393 s

2022-07-28 03:13:15.196267: 
epoch:  229
2022-07-28 03:14:37.855604: train loss : -0.8653
2022-07-28 03:14:46.281373: validation loss: -0.3163
2022-07-28 03:14:46.295467: Average global foreground Dice: [0.6837]
2022-07-28 03:14:46.314365: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:14:47.477493: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 03:14:47.511258: This epoch took 92.312142 s

2022-07-28 03:14:47.536209: 
epoch:  230
2022-07-28 03:16:08.157511: train loss : -0.8580
2022-07-28 03:16:15.925411: validation loss: -0.4134
2022-07-28 03:16:15.969362: Average global foreground Dice: [0.7409]
2022-07-28 03:16:16.010267: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:16:17.245630: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 03:16:17.261459: This epoch took 89.705470 s

2022-07-28 03:16:17.285462: 
epoch:  231
2022-07-28 03:17:34.230684: train loss : -0.8483
2022-07-28 03:17:43.539655: validation loss: -0.3607
2022-07-28 03:17:43.551364: Average global foreground Dice: [0.6915]
2022-07-28 03:17:43.586840: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:17:44.831938: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 03:17:44.835042: This epoch took 87.512002 s

2022-07-28 03:17:44.845016: 
epoch:  232
2022-07-28 03:19:03.914534: train loss : -0.8639
2022-07-28 03:19:12.550613: validation loss: -0.4691
2022-07-28 03:19:12.581286: Average global foreground Dice: [0.7817]
2022-07-28 03:19:12.583999: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:19:13.607313: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 03:19:13.664076: This epoch took 88.815205 s

2022-07-28 03:19:13.731225: 
epoch:  233
2022-07-28 03:20:36.340491: train loss : -0.8783
2022-07-28 03:20:44.059163: validation loss: -0.4225
2022-07-28 03:20:44.104973: Average global foreground Dice: [0.8167]
2022-07-28 03:20:44.155706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:20:45.347592: Suus1 maybe_update_lr lr: 5.7e-05
2022-07-28 03:20:45.372530: This epoch took 91.607161 s

2022-07-28 03:20:45.402970: 
epoch:  234
2022-07-28 03:22:07.799806: train loss : -0.8774
2022-07-28 03:22:14.607817: validation loss: -0.5279
2022-07-28 03:22:14.629533: Average global foreground Dice: [0.8295]
2022-07-28 03:22:14.646563: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:22:15.765477: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 03:22:15.794611: This epoch took 90.361117 s

2022-07-28 03:22:15.822064: 
epoch:  235
2022-07-28 03:23:39.222936: train loss : -0.8764
2022-07-28 03:23:45.996907: validation loss: -0.4418
2022-07-28 03:23:46.055903: Average global foreground Dice: [0.7854]
2022-07-28 03:23:46.069953: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:23:47.278219: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 03:23:47.293115: This epoch took 91.459035 s

2022-07-28 03:23:47.316383: 
epoch:  236
2022-07-28 03:25:06.014768: train loss : -0.8647
2022-07-28 03:25:13.302412: validation loss: -0.3398
2022-07-28 03:25:13.345628: Average global foreground Dice: [0.6885]
2022-07-28 03:25:13.354504: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:25:14.686808: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 03:25:14.729416: This epoch took 87.404001 s

2022-07-28 03:25:14.761045: 
epoch:  237
2022-07-28 03:26:33.298680: train loss : -0.8625
2022-07-28 03:26:41.477828: validation loss: -0.4085
2022-07-28 03:26:41.537997: Average global foreground Dice: [0.7902]
2022-07-28 03:26:41.578167: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:26:42.911915: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 03:26:42.927310: This epoch took 88.109295 s

2022-07-28 03:26:42.930790: 
epoch:  238
2022-07-28 03:28:00.712839: train loss : -0.8532
2022-07-28 03:28:08.062448: validation loss: -0.4673
2022-07-28 03:28:08.091573: Average global foreground Dice: [0.8133]
2022-07-28 03:28:08.111970: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:28:09.281842: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 03:28:09.298916: This epoch took 86.365304 s

2022-07-28 03:28:09.342111: 
epoch:  239
2022-07-28 03:29:30.716413: train loss : -0.8619
2022-07-28 03:29:37.446420: validation loss: -0.4783
2022-07-28 03:29:37.481083: Average global foreground Dice: [0.8469]
2022-07-28 03:29:37.496097: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:29:38.588945: Suus1 maybe_update_lr lr: 5.6e-05
2022-07-28 03:29:38.611902: This epoch took 89.207395 s

2022-07-28 03:29:38.616323: 
epoch:  240
2022-07-28 03:30:58.704796: train loss : -0.8411
2022-07-28 03:31:05.647796: validation loss: -0.2383
2022-07-28 03:31:05.678794: Average global foreground Dice: [0.6975]
2022-07-28 03:31:05.707348: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:31:06.769346: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 03:31:06.830417: This epoch took 88.198893 s

2022-07-28 03:31:06.890729: 
epoch:  241
2022-07-28 03:32:26.321644: train loss : -0.8861
2022-07-28 03:32:33.198297: validation loss: -0.4201
2022-07-28 03:32:33.253718: Average global foreground Dice: [0.7947]
2022-07-28 03:32:33.262957: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:32:34.373806: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 03:32:34.415119: This epoch took 87.496974 s

2022-07-28 03:32:34.481017: 
epoch:  242
2022-07-28 03:33:56.825461: train loss : -0.8445
2022-07-28 03:34:03.292802: validation loss: -0.5233
2022-07-28 03:34:03.324967: Average global foreground Dice: [0.8627]
2022-07-28 03:34:03.348023: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:34:04.655647: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 03:34:04.676193: This epoch took 90.152859 s

2022-07-28 03:34:04.696073: 
epoch:  243
2022-07-28 03:35:27.301931: train loss : -0.8695
2022-07-28 03:35:34.931244: validation loss: -0.4738
2022-07-28 03:35:34.958025: Average global foreground Dice: [0.8302]
2022-07-28 03:35:34.984118: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:35:36.215207: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 03:35:36.235973: This epoch took 91.521960 s

2022-07-28 03:35:36.245465: 
epoch:  244
2022-07-28 03:36:57.601064: train loss : -0.8418
2022-07-28 03:37:03.998074: validation loss: -0.4274
2022-07-28 03:37:04.046813: Average global foreground Dice: [0.7831]
2022-07-28 03:37:04.080106: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:37:05.258296: Suus1 maybe_update_lr lr: 5.5e-05
2022-07-28 03:37:05.260917: This epoch took 89.007163 s

2022-07-28 03:37:05.263250: 
epoch:  245
2022-07-28 03:38:26.294142: train loss : -0.8741
2022-07-28 03:38:33.234199: validation loss: -0.4155
2022-07-28 03:38:33.255347: Average global foreground Dice: [0.8195]
2022-07-28 03:38:33.270253: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:38:34.256971: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 03:38:34.272435: saving best epoch checkpoint...
2022-07-28 03:38:34.774617: saving checkpoint...
2022-07-28 03:38:41.205741: done, saving took 6.91 seconds
2022-07-28 03:38:41.221100: This epoch took 95.955537 s

2022-07-28 03:38:41.223288: 
epoch:  246
2022-07-28 03:40:02.481364: train loss : -0.8574
2022-07-28 03:40:09.336535: validation loss: -0.5138
2022-07-28 03:40:09.368476: Average global foreground Dice: [0.807]
2022-07-28 03:40:09.383640: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:40:10.523238: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 03:40:10.533806: saving best epoch checkpoint...
2022-07-28 03:40:10.940080: saving checkpoint...
2022-07-28 03:40:17.437381: done, saving took 6.88 seconds
2022-07-28 03:40:17.451178: This epoch took 96.225840 s

2022-07-28 03:40:17.453440: 
epoch:  247
2022-07-28 03:41:34.511281: train loss : -0.8821
2022-07-28 03:41:41.875492: validation loss: -0.2923
2022-07-28 03:41:41.901078: Average global foreground Dice: [0.7509]
2022-07-28 03:41:41.933063: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:41:42.966731: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 03:41:42.971328: This epoch took 85.515664 s

2022-07-28 03:41:42.991126: 
epoch:  248
2022-07-28 03:43:05.444523: train loss : -0.8822
2022-07-28 03:43:12.120581: validation loss: -0.4185
2022-07-28 03:43:12.148687: Average global foreground Dice: [0.7896]
2022-07-28 03:43:12.172382: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:43:13.462182: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 03:43:13.496838: This epoch took 90.472838 s

2022-07-28 03:43:13.541103: 
epoch:  249
2022-07-28 03:44:35.515307: train loss : -0.8792
2022-07-28 03:44:41.986262: validation loss: -0.4173
2022-07-28 03:44:42.031288: Average global foreground Dice: [0.7908]
2022-07-28 03:44:42.045953: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:44:43.279504: Suus1 maybe_update_lr lr: 5.4e-05
2022-07-28 03:44:43.288582: saving scheduled checkpoint file...
2022-07-28 03:44:43.744050: saving checkpoint...
2022-07-28 03:44:49.985944: done, saving took 6.68 seconds
2022-07-28 03:44:50.003784: done
2022-07-28 03:44:50.006152: This epoch took 96.441357 s

2022-07-28 03:44:50.008460: 
epoch:  250
2022-07-28 03:46:09.730395: train loss : -0.8572
2022-07-28 03:46:17.589313: validation loss: -0.3207
2022-07-28 03:46:17.594687: Average global foreground Dice: [0.7002]
2022-07-28 03:46:17.643246: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:46:18.993381: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 03:46:19.017102: This epoch took 89.006325 s

2022-07-28 03:46:19.026648: 
epoch:  251
2022-07-28 03:47:39.123223: train loss : -0.8645
2022-07-28 03:47:46.821720: validation loss: -0.5372
2022-07-28 03:47:46.872023: Average global foreground Dice: [0.8595]
2022-07-28 03:47:46.902039: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:47:48.162469: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 03:47:48.197150: This epoch took 89.160553 s

2022-07-28 03:47:48.225028: 
epoch:  252
2022-07-28 03:49:10.857117: train loss : -0.8602
2022-07-28 03:49:17.865244: validation loss: -0.4555
2022-07-28 03:49:17.913925: Average global foreground Dice: [0.7984]
2022-07-28 03:49:17.941993: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:49:19.242525: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 03:49:19.282078: This epoch took 91.036248 s

2022-07-28 03:49:19.310019: 
epoch:  253
2022-07-28 03:50:39.930662: train loss : -0.8742
2022-07-28 03:50:48.097914: validation loss: -0.4090
2022-07-28 03:50:48.203003: Average global foreground Dice: [0.728]
2022-07-28 03:50:48.257025: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:50:49.371297: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 03:50:49.374309: This epoch took 90.042280 s

2022-07-28 03:50:49.376905: 
epoch:  254
2022-07-28 03:52:07.608010: train loss : -0.8492
2022-07-28 03:52:14.399422: validation loss: -0.1467
2022-07-28 03:52:14.436120: Average global foreground Dice: [0.4628]
2022-07-28 03:52:14.474011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:52:15.570168: Suus1 maybe_update_lr lr: 5.3e-05
2022-07-28 03:52:15.636159: This epoch took 86.243975 s

2022-07-28 03:52:15.723081: 
epoch:  255
2022-07-28 03:53:36.452633: train loss : -0.8829
2022-07-28 03:53:43.190621: validation loss: -0.3509
2022-07-28 03:53:43.216393: Average global foreground Dice: [0.8079]
2022-07-28 03:53:43.235643: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:53:44.392769: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 03:53:44.432260: This epoch took 88.660306 s

2022-07-28 03:53:44.437327: 
epoch:  256
2022-07-28 03:55:03.403708: train loss : -0.8840
2022-07-28 03:55:10.712543: validation loss: -0.3025
2022-07-28 03:55:10.747795: Average global foreground Dice: [0.7269]
2022-07-28 03:55:10.767055: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:55:11.959645: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 03:55:11.967901: This epoch took 87.494821 s

2022-07-28 03:55:11.985323: 
epoch:  257
2022-07-28 03:56:32.715093: train loss : -0.8711
2022-07-28 03:56:39.529523: validation loss: -0.2791
2022-07-28 03:56:39.550862: Average global foreground Dice: [0.6949]
2022-07-28 03:56:39.572119: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:56:40.654281: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 03:56:40.676078: This epoch took 88.675971 s

2022-07-28 03:56:40.707017: 
epoch:  258
2022-07-28 03:58:00.822237: train loss : -0.8631
2022-07-28 03:58:07.768126: validation loss: -0.4185
2022-07-28 03:58:07.775952: Average global foreground Dice: [0.7738]
2022-07-28 03:58:07.789057: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:58:08.894180: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 03:58:08.910057: This epoch took 88.174145 s

2022-07-28 03:58:08.934973: 
epoch:  259
2022-07-28 03:59:30.357746: train loss : -0.8906
2022-07-28 03:59:37.190196: validation loss: -0.3614
2022-07-28 03:59:37.220459: Average global foreground Dice: [0.7615]
2022-07-28 03:59:37.237705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 03:59:38.379114: Suus1 maybe_update_lr lr: 5.2e-05
2022-07-28 03:59:38.422143: This epoch took 89.458121 s

2022-07-28 03:59:38.452575: 
epoch:  260
2022-07-28 04:01:00.614706: train loss : -0.8511
2022-07-28 04:01:06.770607: validation loss: -0.3034
2022-07-28 04:01:06.861728: Average global foreground Dice: [0.7412]
2022-07-28 04:01:06.893779: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:01:08.091951: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 04:01:08.131176: This epoch took 89.651973 s

2022-07-28 04:01:08.165406: 
epoch:  261
2022-07-28 04:02:25.679245: train loss : -0.8796
2022-07-28 04:02:33.940751: validation loss: -0.3857
2022-07-28 04:02:33.988495: Average global foreground Dice: [0.7615]
2022-07-28 04:02:34.020064: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:02:35.049653: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 04:02:35.070135: This epoch took 86.860112 s

2022-07-28 04:02:35.072743: 
epoch:  262
2022-07-28 04:03:58.935018: train loss : -0.8792
2022-07-28 04:04:06.165837: validation loss: -0.3453
2022-07-28 04:04:06.171116: Average global foreground Dice: [0.7477]
2022-07-28 04:04:06.194007: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:04:07.425501: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 04:04:07.438699: This epoch took 92.362972 s

2022-07-28 04:04:07.452000: 
epoch:  263
2022-07-28 04:05:28.592829: train loss : -0.8881
2022-07-28 04:05:35.976642: validation loss: -0.2877
2022-07-28 04:05:36.054721: Average global foreground Dice: [0.7128]
2022-07-28 04:05:36.097031: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:05:37.197445: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 04:05:37.200040: This epoch took 89.718005 s

2022-07-28 04:05:37.202438: 
epoch:  264
2022-07-28 04:06:57.342309: train loss : -0.8808
2022-07-28 04:07:03.819914: validation loss: -0.5168
2022-07-28 04:07:03.848258: Average global foreground Dice: [0.8158]
2022-07-28 04:07:03.870225: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:07:04.916091: Suus1 maybe_update_lr lr: 5.1e-05
2022-07-28 04:07:04.944917: This epoch took 87.739064 s

2022-07-28 04:07:04.950403: 
epoch:  265
2022-07-28 04:08:26.025389: train loss : -0.8856
2022-07-28 04:08:33.443355: validation loss: -0.3707
2022-07-28 04:08:33.484782: Average global foreground Dice: [0.8029]
2022-07-28 04:08:33.548314: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:08:34.780839: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 04:08:34.794085: This epoch took 89.818338 s

2022-07-28 04:08:34.815019: 
epoch:  266
2022-07-28 04:09:52.208604: train loss : -0.8821
2022-07-28 04:09:58.406325: validation loss: -0.3493
2022-07-28 04:09:58.423124: Average global foreground Dice: [0.7636]
2022-07-28 04:09:58.446162: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:09:59.828347: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 04:09:59.879193: This epoch took 85.044940 s

2022-07-28 04:09:59.915099: 
epoch:  267
2022-07-28 04:11:18.843665: train loss : -0.8743
2022-07-28 04:11:26.362106: validation loss: -0.3594
2022-07-28 04:11:26.421742: Average global foreground Dice: [0.7596]
2022-07-28 04:11:26.470101: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:11:28.107554: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 04:11:28.140084: This epoch took 88.204021 s

2022-07-28 04:11:28.150896: 
epoch:  268
2022-07-28 04:12:46.372305: train loss : -0.8827
2022-07-28 04:12:52.780843: validation loss: -0.3872
2022-07-28 04:12:52.824124: Average global foreground Dice: [0.7889]
2022-07-28 04:12:52.840034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:12:54.089701: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 04:12:54.145019: This epoch took 85.991315 s

2022-07-28 04:12:54.190997: 
epoch:  269
2022-07-28 04:14:12.672548: train loss : -0.8900
2022-07-28 04:14:18.979293: validation loss: -0.2252
2022-07-28 04:14:19.043785: Average global foreground Dice: [0.6581]
2022-07-28 04:14:19.101010: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:14:20.079110: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 04:14:20.134394: This epoch took 85.908551 s

2022-07-28 04:14:20.169580: 
epoch:  270
2022-07-28 04:15:40.931858: train loss : -0.8531
2022-07-28 04:15:47.677063: validation loss: -0.4448
2022-07-28 04:15:47.747187: Average global foreground Dice: [0.8148]
2022-07-28 04:15:47.787197: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:15:48.914788: Suus1 maybe_update_lr lr: 5e-05
2022-07-28 04:15:48.920717: This epoch took 88.736527 s

2022-07-28 04:15:48.940017: 
epoch:  271
2022-07-28 04:17:09.245740: train loss : -0.8613
2022-07-28 04:17:16.219568: validation loss: -0.4259
2022-07-28 04:17:16.236191: Average global foreground Dice: [0.7705]
2022-07-28 04:17:16.242775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:17:17.533372: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 04:17:17.562315: This epoch took 88.599076 s

2022-07-28 04:17:17.594989: 
epoch:  272
2022-07-28 04:18:36.446194: train loss : -0.8741
2022-07-28 04:18:43.082512: validation loss: -0.4144
2022-07-28 04:18:43.118733: Average global foreground Dice: [0.8125]
2022-07-28 04:18:43.142037: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:18:44.451637: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 04:18:44.472959: This epoch took 86.855973 s

2022-07-28 04:18:44.503177: 
epoch:  273
2022-07-28 04:20:04.216794: train loss : -0.8654
2022-07-28 04:20:12.154048: validation loss: -0.4519
2022-07-28 04:20:12.177314: Average global foreground Dice: [0.8211]
2022-07-28 04:20:12.195845: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:20:13.528488: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 04:20:13.541061: This epoch took 89.029343 s

2022-07-28 04:20:13.561586: 
epoch:  274
2022-07-28 04:21:32.409096: train loss : -0.8789
2022-07-28 04:21:39.352695: validation loss: -0.1886
2022-07-28 04:21:39.388404: Average global foreground Dice: [0.622]
2022-07-28 04:21:39.409650: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:21:40.687344: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 04:21:40.712987: This epoch took 87.133005 s

2022-07-28 04:21:40.733092: 
epoch:  275
2022-07-28 04:23:01.038507: train loss : -0.8863
2022-07-28 04:23:08.872447: validation loss: -0.4299
2022-07-28 04:23:08.918775: Average global foreground Dice: [0.7777]
2022-07-28 04:23:08.941099: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:23:10.112941: Suus1 maybe_update_lr lr: 4.9e-05
2022-07-28 04:23:10.137377: This epoch took 89.373369 s

2022-07-28 04:23:10.152661: 
epoch:  276
2022-07-28 04:24:32.602102: train loss : -0.8613
2022-07-28 04:24:39.715521: validation loss: -0.3652
2022-07-28 04:24:39.734216: Average global foreground Dice: [0.8025]
2022-07-28 04:24:39.742377: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:24:40.813356: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 04:24:40.828921: This epoch took 90.662983 s

2022-07-28 04:24:40.863011: 
epoch:  277
2022-07-28 04:26:00.421354: train loss : -0.8922
2022-07-28 04:26:06.798685: validation loss: -0.2908
2022-07-28 04:26:06.829336: Average global foreground Dice: [0.7044]
2022-07-28 04:26:06.850072: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:26:08.123833: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 04:26:08.131083: This epoch took 87.224062 s

2022-07-28 04:26:08.154760: 
epoch:  278
2022-07-28 04:27:27.068194: train loss : -0.8809
2022-07-28 04:27:34.756377: validation loss: -0.4370
2022-07-28 04:27:34.867343: Average global foreground Dice: [0.8374]
2022-07-28 04:27:34.909374: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:27:36.233227: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 04:27:36.252952: This epoch took 88.083238 s

2022-07-28 04:27:36.269646: 
epoch:  279
2022-07-28 04:28:56.767440: train loss : -0.8718
2022-07-28 04:29:03.203552: validation loss: -0.2457
2022-07-28 04:29:03.217506: Average global foreground Dice: [0.6943]
2022-07-28 04:29:03.233002: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:29:04.399623: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 04:29:04.431386: This epoch took 88.129032 s

2022-07-28 04:29:04.485044: 
epoch:  280
2022-07-28 04:30:27.778575: train loss : -0.8818
2022-07-28 04:30:34.189057: validation loss: -0.4056
2022-07-28 04:30:34.221682: Average global foreground Dice: [0.7979]
2022-07-28 04:30:34.240997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:30:35.626831: Suus1 maybe_update_lr lr: 4.8e-05
2022-07-28 04:30:35.630663: This epoch took 91.079629 s

2022-07-28 04:30:35.642652: 
epoch:  281
2022-07-28 04:31:55.699251: train loss : -0.8850
2022-07-28 04:32:02.696096: validation loss: -0.2838
2022-07-28 04:32:02.727149: Average global foreground Dice: [0.6672]
2022-07-28 04:32:02.749739: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:32:03.812871: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 04:32:03.861208: This epoch took 88.188201 s

2022-07-28 04:32:03.883091: 
epoch:  282
2022-07-28 04:33:20.772743: train loss : -0.8802
2022-07-28 04:33:27.232005: validation loss: -0.3774
2022-07-28 04:33:27.263861: Average global foreground Dice: [0.7876]
2022-07-28 04:33:27.283118: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:33:28.528591: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 04:33:28.539429: This epoch took 84.641659 s

2022-07-28 04:33:28.547074: 
epoch:  283
2022-07-28 04:34:48.254919: train loss : -0.8923
2022-07-28 04:34:54.313029: validation loss: -0.4350
2022-07-28 04:34:54.338743: Average global foreground Dice: [0.8077]
2022-07-28 04:34:54.356292: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:34:55.507881: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 04:34:55.529073: This epoch took 86.973923 s

2022-07-28 04:34:55.553457: 
epoch:  284
2022-07-28 04:36:15.755568: train loss : -0.8792
2022-07-28 04:36:23.015216: validation loss: -0.2621
2022-07-28 04:36:23.040232: Average global foreground Dice: [0.6514]
2022-07-28 04:36:23.092129: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:36:24.255267: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 04:36:24.275172: This epoch took 88.700204 s

2022-07-28 04:36:24.314752: 
epoch:  285
2022-07-28 04:37:45.526718: train loss : -0.8931
2022-07-28 04:37:53.466684: validation loss: -0.3591
2022-07-28 04:37:53.499806: Average global foreground Dice: [0.6951]
2022-07-28 04:37:53.513995: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:37:54.990348: Suus1 maybe_update_lr lr: 4.7e-05
2022-07-28 04:37:55.037596: This epoch took 90.703921 s

2022-07-28 04:37:55.123300: 
epoch:  286
2022-07-28 04:39:16.355652: train loss : -0.8813
2022-07-28 04:39:23.394613: validation loss: -0.4080
2022-07-28 04:39:23.431787: Average global foreground Dice: [0.807]
2022-07-28 04:39:23.453301: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:39:24.717096: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 04:39:24.746176: This epoch took 89.595073 s

2022-07-28 04:39:24.801996: 
epoch:  287
2022-07-28 04:40:46.037379: train loss : -0.8923
2022-07-28 04:40:53.523807: validation loss: -0.4139
2022-07-28 04:40:53.589755: Average global foreground Dice: [0.7723]
2022-07-28 04:40:53.606543: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:40:54.766847: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 04:40:54.799098: This epoch took 89.954831 s

2022-07-28 04:40:54.832041: 
epoch:  288
2022-07-28 04:42:15.163321: train loss : -0.8842
2022-07-28 04:42:23.004529: validation loss: -0.4014
2022-07-28 04:42:23.030875: Average global foreground Dice: [0.7882]
2022-07-28 04:42:23.064048: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:42:24.128108: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 04:42:24.188079: This epoch took 89.307849 s

2022-07-28 04:42:24.227984: 
epoch:  289
2022-07-28 04:43:44.639484: train loss : -0.8884
2022-07-28 04:43:51.920537: validation loss: -0.4306
2022-07-28 04:43:51.940925: Average global foreground Dice: [0.8071]
2022-07-28 04:43:51.961095: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:43:53.075049: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 04:43:53.102375: This epoch took 88.845323 s

2022-07-28 04:43:53.117850: 
epoch:  290
2022-07-28 04:45:11.193423: train loss : -0.8686
2022-07-28 04:45:18.881366: validation loss: -0.5058
2022-07-28 04:45:18.935654: Average global foreground Dice: [0.8113]
2022-07-28 04:45:18.991189: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:45:20.172058: Suus1 maybe_update_lr lr: 4.6e-05
2022-07-28 04:45:20.192191: This epoch took 87.061078 s

2022-07-28 04:45:20.200378: 
epoch:  291
2022-07-28 04:46:39.729530: train loss : -0.8814
2022-07-28 04:46:47.385499: validation loss: -0.2463
2022-07-28 04:46:47.412010: Average global foreground Dice: [0.678]
2022-07-28 04:46:47.434931: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:46:48.670790: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 04:46:48.692068: This epoch took 88.458368 s

2022-07-28 04:46:48.710324: 
epoch:  292
2022-07-28 04:48:09.021392: train loss : -0.8937
2022-07-28 04:48:15.629559: validation loss: -0.4809
2022-07-28 04:48:15.653523: Average global foreground Dice: [0.8503]
2022-07-28 04:48:15.695245: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:48:16.981126: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 04:48:17.005225: This epoch took 88.269442 s

2022-07-28 04:48:17.038939: 
epoch:  293
2022-07-28 04:49:35.042043: train loss : -0.8806
2022-07-28 04:49:41.614142: validation loss: -0.4184
2022-07-28 04:49:41.676257: Average global foreground Dice: [0.7814]
2022-07-28 04:49:41.727854: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:49:42.988791: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 04:49:43.010369: This epoch took 85.952571 s

2022-07-28 04:49:43.046100: 
epoch:  294
2022-07-28 04:50:59.999877: train loss : -0.8923
2022-07-28 04:51:07.546108: validation loss: -0.3341
2022-07-28 04:51:07.569581: Average global foreground Dice: [0.703]
2022-07-28 04:51:07.590995: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:51:08.719299: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 04:51:08.762199: This epoch took 85.682016 s

2022-07-28 04:51:08.796251: 
epoch:  295
2022-07-28 04:52:30.666871: train loss : -0.8822
2022-07-28 04:52:37.153605: validation loss: -0.3663
2022-07-28 04:52:37.198026: Average global foreground Dice: [0.7421]
2022-07-28 04:52:37.221640: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:52:38.298750: Suus1 maybe_update_lr lr: 4.5e-05
2022-07-28 04:52:38.304046: This epoch took 89.476968 s

2022-07-28 04:52:38.307301: 
epoch:  296
2022-07-28 04:53:55.381092: train loss : -0.8722
2022-07-28 04:54:02.363926: validation loss: -0.3770
2022-07-28 04:54:02.417321: Average global foreground Dice: [0.7784]
2022-07-28 04:54:02.443214: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:54:03.625632: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 04:54:03.662335: This epoch took 85.334915 s

2022-07-28 04:54:03.684059: 
epoch:  297
2022-07-28 04:55:22.755818: train loss : -0.8654
2022-07-28 04:55:30.246127: validation loss: -0.4435
2022-07-28 04:55:30.301844: Average global foreground Dice: [0.817]
2022-07-28 04:55:30.330469: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:55:31.383650: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 04:55:31.405353: This epoch took 87.698554 s

2022-07-28 04:55:31.431972: 
epoch:  298
2022-07-28 04:56:50.614732: train loss : -0.8784
2022-07-28 04:56:57.587068: validation loss: -0.5029
2022-07-28 04:56:57.608742: Average global foreground Dice: [0.8559]
2022-07-28 04:56:57.613575: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:56:58.776498: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 04:56:58.807291: This epoch took 87.342311 s

2022-07-28 04:56:58.829166: 
epoch:  299
2022-07-28 04:58:18.470908: train loss : -0.8832
2022-07-28 04:58:24.779907: validation loss: -0.1738
2022-07-28 04:58:24.809514: Average global foreground Dice: [0.5971]
2022-07-28 04:58:24.817873: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 04:58:25.898815: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 04:58:25.926522: saving scheduled checkpoint file...
2022-07-28 04:58:26.334115: saving checkpoint...
2022-07-28 04:58:32.742235: done, saving took 6.78 seconds
2022-07-28 04:58:32.757675: done
2022-07-28 04:58:32.760134: This epoch took 93.908131 s

2022-07-28 04:58:32.762333: 
epoch:  300
2022-07-28 04:59:51.877297: train loss : -0.8701
2022-07-28 04:59:59.910382: validation loss: -0.2360
2022-07-28 04:59:59.946572: Average global foreground Dice: [0.673]
2022-07-28 04:59:59.977296: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:00:01.043833: Suus1 maybe_update_lr lr: 4.4e-05
2022-07-28 05:00:01.047155: This epoch took 88.282546 s

2022-07-28 05:00:01.049905: 
epoch:  301
2022-07-28 05:01:19.060808: train loss : -0.8753
2022-07-28 05:01:26.093814: validation loss: -0.4113
2022-07-28 05:01:26.126814: Average global foreground Dice: [0.7695]
2022-07-28 05:01:26.141054: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:01:27.298563: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 05:01:27.322131: This epoch took 86.269774 s

2022-07-28 05:01:27.326417: 
epoch:  302
2022-07-28 05:02:48.347311: train loss : -0.8833
2022-07-28 05:02:56.072513: validation loss: -0.4226
2022-07-28 05:02:56.093905: Average global foreground Dice: [0.8115]
2022-07-28 05:02:56.113486: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:02:57.254759: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 05:02:57.283025: This epoch took 89.938200 s

2022-07-28 05:02:57.305791: 
epoch:  303
2022-07-28 05:04:20.367452: train loss : -0.8771
2022-07-28 05:04:27.855634: validation loss: -0.5727
2022-07-28 05:04:27.875773: Average global foreground Dice: [0.8483]
2022-07-28 05:04:27.893589: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:04:29.092817: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 05:04:29.095446: This epoch took 91.785792 s

2022-07-28 05:04:29.098159: 
epoch:  304
2022-07-28 05:05:48.712562: train loss : -0.8629
2022-07-28 05:05:55.514761: validation loss: -0.3769
2022-07-28 05:05:55.523535: Average global foreground Dice: [0.7802]
2022-07-28 05:05:55.550224: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:05:56.901282: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 05:05:56.911091: This epoch took 87.806837 s

2022-07-28 05:05:56.924029: 
epoch:  305
2022-07-28 05:07:17.710160: train loss : -0.8793
2022-07-28 05:07:24.515578: validation loss: -0.3732
2022-07-28 05:07:24.536040: Average global foreground Dice: [0.7979]
2022-07-28 05:07:24.539001: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:07:25.695466: Suus1 maybe_update_lr lr: 4.3e-05
2022-07-28 05:07:25.738277: This epoch took 88.802244 s

2022-07-28 05:07:25.788312: 
epoch:  306
2022-07-28 05:08:45.943783: train loss : -0.8946
2022-07-28 05:08:52.796369: validation loss: -0.2435
2022-07-28 05:08:52.866083: Average global foreground Dice: [0.703]
2022-07-28 05:08:52.937061: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:08:54.283749: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 05:08:54.338112: This epoch took 88.521402 s

2022-07-28 05:08:54.365055: 
epoch:  307
2022-07-28 05:10:16.012680: train loss : -0.8937
2022-07-28 05:10:22.456753: validation loss: -0.2764
2022-07-28 05:10:22.515180: Average global foreground Dice: [0.7298]
2022-07-28 05:10:22.590107: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:10:23.933831: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 05:10:23.968329: This epoch took 89.587277 s

2022-07-28 05:10:23.998848: 
epoch:  308
2022-07-28 05:11:44.730484: train loss : -0.8835
2022-07-28 05:11:51.998730: validation loss: -0.4826
2022-07-28 05:11:52.030742: Average global foreground Dice: [0.8374]
2022-07-28 05:11:52.048027: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:11:53.137278: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 05:11:53.148647: This epoch took 89.146114 s

2022-07-28 05:11:53.161701: 
epoch:  309
2022-07-28 05:13:15.797811: train loss : -0.9018
2022-07-28 05:13:24.407564: validation loss: -0.4871
2022-07-28 05:13:24.449917: Average global foreground Dice: [0.8253]
2022-07-28 05:13:24.476937: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:13:25.714149: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 05:13:25.730170: This epoch took 92.555121 s

2022-07-28 05:13:25.758528: 
epoch:  310
2022-07-28 05:14:48.123119: train loss : -0.8741
2022-07-28 05:14:56.490226: validation loss: -0.3459
2022-07-28 05:14:56.508411: Average global foreground Dice: [0.7848]
2022-07-28 05:14:56.538711: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:14:57.799089: Suus1 maybe_update_lr lr: 4.2e-05
2022-07-28 05:14:57.825182: This epoch took 92.012866 s

2022-07-28 05:14:57.868156: 
epoch:  311
2022-07-28 05:16:18.476029: train loss : -0.8869
2022-07-28 05:16:26.123397: validation loss: -0.4099
2022-07-28 05:16:26.135409: Average global foreground Dice: [0.8193]
2022-07-28 05:16:26.145029: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:16:27.221862: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 05:16:27.233695: This epoch took 89.336610 s

2022-07-28 05:16:27.271160: 
epoch:  312
2022-07-28 05:17:46.880013: train loss : -0.8883
2022-07-28 05:17:53.621461: validation loss: -0.2539
2022-07-28 05:17:53.629586: Average global foreground Dice: [0.7432]
2022-07-28 05:17:53.634689: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:17:54.805632: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 05:17:54.818030: This epoch took 87.535544 s

2022-07-28 05:17:54.842990: 
epoch:  313
2022-07-28 05:19:12.746703: train loss : -0.9021
2022-07-28 05:19:21.652843: validation loss: -0.2898
2022-07-28 05:19:21.662777: Average global foreground Dice: [0.7355]
2022-07-28 05:19:21.677016: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:19:22.791945: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 05:19:22.813151: This epoch took 87.941115 s

2022-07-28 05:19:22.832996: 
epoch:  314
2022-07-28 05:20:43.072220: train loss : -0.8735
2022-07-28 05:20:49.842106: validation loss: -0.2679
2022-07-28 05:20:49.880584: Average global foreground Dice: [0.6532]
2022-07-28 05:20:49.888466: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:20:51.015959: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 05:20:51.048107: This epoch took 88.193076 s

2022-07-28 05:20:51.070982: 
epoch:  315
2022-07-28 05:22:10.561846: train loss : -0.8888
2022-07-28 05:22:19.411748: validation loss: -0.3014
2022-07-28 05:22:19.445483: Average global foreground Dice: [0.6752]
2022-07-28 05:22:19.449348: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:22:20.693530: Suus1 maybe_update_lr lr: 4.1e-05
2022-07-28 05:22:20.729261: This epoch took 89.625253 s

2022-07-28 05:22:20.762215: 
epoch:  316
2022-07-28 05:23:41.385483: train loss : -0.8681
2022-07-28 05:23:48.146981: validation loss: -0.3627
2022-07-28 05:23:48.194240: Average global foreground Dice: [0.7482]
2022-07-28 05:23:48.270814: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:23:49.313982: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 05:23:49.351375: This epoch took 88.541060 s

2022-07-28 05:23:49.400568: 
epoch:  317
2022-07-28 05:25:10.499967: train loss : -0.8836
2022-07-28 05:25:17.585923: validation loss: -0.6057
2022-07-28 05:25:17.607954: Average global foreground Dice: [0.8786]
2022-07-28 05:25:17.651043: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:25:18.723619: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 05:25:18.786074: This epoch took 89.321027 s

2022-07-28 05:25:18.816489: 
epoch:  318
2022-07-28 05:26:41.268566: train loss : -0.8817
2022-07-28 05:26:48.458797: validation loss: -0.4579
2022-07-28 05:26:48.462687: Average global foreground Dice: [0.8551]
2022-07-28 05:26:48.465452: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:26:49.610349: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 05:26:49.614965: This epoch took 90.769830 s

2022-07-28 05:26:49.630007: 
epoch:  319
2022-07-28 05:28:12.825232: train loss : -0.8921
2022-07-28 05:28:21.152522: validation loss: -0.3452
2022-07-28 05:28:21.185121: Average global foreground Dice: [0.7601]
2022-07-28 05:28:21.226024: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:28:22.417840: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 05:28:22.475180: This epoch took 92.831375 s

2022-07-28 05:28:22.532251: 
epoch:  320
2022-07-28 05:29:44.170495: train loss : -0.8984
2022-07-28 05:29:51.269292: validation loss: -0.4302
2022-07-28 05:29:51.302193: Average global foreground Dice: [0.8229]
2022-07-28 05:29:51.306027: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:29:52.566004: Suus1 maybe_update_lr lr: 4e-05
2022-07-28 05:29:52.585113: This epoch took 90.020122 s

2022-07-28 05:29:52.597405: 
epoch:  321
2022-07-28 05:31:16.472640: train loss : -0.9018
2022-07-28 05:31:25.211818: validation loss: -0.4082
2022-07-28 05:31:25.259056: Average global foreground Dice: [0.8087]
2022-07-28 05:31:25.286023: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:31:26.399721: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 05:31:26.432142: This epoch took 93.816826 s

2022-07-28 05:31:26.453064: 
epoch:  322
2022-07-28 05:32:46.448451: train loss : -0.8878
2022-07-28 05:32:53.666640: validation loss: -0.2913
2022-07-28 05:32:53.719438: Average global foreground Dice: [0.7061]
2022-07-28 05:32:53.757765: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:32:54.882579: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 05:32:54.910901: This epoch took 88.441802 s

2022-07-28 05:32:54.945361: 
epoch:  323
2022-07-28 05:34:14.434390: train loss : -0.8947
2022-07-28 05:34:21.895541: validation loss: -0.2630
2022-07-28 05:34:21.933932: Average global foreground Dice: [0.7443]
2022-07-28 05:34:21.978159: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:34:23.086653: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 05:34:23.100486: This epoch took 88.134989 s

2022-07-28 05:34:23.122010: 
epoch:  324
2022-07-28 05:35:41.986752: train loss : -0.9030
2022-07-28 05:35:49.630256: validation loss: -0.3933
2022-07-28 05:35:49.665124: Average global foreground Dice: [0.7896]
2022-07-28 05:35:49.687128: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:35:50.961010: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 05:35:50.982058: This epoch took 87.830220 s

2022-07-28 05:35:51.024000: 
epoch:  325
2022-07-28 05:37:11.314296: train loss : -0.8818
2022-07-28 05:37:18.524405: validation loss: -0.4082
2022-07-28 05:37:18.590006: Average global foreground Dice: [0.8171]
2022-07-28 05:37:18.622010: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:37:19.754585: Suus1 maybe_update_lr lr: 3.9e-05
2022-07-28 05:37:19.777186: This epoch took 88.722938 s

2022-07-28 05:37:19.818333: 
epoch:  326
2022-07-28 05:38:42.083421: train loss : -0.8876
2022-07-28 05:38:48.862513: validation loss: -0.4267
2022-07-28 05:38:48.904133: Average global foreground Dice: [0.7934]
2022-07-28 05:38:48.932062: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:38:50.170560: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 05:38:50.191999: This epoch took 90.359125 s

2022-07-28 05:38:50.202931: 
epoch:  327
2022-07-28 05:40:08.774846: train loss : -0.8907
2022-07-28 05:40:15.227433: validation loss: -0.3875
2022-07-28 05:40:15.292056: Average global foreground Dice: [0.7539]
2022-07-28 05:40:15.333225: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:40:16.619948: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 05:40:16.655051: This epoch took 86.447226 s

2022-07-28 05:40:16.700148: 
epoch:  328
2022-07-28 05:41:34.718918: train loss : -0.8897
2022-07-28 05:41:41.065278: validation loss: -0.2829
2022-07-28 05:41:41.102821: Average global foreground Dice: [0.702]
2022-07-28 05:41:41.133104: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:41:42.256669: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 05:41:42.296174: This epoch took 85.565181 s

2022-07-28 05:41:42.351144: 
epoch:  329
2022-07-28 05:43:02.332209: train loss : -0.8887
2022-07-28 05:43:08.485647: validation loss: -0.4056
2022-07-28 05:43:08.525259: Average global foreground Dice: [0.7978]
2022-07-28 05:43:08.560019: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:43:09.698426: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 05:43:09.711080: This epoch took 87.321781 s

2022-07-28 05:43:09.731995: 
epoch:  330
2022-07-28 05:44:28.436510: train loss : -0.8955
2022-07-28 05:44:35.340508: validation loss: -0.4238
2022-07-28 05:44:35.387595: Average global foreground Dice: [0.8192]
2022-07-28 05:44:35.396103: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:44:36.616778: Suus1 maybe_update_lr lr: 3.8e-05
2022-07-28 05:44:36.644515: This epoch took 86.890525 s

2022-07-28 05:44:36.677253: 
epoch:  331
2022-07-28 05:45:55.349246: train loss : -0.8909
2022-07-28 05:46:03.186797: validation loss: -0.2832
2022-07-28 05:46:03.219549: Average global foreground Dice: [0.6993]
2022-07-28 05:46:03.249056: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:46:04.405855: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 05:46:04.409004: This epoch took 87.714490 s

2022-07-28 05:46:04.430969: 
epoch:  332
2022-07-28 05:47:25.030223: train loss : -0.8931
2022-07-28 05:47:33.396691: validation loss: -0.3542
2022-07-28 05:47:33.431636: Average global foreground Dice: [0.782]
2022-07-28 05:47:33.441287: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:47:34.667468: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 05:47:34.670405: This epoch took 90.206411 s

2022-07-28 05:47:34.673420: 
epoch:  333
2022-07-28 05:48:54.897382: train loss : -0.8893
2022-07-28 05:49:01.889143: validation loss: -0.2791
2022-07-28 05:49:01.951376: Average global foreground Dice: [0.6163]
2022-07-28 05:49:01.981943: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:49:03.201972: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 05:49:03.213630: This epoch took 88.525603 s

2022-07-28 05:49:03.220060: 
epoch:  334
2022-07-28 05:50:23.884453: train loss : -0.8863
2022-07-28 05:50:29.916763: validation loss: -0.2740
2022-07-28 05:50:29.960430: Average global foreground Dice: [0.7439]
2022-07-28 05:50:29.982444: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:50:31.056893: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 05:50:31.078068: This epoch took 87.843055 s

2022-07-28 05:50:31.105986: 
epoch:  335
2022-07-28 05:51:51.933622: train loss : -0.8988
2022-07-28 05:51:59.286356: validation loss: -0.3348
2022-07-28 05:51:59.324302: Average global foreground Dice: [0.7514]
2022-07-28 05:51:59.340979: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:52:00.604941: Suus1 maybe_update_lr lr: 3.7e-05
2022-07-28 05:52:00.645331: This epoch took 89.506314 s

2022-07-28 05:52:00.667095: 
epoch:  336
2022-07-28 05:53:21.316143: train loss : -0.8967
2022-07-28 05:53:28.008477: validation loss: -0.4225
2022-07-28 05:53:28.068618: Average global foreground Dice: [0.811]
2022-07-28 05:53:28.101018: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:53:29.237450: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 05:53:29.252077: This epoch took 88.542948 s

2022-07-28 05:53:29.276723: 
epoch:  337
2022-07-28 05:54:50.113215: train loss : -0.9022
2022-07-28 05:54:56.124824: validation loss: -0.4204
2022-07-28 05:54:56.183227: Average global foreground Dice: [0.7793]
2022-07-28 05:54:56.198847: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:54:57.419785: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 05:54:57.458377: This epoch took 88.126188 s

2022-07-28 05:54:57.470910: 
epoch:  338
2022-07-28 05:56:17.946102: train loss : -0.9091
2022-07-28 05:56:25.400905: validation loss: -0.4227
2022-07-28 05:56:25.449571: Average global foreground Dice: [0.7529]
2022-07-28 05:56:25.482067: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:56:26.820859: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 05:56:26.825534: This epoch took 89.342550 s

2022-07-28 05:56:26.845045: 
epoch:  339
2022-07-28 05:57:47.647936: train loss : -0.8739
2022-07-28 05:57:53.462483: validation loss: -0.2155
2022-07-28 05:57:53.473471: Average global foreground Dice: [0.6257]
2022-07-28 05:57:53.504012: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:57:54.612552: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 05:57:54.630256: This epoch took 87.770471 s

2022-07-28 05:57:54.641958: 
epoch:  340
2022-07-28 05:59:14.095970: train loss : -0.8956
2022-07-28 05:59:21.415776: validation loss: -0.3687
2022-07-28 05:59:21.426982: Average global foreground Dice: [0.7902]
2022-07-28 05:59:21.450113: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 05:59:22.604120: Suus1 maybe_update_lr lr: 3.6e-05
2022-07-28 05:59:22.635660: This epoch took 87.989439 s

2022-07-28 05:59:22.667102: 
epoch:  341
2022-07-28 06:00:41.902552: train loss : -0.9021
2022-07-28 06:00:49.322197: validation loss: -0.2825
2022-07-28 06:00:49.347640: Average global foreground Dice: [0.7049]
2022-07-28 06:00:49.369181: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:00:50.634983: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 06:00:50.667419: This epoch took 87.974360 s

2022-07-28 06:00:50.698341: 
epoch:  342
2022-07-28 06:02:09.661350: train loss : -0.8984
2022-07-28 06:02:18.232402: validation loss: -0.3558
2022-07-28 06:02:18.237008: Average global foreground Dice: [0.7856]
2022-07-28 06:02:18.239197: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:02:19.306654: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 06:02:19.345165: This epoch took 88.623862 s

2022-07-28 06:02:19.378227: 
epoch:  343
2022-07-28 06:03:40.826044: train loss : -0.8936
2022-07-28 06:03:47.368560: validation loss: -0.3318
2022-07-28 06:03:47.390118: Average global foreground Dice: [0.7651]
2022-07-28 06:03:47.415387: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:03:48.459140: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 06:03:48.483046: This epoch took 89.085019 s

2022-07-28 06:03:48.493223: 
epoch:  344
2022-07-28 06:05:09.337450: train loss : -0.9037
2022-07-28 06:05:17.245410: validation loss: -0.3466
2022-07-28 06:05:17.298377: Average global foreground Dice: [0.8048]
2022-07-28 06:05:17.319012: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:05:18.360937: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 06:05:18.425131: This epoch took 89.907936 s

2022-07-28 06:05:18.473253: 
epoch:  345
2022-07-28 06:06:39.264151: train loss : -0.8931
2022-07-28 06:06:47.396468: validation loss: -0.3136
2022-07-28 06:06:47.433647: Average global foreground Dice: [0.7513]
2022-07-28 06:06:47.450227: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:06:48.726948: Suus1 maybe_update_lr lr: 3.5e-05
2022-07-28 06:06:48.729215: This epoch took 90.190198 s

2022-07-28 06:06:48.731300: 
epoch:  346
2022-07-28 06:08:06.149802: train loss : -0.8953
2022-07-28 06:08:13.311382: validation loss: -0.4447
2022-07-28 06:08:13.324067: Average global foreground Dice: [0.8172]
2022-07-28 06:08:13.348430: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:08:14.618613: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 06:08:14.643345: This epoch took 85.910189 s

2022-07-28 06:08:14.724014: 
epoch:  347
2022-07-28 06:09:31.031829: train loss : -0.8832
2022-07-28 06:09:37.705224: validation loss: -0.3167
2022-07-28 06:09:37.721334: Average global foreground Dice: [0.7928]
2022-07-28 06:09:37.731753: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:09:39.138778: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 06:09:39.167253: This epoch took 84.409189 s

2022-07-28 06:09:39.186169: 
epoch:  348
2022-07-28 06:10:59.935277: train loss : -0.8941
2022-07-28 06:11:06.853378: validation loss: -0.4215
2022-07-28 06:11:06.928044: Average global foreground Dice: [0.7898]
2022-07-28 06:11:06.983054: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:11:08.113779: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 06:11:08.154455: This epoch took 88.954056 s

2022-07-28 06:11:08.182079: 
epoch:  349
2022-07-28 06:12:29.972756: train loss : -0.8939
2022-07-28 06:12:37.005700: validation loss: -0.2569
2022-07-28 06:12:37.051373: Average global foreground Dice: [0.7374]
2022-07-28 06:12:37.094730: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:12:38.153509: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 06:12:38.168716: saving scheduled checkpoint file...
2022-07-28 06:12:38.724937: saving checkpoint...
2022-07-28 06:12:44.566589: done, saving took 6.38 seconds
2022-07-28 06:12:44.580352: done
2022-07-28 06:12:44.582186: This epoch took 96.361149 s

2022-07-28 06:12:44.583927: 
epoch:  350
2022-07-28 06:14:03.218703: train loss : -0.9126
2022-07-28 06:14:09.637527: validation loss: -0.3077
2022-07-28 06:14:09.701843: Average global foreground Dice: [0.7274]
2022-07-28 06:14:09.722489: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:14:11.452448: Suus1 maybe_update_lr lr: 3.4e-05
2022-07-28 06:14:11.457680: This epoch took 86.871818 s

2022-07-28 06:14:11.468755: 
epoch:  351
2022-07-28 06:15:31.632895: train loss : -0.8967
2022-07-28 06:15:39.309483: validation loss: -0.3395
2022-07-28 06:15:39.339989: Average global foreground Dice: [0.7954]
2022-07-28 06:15:39.347518: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:15:40.397143: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 06:15:40.422349: This epoch took 88.935060 s

2022-07-28 06:15:40.428059: 
epoch:  352
2022-07-28 06:16:59.586304: train loss : -0.8636
2022-07-28 06:17:07.000195: validation loss: -0.2789
2022-07-28 06:17:07.026891: Average global foreground Dice: [0.643]
2022-07-28 06:17:07.029893: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:17:08.329176: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 06:17:08.348119: This epoch took 87.902822 s

2022-07-28 06:17:08.367151: 
epoch:  353
2022-07-28 06:18:27.694129: train loss : -0.8922
2022-07-28 06:18:35.011969: validation loss: -0.4198
2022-07-28 06:18:35.045033: Average global foreground Dice: [0.8184]
2022-07-28 06:18:35.058014: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:18:36.177235: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 06:18:36.195606: This epoch took 87.801577 s

2022-07-28 06:18:36.211111: 
epoch:  354
2022-07-28 06:19:56.843518: train loss : -0.9074
2022-07-28 06:20:03.976182: validation loss: -0.3070
2022-07-28 06:20:04.026986: Average global foreground Dice: [0.7726]
2022-07-28 06:20:04.049026: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:20:05.270034: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 06:20:05.299123: This epoch took 89.084261 s

2022-07-28 06:20:05.332009: 
epoch:  355
2022-07-28 06:21:25.286649: train loss : -0.8971
2022-07-28 06:21:32.781981: validation loss: -0.4433
2022-07-28 06:21:32.825362: Average global foreground Dice: [0.8074]
2022-07-28 06:21:32.851014: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:21:34.135307: Suus1 maybe_update_lr lr: 3.3e-05
2022-07-28 06:21:34.156422: This epoch took 88.780389 s

2022-07-28 06:21:34.178003: 
epoch:  356
2022-07-28 06:22:54.185034: train loss : -0.8962
2022-07-28 06:23:02.401977: validation loss: -0.3788
2022-07-28 06:23:02.439754: Average global foreground Dice: [0.7798]
2022-07-28 06:23:02.446617: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:23:03.828156: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 06:23:03.847307: This epoch took 89.648657 s

2022-07-28 06:23:03.921998: 
epoch:  357
2022-07-28 06:24:25.274081: train loss : -0.8742
2022-07-28 06:24:31.595081: validation loss: -0.3684
2022-07-28 06:24:31.641939: Average global foreground Dice: [0.7487]
2022-07-28 06:24:31.685000: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:24:32.723228: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 06:24:32.768151: This epoch took 88.793100 s

2022-07-28 06:24:32.808070: 
epoch:  358
2022-07-28 06:25:52.234478: train loss : -0.8983
2022-07-28 06:25:58.651615: validation loss: -0.3411
2022-07-28 06:25:58.696860: Average global foreground Dice: [0.8097]
2022-07-28 06:25:58.724054: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:25:59.978282: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 06:26:00.002843: This epoch took 87.155732 s

2022-07-28 06:26:00.019111: 
epoch:  359
2022-07-28 06:27:21.187479: train loss : -0.9034
2022-07-28 06:27:27.312053: validation loss: -0.2931
2022-07-28 06:27:27.362468: Average global foreground Dice: [0.725]
2022-07-28 06:27:27.382549: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:27:28.568240: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 06:27:28.602217: This epoch took 88.569335 s

2022-07-28 06:27:28.646118: 
epoch:  360
2022-07-28 06:28:49.089483: train loss : -0.8990
2022-07-28 06:28:56.688979: validation loss: -0.3459
2022-07-28 06:28:56.740702: Average global foreground Dice: [0.7579]
2022-07-28 06:28:56.759845: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:28:58.114810: Suus1 maybe_update_lr lr: 3.2e-05
2022-07-28 06:28:58.135451: This epoch took 89.440923 s

2022-07-28 06:28:58.157021: 
epoch:  361
2022-07-28 06:30:20.210932: train loss : -0.9120
2022-07-28 06:30:28.609373: validation loss: -0.3747
2022-07-28 06:30:28.634097: Average global foreground Dice: [0.776]
2022-07-28 06:30:28.654675: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:30:30.060057: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 06:30:30.104198: This epoch took 91.924950 s

2022-07-28 06:30:30.148124: 
epoch:  362
2022-07-28 06:31:49.373342: train loss : -0.8894
2022-07-28 06:31:57.793966: validation loss: -0.3693
2022-07-28 06:31:57.859430: Average global foreground Dice: [0.787]
2022-07-28 06:31:57.908295: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:31:59.217693: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 06:31:59.245265: This epoch took 89.053241 s

2022-07-28 06:31:59.285156: 
epoch:  363
2022-07-28 06:33:20.187656: train loss : -0.9058
2022-07-28 06:33:29.248265: validation loss: -0.4187
2022-07-28 06:33:29.277347: Average global foreground Dice: [0.777]
2022-07-28 06:33:29.292188: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:33:30.471016: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 06:33:30.497567: This epoch took 91.180517 s

2022-07-28 06:33:30.518718: 
epoch:  364
2022-07-28 06:34:49.947500: train loss : -0.8993
2022-07-28 06:34:58.514567: validation loss: -0.4766
2022-07-28 06:34:58.526074: Average global foreground Dice: [0.8295]
2022-07-28 06:34:58.548185: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:34:59.681991: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 06:34:59.704679: This epoch took 89.172630 s

2022-07-28 06:34:59.733800: 
epoch:  365
2022-07-28 06:36:19.679669: train loss : -0.9135
2022-07-28 06:36:28.525806: validation loss: -0.4315
2022-07-28 06:36:28.552893: Average global foreground Dice: [0.8176]
2022-07-28 06:36:28.574020: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:36:29.793746: Suus1 maybe_update_lr lr: 3.1e-05
2022-07-28 06:36:29.797579: This epoch took 90.042711 s

2022-07-28 06:36:29.830160: 
epoch:  366
2022-07-28 06:37:48.189864: train loss : -0.8948
2022-07-28 06:37:56.754197: validation loss: -0.4291
2022-07-28 06:37:56.800403: Average global foreground Dice: [0.7758]
2022-07-28 06:37:56.843012: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:37:58.022273: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 06:37:58.026514: This epoch took 88.191230 s

2022-07-28 06:37:58.068449: 
epoch:  367
2022-07-28 06:39:18.047575: train loss : -0.8752
2022-07-28 06:39:26.295648: validation loss: -0.3415
2022-07-28 06:39:26.328002: Average global foreground Dice: [0.7929]
2022-07-28 06:39:26.374514: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:39:27.621270: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 06:39:27.654164: This epoch took 89.548066 s

2022-07-28 06:39:27.719995: 
epoch:  368
2022-07-28 06:40:46.686349: train loss : -0.9015
2022-07-28 06:40:54.179165: validation loss: -0.3285
2022-07-28 06:40:54.238964: Average global foreground Dice: [0.7925]
2022-07-28 06:40:54.307088: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:40:55.368602: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 06:40:55.392344: This epoch took 87.626327 s

2022-07-28 06:40:55.421088: 
epoch:  369
2022-07-28 06:42:15.676589: train loss : -0.9132
2022-07-28 06:42:23.997635: validation loss: -0.4269
2022-07-28 06:42:24.064778: Average global foreground Dice: [0.8346]
2022-07-28 06:42:24.108096: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:42:25.290254: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 06:42:25.325167: This epoch took 89.859086 s

2022-07-28 06:42:25.360262: 
epoch:  370
2022-07-28 06:43:43.863724: train loss : -0.8885
2022-07-28 06:43:52.060721: validation loss: -0.5670
2022-07-28 06:43:52.119072: Average global foreground Dice: [0.8553]
2022-07-28 06:43:52.184008: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:43:53.415680: Suus1 maybe_update_lr lr: 3e-05
2022-07-28 06:43:53.425500: saving best epoch checkpoint...
2022-07-28 06:43:53.820895: saving checkpoint...
2022-07-28 06:43:59.278838: done, saving took 5.84 seconds
2022-07-28 06:43:59.288353: This epoch took 93.900181 s

2022-07-28 06:43:59.290484: 
epoch:  371
2022-07-28 06:45:15.268498: train loss : -0.8980
2022-07-28 06:45:22.685672: validation loss: -0.3159
2022-07-28 06:45:22.692440: Average global foreground Dice: [0.7329]
2022-07-28 06:45:22.695611: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:45:23.900977: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 06:45:23.920695: This epoch took 84.628107 s

2022-07-28 06:45:23.938415: 
epoch:  372
2022-07-28 06:46:43.645212: train loss : -0.9024
2022-07-28 06:46:51.553195: validation loss: -0.3823
2022-07-28 06:46:51.595986: Average global foreground Dice: [0.7555]
2022-07-28 06:46:51.647611: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:46:52.694104: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 06:46:52.712897: This epoch took 88.760847 s

2022-07-28 06:46:52.716213: 
epoch:  373
2022-07-28 06:48:12.235533: train loss : -0.9050
2022-07-28 06:48:19.923320: validation loss: -0.3904
2022-07-28 06:48:19.987270: Average global foreground Dice: [0.8038]
2022-07-28 06:48:20.037153: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:48:21.363876: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 06:48:21.391309: This epoch took 88.672074 s

2022-07-28 06:48:21.408702: 
epoch:  374
2022-07-28 06:49:41.586941: train loss : -0.9008
2022-07-28 06:49:48.815135: validation loss: -0.3603
2022-07-28 06:49:48.831050: Average global foreground Dice: [0.798]
2022-07-28 06:49:48.844387: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:49:50.154621: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 06:49:50.199112: This epoch took 88.755049 s

2022-07-28 06:49:50.232018: 
epoch:  375
2022-07-28 06:51:09.615161: train loss : -0.9006
2022-07-28 06:51:17.404906: validation loss: -0.3698
2022-07-28 06:51:17.470112: Average global foreground Dice: [0.7897]
2022-07-28 06:51:17.481564: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:51:18.569832: Suus1 maybe_update_lr lr: 2.9e-05
2022-07-28 06:51:18.580213: This epoch took 88.301197 s

2022-07-28 06:51:18.601972: 
epoch:  376
2022-07-28 06:52:42.055595: train loss : -0.9115
2022-07-28 06:52:48.859012: validation loss: -0.4125
2022-07-28 06:52:48.881921: Average global foreground Dice: [0.8115]
2022-07-28 06:52:48.911535: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:52:50.120762: Suus1 maybe_update_lr lr: 2.8e-05
2022-07-28 06:52:50.126194: This epoch took 91.502132 s

2022-07-28 06:52:50.151997: 
epoch:  377
2022-07-28 06:54:11.418516: train loss : -0.9027
2022-07-28 06:54:20.210612: validation loss: -0.4074
2022-07-28 06:54:20.253467: Average global foreground Dice: [0.8093]
2022-07-28 06:54:20.286993: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:54:21.553081: Suus1 maybe_update_lr lr: 2.8e-05
2022-07-28 06:54:21.566300: This epoch took 91.410157 s

2022-07-28 06:54:21.653114: 
epoch:  378
2022-07-28 06:55:43.981191: train loss : -0.8960
2022-07-28 06:55:50.636445: validation loss: -0.3199
2022-07-28 06:55:50.650425: Average global foreground Dice: [0.7959]
2022-07-28 06:55:50.686235: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:55:51.874034: Suus1 maybe_update_lr lr: 2.8e-05
2022-07-28 06:55:51.881732: This epoch took 90.208455 s

2022-07-28 06:55:51.896055: 
epoch:  379
2022-07-28 06:57:08.892817: train loss : -0.8918
2022-07-28 06:57:17.585931: validation loss: -0.3984
2022-07-28 06:57:17.596958: Average global foreground Dice: [0.8217]
2022-07-28 06:57:17.622061: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:57:18.678600: Suus1 maybe_update_lr lr: 2.8e-05
2022-07-28 06:57:18.715285: saving best epoch checkpoint...
2022-07-28 06:57:18.999959: saving checkpoint...
2022-07-28 06:57:24.324289: done, saving took 5.55 seconds
2022-07-28 06:57:24.335277: This epoch took 92.423711 s

2022-07-28 06:57:24.337294: 
epoch:  380
2022-07-28 06:58:44.055610: train loss : -0.9024
2022-07-28 06:58:50.652771: validation loss: -0.3561
2022-07-28 06:58:50.657305: Average global foreground Dice: [0.7413]
2022-07-28 06:58:50.659469: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 06:58:51.864475: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 06:58:51.877213: This epoch took 87.538083 s

2022-07-28 06:58:51.893106: 
epoch:  381
2022-07-28 07:00:14.108902: train loss : -0.9118
2022-07-28 07:00:21.652156: validation loss: -0.4274
2022-07-28 07:00:21.674101: Average global foreground Dice: [0.7924]
2022-07-28 07:00:21.690750: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:00:22.764247: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 07:00:22.777025: This epoch took 90.854730 s

2022-07-28 07:00:22.807016: 
epoch:  382
2022-07-28 07:01:43.055100: train loss : -0.9052
2022-07-28 07:01:50.414068: validation loss: -0.5215
2022-07-28 07:01:50.447895: Average global foreground Dice: [0.8531]
2022-07-28 07:01:50.469009: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:01:51.798812: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 07:01:51.803109: saving best epoch checkpoint...
2022-07-28 07:01:52.086556: saving checkpoint...
2022-07-28 07:01:58.068509: done, saving took 6.26 seconds
2022-07-28 07:01:58.080454: This epoch took 95.247437 s

2022-07-28 07:01:58.082576: 
epoch:  383
2022-07-28 07:03:16.167822: train loss : -0.9148
2022-07-28 07:03:22.580209: validation loss: -0.4144
2022-07-28 07:03:22.629231: Average global foreground Dice: [0.778]
2022-07-28 07:03:22.647655: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:03:23.975848: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 07:03:24.000555: This epoch took 85.916069 s

2022-07-28 07:03:24.015070: 
epoch:  384
2022-07-28 07:04:43.404233: train loss : -0.9040
2022-07-28 07:04:50.584821: validation loss: -0.4506
2022-07-28 07:04:50.619637: Average global foreground Dice: [0.8163]
2022-07-28 07:04:50.672082: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:04:51.862811: Suus1 maybe_update_lr lr: 2.7e-05
2022-07-28 07:04:51.871187: saving best epoch checkpoint...
2022-07-28 07:04:52.361216: saving checkpoint...
2022-07-28 07:04:57.788917: done, saving took 5.88 seconds
2022-07-28 07:04:57.804546: This epoch took 93.784069 s

2022-07-28 07:04:57.806471: 
epoch:  385
2022-07-28 07:06:16.080595: train loss : -0.9072
2022-07-28 07:06:22.681452: validation loss: -0.3836
2022-07-28 07:06:22.713514: Average global foreground Dice: [0.8133]
2022-07-28 07:06:22.760628: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:06:23.752805: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 07:06:23.784136: saving best epoch checkpoint...
2022-07-28 07:06:24.102641: saving checkpoint...
2022-07-28 07:06:29.182838: done, saving took 5.38 seconds
2022-07-28 07:06:29.197138: This epoch took 91.388808 s

2022-07-28 07:06:29.199057: 
epoch:  386
2022-07-28 07:07:47.837479: train loss : -0.8830
2022-07-28 07:07:55.034535: validation loss: -0.4466
2022-07-28 07:07:55.083005: Average global foreground Dice: [0.8186]
2022-07-28 07:07:55.126014: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:07:56.357822: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 07:07:56.380668: saving best epoch checkpoint...
2022-07-28 07:07:56.702501: saving checkpoint...
2022-07-28 07:08:02.048354: done, saving took 5.65 seconds
2022-07-28 07:08:02.062145: This epoch took 92.861192 s

2022-07-28 07:08:02.064229: 
epoch:  387
2022-07-28 07:09:20.134500: train loss : -0.8973
2022-07-28 07:09:27.344043: validation loss: -0.3405
2022-07-28 07:09:27.353520: Average global foreground Dice: [0.7939]
2022-07-28 07:09:27.417039: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:09:28.721020: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 07:09:28.755365: This epoch took 86.689217 s

2022-07-28 07:09:28.777112: 
epoch:  388
2022-07-28 07:10:49.605335: train loss : -0.9033
2022-07-28 07:10:56.066303: validation loss: -0.3330
2022-07-28 07:10:56.074255: Average global foreground Dice: [0.7782]
2022-07-28 07:10:56.076673: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:10:57.256470: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 07:10:57.276737: This epoch took 88.464695 s

2022-07-28 07:10:57.297143: 
epoch:  389
2022-07-28 07:12:15.645233: train loss : -0.8922
2022-07-28 07:12:22.670005: validation loss: -0.2716
2022-07-28 07:12:22.696258: Average global foreground Dice: [0.7305]
2022-07-28 07:12:22.723934: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:12:24.118786: Suus1 maybe_update_lr lr: 2.6e-05
2022-07-28 07:12:24.155544: This epoch took 86.838156 s

2022-07-28 07:12:24.161179: 
epoch:  390
2022-07-28 07:13:43.936727: train loss : -0.9170
2022-07-28 07:13:50.190568: validation loss: -0.3547
2022-07-28 07:13:50.262452: Average global foreground Dice: [0.7778]
2022-07-28 07:13:50.309126: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:13:51.728912: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 07:13:51.760751: This epoch took 87.572609 s

2022-07-28 07:13:51.782166: 
epoch:  391
2022-07-28 07:15:11.484746: train loss : -0.8996
2022-07-28 07:15:17.853933: validation loss: -0.4892
2022-07-28 07:15:17.868617: Average global foreground Dice: [0.8492]
2022-07-28 07:15:17.885769: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:15:19.148270: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 07:15:19.168534: This epoch took 87.364645 s

2022-07-28 07:15:19.195773: 
epoch:  392
2022-07-28 07:16:38.574206: train loss : -0.9099
2022-07-28 07:16:46.501112: validation loss: -0.2985
2022-07-28 07:16:46.514824: Average global foreground Dice: [0.7328]
2022-07-28 07:16:46.539056: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:16:47.797218: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 07:16:47.832668: This epoch took 88.614794 s

2022-07-28 07:16:47.836975: 
epoch:  393
2022-07-28 07:18:05.838733: train loss : -0.8955
2022-07-28 07:18:12.564186: validation loss: -0.4051
2022-07-28 07:18:12.607673: Average global foreground Dice: [0.8161]
2022-07-28 07:18:12.635585: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:18:13.667741: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 07:18:13.675265: This epoch took 85.822734 s

2022-07-28 07:18:13.706516: 
epoch:  394
2022-07-28 07:19:31.948632: train loss : -0.9191
2022-07-28 07:19:38.478300: validation loss: -0.3852
2022-07-28 07:19:38.511333: Average global foreground Dice: [0.8329]
2022-07-28 07:19:38.520501: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:19:39.676120: Suus1 maybe_update_lr lr: 2.5e-05
2022-07-28 07:19:39.697207: This epoch took 85.964268 s

2022-07-28 07:19:39.707973: 
epoch:  395
2022-07-28 07:20:58.212109: train loss : -0.9120
2022-07-28 07:21:05.377871: validation loss: -0.3479
2022-07-28 07:21:05.418867: Average global foreground Dice: [0.7505]
2022-07-28 07:21:05.442084: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:21:06.612527: Suus1 maybe_update_lr lr: 2.4e-05
2022-07-28 07:21:06.634530: This epoch took 86.893518 s

2022-07-28 07:21:06.648971: 
epoch:  396
2022-07-28 07:22:23.518978: train loss : -0.8982
2022-07-28 07:22:30.168321: validation loss: -0.3816
2022-07-28 07:22:30.185229: Average global foreground Dice: [0.8022]
2022-07-28 07:22:30.215045: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:22:31.821246: Suus1 maybe_update_lr lr: 2.4e-05
2022-07-28 07:22:31.850641: This epoch took 85.173445 s

2022-07-28 07:22:31.875797: 
epoch:  397
2022-07-28 07:23:51.619668: train loss : -0.9152
2022-07-28 07:23:58.867944: validation loss: -0.3441
2022-07-28 07:23:58.914916: Average global foreground Dice: [0.7412]
2022-07-28 07:23:58.926582: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:24:00.050544: Suus1 maybe_update_lr lr: 2.4e-05
2022-07-28 07:24:00.093778: This epoch took 88.207189 s

2022-07-28 07:24:00.136974: 
epoch:  398
2022-07-28 07:25:18.838698: train loss : -0.9009
2022-07-28 07:25:25.576093: validation loss: -0.2860
2022-07-28 07:25:25.600863: Average global foreground Dice: [0.7494]
2022-07-28 07:25:25.620332: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:25:27.371780: Suus1 maybe_update_lr lr: 2.4e-05
2022-07-28 07:25:27.374494: This epoch took 87.182481 s

2022-07-28 07:25:27.377464: 
epoch:  399
2022-07-28 07:26:46.721838: train loss : -0.9137
2022-07-28 07:26:53.974294: validation loss: -0.3858
2022-07-28 07:26:54.014858: Average global foreground Dice: [0.7996]
2022-07-28 07:26:54.056050: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:26:55.310735: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 07:26:55.345086: saving scheduled checkpoint file...
2022-07-28 07:26:55.577782: saving checkpoint...
2022-07-28 07:27:00.581541: done, saving took 5.23 seconds
2022-07-28 07:27:00.614573: done
2022-07-28 07:27:00.616850: This epoch took 93.219435 s

2022-07-28 07:27:00.618873: 
epoch:  400
2022-07-28 07:28:17.629398: train loss : -0.9089
2022-07-28 07:28:23.698116: validation loss: -0.4365
2022-07-28 07:28:23.734965: Average global foreground Dice: [0.825]
2022-07-28 07:28:23.754575: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:28:24.943065: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 07:28:24.970623: This epoch took 84.349814 s

2022-07-28 07:28:24.977847: 
epoch:  401
2022-07-28 07:29:44.870931: train loss : -0.9115
2022-07-28 07:29:51.447435: validation loss: -0.4576
2022-07-28 07:29:51.520200: Average global foreground Dice: [0.8416]
2022-07-28 07:29:51.543755: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:29:52.904031: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 07:29:52.957091: This epoch took 87.933878 s

2022-07-28 07:29:53.009159: 
epoch:  402
2022-07-28 07:31:14.338684: train loss : -0.8985
2022-07-28 07:31:20.996903: validation loss: -0.4045
2022-07-28 07:31:21.018704: Average global foreground Dice: [0.8034]
2022-07-28 07:31:21.049043: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:31:22.281681: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 07:31:22.298908: This epoch took 89.234754 s

2022-07-28 07:31:22.324413: 
epoch:  403
2022-07-28 07:32:39.777759: train loss : -0.9083
2022-07-28 07:32:46.748771: validation loss: -0.4303
2022-07-28 07:32:46.752540: Average global foreground Dice: [0.8371]
2022-07-28 07:32:46.772017: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:32:47.991793: Suus1 maybe_update_lr lr: 2.3e-05
2022-07-28 07:32:48.024094: This epoch took 85.680157 s

2022-07-28 07:32:48.058283: 
epoch:  404
2022-07-28 07:34:07.061924: train loss : -0.9080
2022-07-28 07:34:14.199655: validation loss: -0.5142
2022-07-28 07:34:14.217705: Average global foreground Dice: [0.8333]
2022-07-28 07:34:14.237002: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:34:15.593805: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 07:34:15.617248: saving best epoch checkpoint...
2022-07-28 07:34:16.174825: saving checkpoint...
2022-07-28 07:34:21.302309: done, saving took 5.66 seconds
2022-07-28 07:34:21.319336: This epoch took 93.241124 s

2022-07-28 07:34:21.321251: 
epoch:  405
2022-07-28 07:35:40.634727: train loss : -0.9217
2022-07-28 07:35:48.212377: validation loss: -0.4240
2022-07-28 07:35:48.240662: Average global foreground Dice: [0.7559]
2022-07-28 07:35:48.323096: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:35:49.543449: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 07:35:49.574848: This epoch took 88.251612 s

2022-07-28 07:35:49.599063: 
epoch:  406
2022-07-28 07:37:09.517064: train loss : -0.9206
2022-07-28 07:37:16.419549: validation loss: -0.3053
2022-07-28 07:37:16.446456: Average global foreground Dice: [0.7384]
2022-07-28 07:37:16.454401: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:37:17.746470: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 07:37:17.795449: This epoch took 88.163392 s

2022-07-28 07:37:17.807088: 
epoch:  407
2022-07-28 07:38:38.866170: train loss : -0.9037
2022-07-28 07:38:45.657988: validation loss: -0.4141
2022-07-28 07:38:45.682025: Average global foreground Dice: [0.7995]
2022-07-28 07:38:45.709064: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:38:47.987929: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 07:38:47.995868: This epoch took 90.145089 s

2022-07-28 07:38:47.998356: 
epoch:  408
2022-07-28 07:40:07.667323: train loss : -0.9164
2022-07-28 07:40:14.874080: validation loss: -0.3423
2022-07-28 07:40:14.920591: Average global foreground Dice: [0.7675]
2022-07-28 07:40:14.957123: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:40:16.429385: Suus1 maybe_update_lr lr: 2.2e-05
2022-07-28 07:40:16.445058: This epoch took 88.444498 s

2022-07-28 07:40:16.462497: 
epoch:  409
2022-07-28 07:41:36.072668: train loss : -0.9155
2022-07-28 07:41:43.709697: validation loss: -0.4401
2022-07-28 07:41:43.756974: Average global foreground Dice: [0.8337]
2022-07-28 07:41:43.786169: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:41:45.077774: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 07:41:45.102110: This epoch took 88.601095 s

2022-07-28 07:41:45.118230: 
epoch:  410
2022-07-28 07:43:04.688420: train loss : -0.9046
2022-07-28 07:43:11.792058: validation loss: -0.4003
2022-07-28 07:43:11.824643: Average global foreground Dice: [0.8272]
2022-07-28 07:43:11.857007: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:43:13.084602: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 07:43:13.112799: This epoch took 87.977720 s

2022-07-28 07:43:13.115959: 
epoch:  411
2022-07-28 07:44:32.435488: train loss : -0.9221
2022-07-28 07:44:39.584046: validation loss: -0.4080
2022-07-28 07:44:39.635812: Average global foreground Dice: [0.7855]
2022-07-28 07:44:39.665011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:44:40.722360: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 07:44:40.730409: This epoch took 87.594385 s

2022-07-28 07:44:40.734767: 
epoch:  412
2022-07-28 07:46:01.708333: train loss : -0.9180
2022-07-28 07:46:09.048148: validation loss: -0.2631
2022-07-28 07:46:09.107972: Average global foreground Dice: [0.7152]
2022-07-28 07:46:09.151206: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:46:10.541766: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 07:46:10.569197: This epoch took 89.831902 s

2022-07-28 07:46:10.607123: 
epoch:  413
2022-07-28 07:47:31.046702: train loss : -0.9045
2022-07-28 07:47:38.421363: validation loss: -0.4115
2022-07-28 07:47:38.448435: Average global foreground Dice: [0.7986]
2022-07-28 07:47:38.468892: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:47:39.794733: Suus1 maybe_update_lr lr: 2.1e-05
2022-07-28 07:47:39.806312: This epoch took 89.145249 s

2022-07-28 07:47:39.824991: 
epoch:  414
2022-07-28 07:48:59.736556: train loss : -0.9052
2022-07-28 07:49:06.776608: validation loss: -0.4534
2022-07-28 07:49:06.829060: Average global foreground Dice: [0.792]
2022-07-28 07:49:06.882804: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:49:08.206614: Suus1 maybe_update_lr lr: 2e-05
2022-07-28 07:49:08.209107: This epoch took 88.374743 s

2022-07-28 07:49:08.211178: 
epoch:  415
2022-07-28 07:50:27.172179: train loss : -0.9236
2022-07-28 07:50:34.535882: validation loss: -0.3344
2022-07-28 07:50:34.573867: Average global foreground Dice: [0.755]
2022-07-28 07:50:34.622467: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:50:35.781407: Suus1 maybe_update_lr lr: 2e-05
2022-07-28 07:50:35.790748: This epoch took 87.576778 s

2022-07-28 07:50:35.793296: 
epoch:  416
2022-07-28 07:51:54.688536: train loss : -0.9160
2022-07-28 07:52:02.355583: validation loss: -0.3691
2022-07-28 07:52:02.360251: Average global foreground Dice: [0.8035]
2022-07-28 07:52:02.362797: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:52:03.618500: Suus1 maybe_update_lr lr: 2e-05
2022-07-28 07:52:03.646447: This epoch took 87.851019 s

2022-07-28 07:52:03.669447: 
epoch:  417
2022-07-28 07:53:20.202986: train loss : -0.9045
2022-07-28 07:53:27.536859: validation loss: -0.4107
2022-07-28 07:53:27.566164: Average global foreground Dice: [0.8065]
2022-07-28 07:53:27.598034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:53:28.702795: Suus1 maybe_update_lr lr: 2e-05
2022-07-28 07:53:28.716995: This epoch took 85.022777 s

2022-07-28 07:53:28.730100: 
epoch:  418
2022-07-28 07:54:46.005569: train loss : -0.9083
2022-07-28 07:54:53.332206: validation loss: -0.4293
2022-07-28 07:54:53.369664: Average global foreground Dice: [0.8456]
2022-07-28 07:54:53.397758: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:54:54.698822: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 07:54:54.731189: This epoch took 85.977114 s

2022-07-28 07:54:54.775032: 
epoch:  419
2022-07-28 07:56:13.211790: train loss : -0.9037
2022-07-28 07:56:19.548486: validation loss: -0.3609
2022-07-28 07:56:19.552768: Average global foreground Dice: [0.7547]
2022-07-28 07:56:19.555026: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:56:20.654838: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 07:56:20.676170: This epoch took 85.857138 s

2022-07-28 07:56:20.709155: 
epoch:  420
2022-07-28 07:57:39.225423: train loss : -0.9196
2022-07-28 07:57:46.435303: validation loss: -0.2682
2022-07-28 07:57:46.487950: Average global foreground Dice: [0.7377]
2022-07-28 07:57:46.520009: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:57:47.743045: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 07:57:47.769049: This epoch took 87.023561 s

2022-07-28 07:57:47.790995: 
epoch:  421
2022-07-28 07:59:07.345382: train loss : -0.9035
2022-07-28 07:59:14.456036: validation loss: -0.3843
2022-07-28 07:59:14.518167: Average global foreground Dice: [0.8009]
2022-07-28 07:59:14.533243: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 07:59:15.861664: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 07:59:15.922378: This epoch took 88.105378 s

2022-07-28 07:59:15.987258: 
epoch:  422
2022-07-28 08:00:33.814186: train loss : -0.9068
2022-07-28 08:00:40.996080: validation loss: -0.4847
2022-07-28 08:00:41.020087: Average global foreground Dice: [0.8536]
2022-07-28 08:00:41.042120: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:00:42.357163: Suus1 maybe_update_lr lr: 1.9e-05
2022-07-28 08:00:42.403562: This epoch took 86.357508 s

2022-07-28 08:00:42.436020: 
epoch:  423
2022-07-28 08:02:01.308425: train loss : -0.9097
2022-07-28 08:02:08.928397: validation loss: -0.4453
2022-07-28 08:02:08.949071: Average global foreground Dice: [0.8273]
2022-07-28 08:02:08.966056: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:02:10.147985: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-28 08:02:10.168691: This epoch took 87.699224 s

2022-07-28 08:02:10.171196: 
epoch:  424
2022-07-28 08:03:29.471113: train loss : -0.9099
2022-07-28 08:03:37.814537: validation loss: -0.3912
2022-07-28 08:03:37.845595: Average global foreground Dice: [0.7907]
2022-07-28 08:03:37.864017: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:03:39.088639: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-28 08:03:39.125087: This epoch took 88.951551 s

2022-07-28 08:03:39.157007: 
epoch:  425
2022-07-28 08:04:55.558735: train loss : -0.9090
2022-07-28 08:05:02.197553: validation loss: -0.4126
2022-07-28 08:05:02.260804: Average global foreground Dice: [0.8197]
2022-07-28 08:05:02.287178: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:05:03.708699: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-28 08:05:03.723120: This epoch took 84.533078 s

2022-07-28 08:05:03.730142: 
epoch:  426
2022-07-28 08:06:24.171208: train loss : -0.9158
2022-07-28 08:06:30.828445: validation loss: -0.4011
2022-07-28 08:06:30.833694: Average global foreground Dice: [0.8182]
2022-07-28 08:06:30.835684: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:06:31.953396: Suus1 maybe_update_lr lr: 1.8e-05
2022-07-28 08:06:31.959606: This epoch took 88.222496 s

2022-07-28 08:06:31.965250: 
epoch:  427
2022-07-28 08:07:51.098691: train loss : -0.9120
2022-07-28 08:07:57.758597: validation loss: -0.4461
2022-07-28 08:07:57.787638: Average global foreground Dice: [0.7782]
2022-07-28 08:07:57.813935: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:07:59.022719: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 08:07:59.039271: This epoch took 87.038795 s

2022-07-28 08:07:59.063745: 
epoch:  428
2022-07-28 08:09:19.212455: train loss : -0.9108
2022-07-28 08:09:26.176097: validation loss: -0.4107
2022-07-28 08:09:26.209812: Average global foreground Dice: [0.788]
2022-07-28 08:09:26.225013: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:09:27.444012: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 08:09:27.466689: This epoch took 88.372370 s

2022-07-28 08:09:27.490293: 
epoch:  429
2022-07-28 08:10:49.586018: train loss : -0.9150
2022-07-28 08:10:57.563555: validation loss: -0.3144
2022-07-28 08:10:57.592817: Average global foreground Dice: [0.7844]
2022-07-28 08:10:57.601005: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:10:58.766113: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 08:10:58.769130: This epoch took 91.264107 s

2022-07-28 08:10:58.771324: 
epoch:  430
2022-07-28 08:12:17.758836: train loss : -0.9065
2022-07-28 08:12:24.502811: validation loss: -0.3957
2022-07-28 08:12:24.544951: Average global foreground Dice: [0.7996]
2022-07-28 08:12:24.559674: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:12:25.519804: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 08:12:25.563782: This epoch took 86.790201 s

2022-07-28 08:12:25.567934: 
epoch:  431
2022-07-28 08:13:45.669052: train loss : -0.9231
2022-07-28 08:13:51.619612: validation loss: -0.2524
2022-07-28 08:13:51.668032: Average global foreground Dice: [0.7321]
2022-07-28 08:13:51.690006: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:13:53.119401: Suus1 maybe_update_lr lr: 1.7e-05
2022-07-28 08:13:53.201262: This epoch took 87.620585 s

2022-07-28 08:13:53.281292: 
epoch:  432
2022-07-28 08:15:11.276754: train loss : -0.9115
2022-07-28 08:15:17.547017: validation loss: -0.3432
2022-07-28 08:15:17.571114: Average global foreground Dice: [0.7764]
2022-07-28 08:15:17.626130: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:15:18.909382: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 08:15:18.927037: This epoch took 85.596997 s

2022-07-28 08:15:18.938983: 
epoch:  433
2022-07-28 08:16:37.471800: train loss : -0.9182
2022-07-28 08:16:45.019157: validation loss: -0.3483
2022-07-28 08:16:45.036938: Average global foreground Dice: [0.7655]
2022-07-28 08:16:45.061149: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:16:46.462316: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 08:16:46.548651: This epoch took 87.594617 s

2022-07-28 08:16:46.568387: 
epoch:  434
2022-07-28 08:18:03.926265: train loss : -0.9183
2022-07-28 08:18:10.602145: validation loss: -0.4189
2022-07-28 08:18:10.641285: Average global foreground Dice: [0.8088]
2022-07-28 08:18:10.703287: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:18:11.861547: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 08:18:11.887462: This epoch took 85.298896 s

2022-07-28 08:18:11.913042: 
epoch:  435
2022-07-28 08:19:33.121042: train loss : -0.9148
2022-07-28 08:19:40.192743: validation loss: -0.3767
2022-07-28 08:19:40.217804: Average global foreground Dice: [0.809]
2022-07-28 08:19:40.248003: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:19:41.446984: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 08:19:41.455769: This epoch took 89.484573 s

2022-07-28 08:19:41.461999: 
epoch:  436
2022-07-28 08:21:01.938693: train loss : -0.9212
2022-07-28 08:21:09.123260: validation loss: -0.3621
2022-07-28 08:21:09.193525: Average global foreground Dice: [0.8075]
2022-07-28 08:21:09.214505: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:21:10.501714: Suus1 maybe_update_lr lr: 1.6e-05
2022-07-28 08:21:10.537320: This epoch took 89.072245 s

2022-07-28 08:21:10.558981: 
epoch:  437
2022-07-28 08:22:31.642184: train loss : -0.9133
2022-07-28 08:22:39.648747: validation loss: -0.4171
2022-07-28 08:22:39.689590: Average global foreground Dice: [0.8343]
2022-07-28 08:22:39.693481: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:22:41.027286: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-28 08:22:41.031300: This epoch took 90.450264 s

2022-07-28 08:22:41.033645: 
epoch:  438
2022-07-28 08:24:02.412808: train loss : -0.9240
2022-07-28 08:24:10.367816: validation loss: -0.5488
2022-07-28 08:24:10.397246: Average global foreground Dice: [0.8684]
2022-07-28 08:24:10.413537: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:24:11.580760: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-28 08:24:11.612557: saving best epoch checkpoint...
2022-07-28 08:24:11.926189: saving checkpoint...
2022-07-28 08:24:17.933148: done, saving took 6.30 seconds
2022-07-28 08:24:17.945527: This epoch took 96.878463 s

2022-07-28 08:24:17.947429: 
epoch:  439
2022-07-28 08:25:34.543894: train loss : -0.9130
2022-07-28 08:25:42.238693: validation loss: -0.3565
2022-07-28 08:25:42.249012: Average global foreground Dice: [0.7953]
2022-07-28 08:25:42.282127: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:25:43.752408: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-28 08:25:43.754934: This epoch took 85.805794 s

2022-07-28 08:25:43.767576: 
epoch:  440
2022-07-28 08:27:02.860827: train loss : -0.9112
2022-07-28 08:27:11.769400: validation loss: -0.1812
2022-07-28 08:27:11.798877: Average global foreground Dice: [0.6146]
2022-07-28 08:27:11.848498: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:27:12.899011: Suus1 maybe_update_lr lr: 1.5e-05
2022-07-28 08:27:12.944122: This epoch took 89.134095 s

2022-07-28 08:27:12.948926: 
epoch:  441
2022-07-28 08:28:32.056132: train loss : -0.9254
2022-07-28 08:28:40.121859: validation loss: -0.3639
2022-07-28 08:28:40.146191: Average global foreground Dice: [0.7913]
2022-07-28 08:28:40.168186: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:28:41.510813: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-28 08:28:41.520237: This epoch took 88.541218 s

2022-07-28 08:28:41.553687: 
epoch:  442
2022-07-28 08:30:00.803294: train loss : -0.9180
2022-07-28 08:30:09.447212: validation loss: -0.3716
2022-07-28 08:30:09.486964: Average global foreground Dice: [0.798]
2022-07-28 08:30:09.489734: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:30:10.918175: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-28 08:30:10.988356: This epoch took 89.393320 s

2022-07-28 08:30:11.020184: 
epoch:  443
2022-07-28 08:31:28.638450: train loss : -0.9106
2022-07-28 08:31:38.102091: validation loss: -0.3965
2022-07-28 08:31:38.114757: Average global foreground Dice: [0.8091]
2022-07-28 08:31:38.121062: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:31:39.203660: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-28 08:31:39.211107: This epoch took 88.137041 s

2022-07-28 08:31:39.247048: 
epoch:  444
2022-07-28 08:32:55.481121: train loss : -0.9091
2022-07-28 08:33:03.312441: validation loss: -0.4149
2022-07-28 08:33:03.363727: Average global foreground Dice: [0.8279]
2022-07-28 08:33:03.376441: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:33:04.521169: Suus1 maybe_update_lr lr: 1.4e-05
2022-07-28 08:33:04.558195: This epoch took 85.272193 s

2022-07-28 08:33:04.611021: 
epoch:  445
2022-07-28 08:34:14.638054: train loss : -0.9089
2022-07-28 08:34:19.917459: validation loss: -0.2713
2022-07-28 08:34:19.947694: Average global foreground Dice: [0.7308]
2022-07-28 08:34:19.949927: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:34:20.678535: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 08:34:20.680497: This epoch took 76.003493 s

2022-07-28 08:34:20.682292: 
epoch:  446
2022-07-28 08:35:29.143806: train loss : -0.9137
2022-07-28 08:35:34.664417: validation loss: -0.4101
2022-07-28 08:35:34.680709: Average global foreground Dice: [0.8213]
2022-07-28 08:35:34.689382: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:35:35.316671: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 08:35:35.318877: This epoch took 74.634733 s

2022-07-28 08:35:35.320833: 
epoch:  447
2022-07-28 08:36:43.826159: train loss : -0.9154
2022-07-28 08:36:50.087375: validation loss: -0.4135
2022-07-28 08:36:50.112831: Average global foreground Dice: [0.8297]
2022-07-28 08:36:50.140996: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:36:51.005165: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 08:36:51.007336: This epoch took 75.684628 s

2022-07-28 08:36:51.009103: 
epoch:  448
2022-07-28 08:38:00.392572: train loss : -0.9059
2022-07-28 08:38:06.524072: validation loss: -0.3111
2022-07-28 08:38:06.550000: Average global foreground Dice: [0.7543]
2022-07-28 08:38:06.562986: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:38:07.517608: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 08:38:07.520782: This epoch took 76.509837 s

2022-07-28 08:38:07.523589: 
epoch:  449
2022-07-28 08:39:19.227843: train loss : -0.9130
2022-07-28 08:39:25.575963: validation loss: -0.2710
2022-07-28 08:39:25.605521: Average global foreground Dice: [0.6786]
2022-07-28 08:39:25.643070: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:39:26.780030: Suus1 maybe_update_lr lr: 1.3e-05
2022-07-28 08:39:26.782588: saving scheduled checkpoint file...
2022-07-28 08:39:27.002162: saving checkpoint...
2022-07-28 08:39:31.556815: done, saving took 4.77 seconds
2022-07-28 08:39:31.567879: done
2022-07-28 08:39:31.570349: This epoch took 84.044775 s

2022-07-28 08:39:31.572119: 
epoch:  450
2022-07-28 08:40:43.943364: train loss : -0.9183
2022-07-28 08:40:49.766522: validation loss: -0.2262
2022-07-28 08:40:49.807588: Average global foreground Dice: [0.6687]
2022-07-28 08:40:49.830596: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:40:51.038435: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-28 08:40:51.041322: This epoch took 79.467321 s

2022-07-28 08:40:51.043505: 
epoch:  451
2022-07-28 08:42:02.150648: train loss : -0.9247
2022-07-28 08:42:09.157307: validation loss: -0.4137
2022-07-28 08:42:09.193848: Average global foreground Dice: [0.8]
2022-07-28 08:42:09.209103: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:42:10.327825: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-28 08:42:10.385081: This epoch took 79.338655 s

2022-07-28 08:42:10.444752: 
epoch:  452
2022-07-28 08:43:22.181300: train loss : -0.9104
2022-07-28 08:43:28.077613: validation loss: -0.3496
2022-07-28 08:43:28.081205: Average global foreground Dice: [0.7669]
2022-07-28 08:43:28.083840: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:43:28.724989: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-28 08:43:28.727007: This epoch took 78.245842 s

2022-07-28 08:43:28.729182: 
epoch:  453
2022-07-28 08:44:44.685014: train loss : -0.9227
2022-07-28 08:44:50.641779: validation loss: -0.3902
2022-07-28 08:44:50.678966: Average global foreground Dice: [0.7928]
2022-07-28 08:44:50.699653: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:44:52.092717: Suus1 maybe_update_lr lr: 1.2e-05
2022-07-28 08:44:52.111189: This epoch took 83.380154 s

2022-07-28 08:44:52.137976: 
epoch:  454
2022-07-28 08:46:06.070477: train loss : -0.9211
2022-07-28 08:46:11.359978: validation loss: -0.4285
2022-07-28 08:46:11.363553: Average global foreground Dice: [0.815]
2022-07-28 08:46:11.366063: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:46:11.956045: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 08:46:11.958139: This epoch took 79.802564 s

2022-07-28 08:46:11.960014: 
epoch:  455
2022-07-28 08:47:21.444688: train loss : -0.9254
2022-07-28 08:47:26.700215: validation loss: -0.3714
2022-07-28 08:47:26.705393: Average global foreground Dice: [0.7942]
2022-07-28 08:47:26.732966: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:47:27.467745: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 08:47:27.470062: This epoch took 75.508298 s

2022-07-28 08:47:27.471962: 
epoch:  456
2022-07-28 08:48:36.252011: train loss : -0.9204
2022-07-28 08:48:41.447001: validation loss: -0.2954
2022-07-28 08:48:41.450141: Average global foreground Dice: [0.7404]
2022-07-28 08:48:41.478920: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:48:42.455436: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 08:48:42.457552: This epoch took 74.983694 s

2022-07-28 08:48:42.459465: 
epoch:  457
2022-07-28 08:49:50.096635: train loss : -0.9208
2022-07-28 08:49:55.647902: validation loss: -0.3724
2022-07-28 08:49:55.681121: Average global foreground Dice: [0.7893]
2022-07-28 08:49:55.723097: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:49:56.572173: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 08:49:56.574907: This epoch took 74.113505 s

2022-07-28 08:49:56.576790: 
epoch:  458
2022-07-28 08:51:05.055021: train loss : -0.9166
2022-07-28 08:51:10.543185: validation loss: -0.3503
2022-07-28 08:51:10.546207: Average global foreground Dice: [0.7674]
2022-07-28 08:51:10.548358: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:51:11.109823: Suus1 maybe_update_lr lr: 1.1e-05
2022-07-28 08:51:11.112186: This epoch took 74.533577 s

2022-07-28 08:51:11.114053: 
epoch:  459
2022-07-28 08:52:18.749057: train loss : -0.9237
2022-07-28 08:52:24.706473: validation loss: -0.5172
2022-07-28 08:52:24.727489: Average global foreground Dice: [0.8544]
2022-07-28 08:52:24.754011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:52:25.800781: Suus1 maybe_update_lr lr: 1e-05
2022-07-28 08:52:25.802969: This epoch took 74.687145 s

2022-07-28 08:52:25.804765: 
epoch:  460
2022-07-28 08:53:33.715510: train loss : -0.9214
2022-07-28 08:53:39.124954: validation loss: -0.4385
2022-07-28 08:53:39.154106: Average global foreground Dice: [0.7741]
2022-07-28 08:53:39.156846: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:53:39.882420: Suus1 maybe_update_lr lr: 1e-05
2022-07-28 08:53:39.884432: This epoch took 74.077784 s

2022-07-28 08:53:39.886358: 
epoch:  461
2022-07-28 08:54:47.462183: train loss : -0.9259
2022-07-28 08:54:52.850996: validation loss: -0.4419
2022-07-28 08:54:52.860615: Average global foreground Dice: [0.8214]
2022-07-28 08:54:52.862688: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:54:53.599767: Suus1 maybe_update_lr lr: 1e-05
2022-07-28 08:54:53.602142: This epoch took 73.713974 s

2022-07-28 08:54:53.603991: 
epoch:  462
2022-07-28 08:56:02.018899: train loss : -0.9068
2022-07-28 08:56:08.198022: validation loss: -0.4793
2022-07-28 08:56:08.206986: Average global foreground Dice: [0.8307]
2022-07-28 08:56:08.214505: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:56:09.131944: Suus1 maybe_update_lr lr: 1e-05
2022-07-28 08:56:09.134590: This epoch took 75.528879 s

2022-07-28 08:56:09.136832: 
epoch:  463
2022-07-28 08:57:17.280067: train loss : -0.9154
2022-07-28 08:57:22.688874: validation loss: -0.4271
2022-07-28 08:57:22.724723: Average global foreground Dice: [0.8431]
2022-07-28 08:57:22.753515: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:57:23.597040: Suus1 maybe_update_lr lr: 9e-06
2022-07-28 08:57:23.599215: This epoch took 74.460265 s

2022-07-28 08:57:23.601150: 
epoch:  464
2022-07-28 08:58:32.284427: train loss : -0.9174
2022-07-28 08:58:37.607643: validation loss: -0.4750
2022-07-28 08:58:37.623508: Average global foreground Dice: [0.8244]
2022-07-28 08:58:37.643210: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:58:38.275012: Suus1 maybe_update_lr lr: 9e-06
2022-07-28 08:58:38.277162: This epoch took 74.674084 s

2022-07-28 08:58:38.278991: 
epoch:  465
2022-07-28 08:59:46.149880: train loss : -0.9022
2022-07-28 08:59:51.987984: validation loss: -0.4824
2022-07-28 08:59:52.023148: Average global foreground Dice: [0.7959]
2022-07-28 08:59:52.044293: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 08:59:52.777106: Suus1 maybe_update_lr lr: 9e-06
2022-07-28 08:59:52.779384: This epoch took 74.498623 s

2022-07-28 08:59:52.781349: 
epoch:  466
2022-07-28 09:01:00.955381: train loss : -0.9137
2022-07-28 09:01:06.456310: validation loss: -0.4819
2022-07-28 09:01:06.470993: Average global foreground Dice: [0.8576]
2022-07-28 09:01:06.480839: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:01:07.265004: Suus1 maybe_update_lr lr: 9e-06
2022-07-28 09:01:07.267034: saving best epoch checkpoint...
2022-07-28 09:01:07.426802: saving checkpoint...
2022-07-28 09:01:11.825348: done, saving took 4.56 seconds
2022-07-28 09:01:11.838232: This epoch took 79.054961 s

2022-07-28 09:01:11.840383: 
epoch:  467
2022-07-28 09:02:20.079120: train loss : -0.9157
2022-07-28 09:02:25.680141: validation loss: -0.4272
2022-07-28 09:02:25.705679: Average global foreground Dice: [0.8121]
2022-07-28 09:02:25.730010: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:02:26.659182: Suus1 maybe_update_lr lr: 8e-06
2022-07-28 09:02:26.661278: saving best epoch checkpoint...
2022-07-28 09:02:26.782538: saving checkpoint...
2022-07-28 09:02:31.139351: done, saving took 4.48 seconds
2022-07-28 09:02:31.146845: This epoch took 79.304641 s

2022-07-28 09:02:31.148751: 
epoch:  468
2022-07-28 09:03:40.213765: train loss : -0.9221
2022-07-28 09:03:45.551945: validation loss: -0.4293
2022-07-28 09:03:45.585912: Average global foreground Dice: [0.8023]
2022-07-28 09:03:45.601699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:03:46.333632: Suus1 maybe_update_lr lr: 8e-06
2022-07-28 09:03:46.335757: This epoch took 75.185190 s

2022-07-28 09:03:46.337566: 
epoch:  469
2022-07-28 09:04:54.064168: train loss : -0.9095
2022-07-28 09:04:59.509799: validation loss: -0.3499
2022-07-28 09:04:59.533245: Average global foreground Dice: [0.7517]
2022-07-28 09:04:59.562277: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:05:00.411899: Suus1 maybe_update_lr lr: 8e-06
2022-07-28 09:05:00.414033: This epoch took 74.074584 s

2022-07-28 09:05:00.415939: 
epoch:  470
2022-07-28 09:06:08.148182: train loss : -0.9210
2022-07-28 09:06:13.684388: validation loss: -0.4024
2022-07-28 09:06:13.710630: Average global foreground Dice: [0.8189]
2022-07-28 09:06:13.714976: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:06:14.439903: Suus1 maybe_update_lr lr: 8e-06
2022-07-28 09:06:14.442386: This epoch took 74.025089 s

2022-07-28 09:06:14.444532: 
epoch:  471
2022-07-28 09:07:23.446619: train loss : -0.9037
2022-07-28 09:07:29.033983: validation loss: -0.3429
2022-07-28 09:07:29.054680: Average global foreground Dice: [0.7391]
2022-07-28 09:07:29.064544: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:07:29.893755: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 09:07:29.895861: This epoch took 75.448539 s

2022-07-28 09:07:29.897693: 
epoch:  472
2022-07-28 09:08:37.665405: train loss : -0.9222
2022-07-28 09:08:43.099617: validation loss: -0.4324
2022-07-28 09:08:43.118650: Average global foreground Dice: [0.8123]
2022-07-28 09:08:43.123568: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:08:43.732317: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 09:08:43.734512: This epoch took 73.834997 s

2022-07-28 09:08:43.736602: 
epoch:  473
2022-07-28 09:09:51.387943: train loss : -0.9157
2022-07-28 09:09:56.682072: validation loss: -0.3683
2022-07-28 09:09:56.714744: Average global foreground Dice: [0.7686]
2022-07-28 09:09:56.735253: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:09:57.637493: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 09:09:57.639935: This epoch took 73.901516 s

2022-07-28 09:09:57.641948: 
epoch:  474
2022-07-28 09:11:07.290452: train loss : -0.9256
2022-07-28 09:11:12.911085: validation loss: -0.4953
2022-07-28 09:11:12.952229: Average global foreground Dice: [0.8448]
2022-07-28 09:11:12.964020: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:11:13.849315: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 09:11:13.851971: This epoch took 76.208127 s

2022-07-28 09:11:13.853914: 
epoch:  475
2022-07-28 09:12:24.165344: train loss : -0.9169
2022-07-28 09:12:29.634369: validation loss: -0.3316
2022-07-28 09:12:29.666491: Average global foreground Dice: [0.7483]
2022-07-28 09:12:29.687832: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:12:30.515833: Suus1 maybe_update_lr lr: 7e-06
2022-07-28 09:12:30.517933: This epoch took 76.662161 s

2022-07-28 09:12:30.519908: 
epoch:  476
2022-07-28 09:13:39.128257: train loss : -0.9325
2022-07-28 09:13:44.809366: validation loss: -0.4556
2022-07-28 09:13:44.829286: Average global foreground Dice: [0.8342]
2022-07-28 09:13:44.836274: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:13:45.682221: Suus1 maybe_update_lr lr: 6e-06
2022-07-28 09:13:45.684453: This epoch took 75.163200 s

2022-07-28 09:13:45.686394: 
epoch:  477
2022-07-28 09:14:54.254241: train loss : -0.9163
2022-07-28 09:15:00.015333: validation loss: -0.3111
2022-07-28 09:15:00.039700: Average global foreground Dice: [0.7476]
2022-07-28 09:15:00.044043: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:15:00.980468: Suus1 maybe_update_lr lr: 6e-06
2022-07-28 09:15:00.983451: This epoch took 75.295242 s

2022-07-28 09:15:00.985641: 
epoch:  478
2022-07-28 09:16:09.298390: train loss : -0.9190
2022-07-28 09:16:15.075426: validation loss: -0.4651
2022-07-28 09:16:15.087446: Average global foreground Dice: [0.8299]
2022-07-28 09:16:15.106762: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:16:15.949628: Suus1 maybe_update_lr lr: 6e-06
2022-07-28 09:16:15.951804: This epoch took 74.964087 s

2022-07-28 09:16:15.953631: 
epoch:  479
2022-07-28 09:17:24.945639: train loss : -0.9091
2022-07-28 09:17:30.454979: validation loss: -0.3898
2022-07-28 09:17:30.484378: Average global foreground Dice: [0.8086]
2022-07-28 09:17:30.486973: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:17:31.092558: Suus1 maybe_update_lr lr: 6e-06
2022-07-28 09:17:31.094672: This epoch took 75.139222 s

2022-07-28 09:17:31.096586: 
epoch:  480
2022-07-28 09:18:38.812497: train loss : -0.9229
2022-07-28 09:18:44.271210: validation loss: -0.3115
2022-07-28 09:18:44.274557: Average global foreground Dice: [0.7488]
2022-07-28 09:18:44.277107: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:18:45.250040: Suus1 maybe_update_lr lr: 5e-06
2022-07-28 09:18:45.252177: This epoch took 74.153762 s

2022-07-28 09:18:45.254095: 
epoch:  481
2022-07-28 09:19:55.308806: train loss : -0.9170
2022-07-28 09:20:01.214382: validation loss: -0.4483
2022-07-28 09:20:01.226782: Average global foreground Dice: [0.8346]
2022-07-28 09:20:01.229221: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:20:01.873900: Suus1 maybe_update_lr lr: 5e-06
2022-07-28 09:20:01.876279: This epoch took 76.620162 s

2022-07-28 09:20:01.878125: 
epoch:  482
2022-07-28 09:21:10.125414: train loss : -0.9274
2022-07-28 09:21:15.673631: validation loss: -0.5091
2022-07-28 09:21:15.695228: Average global foreground Dice: [0.855]
2022-07-28 09:21:15.697680: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:21:16.690377: Suus1 maybe_update_lr lr: 5e-06
2022-07-28 09:21:16.693015: This epoch took 74.813005 s

2022-07-28 09:21:16.695207: 
epoch:  483
2022-07-28 09:22:25.416169: train loss : -0.9243
2022-07-28 09:22:30.914218: validation loss: -0.2905
2022-07-28 09:22:30.945458: Average global foreground Dice: [0.7516]
2022-07-28 09:22:30.958950: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:22:31.707021: Suus1 maybe_update_lr lr: 5e-06
2022-07-28 09:22:31.709137: This epoch took 75.012003 s

2022-07-28 09:22:31.711039: 
epoch:  484
2022-07-28 09:23:40.539327: train loss : -0.9190
2022-07-28 09:23:45.933060: validation loss: -0.2796
2022-07-28 09:23:45.937698: Average global foreground Dice: [0.7517]
2022-07-28 09:23:45.941548: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:23:47.050587: Suus1 maybe_update_lr lr: 4e-06
2022-07-28 09:23:47.052744: This epoch took 75.339791 s

2022-07-28 09:23:47.054615: 
epoch:  485
2022-07-28 09:24:54.671513: train loss : -0.9254
2022-07-28 09:24:59.970045: validation loss: -0.5220
2022-07-28 09:24:59.992760: Average global foreground Dice: [0.8456]
2022-07-28 09:25:00.001859: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:25:00.578549: Suus1 maybe_update_lr lr: 4e-06
2022-07-28 09:25:00.581075: This epoch took 73.524658 s

2022-07-28 09:25:00.582951: 
epoch:  486
2022-07-28 09:26:08.684540: train loss : -0.9190
2022-07-28 09:26:14.084328: validation loss: -0.3628
2022-07-28 09:26:14.086839: Average global foreground Dice: [0.8201]
2022-07-28 09:26:14.088838: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:26:14.809530: Suus1 maybe_update_lr lr: 4e-06
2022-07-28 09:26:14.811736: This epoch took 74.226910 s

2022-07-28 09:26:14.813587: 
epoch:  487
2022-07-28 09:27:23.640404: train loss : -0.9268
2022-07-28 09:27:29.277980: validation loss: -0.3583
2022-07-28 09:27:29.301977: Average global foreground Dice: [0.773]
2022-07-28 09:27:29.321387: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:27:30.015965: Suus1 maybe_update_lr lr: 3e-06
2022-07-28 09:27:30.018091: This epoch took 75.202640 s

2022-07-28 09:27:30.020009: 
epoch:  488
2022-07-28 09:28:37.757935: train loss : -0.9149
2022-07-28 09:28:43.424285: validation loss: -0.3809
2022-07-28 09:28:43.464129: Average global foreground Dice: [0.8139]
2022-07-28 09:28:43.491152: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:28:44.317153: Suus1 maybe_update_lr lr: 3e-06
2022-07-28 09:28:44.319144: This epoch took 74.297316 s

2022-07-28 09:28:44.320971: 
epoch:  489
2022-07-28 09:29:53.842683: train loss : -0.9182
2022-07-28 09:29:59.682743: validation loss: -0.4924
2022-07-28 09:29:59.713040: Average global foreground Dice: [0.8296]
2022-07-28 09:29:59.731654: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:30:00.541493: Suus1 maybe_update_lr lr: 3e-06
2022-07-28 09:30:00.543673: This epoch took 76.220903 s

2022-07-28 09:30:00.545571: 
epoch:  490
2022-07-28 09:31:09.303017: train loss : -0.9189
2022-07-28 09:31:14.890009: validation loss: -0.4259
2022-07-28 09:31:14.911144: Average global foreground Dice: [0.8299]
2022-07-28 09:31:14.924004: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:31:15.719782: Suus1 maybe_update_lr lr: 3e-06
2022-07-28 09:31:15.721879: saving best epoch checkpoint...
2022-07-28 09:31:15.870250: saving checkpoint...
2022-07-28 09:31:20.227521: done, saving took 4.50 seconds
2022-07-28 09:31:20.239461: This epoch took 79.692120 s

2022-07-28 09:31:20.241667: 
epoch:  491
2022-07-28 09:32:30.283715: train loss : -0.9221
2022-07-28 09:32:35.797227: validation loss: -0.2560
2022-07-28 09:32:35.814978: Average global foreground Dice: [0.7516]
2022-07-28 09:32:35.836977: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:32:36.592397: Suus1 maybe_update_lr lr: 2e-06
2022-07-28 09:32:36.594670: This epoch took 76.350805 s

2022-07-28 09:32:36.596510: 
epoch:  492
2022-07-28 09:33:44.319507: train loss : -0.9144
2022-07-28 09:33:49.961553: validation loss: -0.3752
2022-07-28 09:33:49.999567: Average global foreground Dice: [0.7913]
2022-07-28 09:33:50.024991: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:33:50.627900: Suus1 maybe_update_lr lr: 2e-06
2022-07-28 09:33:50.630122: This epoch took 74.031646 s

2022-07-28 09:33:50.632176: 
epoch:  493
2022-07-28 09:34:59.814790: train loss : -0.9127
2022-07-28 09:35:05.130356: validation loss: -0.5111
2022-07-28 09:35:05.143932: Average global foreground Dice: [0.819]
2022-07-28 09:35:05.171485: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:35:05.849224: Suus1 maybe_update_lr lr: 2e-06
2022-07-28 09:35:05.851541: This epoch took 75.217505 s

2022-07-28 09:35:05.853333: 
epoch:  494
2022-07-28 09:36:14.121387: train loss : -0.9087
2022-07-28 09:36:19.423707: validation loss: -0.3145
2022-07-28 09:36:19.432019: Average global foreground Dice: [0.7517]
2022-07-28 09:36:19.442423: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:36:20.105582: Suus1 maybe_update_lr lr: 2e-06
2022-07-28 09:36:20.107752: This epoch took 74.252599 s

2022-07-28 09:36:20.109496: 
epoch:  495
2022-07-28 09:37:28.071865: train loss : -0.9207
2022-07-28 09:37:33.854289: validation loss: -0.3164
2022-07-28 09:37:33.885219: Average global foreground Dice: [0.7293]
2022-07-28 09:37:33.887529: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:37:34.484852: Suus1 maybe_update_lr lr: 1e-06
2022-07-28 09:37:34.487025: This epoch took 74.375711 s

2022-07-28 09:37:34.488792: 
epoch:  496
2022-07-28 09:38:43.646638: train loss : -0.9300
2022-07-28 09:38:49.604848: validation loss: -0.4696
2022-07-28 09:38:49.635484: Average global foreground Dice: [0.8409]
2022-07-28 09:38:49.643967: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:38:50.238832: Suus1 maybe_update_lr lr: 1e-06
2022-07-28 09:38:50.241098: This epoch took 75.750570 s

2022-07-28 09:38:50.243189: 
epoch:  497
2022-07-28 09:39:58.051261: train loss : -0.9277
2022-07-28 09:40:03.378708: validation loss: -0.4870
2022-07-28 09:40:03.393800: Average global foreground Dice: [0.8329]
2022-07-28 09:40:03.420395: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:40:04.399515: Suus1 maybe_update_lr lr: 1e-06
2022-07-28 09:40:04.401967: This epoch took 74.156849 s

2022-07-28 09:40:04.403788: 
epoch:  498
2022-07-28 09:41:12.575433: train loss : -0.9182
2022-07-28 09:41:17.865780: validation loss: -0.4247
2022-07-28 09:41:17.876059: Average global foreground Dice: [0.803]
2022-07-28 09:41:17.889928: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:41:18.574456: Suus1 maybe_update_lr lr: 0.0
2022-07-28 09:41:18.576787: This epoch took 74.171214 s

2022-07-28 09:41:18.578546: 
epoch:  499
2022-07-28 09:42:26.755645: train loss : -0.9122
2022-07-28 09:42:32.438442: validation loss: -0.3980
2022-07-28 09:42:32.467946: Average global foreground Dice: [0.8056]
2022-07-28 09:42:32.472283: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-07-28 09:42:33.424294: Suus1 maybe_update_lr lr: 0.0
2022-07-28 09:42:33.426733: saving scheduled checkpoint file...
2022-07-28 09:42:33.599077: saving checkpoint...
2022-07-28 09:42:38.024470: done, saving took 4.60 seconds
2022-07-28 09:42:38.036904: done
2022-07-28 09:42:38.040310: This epoch took 79.459977 s

2022-07-28 09:42:38.175377: saving checkpoint...
2022-07-28 09:42:42.337326: done, saving took 4.29 seconds
panc_0002 (2, 139, 481, 481)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
computing Gaussian
done
prediction done
suus panc_0002 transposed
suus panc_0002 not saving softmax
suus panc_0002 voeg toe aan pred_gt tuples voor later
panc_0003 (2, 198, 599, 599)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
suus panc_0003 transposed
suus panc_0003 not saving softmax
suus panc_0003 voeg toe aan pred_gt tuples voor later
panc_0005 (2, 117, 604, 604)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
suus panc_0005 transposed
suus panc_0005 not saving softmax
suus panc_0005 voeg toe aan pred_gt tuples voor later
panc_0006 (2, 131, 468, 468)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
suus panc_0006 transposed
suus panc_0006 not saving softmax
suus panc_0006 voeg toe aan pred_gt tuples voor later
panc_0021 (2, 143, 456, 456)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
suus panc_0021 transposed
suus panc_0021 not saving softmax
suus panc_0021 voeg toe aan pred_gt tuples voor later
2022-07-28 09:47:05.559198: finished prediction
2022-07-28 09:47:05.562548: evaluation of raw predictions
2022-07-28 09:47:11.725136: determining postprocessing
Foreground vs background
before: 0.006783458799476857
after:  0.0
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0002.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0003.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0005.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0006.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0021.nii.gz
done
Done training all the folds! Now start the same command but with continue option, to generate log files


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='521', fold='0', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 09:47:29.961694: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 09:47:30.042570: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 09:47:30.046781: The split file contains 5 splits.
2022-07-28 09:47:30.048748: Desired fold for training: 0
2022-07-28 09:47:30.050589: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 09:47:32.122203: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 09:48:04.432294: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 09:48:17.736854: Unable to plot network architecture:
2022-07-28 09:48:17.739161: local variable 'g' referenced before assignment
2022-07-28 09:48:17.741076: 
printing the network instead:

2022-07-28 09:48:17.742876: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 09:48:17.748206: 

2022-07-28 09:48:18.001540: saving checkpoint...
2022-07-28 09:48:22.720588: done, saving took 4.97 seconds
suus panc_0001 voeg toe aan pred_gt tuples voor later
suus panc_0004 voeg toe aan pred_gt tuples voor later
suus panc_0024 voeg toe aan pred_gt tuples voor later
suus panc_0027 voeg toe aan pred_gt tuples voor later
suus panc_0031 voeg toe aan pred_gt tuples voor later
suus panc_0037 voeg toe aan pred_gt tuples voor later
2022-07-28 09:48:23.040759: finished prediction
2022-07-28 09:48:23.042952: evaluation of raw predictions
2022-07-28 09:48:26.021722: determining postprocessing
Foreground vs background
before: 0.02121677984188196
after:  0.00015500876805743294
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='521', fold='1', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 09:48:41.049089: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 09:48:41.099321: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 09:48:41.102882: The split file contains 5 splits.
2022-07-28 09:48:41.104677: Desired fold for training: 1
2022-07-28 09:48:41.106488: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 09:48:43.128373: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 09:49:14.380551: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 09:49:25.563356: Unable to plot network architecture:
2022-07-28 09:49:25.583991: local variable 'g' referenced before assignment
2022-07-28 09:49:25.605128: 
printing the network instead:

2022-07-28 09:49:25.607396: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 09:49:25.615988: 

2022-07-28 09:49:26.514035: saving checkpoint...
2022-07-28 09:49:33.259022: done, saving took 7.63 seconds
suus panc_0007 voeg toe aan pred_gt tuples voor later
suus panc_0009 voeg toe aan pred_gt tuples voor later
suus panc_0010 voeg toe aan pred_gt tuples voor later
suus panc_0032 voeg toe aan pred_gt tuples voor later
suus panc_0034 voeg toe aan pred_gt tuples voor later
suus panc_0035 voeg toe aan pred_gt tuples voor later
2022-07-28 09:49:33.756676: finished prediction
2022-07-28 09:49:33.758821: evaluation of raw predictions
2022-07-28 09:49:36.661808: determining postprocessing
Foreground vs background
before: 0.15891951935136564
after:  0.15891951935136564
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='521', fold='2', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 09:49:52.471561: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 09:49:52.519458: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 09:49:52.522753: The split file contains 5 splits.
2022-07-28 09:49:52.524518: Desired fold for training: 2
2022-07-28 09:49:52.526321: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 09:49:54.473537: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 09:50:31.857414: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 09:50:43.717744: Unable to plot network architecture:
2022-07-28 09:50:43.729939: local variable 'g' referenced before assignment
2022-07-28 09:50:43.751946: 
printing the network instead:

2022-07-28 09:50:43.773944: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 09:50:43.804607: 

2022-07-28 09:50:44.407779: saving checkpoint...
2022-07-28 09:50:49.642108: done, saving took 5.81 seconds
suus panc_0022 voeg toe aan pred_gt tuples voor later
suus panc_0023 voeg toe aan pred_gt tuples voor later
suus panc_0025 voeg toe aan pred_gt tuples voor later
suus panc_0028 voeg toe aan pred_gt tuples voor later
suus panc_0030 voeg toe aan pred_gt tuples voor later
suus panc_0033 voeg toe aan pred_gt tuples voor later
2022-07-28 09:50:49.940620: finished prediction
2022-07-28 09:50:49.943532: evaluation of raw predictions
2022-07-28 09:50:52.855939: determining postprocessing
Foreground vs background
before: 0.001098913421321163
after:  0.0
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='521', fold='3', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 09:51:06.928189: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 09:51:07.004706: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 09:51:07.008149: The split file contains 5 splits.
2022-07-28 09:51:07.010074: Desired fold for training: 3
2022-07-28 09:51:07.011892: This split has 23 training and 6 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 09:51:09.012246: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 09:51:10.003694: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 09:51:19.422939: Unable to plot network architecture:
2022-07-28 09:51:19.425498: local variable 'g' referenced before assignment
2022-07-28 09:51:19.456547: 
printing the network instead:

2022-07-28 09:51:19.467534: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 09:51:19.484981: 

2022-07-28 09:51:20.433415: saving checkpoint...
2022-07-28 09:51:26.759586: done, saving took 7.27 seconds
suus panc_0026 voeg toe aan pred_gt tuples voor later
suus panc_0029 voeg toe aan pred_gt tuples voor later
suus panc_0036 voeg toe aan pred_gt tuples voor later
suus panc_0038 voeg toe aan pred_gt tuples voor later
suus panc_0039 voeg toe aan pred_gt tuples voor later
suus panc_0040 voeg toe aan pred_gt tuples voor later
2022-07-28 09:51:27.134930: finished prediction
2022-07-28 09:51:27.136619: evaluation of raw predictions
2022-07-28 09:51:29.536203: determining postprocessing
Foreground vs background
before: 0.16183810748119873
after:  0.16241444353584972
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='521', fold='4', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 96, 160, 160]), 'median_patient_size_in_voxels': array([147, 258, 258]), 'current_spacing': array([3.03      , 1.52509646, 1.52509646]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [3, 5, 5], 'patch_size': array([ 48, 192, 192]), 'median_patient_size_in_voxels': array([148, 512, 512]), 'current_spacing': array([3.        , 0.76757801, 0.76757801]), 'original_spacing': array([3.        , 0.76757801, 0.76757801]), 'do_dummy_2D_data_aug': True, 'pool_op_kernel_sizes': [[1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 09:51:53.486215: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-07-28 09:51:53.533203: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task521/splits_final.pkl
2022-07-28 09:51:53.536542: The split file contains 5 splits.
2022-07-28 09:51:53.538145: Desired fold for training: 4
2022-07-28 09:51:53.539824: This split has 24 training and 5 validation cases.
unpacking dataset
done
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-07-28 09:51:55.462916: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-07-28 09:52:04.469349: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-07-28 09:52:13.820979: Unable to plot network architecture:
2022-07-28 09:52:13.842947: local variable 'g' referenced before assignment
2022-07-28 09:52:13.859846: 
printing the network instead:

2022-07-28 09:52:13.870034: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-07-28 09:52:13.882347: 

2022-07-28 09:52:14.430028: saving checkpoint...
2022-07-28 09:52:21.871485: done, saving took 7.99 seconds
suus panc_0002 voeg toe aan pred_gt tuples voor later
suus panc_0003 voeg toe aan pred_gt tuples voor later
suus panc_0005 voeg toe aan pred_gt tuples voor later
suus panc_0006 voeg toe aan pred_gt tuples voor later
suus panc_0021 voeg toe aan pred_gt tuples voor later
2022-07-28 09:52:22.130830: finished prediction
2022-07-28 09:52:22.132835: evaluation of raw predictions
2022-07-28 09:52:24.741125: determining postprocessing
Foreground vs background
before: 0.006783458799476857
after:  0.0
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done
Start postprocessing..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Foreground vs background
before: 0.07215024877834446
after:  0.06651495965281505
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done
Done postprocessing! Now start inferencing its own train and test files.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

using model stored in  /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1
This model expects 1 input modalities for each image
Found 26 unique case ids, here are some examples: ['panc_0001' 'panc_0002' 'panc_0034' 'panc_0007' 'panc_0010' 'panc_0004'
 'panc_0002' 'panc_0001' 'panc_0030' 'panc_0036']
If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc
number of cases: 26
number of cases that still need to be predicted: 26
emptying cuda cache
loading parameters for folds, None
folds is None so we will automatically look for output folders (not using 'all'!)
found the following folds:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_0', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_1', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_2', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_3', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4']
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus5 - zet de plans properties
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 09:53:14.063766: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
using the following model files:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']
starting preprocessing generator
starting prediction...
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0004.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 140, 512, 512) after crop: (1, 140, 512, 512) spacing: [3.      0.59375 0.59375] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.     , 0.59375, 0.59375]), 'spacing_transposed': array([3.     , 0.59375, 0.59375]), 'data.shape (data is transposed)': (1, 140, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 140, 396, 396)} 

(1, 140, 396, 396)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0022.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 89, 512, 512) after crop: (1, 89, 512, 512) spacing: [5.         0.76757801 0.76757801] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.76757801, 0.76757801]), 'spacing_transposed': array([5.        , 0.76757801, 0.76757801]), 'data.shape (data is transposed)': (1, 89, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 148, 512, 512)} 

(1, 148, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0028.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 89, 512, 512) after crop: (1, 89, 512, 512) spacing: [5.       0.796875 0.796875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.      , 0.796875, 0.796875]), 'spacing_transposed': array([5.      , 0.796875, 0.796875]), 'data.shape (data is transposed)': (1, 89, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 148, 532, 532)} 

(1, 148, 532, 532)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0034.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 98, 512, 512) after crop: (1, 98, 512, 512) spacing: [5.       0.671875 0.671875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.      , 0.671875, 0.671875]), 'spacing_transposed': array([5.      , 0.671875, 0.671875]), 'data.shape (data is transposed)': (1, 98, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 163, 448, 448)} 

(1, 163, 448, 448)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0005.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 117, 512, 512) after crop: (1, 117, 512, 512) spacing: [3.      0.90625 0.90625] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.     , 0.90625, 0.90625]), 'spacing_transposed': array([3.     , 0.90625, 0.90625]), 'data.shape (data is transposed)': (1, 117, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 117, 604, 604)} 

(1, 117, 604, 604)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0023.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 96, 512, 512) after crop: (1, 96, 512, 512) spacing: [5.         0.70703125 0.70703125] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.70703125, 0.70703125]), 'spacing_transposed': array([5.        , 0.70703125, 0.70703125]), 'data.shape (data is transposed)': (1, 96, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 160, 472, 472)} 

(1, 160, 472, 472)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0029.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 100, 512, 512) after crop: (1, 100, 512, 512) spacing: [3.    0.875 0.875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.   , 0.875, 0.875]), 'spacing_transposed': array([3.   , 0.875, 0.875]), 'data.shape (data is transposed)': (1, 100, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 100, 584, 584)} 

(1, 100, 584, 584)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0036.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 184, 512, 512) after crop: (1, 184, 512, 512) spacing: [3.         0.74609375 0.74609375] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.74609375, 0.74609375]), 'spacing_transposed': array([3.        , 0.74609375, 0.74609375]), 'data.shape (data is transposed)': (1, 184, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 184, 498, 498)} 

(1, 184, 498, 498)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0007.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 163, 512, 512) after crop: (1, 163, 512, 512) spacing: [3.         0.74804688 0.74804688] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.74804688, 0.74804688]), 'spacing_transposed': array([3.        , 0.74804688, 0.74804688]), 'data.shape (data is transposed)': (1, 163, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 163, 499, 499)} 

(1, 163, 499, 499)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0024.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 124, 512, 512) after crop: (1, 124, 512, 512) spacing: [3.         0.68554688 0.68554688] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.68554688, 0.68554688]), 'spacing_transposed': array([3.        , 0.68554688, 0.68554688]), 'data.shape (data is transposed)': (1, 124, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 124, 457, 457)} 

(1, 124, 457, 457)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0030.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 153, 512, 512) after crop: (1, 153, 512, 512) spacing: [3.        0.7421875 0.7421875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.       , 0.7421875, 0.7421875]), 'spacing_transposed': array([3.       , 0.7421875, 0.7421875]), 'data.shape (data is transposed)': (1, 153, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 153, 495, 495)} 

(1, 153, 495, 495)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0037.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 99, 512, 512) after crop: (1, 99, 512, 512) spacing: [5.       0.703125 0.703125] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.      , 0.703125, 0.703125]), 'spacing_transposed': array([5.      , 0.703125, 0.703125]), 'data.shape (data is transposed)': (1, 99, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 165, 469, 469)} 

(1, 165, 469, 469)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0003.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 198, 512, 512) after crop: (1, 198, 512, 512) spacing: [3.        0.8984375 0.8984375] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([3.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 198, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 198, 599, 599)} 

(1, 198, 599, 599)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0021.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 143, 512, 512) after crop: (1, 143, 512, 512) spacing: [3.         0.68359375 0.68359375] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.68359375, 0.68359375]), 'spacing_transposed': array([3.        , 0.68359375, 0.68359375]), 'data.shape (data is transposed)': (1, 143, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 143, 456, 456)} 

(1, 143, 456, 456)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0027.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 88, 512, 512) after crop: (1, 88, 512, 512) spacing: [5.         0.77539098 0.77539098] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.77539098, 0.77539098]), 'spacing_transposed': array([5.        , 0.77539098, 0.77539098]), 'data.shape (data is transposed)': (1, 88, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 147, 517, 517)} 

(1, 147, 517, 517)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0033.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 104, 512, 512) after crop: (1, 104, 512, 512) spacing: [5.         0.81445301 0.81445301] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.81445301, 0.81445301]), 'spacing_transposed': array([5.        , 0.81445301, 0.81445301]), 'data.shape (data is transposed)': (1, 104, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 173, 543, 543)} 

(1, 173, 543, 543)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0002.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 139, 512, 512) after crop: (1, 139, 512, 512) spacing: [3.         0.72070312 0.72070312] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.72070312, 0.72070312]), 'spacing_transposed': array([3.        , 0.72070312, 0.72070312]), 'data.shape (data is transposed)': (1, 139, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 139, 481, 481)} 

(1, 139, 481, 481)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0010.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 148, 512, 512) after crop: (1, 148, 512, 512) spacing: [3.      0.78125 0.78125] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.     , 0.78125, 0.78125]), 'spacing_transposed': array([3.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 148, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 148, 521, 521)} 

(1, 148, 521, 521)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0026.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 131, 512, 512) after crop: (1, 131, 512, 512) spacing: [5.         0.77929699 0.77929699] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.77929699, 0.77929699]), 'spacing_transposed': array([5.        , 0.77929699, 0.77929699]), 'data.shape (data is transposed)': (1, 131, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 218, 520, 520)} 

(1, 218, 520, 520)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0032.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 144, 512, 512) after crop: (1, 144, 512, 512) spacing: [3.         0.74023438 0.74023438] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.74023438, 0.74023438]), 'spacing_transposed': array([3.        , 0.74023438, 0.74023438]), 'data.shape (data is transposed)': (1, 144, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 144, 494, 494)} 

(1, 144, 494, 494)
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0040.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 195, 512, 512) after crop: (1, 195, 512, 512) spacing: [3.        0.7421875 0.7421875] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.       , 0.7421875, 0.7421875]), 'spacing_transposed': array([3.       , 0.7421875, 0.7421875]), 'data.shape (data is transposed)': (1, 195, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 195, 495, 495)} 

(1, 195, 495, 495)
This worker has ended successfully, no errors to report
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0002.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0005.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0003.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0022.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0023.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0021.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0028.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0029.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0027.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0034.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0036.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0033.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0038.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0004.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0001.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0007.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0010.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0009.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0024.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0026.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0025.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0030.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0032.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0031.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0037.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0040.nii.gz
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0004.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
computing Gaussian
done
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 140, 396, 396)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92], [0, 68, 136, 204], [0, 68, 136, 204]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0002.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 139, 481, 481)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 68, 91], [0, 72, 144, 217, 289], [0, 72, 144, 217, 289]]
number of tiles: 125
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0001.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 446, 446)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 85, 169, 254], [0, 85, 169, 254]]
number of tiles: 96
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0005.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 117, 604, 604)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69], [0, 82, 165, 247, 330, 412], [0, 82, 165, 247, 330, 412]]
number of tiles: 144
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0007.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 499, 499)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 77, 154, 230, 307], [0, 77, 154, 230, 307]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0003.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 599, 599)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 43, 64, 86, 107, 129, 150], [0, 81, 163, 244, 326, 407], [0, 81, 163, 244, 326, 407]]
number of tiles: 288
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0010.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 521, 521)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 82, 164, 247, 329], [0, 82, 164, 247, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0022.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 512, 512)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 80, 160, 240, 320], [0, 80, 160, 240, 320]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0009.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 180
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0023.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 160, 472, 472)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45, 67, 90, 112], [0, 93, 187, 280], [0, 93, 187, 280]]
number of tiles: 96
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0024.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 124, 457, 457)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 19, 38, 57, 76], [0, 88, 177, 265], [0, 88, 177, 265]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0021.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 143, 456, 456)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95], [0, 88, 176, 264], [0, 88, 176, 264]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0026.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 520, 520)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 64, 85, 106, 128, 149, 170], [0, 82, 164, 246, 328], [0, 82, 164, 246, 328]]
number of tiles: 225
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0028.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 532, 532)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 85, 170, 255, 340], [0, 85, 170, 255, 340]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0025.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 142, 555, 555)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 47, 70, 94], [0, 91, 182, 272, 363], [0, 91, 182, 272, 363]]
number of tiles: 125
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0029.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 100, 584, 584)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 17, 35, 52], [0, 78, 157, 235, 314, 392], [0, 78, 157, 235, 314, 392]]
number of tiles: 144
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0030.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 153, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0027.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 147, 517, 517)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 59, 79, 99], [0, 81, 162, 244, 325], [0, 81, 162, 244, 325]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0032.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 144, 494, 494)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 72, 96], [0, 76, 151, 226, 302], [0, 76, 151, 226, 302]]
number of tiles: 125
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0034.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 163, 448, 448)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 46, 69, 92, 115], [0, 85, 171, 256], [0, 85, 171, 256]]
number of tiles: 96
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0031.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 93, 559, 559)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 45], [0, 92, 184, 275, 367], [0, 92, 184, 275, 367]]
number of tiles: 75
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0036.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 184, 498, 498)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 45, 68, 91, 113, 136], [0, 76, 153, 230, 306], [0, 76, 153, 230, 306]]
number of tiles: 175
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0037.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 165, 469, 469)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 23, 47, 70, 94, 117], [0, 92, 185, 277], [0, 92, 185, 277]]
number of tiles: 96
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0033.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 173, 543, 543)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83, 104, 125], [0, 88, 176, 263, 351], [0, 88, 176, 263, 351]]
number of tiles: 175
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0040.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 495, 495)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 63, 84, 105, 126, 147], [0, 76, 152, 227, 303], [0, 76, 152, 227, 303]]
number of tiles: 200
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTr/panc_0038.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 167, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 24, 48, 71, 95, 119], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
inference done. Now waiting for the segmentation export to finish...
postprocessing...


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

using model stored in  /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1
This model expects 1 input modalities for each image
Found 4 unique case ids, here are some examples: ['panc_0008' 'panc_0008' 'panc_0006' 'panc_0039']
If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc
number of cases: 4
number of cases that still need to be predicted: 4
emptying cuda cache
loading parameters for folds, None
folds is None so we will automatically look for output folders (not using 'all'!)
found the following folds:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_0', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_1', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_2', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_3', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4']
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus5 - zet de plans properties
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
2022-07-28 11:44:21.913772: Using dummy2d data augmentation
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
Img size: [ 48 192 192]
Patch size: (8, 16, 16)
Feature size: (6, 12, 12)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=8, p2=16, p3=16)
          (1): Linear(in_features=2048, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(1, 2, 2), stride=(1, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
using the following model files:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task521/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']
starting preprocessing generator
starting prediction...
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0006.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 131, 512, 512) after crop: (1, 131, 512, 512) spacing: [3.         0.70117188 0.70117188] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.70117188, 0.70117188]), 'spacing_transposed': array([3.        , 0.70117188, 0.70117188]), 'data.shape (data is transposed)': (1, 131, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 131, 468, 468)} 

(1, 131, 468, 468)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0008.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 148, 512, 512) after crop: (1, 148, 512, 512) spacing: [3.         0.72851562 0.72851562] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([3.        , 0.72851562, 0.72851562]), 'spacing_transposed': array([3.        , 0.72851562, 0.72851562]), 'data.shape (data is transposed)': (1, 148, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 148, 486, 486)} 

(1, 148, 486, 486)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0035.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 94, 512, 512) after crop: (1, 94, 512, 512) spacing: [5.         0.91992199 0.91992199] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.91992199, 0.91992199]), 'spacing_transposed': array([5.        , 0.91992199, 0.91992199]), 'data.shape (data is transposed)': (1, 94, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 157, 614, 614)} 

(1, 157, 614, 614)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0039.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 90, 512, 512) after crop: (1, 90, 512, 512) spacing: [5.         0.97600001 0.97600001] 

separate z, order in z is 0 order inplane is 3
separate z, order in z is 0 order inplane is 1
before: {'spacing': array([5.        , 0.97600001, 0.97600001]), 'spacing_transposed': array([5.        , 0.97600001, 0.97600001]), 'data.shape (data is transposed)': (1, 90, 512, 512)} 
after:  {'spacing': array([3.        , 0.76757801, 0.76757801]), 'data.shape (data is resampled)': (1, 150, 651, 651)} 

(1, 150, 651, 651)
This worker has ended successfully, no errors to report
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0006.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0035.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0008.nii.gz
force_separate_z: None interpolation order: 1
separate z: True lowres axis [0]
separate z, order in z is 0 order inplane is 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0039.nii.gz
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0006.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
computing Gaussian
done
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 131, 468, 468)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 21, 42, 62, 83], [0, 92, 184, 276], [0, 92, 184, 276]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0008.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 148, 486, 486)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 40, 60, 80, 100], [0, 74, 147, 220, 294], [0, 74, 147, 220, 294]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0035.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 157, 614, 614)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 22, 44, 65, 87, 109], [0, 84, 169, 253, 338, 422], [0, 84, 169, 253, 338, 422]]
number of tiles: 216
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/521/3d_fullres/nnUNetTrainerV2_Hybrid2LR/521/imagesTs/panc_0039.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 150, 651, 651)
patch size: [ 48 192 192]
steps (x, y, and z): [[0, 20, 41, 61, 82, 102], [0, 92, 184, 275, 367, 459], [0, 92, 184, 275, 367, 459]]
number of tiles: 216
using precomputed Gaussian
prediction done
inference done. Now waiting for the segmentation export to finish...
postprocessing...
Done inferencing! Now start the evaluation.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet



Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Program finished with exit code 0 at: Sun Jul 24 23:22:11 CEST 2022
