Starting at Thu Jul 28 18:37:05 CEST 2022
Running on hosts: res-hpc-lkeb09
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 10.
Account: div2-lkeb
Job ID: 11069597
Job name: PancreasAll
Node running script: res-hpc-lkeb09
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Tue Aug  2 18:22:18 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A6000    On   | 00000000:D8:00.0 Off |                  Off |
| 42%   65C    P3    72W / 300W |      0MiB / 48685MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-cqm9bqua/hiddenlayer_de457d92cdb342548f3c5160c5c15b6e
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Using legacy 'setup.py install' for hiddenlayer, since package 'wheel' is not installed.
Installing collected packages: hiddenlayer
    Running setup.py install for hiddenlayer: started
    Running setup.py install for hiddenlayer: finished with status 'done'
Successfully installed hiddenlayer-0.2
Start preprocessing..
Done preprocessing! Start training all the folds..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='501', fold='4', validation_only=False, continue_training=False, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-02 18:22:51.295576: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-02 18:22:51.307755: The split file contains 5 splits.
2022-08-02 18:22:51.310928: Desired fold for training: 4
2022-08-02 18:22:51.313220: This split has 55 training and 13 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusB run_training - zet learning rate als  
2022-08-02 18:22:53.868895: Suus1 maybe_update_lr lr: 0.0001
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-02 18:23:16.756061: Unable to plot network architecture:
2022-08-02 18:23:16.759814: local variable 'g' referenced before assignment
2022-08-02 18:23:16.762339: 
printing the network instead:

2022-08-02 18:23:16.764776: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-02 18:23:16.773929: 

2022-08-02 18:23:16.776491: 
epoch:  0
2022-08-02 18:25:12.119888: train loss : 0.1567
2022-08-02 18:25:22.594203: validation loss: 0.0667
2022-08-02 18:25:22.626536: Average global foreground Dice: [0.0822]
2022-08-02 18:25:22.649749: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:25:24.401232: Suus1 maybe_update_lr lr: 0.0001
2022-08-02 18:25:24.440753: This epoch took 127.661920 s

2022-08-02 18:25:24.477868: 
epoch:  1
2022-08-02 18:27:19.798396: train loss : 0.0322
2022-08-02 18:27:29.608361: validation loss: -0.0026
2022-08-02 18:27:29.633183: Average global foreground Dice: [0.196]
2022-08-02 18:27:29.675705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:27:31.136842: Suus1 maybe_update_lr lr: 0.0001
2022-08-02 18:27:31.156824: saving best epoch checkpoint...
2022-08-02 18:27:31.514465: saving checkpoint...
2022-08-02 18:27:38.129687: done, saving took 6.95 seconds
2022-08-02 18:27:38.141716: This epoch took 133.642039 s

2022-08-02 18:27:38.144752: 
epoch:  2
2022-08-02 18:29:24.458521: train loss : -0.0564
2022-08-02 18:29:35.641494: validation loss: -0.0521
2022-08-02 18:29:35.672377: Average global foreground Dice: [0.2197]
2022-08-02 18:29:35.693704: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:29:36.700294: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-02 18:29:36.718756: saving best epoch checkpoint...
2022-08-02 18:29:37.011764: saving checkpoint...
2022-08-02 18:29:43.561405: done, saving took 6.81 seconds
2022-08-02 18:29:43.572186: This epoch took 125.424936 s

2022-08-02 18:29:43.574781: 
epoch:  3
2022-08-02 18:31:36.042099: train loss : -0.1431
2022-08-02 18:31:46.189821: validation loss: -0.1939
2022-08-02 18:31:46.233845: Average global foreground Dice: [0.3833]
2022-08-02 18:31:46.259716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:31:46.899513: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-02 18:31:46.921767: saving best epoch checkpoint...
2022-08-02 18:31:47.240898: saving checkpoint...
2022-08-02 18:31:53.306251: done, saving took 6.35 seconds
2022-08-02 18:31:53.315779: This epoch took 129.738577 s

2022-08-02 18:31:53.317757: 
epoch:  4
2022-08-02 18:33:47.467165: train loss : -0.2240
2022-08-02 18:33:59.345326: validation loss: -0.2058
2022-08-02 18:33:59.366682: Average global foreground Dice: [0.3567]
2022-08-02 18:33:59.402951: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:34:00.716639: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-02 18:34:00.753810: saving best epoch checkpoint...
2022-08-02 18:34:01.048500: saving checkpoint...
2022-08-02 18:34:07.779142: done, saving took 7.00 seconds
2022-08-02 18:34:07.788348: This epoch took 134.468521 s

2022-08-02 18:34:07.790548: 
epoch:  5
2022-08-02 18:36:02.849543: train loss : -0.2641
2022-08-02 18:36:16.167758: validation loss: -0.2667
2022-08-02 18:36:16.208292: Average global foreground Dice: [0.4364]
2022-08-02 18:36:16.249715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:36:17.732108: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-02 18:36:17.749221: saving best epoch checkpoint...
2022-08-02 18:36:18.163783: saving checkpoint...
2022-08-02 18:36:24.141244: done, saving took 6.37 seconds
2022-08-02 18:36:24.149808: This epoch took 136.357038 s

2022-08-02 18:36:24.151832: 
epoch:  6
2022-08-02 18:38:16.424135: train loss : -0.2813
2022-08-02 18:38:27.890050: validation loss: -0.2056
2022-08-02 18:38:27.924361: Average global foreground Dice: [0.3559]
2022-08-02 18:38:27.946713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:38:28.736483: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-02 18:38:28.749443: saving best epoch checkpoint...
2022-08-02 18:38:28.993014: saving checkpoint...
2022-08-02 18:38:34.978273: done, saving took 6.23 seconds
2022-08-02 18:38:34.998402: This epoch took 130.844498 s

2022-08-02 18:38:35.000487: 
epoch:  7
2022-08-02 18:40:24.311369: train loss : -0.3309
2022-08-02 18:40:33.324194: validation loss: -0.2882
2022-08-02 18:40:33.354419: Average global foreground Dice: [0.4153]
2022-08-02 18:40:33.386723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:40:34.420873: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-02 18:40:34.479769: saving best epoch checkpoint...
2022-08-02 18:40:34.834209: saving checkpoint...
2022-08-02 18:40:41.403668: done, saving took 6.90 seconds
2022-08-02 18:40:41.416975: This epoch took 126.414432 s

2022-08-02 18:40:41.419313: 
epoch:  8
2022-08-02 18:42:33.004315: train loss : -0.3278
2022-08-02 18:42:44.534925: validation loss: -0.3042
2022-08-02 18:42:44.568552: Average global foreground Dice: [0.4521]
2022-08-02 18:42:44.622738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:42:46.198123: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-02 18:42:46.225825: saving best epoch checkpoint...
2022-08-02 18:42:46.644211: saving checkpoint...
2022-08-02 18:42:53.153032: done, saving took 6.92 seconds
2022-08-02 18:42:53.168693: This epoch took 131.747057 s

2022-08-02 18:42:53.170905: 
epoch:  9
2022-08-02 18:44:45.397347: train loss : -0.3553
2022-08-02 18:44:55.978231: validation loss: -0.3352
2022-08-02 18:44:56.004783: Average global foreground Dice: [0.4649]
2022-08-02 18:44:56.018245: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:44:57.273577: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-02 18:44:57.304882: saving best epoch checkpoint...
2022-08-02 18:44:57.747078: saving checkpoint...
2022-08-02 18:45:04.045916: done, saving took 6.71 seconds
2022-08-02 18:45:04.060982: This epoch took 130.887994 s

2022-08-02 18:45:04.063411: 
epoch:  10
2022-08-02 18:46:52.161553: train loss : -0.3735
2022-08-02 18:47:07.115720: validation loss: -0.4086
2022-08-02 18:47:07.139215: Average global foreground Dice: [0.5203]
2022-08-02 18:47:07.171691: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:47:08.946156: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-02 18:47:08.978038: saving best epoch checkpoint...
2022-08-02 18:47:09.180752: saving checkpoint...
2022-08-02 18:47:14.937046: done, saving took 5.93 seconds
2022-08-02 18:47:14.948811: This epoch took 130.883171 s

2022-08-02 18:47:14.951361: 
epoch:  11
2022-08-02 18:49:09.917056: train loss : -0.3793
2022-08-02 18:49:19.569654: validation loss: -0.4002
2022-08-02 18:49:19.603426: Average global foreground Dice: [0.5221]
2022-08-02 18:49:19.635755: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:49:20.899495: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-02 18:49:20.929769: saving best epoch checkpoint...
2022-08-02 18:49:21.151630: saving checkpoint...
2022-08-02 18:49:27.782750: done, saving took 6.83 seconds
2022-08-02 18:49:27.794901: This epoch took 132.841305 s

2022-08-02 18:49:27.797503: 
epoch:  12
2022-08-02 18:51:14.290945: train loss : -0.3724
2022-08-02 18:51:27.080844: validation loss: -0.3772
2022-08-02 18:51:27.130044: Average global foreground Dice: [0.5013]
2022-08-02 18:51:27.159070: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:51:28.225894: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-02 18:51:28.228514: saving best epoch checkpoint...
2022-08-02 18:51:28.785673: saving checkpoint...
2022-08-02 18:51:36.585574: done, saving took 8.34 seconds
2022-08-02 18:51:36.604773: This epoch took 128.805190 s

2022-08-02 18:51:36.607203: 
epoch:  13
2022-08-02 18:53:27.856525: train loss : -0.3846
2022-08-02 18:53:40.303280: validation loss: -0.3753
2022-08-02 18:53:40.363356: Average global foreground Dice: [0.4985]
2022-08-02 18:53:40.385715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:53:41.982652: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-02 18:53:42.000886: saving best epoch checkpoint...
2022-08-02 18:53:42.534274: saving checkpoint...
2022-08-02 18:53:49.333798: done, saving took 7.31 seconds
2022-08-02 18:53:49.343845: This epoch took 132.734112 s

2022-08-02 18:53:49.346034: 
epoch:  14
2022-08-02 18:55:42.366451: train loss : -0.3915
2022-08-02 18:55:54.429019: validation loss: -0.3835
2022-08-02 18:55:54.453550: Average global foreground Dice: [0.509]
2022-08-02 18:55:54.479739: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:55:55.439011: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-02 18:55:55.458762: saving best epoch checkpoint...
2022-08-02 18:55:55.914425: saving checkpoint...
2022-08-02 18:56:02.774970: done, saving took 7.30 seconds
2022-08-02 18:56:02.791433: This epoch took 133.443274 s

2022-08-02 18:56:02.793552: 
epoch:  15
2022-08-02 18:57:55.106848: train loss : -0.3934
2022-08-02 18:58:06.823923: validation loss: -0.4146
2022-08-02 18:58:06.868267: Average global foreground Dice: [0.5135]
2022-08-02 18:58:06.900719: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 18:58:08.025746: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-02 18:58:08.051678: saving best epoch checkpoint...
2022-08-02 18:58:08.477936: saving checkpoint...
2022-08-02 18:58:16.156853: done, saving took 8.07 seconds
2022-08-02 18:58:16.168234: This epoch took 133.372455 s

2022-08-02 18:58:16.170468: 
epoch:  16
2022-08-02 19:00:06.267862: train loss : -0.4149
2022-08-02 19:00:17.087178: validation loss: -0.4296
2022-08-02 19:00:17.123426: Average global foreground Dice: [0.5361]
2022-08-02 19:00:17.140733: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:00:18.662164: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-02 19:00:18.707783: saving best epoch checkpoint...
2022-08-02 19:00:19.082840: saving checkpoint...
2022-08-02 19:00:25.796359: done, saving took 7.06 seconds
2022-08-02 19:00:25.814493: This epoch took 129.641784 s

2022-08-02 19:00:25.816820: 
epoch:  17
2022-08-02 19:02:17.258540: train loss : -0.4333
2022-08-02 19:02:28.004922: validation loss: -0.3976
2022-08-02 19:02:28.037736: Average global foreground Dice: [0.5277]
2022-08-02 19:02:28.060744: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:02:29.122168: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-02 19:02:29.152358: saving best epoch checkpoint...
2022-08-02 19:02:29.574193: saving checkpoint...
2022-08-02 19:02:36.759690: done, saving took 7.60 seconds
2022-08-02 19:02:36.774816: This epoch took 130.955497 s

2022-08-02 19:02:36.777312: 
epoch:  18
2022-08-02 19:04:26.259569: train loss : -0.4296
2022-08-02 19:04:38.254198: validation loss: -0.4258
2022-08-02 19:04:38.280938: Average global foreground Dice: [0.5428]
2022-08-02 19:04:38.309737: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:04:39.766859: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-02 19:04:39.797795: saving best epoch checkpoint...
2022-08-02 19:04:40.329099: saving checkpoint...
2022-08-02 19:04:47.679950: done, saving took 7.85 seconds
2022-08-02 19:04:47.698217: This epoch took 130.918571 s

2022-08-02 19:04:47.700619: 
epoch:  19
2022-08-02 19:06:39.698917: train loss : -0.4149
2022-08-02 19:06:49.192590: validation loss: -0.4561
2022-08-02 19:06:49.238385: Average global foreground Dice: [0.5669]
2022-08-02 19:06:49.270754: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:06:50.502107: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-02 19:06:50.532795: saving best epoch checkpoint...
2022-08-02 19:06:51.160581: saving checkpoint...
2022-08-02 19:06:58.070887: done, saving took 7.50 seconds
2022-08-02 19:06:58.095407: This epoch took 130.392521 s

2022-08-02 19:06:58.097757: 
epoch:  20
2022-08-02 19:08:48.746219: train loss : -0.4418
2022-08-02 19:08:59.177953: validation loss: -0.4748
2022-08-02 19:08:59.211673: Average global foreground Dice: [0.5878]
2022-08-02 19:08:59.239767: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:09:00.475814: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-02 19:09:00.512766: saving best epoch checkpoint...
2022-08-02 19:09:01.130139: saving checkpoint...
2022-08-02 19:09:07.660978: done, saving took 7.09 seconds
2022-08-02 19:09:07.676121: This epoch took 129.576148 s

2022-08-02 19:09:07.678265: 
epoch:  21
2022-08-02 19:11:06.019711: train loss : -0.4411
2022-08-02 19:11:17.661946: validation loss: -0.4466
2022-08-02 19:11:17.708270: Average global foreground Dice: [0.5621]
2022-08-02 19:11:17.739716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:11:19.067386: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-02 19:11:19.086838: saving best epoch checkpoint...
2022-08-02 19:11:19.487554: saving checkpoint...
2022-08-02 19:11:26.546896: done, saving took 7.43 seconds
2022-08-02 19:11:26.561741: This epoch took 138.881272 s

2022-08-02 19:11:26.563855: 
epoch:  22
2022-08-02 19:13:19.223801: train loss : -0.4627
2022-08-02 19:13:31.124277: validation loss: -0.5097
2022-08-02 19:13:31.152551: Average global foreground Dice: [0.6255]
2022-08-02 19:13:31.178707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:13:32.627937: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-02 19:13:32.657799: saving best epoch checkpoint...
2022-08-02 19:13:33.004907: saving checkpoint...
2022-08-02 19:13:39.898776: done, saving took 7.21 seconds
2022-08-02 19:13:39.914433: This epoch took 133.348223 s

2022-08-02 19:13:39.916733: 
epoch:  23
2022-08-02 19:15:31.378326: train loss : -0.4827
2022-08-02 19:15:44.151856: validation loss: -0.4575
2022-08-02 19:15:44.209225: Average global foreground Dice: [0.5696]
2022-08-02 19:15:44.240188: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:15:45.367161: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-02 19:15:45.385771: saving best epoch checkpoint...
2022-08-02 19:15:45.701110: saving checkpoint...
2022-08-02 19:15:52.019074: done, saving took 6.60 seconds
2022-08-02 19:15:52.032091: This epoch took 132.113277 s

2022-08-02 19:15:52.034244: 
epoch:  24
2022-08-02 19:17:44.678505: train loss : -0.4926
2022-08-02 19:17:58.897064: validation loss: -0.4775
2022-08-02 19:17:58.938457: Average global foreground Dice: [0.5811]
2022-08-02 19:17:58.957390: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:18:00.217110: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-02 19:18:00.258772: saving best epoch checkpoint...
2022-08-02 19:18:00.672484: saving checkpoint...
2022-08-02 19:18:06.722306: done, saving took 6.43 seconds
2022-08-02 19:18:06.737363: This epoch took 134.700937 s

2022-08-02 19:18:06.739921: 
epoch:  25
2022-08-02 19:19:55.992254: train loss : -0.4743
2022-08-02 19:20:08.272545: validation loss: -0.4684
2022-08-02 19:20:08.306317: Average global foreground Dice: [0.5904]
2022-08-02 19:20:08.309274: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:20:09.378763: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-02 19:20:09.399794: saving best epoch checkpoint...
2022-08-02 19:20:09.729936: saving checkpoint...
2022-08-02 19:20:16.000450: done, saving took 6.58 seconds
2022-08-02 19:20:16.014352: This epoch took 129.271912 s

2022-08-02 19:20:16.016698: 
epoch:  26
2022-08-02 19:22:09.358267: train loss : -0.5051
2022-08-02 19:22:19.075317: validation loss: -0.4790
2022-08-02 19:22:19.107309: Average global foreground Dice: [0.586]
2022-08-02 19:22:19.129718: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:22:20.099260: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-02 19:22:20.127844: saving best epoch checkpoint...
2022-08-02 19:22:20.443832: saving checkpoint...
2022-08-02 19:22:27.888868: done, saving took 7.74 seconds
2022-08-02 19:22:27.905042: This epoch took 131.885948 s

2022-08-02 19:22:27.907346: 
epoch:  27
2022-08-02 19:24:14.294978: train loss : -0.5155
2022-08-02 19:24:25.940669: validation loss: -0.5343
2022-08-02 19:24:25.974557: Average global foreground Dice: [0.6412]
2022-08-02 19:24:26.006763: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:24:27.412603: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-02 19:24:27.443792: saving best epoch checkpoint...
2022-08-02 19:24:27.708382: saving checkpoint...
2022-08-02 19:24:34.757360: done, saving took 7.28 seconds
2022-08-02 19:24:34.773334: This epoch took 126.863636 s

2022-08-02 19:24:34.775973: 
epoch:  28
2022-08-02 19:26:24.458991: train loss : -0.5278
2022-08-02 19:26:36.318660: validation loss: -0.5300
2022-08-02 19:26:36.339703: Average global foreground Dice: [0.6287]
2022-08-02 19:26:36.359485: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:26:37.196237: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-02 19:26:37.216725: saving best epoch checkpoint...
2022-08-02 19:26:37.663250: saving checkpoint...
2022-08-02 19:26:44.350827: done, saving took 7.10 seconds
2022-08-02 19:26:44.371854: This epoch took 129.593450 s

2022-08-02 19:26:44.374074: 
epoch:  29
2022-08-02 19:28:34.243939: train loss : -0.5226
2022-08-02 19:28:45.183049: validation loss: -0.5395
2022-08-02 19:28:45.214294: Average global foreground Dice: [0.6435]
2022-08-02 19:28:45.240312: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:28:46.194291: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-02 19:28:46.225791: saving best epoch checkpoint...
2022-08-02 19:28:46.565277: saving checkpoint...
2022-08-02 19:28:54.119846: done, saving took 7.86 seconds
2022-08-02 19:28:54.151224: This epoch took 129.774959 s

2022-08-02 19:28:54.154464: 
epoch:  30
2022-08-02 19:30:45.483236: train loss : -0.5206
2022-08-02 19:30:56.787868: validation loss: -0.5334
2022-08-02 19:30:56.821267: Average global foreground Dice: [0.6259]
2022-08-02 19:30:56.846845: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:30:58.190630: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-02 19:30:58.205773: saving best epoch checkpoint...
2022-08-02 19:30:58.811682: saving checkpoint...
2022-08-02 19:31:05.550846: done, saving took 7.32 seconds
2022-08-02 19:31:05.566109: This epoch took 131.408854 s

2022-08-02 19:31:05.568484: 
epoch:  31
2022-08-02 19:33:01.970616: train loss : -0.5297
2022-08-02 19:33:14.183543: validation loss: -0.5139
2022-08-02 19:33:14.215462: Average global foreground Dice: [0.6174]
2022-08-02 19:33:14.239731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:33:15.268896: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-02 19:33:15.276811: saving best epoch checkpoint...
2022-08-02 19:33:15.659582: saving checkpoint...
2022-08-02 19:33:20.805712: done, saving took 5.51 seconds
2022-08-02 19:33:20.816424: This epoch took 135.245603 s

2022-08-02 19:33:20.818460: 
epoch:  32
2022-08-02 19:35:05.063742: train loss : -0.5501
2022-08-02 19:35:16.631244: validation loss: -0.5415
2022-08-02 19:35:16.647773: Average global foreground Dice: [0.6426]
2022-08-02 19:35:16.664714: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:35:17.700292: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-02 19:35:17.729832: saving best epoch checkpoint...
2022-08-02 19:35:18.096781: saving checkpoint...
2022-08-02 19:35:24.349331: done, saving took 6.60 seconds
2022-08-02 19:35:24.359524: This epoch took 123.539145 s

2022-08-02 19:35:24.361747: 
epoch:  33
2022-08-02 19:37:09.742081: train loss : -0.5445
2022-08-02 19:37:19.826840: validation loss: -0.5627
2022-08-02 19:37:19.859711: Average global foreground Dice: [0.6648]
2022-08-02 19:37:19.891222: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:37:21.074265: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-02 19:37:21.103805: saving best epoch checkpoint...
2022-08-02 19:37:21.481065: saving checkpoint...
2022-08-02 19:37:28.311439: done, saving took 7.19 seconds
2022-08-02 19:37:28.323903: This epoch took 123.959980 s

2022-08-02 19:37:28.326123: 
epoch:  34
2022-08-02 19:39:19.294684: train loss : -0.5489
2022-08-02 19:39:31.974241: validation loss: -0.5515
2022-08-02 19:39:32.002171: Average global foreground Dice: [0.66]
2022-08-02 19:39:32.023718: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:39:33.203048: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-02 19:39:33.220770: saving best epoch checkpoint...
2022-08-02 19:39:33.520654: saving checkpoint...
2022-08-02 19:39:40.048280: done, saving took 6.81 seconds
2022-08-02 19:39:40.064603: This epoch took 131.735961 s

2022-08-02 19:39:40.066766: 
epoch:  35
2022-08-02 19:41:25.035156: train loss : -0.5493
2022-08-02 19:41:34.945911: validation loss: -0.5377
2022-08-02 19:41:34.979428: Average global foreground Dice: [0.6393]
2022-08-02 19:41:35.000740: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:41:36.419944: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-02 19:41:36.445792: saving best epoch checkpoint...
2022-08-02 19:41:36.777985: saving checkpoint...
2022-08-02 19:41:45.295928: done, saving took 8.82 seconds
2022-08-02 19:41:45.316994: This epoch took 125.248153 s

2022-08-02 19:41:45.319625: 
epoch:  36
2022-08-02 19:43:38.220132: train loss : -0.5608
2022-08-02 19:43:49.574225: validation loss: -0.5834
2022-08-02 19:43:49.612381: Average global foreground Dice: [0.6836]
2022-08-02 19:43:49.624706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:43:51.051841: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-02 19:43:51.081780: saving best epoch checkpoint...
2022-08-02 19:43:51.478043: saving checkpoint...
2022-08-02 19:43:58.390536: done, saving took 7.28 seconds
2022-08-02 19:43:58.403825: This epoch took 133.081710 s

2022-08-02 19:43:58.406058: 
epoch:  37
2022-08-02 19:45:49.874966: train loss : -0.5590
2022-08-02 19:46:02.836522: validation loss: -0.5376
2022-08-02 19:46:02.868222: Average global foreground Dice: [0.6384]
2022-08-02 19:46:02.897778: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:46:04.102189: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-02 19:46:04.126750: saving best epoch checkpoint...
2022-08-02 19:46:04.570817: saving checkpoint...
2022-08-02 19:46:11.025438: done, saving took 6.88 seconds
2022-08-02 19:46:11.039049: This epoch took 132.630769 s

2022-08-02 19:46:11.041330: 
epoch:  38
2022-08-02 19:47:58.463235: train loss : -0.5700
2022-08-02 19:48:07.483075: validation loss: -0.5909
2022-08-02 19:48:07.525440: Average global foreground Dice: [0.6854]
2022-08-02 19:48:07.548707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:48:08.631098: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-02 19:48:08.648759: saving best epoch checkpoint...
2022-08-02 19:48:09.091390: saving checkpoint...
2022-08-02 19:48:15.579933: done, saving took 6.91 seconds
2022-08-02 19:48:15.595648: This epoch took 124.552008 s

2022-08-02 19:48:15.598026: 
epoch:  39
2022-08-02 19:49:58.900025: train loss : -0.5577
2022-08-02 19:50:10.548711: validation loss: -0.5589
2022-08-02 19:50:10.580340: Average global foreground Dice: [0.6655]
2022-08-02 19:50:10.602745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:50:11.806732: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-02 19:50:11.827766: saving best epoch checkpoint...
2022-08-02 19:50:12.216651: saving checkpoint...
2022-08-02 19:50:18.884059: done, saving took 7.03 seconds
2022-08-02 19:50:18.897959: This epoch took 123.297827 s

2022-08-02 19:50:18.900318: 
epoch:  40
2022-08-02 19:52:00.421106: train loss : -0.5874
2022-08-02 19:52:13.162460: validation loss: -0.6024
2022-08-02 19:52:13.174488: Average global foreground Dice: [0.6951]
2022-08-02 19:52:13.177368: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:52:15.281289: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-02 19:52:15.314734: saving best epoch checkpoint...
2022-08-02 19:52:15.556777: saving checkpoint...
2022-08-02 19:52:21.897738: done, saving took 6.55 seconds
2022-08-02 19:52:21.909861: This epoch took 123.007515 s

2022-08-02 19:52:21.912435: 
epoch:  41
2022-08-02 19:54:16.485157: train loss : -0.5774
2022-08-02 19:54:29.309824: validation loss: -0.5628
2022-08-02 19:54:29.352531: Average global foreground Dice: [0.6642]
2022-08-02 19:54:29.386434: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:54:30.861331: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-02 19:54:30.886027: saving best epoch checkpoint...
2022-08-02 19:54:31.213435: saving checkpoint...
2022-08-02 19:54:37.775298: done, saving took 6.87 seconds
2022-08-02 19:54:37.784805: This epoch took 135.869961 s

2022-08-02 19:54:37.786878: 
epoch:  42
2022-08-02 19:56:31.882029: train loss : -0.5955
2022-08-02 19:56:43.875507: validation loss: -0.6139
2022-08-02 19:56:43.902828: Average global foreground Dice: [0.7067]
2022-08-02 19:56:43.924717: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:56:45.241635: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-02 19:56:45.251838: saving best epoch checkpoint...
2022-08-02 19:56:45.486957: saving checkpoint...
2022-08-02 19:56:52.576975: done, saving took 7.29 seconds
2022-08-02 19:56:52.586310: This epoch took 134.797401 s

2022-08-02 19:56:52.588576: 
epoch:  43
2022-08-02 19:58:40.808870: train loss : -0.5972
2022-08-02 19:58:51.746170: validation loss: -0.4730
2022-08-02 19:58:51.775316: Average global foreground Dice: [0.6015]
2022-08-02 19:58:51.807746: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 19:58:53.492667: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-02 19:58:53.535865: This epoch took 120.945209 s

2022-08-02 19:58:53.557701: 
epoch:  44
2022-08-02 20:00:49.882541: train loss : -0.5844
2022-08-02 20:01:01.789289: validation loss: -0.5702
2022-08-02 20:01:01.836412: Average global foreground Dice: [0.6652]
2022-08-02 20:01:01.857704: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:01:02.874592: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-02 20:01:02.877247: This epoch took 129.293541 s

2022-08-02 20:01:02.895525: 
epoch:  45
2022-08-02 20:03:07.709889: train loss : -0.5959
2022-08-02 20:03:18.382656: validation loss: -0.5736
2022-08-02 20:03:18.414714: Average global foreground Dice: [0.6732]
2022-08-02 20:03:18.433748: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:03:19.487906: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-02 20:03:19.519785: saving best epoch checkpoint...
2022-08-02 20:03:20.138385: saving checkpoint...
2022-08-02 20:03:27.040295: done, saving took 7.50 seconds
2022-08-02 20:03:27.056398: This epoch took 144.158363 s

2022-08-02 20:03:27.058936: 
epoch:  46
2022-08-02 20:05:15.594881: train loss : -0.6026
2022-08-02 20:05:28.862196: validation loss: -0.6111
2022-08-02 20:05:28.897214: Average global foreground Dice: [0.7034]
2022-08-02 20:05:28.929704: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:05:30.079301: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-02 20:05:30.097446: saving best epoch checkpoint...
2022-08-02 20:05:30.428250: saving checkpoint...
2022-08-02 20:05:37.706048: done, saving took 7.59 seconds
2022-08-02 20:05:37.718896: This epoch took 130.657272 s

2022-08-02 20:05:37.722148: 
epoch:  47
2022-08-02 20:07:28.900488: train loss : -0.6094
2022-08-02 20:07:40.290868: validation loss: -0.5672
2022-08-02 20:07:40.323394: Average global foreground Dice: [0.6676]
2022-08-02 20:07:40.353736: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:07:41.656240: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-02 20:07:41.671770: saving best epoch checkpoint...
2022-08-02 20:07:42.295431: saving checkpoint...
2022-08-02 20:07:48.383940: done, saving took 6.68 seconds
2022-08-02 20:07:48.398167: This epoch took 130.673729 s

2022-08-02 20:07:48.400635: 
epoch:  48
2022-08-02 20:09:30.672447: train loss : -0.6291
2022-08-02 20:09:41.554175: validation loss: -0.5903
2022-08-02 20:09:41.583556: Average global foreground Dice: [0.6841]
2022-08-02 20:09:41.610714: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:09:42.742314: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-02 20:09:42.773755: saving best epoch checkpoint...
2022-08-02 20:09:43.101307: saving checkpoint...
2022-08-02 20:09:50.664159: done, saving took 7.86 seconds
2022-08-02 20:09:50.683435: This epoch took 122.280736 s

2022-08-02 20:09:50.685714: 
epoch:  49
2022-08-02 20:11:39.068585: train loss : -0.6174
2022-08-02 20:11:50.756042: validation loss: -0.5752
2022-08-02 20:11:50.788329: Average global foreground Dice: [0.6697]
2022-08-02 20:11:50.822119: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:11:51.797596: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-02 20:11:51.800869: saving scheduled checkpoint file...
2022-08-02 20:11:52.052540: saving checkpoint...
2022-08-02 20:11:58.064550: done, saving took 6.26 seconds
2022-08-02 20:11:58.083806: done
2022-08-02 20:11:58.086528: saving best epoch checkpoint...
2022-08-02 20:11:58.237034: saving checkpoint...
2022-08-02 20:12:02.944355: done, saving took 4.86 seconds
2022-08-02 20:12:02.960318: This epoch took 132.272319 s

2022-08-02 20:12:02.962659: 
epoch:  50
2022-08-02 20:13:46.829194: train loss : -0.6231
2022-08-02 20:13:57.722928: validation loss: -0.6071
2022-08-02 20:13:57.751236: Average global foreground Dice: [0.6969]
2022-08-02 20:13:57.786518: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:13:58.914827: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-02 20:13:58.936108: saving best epoch checkpoint...
2022-08-02 20:13:59.365321: saving checkpoint...
2022-08-02 20:14:05.973109: done, saving took 7.01 seconds
2022-08-02 20:14:05.985096: This epoch took 123.020118 s

2022-08-02 20:14:05.987652: 
epoch:  51
2022-08-02 20:15:52.366109: train loss : -0.6178
2022-08-02 20:16:02.477407: validation loss: -0.6008
2022-08-02 20:16:02.500839: Average global foreground Dice: [0.6891]
2022-08-02 20:16:02.509551: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:16:03.398929: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-02 20:16:03.417752: saving best epoch checkpoint...
2022-08-02 20:16:03.720703: saving checkpoint...
2022-08-02 20:16:10.926892: done, saving took 7.49 seconds
2022-08-02 20:16:10.939529: This epoch took 124.949227 s

2022-08-02 20:16:10.942074: 
epoch:  52
2022-08-02 20:17:55.499744: train loss : -0.6389
2022-08-02 20:18:05.556960: validation loss: -0.6052
2022-08-02 20:18:05.590241: Average global foreground Dice: [0.7029]
2022-08-02 20:18:05.622730: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:18:06.795878: Suus1 maybe_update_lr lr: 9e-05
2022-08-02 20:18:06.824747: saving best epoch checkpoint...
2022-08-02 20:18:07.250817: saving checkpoint...
2022-08-02 20:18:15.400278: done, saving took 8.53 seconds
2022-08-02 20:18:15.418753: This epoch took 124.474376 s

2022-08-02 20:18:15.421413: 
epoch:  53
2022-08-02 20:20:07.110833: train loss : -0.6309
2022-08-02 20:20:19.041671: validation loss: -0.6128
2022-08-02 20:20:19.086383: Average global foreground Dice: [0.6995]
2022-08-02 20:20:19.127553: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:20:20.153624: Suus1 maybe_update_lr lr: 9e-05
2022-08-02 20:20:20.176783: saving best epoch checkpoint...
2022-08-02 20:20:20.542914: saving checkpoint...
2022-08-02 20:20:26.647752: done, saving took 6.45 seconds
2022-08-02 20:20:26.660303: This epoch took 131.235963 s

2022-08-02 20:20:26.662646: 
epoch:  54
2022-08-02 20:22:14.364721: train loss : -0.6253
2022-08-02 20:22:27.803511: validation loss: -0.5994
2022-08-02 20:22:27.826278: Average global foreground Dice: [0.6903]
2022-08-02 20:22:27.848710: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:22:29.120063: Suus1 maybe_update_lr lr: 9e-05
2022-08-02 20:22:29.143789: saving best epoch checkpoint...
2022-08-02 20:22:29.414793: saving checkpoint...
2022-08-02 20:22:36.378977: done, saving took 7.21 seconds
2022-08-02 20:22:36.391551: This epoch took 129.726824 s

2022-08-02 20:22:36.393701: 
epoch:  55
2022-08-02 20:24:25.449843: train loss : -0.6310
2022-08-02 20:24:38.308296: validation loss: -0.5878
2022-08-02 20:24:38.340412: Average global foreground Dice: [0.6792]
2022-08-02 20:24:38.363727: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:24:39.857937: Suus1 maybe_update_lr lr: 9e-05
2022-08-02 20:24:39.877808: saving best epoch checkpoint...
2022-08-02 20:24:40.379770: saving checkpoint...
2022-08-02 20:24:47.543375: done, saving took 7.64 seconds
2022-08-02 20:24:47.563402: This epoch took 131.167696 s

2022-08-02 20:24:47.566564: 
epoch:  56
2022-08-02 20:26:35.310858: train loss : -0.6394
2022-08-02 20:26:48.839059: validation loss: -0.6284
2022-08-02 20:26:48.881352: Average global foreground Dice: [0.7226]
2022-08-02 20:26:48.894715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:26:50.088666: Suus1 maybe_update_lr lr: 9e-05
2022-08-02 20:26:50.091500: saving best epoch checkpoint...
2022-08-02 20:26:50.599276: saving checkpoint...
2022-08-02 20:26:57.127349: done, saving took 7.01 seconds
2022-08-02 20:26:57.148707: This epoch took 129.579669 s

2022-08-02 20:26:57.151128: 
epoch:  57
2022-08-02 20:28:44.927658: train loss : -0.6283
2022-08-02 20:28:54.897535: validation loss: -0.6036
2022-08-02 20:28:54.929870: Average global foreground Dice: [0.6964]
2022-08-02 20:28:54.960026: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:28:56.413987: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-02 20:28:56.432822: saving best epoch checkpoint...
2022-08-02 20:28:56.930139: saving checkpoint...
2022-08-02 20:29:03.680751: done, saving took 7.23 seconds
2022-08-02 20:29:03.695070: This epoch took 126.541612 s

2022-08-02 20:29:03.697688: 
epoch:  58
2022-08-02 20:30:50.113992: train loss : -0.6421
2022-08-02 20:31:01.018191: validation loss: -0.6277
2022-08-02 20:31:01.038999: Average global foreground Dice: [0.7161]
2022-08-02 20:31:01.058582: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:31:02.738576: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-02 20:31:02.765166: saving best epoch checkpoint...
2022-08-02 20:31:03.299996: saving checkpoint...
2022-08-02 20:31:10.191269: done, saving took 7.40 seconds
2022-08-02 20:31:10.208909: This epoch took 126.508502 s

2022-08-02 20:31:10.211191: 
epoch:  59
2022-08-02 20:32:59.326989: train loss : -0.6525
2022-08-02 20:33:11.462948: validation loss: -0.5993
2022-08-02 20:33:11.496468: Average global foreground Dice: [0.6877]
2022-08-02 20:33:11.528756: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:33:12.993957: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-02 20:33:13.013717: saving best epoch checkpoint...
2022-08-02 20:33:13.524212: saving checkpoint...
2022-08-02 20:33:20.170483: done, saving took 7.12 seconds
2022-08-02 20:33:20.184660: This epoch took 129.970986 s

2022-08-02 20:33:20.186822: 
epoch:  60
2022-08-02 20:34:58.577589: train loss : -0.6425
2022-08-02 20:35:10.293530: validation loss: -0.6319
2022-08-02 20:35:10.327647: Average global foreground Dice: [0.7187]
2022-08-02 20:35:10.361706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:35:11.810666: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-02 20:35:11.839825: saving best epoch checkpoint...
2022-08-02 20:35:12.238566: saving checkpoint...
2022-08-02 20:35:19.503771: done, saving took 7.63 seconds
2022-08-02 20:35:19.516872: This epoch took 119.327965 s

2022-08-02 20:35:19.519456: 
epoch:  61
2022-08-02 20:37:07.755285: train loss : -0.6296
2022-08-02 20:37:21.019596: validation loss: -0.6537
2022-08-02 20:37:21.051306: Average global foreground Dice: [0.7306]
2022-08-02 20:37:21.063706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:37:22.390372: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-02 20:37:22.414795: saving best epoch checkpoint...
2022-08-02 20:37:22.708736: saving checkpoint...
2022-08-02 20:37:29.247266: done, saving took 6.81 seconds
2022-08-02 20:37:29.257514: This epoch took 129.735622 s

2022-08-02 20:37:29.259704: 
epoch:  62
2022-08-02 20:39:21.866584: train loss : -0.6499
2022-08-02 20:39:32.378229: validation loss: -0.6130
2022-08-02 20:39:32.409548: Average global foreground Dice: [0.7049]
2022-08-02 20:39:32.438957: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:39:33.708026: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-02 20:39:33.739798: saving best epoch checkpoint...
2022-08-02 20:39:34.019805: saving checkpoint...
2022-08-02 20:39:41.066435: done, saving took 7.29 seconds
2022-08-02 20:39:41.078111: This epoch took 131.816338 s

2022-08-02 20:39:41.080166: 
epoch:  63
2022-08-02 20:41:36.058167: train loss : -0.6547
2022-08-02 20:41:46.884839: validation loss: -0.6316
2022-08-02 20:41:46.913124: Average global foreground Dice: [0.7216]
2022-08-02 20:41:46.927367: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:41:48.217142: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-02 20:41:48.240828: saving best epoch checkpoint...
2022-08-02 20:41:48.713060: saving checkpoint...
2022-08-02 20:41:55.141979: done, saving took 6.87 seconds
2022-08-02 20:41:55.156048: This epoch took 134.073764 s

2022-08-02 20:41:55.158394: 
epoch:  64
2022-08-02 20:43:39.066539: train loss : -0.6554
2022-08-02 20:43:50.610172: validation loss: -0.6468
2022-08-02 20:43:50.642652: Average global foreground Dice: [0.728]
2022-08-02 20:43:50.674752: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:43:51.836001: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-02 20:43:51.866941: saving best epoch checkpoint...
2022-08-02 20:43:52.239432: saving checkpoint...
2022-08-02 20:43:58.551447: done, saving took 6.65 seconds
2022-08-02 20:43:58.564932: This epoch took 123.404261 s

2022-08-02 20:43:58.567281: 
epoch:  65
2022-08-02 20:45:49.533320: train loss : -0.6573
2022-08-02 20:45:59.913829: validation loss: -0.6037
2022-08-02 20:45:59.960391: Average global foreground Dice: [0.6946]
2022-08-02 20:45:59.981738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:46:01.068337: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-02 20:46:01.084656: This epoch took 122.515302 s

2022-08-02 20:46:01.097693: 
epoch:  66
2022-08-02 20:48:00.657726: train loss : -0.6584
2022-08-02 20:48:13.336257: validation loss: -0.6205
2022-08-02 20:48:13.350418: Average global foreground Dice: [0.7067]
2022-08-02 20:48:13.370736: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:48:14.423207: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-02 20:48:14.454835: saving best epoch checkpoint...
2022-08-02 20:48:14.837554: saving checkpoint...
2022-08-02 20:48:21.202166: done, saving took 6.73 seconds
2022-08-02 20:48:21.216261: This epoch took 140.101509 s

2022-08-02 20:48:21.218713: 
epoch:  67
2022-08-02 20:50:14.068475: train loss : -0.6509
2022-08-02 20:50:25.234955: validation loss: -0.6483
2022-08-02 20:50:25.270927: Average global foreground Dice: [0.7341]
2022-08-02 20:50:25.286726: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:50:26.256980: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-02 20:50:26.259968: saving best epoch checkpoint...
2022-08-02 20:50:26.523027: saving checkpoint...
2022-08-02 20:50:32.601208: done, saving took 6.34 seconds
2022-08-02 20:50:32.615394: This epoch took 131.394364 s

2022-08-02 20:50:32.617417: 
epoch:  68
2022-08-02 20:52:20.736062: train loss : -0.6636
2022-08-02 20:52:29.770367: validation loss: -0.6352
2022-08-02 20:52:29.821509: Average global foreground Dice: [0.7206]
2022-08-02 20:52:29.845737: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:52:31.558731: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-02 20:52:31.583736: saving best epoch checkpoint...
2022-08-02 20:52:32.338573: saving checkpoint...
2022-08-02 20:52:40.175151: done, saving took 8.53 seconds
2022-08-02 20:52:40.197302: This epoch took 127.577946 s

2022-08-02 20:52:40.200151: 
epoch:  69
2022-08-02 20:54:38.803163: train loss : -0.6513
2022-08-02 20:54:48.944285: validation loss: -0.6717
2022-08-02 20:54:48.989325: Average global foreground Dice: [0.7555]
2022-08-02 20:54:49.000700: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:54:50.355642: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-02 20:54:50.383040: saving best epoch checkpoint...
2022-08-02 20:54:50.590415: saving checkpoint...
2022-08-02 20:54:56.186512: done, saving took 5.79 seconds
2022-08-02 20:54:56.197358: This epoch took 135.994500 s

2022-08-02 20:54:56.199406: 
epoch:  70
2022-08-02 20:56:46.740458: train loss : -0.6558
2022-08-02 20:56:58.679980: validation loss: -0.6451
2022-08-02 20:56:58.709434: Average global foreground Dice: [0.7412]
2022-08-02 20:56:58.742717: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:57:00.127765: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-02 20:57:00.150764: saving best epoch checkpoint...
2022-08-02 20:57:00.343838: saving checkpoint...
2022-08-02 20:57:06.177273: done, saving took 6.00 seconds
2022-08-02 20:57:06.186949: This epoch took 129.985605 s

2022-08-02 20:57:06.189369: 
epoch:  71
2022-08-02 20:58:58.219105: train loss : -0.6532
2022-08-02 20:59:07.228037: validation loss: -0.6179
2022-08-02 20:59:07.252379: Average global foreground Dice: [0.7073]
2022-08-02 20:59:07.284764: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 20:59:08.540106: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-02 20:59:08.562715: This epoch took 122.371043 s

2022-08-02 20:59:08.584583: 
epoch:  72
2022-08-02 21:01:02.165145: train loss : -0.6681
2022-08-02 21:01:13.732831: validation loss: -0.6610
2022-08-02 21:01:13.761824: Average global foreground Dice: [0.7428]
2022-08-02 21:01:13.785731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:01:14.751621: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-02 21:01:14.783749: saving best epoch checkpoint...
2022-08-02 21:01:15.262415: saving checkpoint...
2022-08-02 21:01:23.010356: done, saving took 8.20 seconds
2022-08-02 21:01:23.023110: This epoch took 134.433651 s

2022-08-02 21:01:23.025309: 
epoch:  73
2022-08-02 21:03:17.429249: train loss : -0.6732
2022-08-02 21:03:26.900810: validation loss: -0.6791
2022-08-02 21:03:26.929546: Average global foreground Dice: [0.7537]
2022-08-02 21:03:26.968721: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:03:28.064992: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-02 21:03:28.094919: saving best epoch checkpoint...
2022-08-02 21:03:28.440352: saving checkpoint...
2022-08-02 21:03:34.894814: done, saving took 6.77 seconds
2022-08-02 21:03:34.908706: This epoch took 131.881218 s

2022-08-02 21:03:34.910957: 
epoch:  74
2022-08-02 21:05:26.731886: train loss : -0.6761
2022-08-02 21:05:37.740331: validation loss: -0.6665
2022-08-02 21:05:37.761356: Average global foreground Dice: [0.7457]
2022-08-02 21:05:37.780867: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:05:38.900214: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-02 21:05:38.927761: saving best epoch checkpoint...
2022-08-02 21:05:39.278526: saving checkpoint...
2022-08-02 21:05:46.234293: done, saving took 7.27 seconds
2022-08-02 21:05:46.248664: This epoch took 131.335579 s

2022-08-02 21:05:46.250777: 
epoch:  75
2022-08-02 21:07:35.382472: train loss : -0.6764
2022-08-02 21:07:46.777589: validation loss: -0.6434
2022-08-02 21:07:46.834190: Average global foreground Dice: [0.7269]
2022-08-02 21:07:46.860314: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:07:48.124687: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-02 21:07:48.155960: saving best epoch checkpoint...
2022-08-02 21:07:48.682854: saving checkpoint...
2022-08-02 21:07:56.439025: done, saving took 8.27 seconds
2022-08-02 21:07:56.459437: This epoch took 130.206687 s

2022-08-02 21:07:56.461636: 
epoch:  76
2022-08-02 21:09:47.102864: train loss : -0.6601
2022-08-02 21:09:58.327880: validation loss: -0.6504
2022-08-02 21:09:58.372816: Average global foreground Dice: [0.7437]
2022-08-02 21:09:58.394745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:09:59.936433: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-02 21:09:59.976741: saving best epoch checkpoint...
2022-08-02 21:10:00.393752: saving checkpoint...
2022-08-02 21:10:07.132846: done, saving took 7.10 seconds
2022-08-02 21:10:07.152672: This epoch took 130.688789 s

2022-08-02 21:10:07.154999: 
epoch:  77
2022-08-02 21:11:55.342220: train loss : -0.6938
2022-08-02 21:12:02.085988: validation loss: -0.6671
2022-08-02 21:12:02.090869: Average global foreground Dice: [0.7426]
2022-08-02 21:12:02.093531: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:12:02.913856: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-02 21:12:02.916660: saving best epoch checkpoint...
2022-08-02 21:12:03.190113: saving checkpoint...
2022-08-02 21:12:10.917155: done, saving took 8.00 seconds
2022-08-02 21:12:10.932925: This epoch took 123.775594 s

2022-08-02 21:12:10.936015: 
epoch:  78
2022-08-02 21:13:59.017606: train loss : -0.6828
2022-08-02 21:14:10.171483: validation loss: -0.6205
2022-08-02 21:14:10.192346: Average global foreground Dice: [0.7041]
2022-08-02 21:14:10.226344: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:14:11.356471: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-02 21:14:11.373573: This epoch took 120.435261 s

2022-08-02 21:14:11.387753: 
epoch:  79
2022-08-02 21:15:58.291149: train loss : -0.6681
2022-08-02 21:16:09.683502: validation loss: -0.6281
2022-08-02 21:16:09.713266: Average global foreground Dice: [0.7186]
2022-08-02 21:16:09.751721: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:16:11.179790: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-02 21:16:11.189859: This epoch took 119.777322 s

2022-08-02 21:16:11.192195: 
epoch:  80
2022-08-02 21:18:16.008165: train loss : -0.6709
2022-08-02 21:18:25.304065: validation loss: -0.6668
2022-08-02 21:18:25.315118: Average global foreground Dice: [0.7551]
2022-08-02 21:18:25.353714: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:18:26.467461: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-02 21:18:26.499850: saving best epoch checkpoint...
2022-08-02 21:18:26.776881: saving checkpoint...
2022-08-02 21:18:33.970477: done, saving took 7.45 seconds
2022-08-02 21:18:33.985593: This epoch took 142.790349 s

2022-08-02 21:18:33.987879: 
epoch:  81
2022-08-02 21:20:29.939078: train loss : -0.6676
2022-08-02 21:20:40.916225: validation loss: -0.6774
2022-08-02 21:20:40.956225: Average global foreground Dice: [0.7586]
2022-08-02 21:20:40.981722: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:20:42.268685: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-02 21:20:42.295747: saving best epoch checkpoint...
2022-08-02 21:20:42.700394: saving checkpoint...
2022-08-02 21:20:50.853550: done, saving took 8.54 seconds
2022-08-02 21:20:50.863062: This epoch took 136.870109 s

2022-08-02 21:20:50.865288: 
epoch:  82
2022-08-02 21:22:37.467448: train loss : -0.6653
2022-08-02 21:22:49.134806: validation loss: -0.6669
2022-08-02 21:22:49.164532: Average global foreground Dice: [0.748]
2022-08-02 21:22:49.195707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:22:50.622174: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-02 21:22:50.665760: saving best epoch checkpoint...
2022-08-02 21:22:50.990511: saving checkpoint...
2022-08-02 21:22:57.970198: done, saving took 7.27 seconds
2022-08-02 21:22:57.980486: This epoch took 127.112982 s

2022-08-02 21:22:57.982519: 
epoch:  83
2022-08-02 21:24:49.924500: train loss : -0.6888
2022-08-02 21:24:59.967060: validation loss: -0.6621
2022-08-02 21:24:59.995192: Average global foreground Dice: [0.7372]
2022-08-02 21:25:00.019841: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:25:01.085759: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-02 21:25:01.099707: saving best epoch checkpoint...
2022-08-02 21:25:01.437901: saving checkpoint...
2022-08-02 21:25:08.879473: done, saving took 7.76 seconds
2022-08-02 21:25:08.895955: This epoch took 130.911198 s

2022-08-02 21:25:08.898461: 
epoch:  84
2022-08-02 21:27:00.697946: train loss : -0.7003
2022-08-02 21:27:09.364981: validation loss: -0.6779
2022-08-02 21:27:09.408679: Average global foreground Dice: [0.7541]
2022-08-02 21:27:09.437997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:27:10.270145: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-02 21:27:10.272743: saving best epoch checkpoint...
2022-08-02 21:27:10.627272: saving checkpoint...
2022-08-02 21:27:17.343071: done, saving took 7.07 seconds
2022-08-02 21:27:17.357175: This epoch took 128.456360 s

2022-08-02 21:27:17.359467: 
epoch:  85
2022-08-02 21:29:09.424079: train loss : -0.6943
2022-08-02 21:29:19.310263: validation loss: -0.6577
2022-08-02 21:29:19.341089: Average global foreground Dice: [0.7309]
2022-08-02 21:29:19.359057: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:29:20.148140: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-02 21:29:20.179835: This epoch took 122.817975 s

2022-08-02 21:29:20.202705: 
epoch:  86
2022-08-02 21:31:13.254386: train loss : -0.6936
2022-08-02 21:31:23.139034: validation loss: -0.6693
2022-08-02 21:31:23.171422: Average global foreground Dice: [0.7522]
2022-08-02 21:31:23.193731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:31:24.494295: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-02 21:31:24.515813: saving best epoch checkpoint...
2022-08-02 21:31:24.963235: saving checkpoint...
2022-08-02 21:31:31.898015: done, saving took 7.35 seconds
2022-08-02 21:31:31.914186: This epoch took 131.693214 s

2022-08-02 21:31:31.916589: 
epoch:  87
2022-08-02 21:33:18.238505: train loss : -0.6987
2022-08-02 21:33:27.938090: validation loss: -0.6629
2022-08-02 21:33:27.972402: Average global foreground Dice: [0.7395]
2022-08-02 21:33:27.993713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:33:29.760422: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-02 21:33:29.790767: saving best epoch checkpoint...
2022-08-02 21:33:30.385508: saving checkpoint...
2022-08-02 21:33:36.713021: done, saving took 6.89 seconds
2022-08-02 21:33:36.733342: This epoch took 124.814532 s

2022-08-02 21:33:36.735780: 
epoch:  88
2022-08-02 21:35:18.511322: train loss : -0.6987
2022-08-02 21:35:31.265687: validation loss: -0.6347
2022-08-02 21:35:31.297467: Average global foreground Dice: [0.7182]
2022-08-02 21:35:31.327747: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:35:32.437053: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-02 21:35:32.466821: This epoch took 115.728347 s

2022-08-02 21:35:32.499706: 
epoch:  89
2022-08-02 21:37:25.723964: train loss : -0.6952
2022-08-02 21:37:38.015552: validation loss: -0.6475
2022-08-02 21:37:38.062387: Average global foreground Dice: [0.7344]
2022-08-02 21:37:38.094823: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:37:39.327484: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-02 21:37:39.359786: This epoch took 126.826061 s

2022-08-02 21:37:39.422727: 
epoch:  90
2022-08-02 21:39:35.567295: train loss : -0.6781
2022-08-02 21:39:48.431926: validation loss: -0.6660
2022-08-02 21:39:48.447756: Average global foreground Dice: [0.7458]
2022-08-02 21:39:48.463620: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:39:49.870701: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-02 21:39:49.896839: This epoch took 130.441105 s

2022-08-02 21:39:49.929710: 
epoch:  91
2022-08-02 21:41:48.615593: train loss : -0.6963
2022-08-02 21:42:00.163079: validation loss: -0.6900
2022-08-02 21:42:00.195860: Average global foreground Dice: [0.7689]
2022-08-02 21:42:00.216930: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:42:01.267909: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-02 21:42:01.289373: saving best epoch checkpoint...
2022-08-02 21:42:01.501103: saving checkpoint...
2022-08-02 21:42:08.306191: done, saving took 7.00 seconds
2022-08-02 21:42:08.318702: This epoch took 138.366962 s

2022-08-02 21:42:08.320980: 
epoch:  92
2022-08-02 21:44:01.894482: train loss : -0.6981
2022-08-02 21:44:12.044168: validation loss: -0.6914
2022-08-02 21:44:12.084669: Average global foreground Dice: [0.7654]
2022-08-02 21:44:12.107728: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:44:13.481646: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-02 21:44:13.521750: saving best epoch checkpoint...
2022-08-02 21:44:13.883279: saving checkpoint...
2022-08-02 21:44:20.212394: done, saving took 6.66 seconds
2022-08-02 21:44:20.221743: This epoch took 131.898586 s

2022-08-02 21:44:20.223899: 
epoch:  93
2022-08-02 21:46:11.313923: train loss : -0.7078
2022-08-02 21:46:21.486975: validation loss: -0.6546
2022-08-02 21:46:21.505587: Average global foreground Dice: [0.7332]
2022-08-02 21:46:21.530719: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:46:23.131846: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-02 21:46:23.151806: This epoch took 122.925900 s

2022-08-02 21:46:23.184702: 
epoch:  94
2022-08-02 21:48:15.396054: train loss : -0.7099
2022-08-02 21:48:29.071431: validation loss: -0.6816
2022-08-02 21:48:29.113153: Average global foreground Dice: [0.7597]
2022-08-02 21:48:29.133751: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:48:30.346952: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-02 21:48:30.388775: saving best epoch checkpoint...
2022-08-02 21:48:30.780559: saving checkpoint...
2022-08-02 21:48:37.091466: done, saving took 6.67 seconds
2022-08-02 21:48:37.102077: This epoch took 133.884326 s

2022-08-02 21:48:37.104438: 
epoch:  95
2022-08-02 21:50:25.177884: train loss : -0.7157
2022-08-02 21:50:36.920256: validation loss: -0.6657
2022-08-02 21:50:36.958205: Average global foreground Dice: [0.7471]
2022-08-02 21:50:36.984704: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:50:38.270402: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-02 21:50:38.300817: saving best epoch checkpoint...
2022-08-02 21:50:38.643583: saving checkpoint...
2022-08-02 21:50:45.162397: done, saving took 6.83 seconds
2022-08-02 21:50:45.178382: This epoch took 128.071823 s

2022-08-02 21:50:45.180760: 
epoch:  96
2022-08-02 21:52:32.363716: train loss : -0.7107
2022-08-02 21:52:43.895345: validation loss: -0.6669
2022-08-02 21:52:43.929295: Average global foreground Dice: [0.7416]
2022-08-02 21:52:43.950718: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:52:45.478931: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-02 21:52:45.510083: This epoch took 120.326645 s

2022-08-02 21:52:45.531735: 
epoch:  97
2022-08-02 21:54:51.499186: train loss : -0.6985
2022-08-02 21:55:03.983283: validation loss: -0.6565
2022-08-02 21:55:04.022336: Average global foreground Dice: [0.7332]
2022-08-02 21:55:04.056292: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:55:05.195740: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-02 21:55:05.214787: This epoch took 139.649086 s

2022-08-02 21:55:05.247701: 
epoch:  98
2022-08-02 21:57:03.421081: train loss : -0.6974
2022-08-02 21:57:15.101000: validation loss: -0.6674
2022-08-02 21:57:15.121627: Average global foreground Dice: [0.751]
2022-08-02 21:57:15.140703: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:57:17.048477: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-02 21:57:17.081781: This epoch took 131.801056 s

2022-08-02 21:57:17.114695: 
epoch:  99
2022-08-02 21:59:13.331328: train loss : -0.7100
2022-08-02 21:59:22.269750: validation loss: -0.6670
2022-08-02 21:59:22.313538: Average global foreground Dice: [0.7469]
2022-08-02 21:59:22.337801: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 21:59:23.260649: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-02 21:59:23.263669: saving scheduled checkpoint file...
2022-08-02 21:59:23.655692: saving checkpoint...
2022-08-02 21:59:30.524026: done, saving took 7.25 seconds
2022-08-02 21:59:30.541711: done
2022-08-02 21:59:30.544007: saving best epoch checkpoint...
2022-08-02 21:59:30.693269: saving checkpoint...
2022-08-02 21:59:35.373463: done, saving took 4.83 seconds
2022-08-02 21:59:35.387650: This epoch took 138.239960 s

2022-08-02 21:59:35.389846: 
epoch:  100
2022-08-02 22:01:33.623850: train loss : -0.6931
2022-08-02 22:01:43.320130: validation loss: -0.6803
2022-08-02 22:01:43.357281: Average global foreground Dice: [0.7562]
2022-08-02 22:01:43.376722: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:01:44.452093: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-02 22:01:44.472739: saving best epoch checkpoint...
2022-08-02 22:01:44.858782: saving checkpoint...
2022-08-02 22:01:51.522660: done, saving took 7.04 seconds
2022-08-02 22:01:51.543234: This epoch took 136.151177 s

2022-08-02 22:01:51.546168: 
epoch:  101
2022-08-02 22:03:41.894843: train loss : -0.7068
2022-08-02 22:03:52.789748: validation loss: -0.6732
2022-08-02 22:03:52.833478: Average global foreground Dice: [0.7522]
2022-08-02 22:03:52.856706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:03:54.461984: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-02 22:03:54.479827: saving best epoch checkpoint...
2022-08-02 22:03:54.902793: saving checkpoint...
2022-08-02 22:04:02.527095: done, saving took 8.02 seconds
2022-08-02 22:04:02.582734: This epoch took 131.034003 s

2022-08-02 22:04:02.593004: 
epoch:  102
2022-08-02 22:05:49.284845: train loss : -0.7114
2022-08-02 22:06:02.011014: validation loss: -0.6819
2022-08-02 22:06:02.032271: Average global foreground Dice: [0.7578]
2022-08-02 22:06:02.066374: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:06:03.539529: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-02 22:06:03.572746: saving best epoch checkpoint...
2022-08-02 22:06:03.881773: saving checkpoint...
2022-08-02 22:06:09.912184: done, saving took 6.31 seconds
2022-08-02 22:06:09.923950: This epoch took 127.303128 s

2022-08-02 22:06:09.926203: 
epoch:  103
2022-08-02 22:07:58.707700: train loss : -0.7198
2022-08-02 22:08:08.734160: validation loss: -0.6860
2022-08-02 22:08:08.766002: Average global foreground Dice: [0.762]
2022-08-02 22:08:08.782795: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:08:09.927334: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-02 22:08:09.958123: saving best epoch checkpoint...
2022-08-02 22:08:10.211002: saving checkpoint...
2022-08-02 22:08:17.631364: done, saving took 7.66 seconds
2022-08-02 22:08:17.643463: This epoch took 127.715265 s

2022-08-02 22:08:17.645724: 
epoch:  104
2022-08-02 22:10:09.933969: train loss : -0.7193
2022-08-02 22:10:21.531646: validation loss: -0.6681
2022-08-02 22:10:21.558640: Average global foreground Dice: [0.7541]
2022-08-02 22:10:21.577688: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:10:23.151375: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-02 22:10:23.154541: saving best epoch checkpoint...
2022-08-02 22:10:23.449632: saving checkpoint...
2022-08-02 22:10:30.154750: done, saving took 6.98 seconds
2022-08-02 22:10:30.172165: This epoch took 132.524410 s

2022-08-02 22:10:30.174685: 
epoch:  105
2022-08-02 22:12:22.817054: train loss : -0.7274
2022-08-02 22:12:34.676806: validation loss: -0.7100
2022-08-02 22:12:34.739361: Average global foreground Dice: [0.7821]
2022-08-02 22:12:34.758982: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:12:35.928655: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-02 22:12:35.959823: saving best epoch checkpoint...
2022-08-02 22:12:36.476403: saving checkpoint...
2022-08-02 22:12:42.502460: done, saving took 6.51 seconds
2022-08-02 22:12:42.518771: This epoch took 132.341854 s

2022-08-02 22:12:42.521267: 
epoch:  106
2022-08-02 22:14:31.826969: train loss : -0.7295
2022-08-02 22:14:44.115197: validation loss: -0.6948
2022-08-02 22:14:44.152354: Average global foreground Dice: [0.7705]
2022-08-02 22:14:44.183477: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:14:45.658773: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-02 22:14:45.687763: saving best epoch checkpoint...
2022-08-02 22:14:46.060300: saving checkpoint...
2022-08-02 22:14:53.509903: done, saving took 7.80 seconds
2022-08-02 22:14:53.525357: This epoch took 131.001772 s

2022-08-02 22:14:53.527877: 
epoch:  107
2022-08-02 22:16:39.141291: train loss : -0.6909
2022-08-02 22:16:50.264807: validation loss: -0.6825
2022-08-02 22:16:50.298361: Average global foreground Dice: [0.7625]
2022-08-02 22:16:50.320696: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:16:51.574777: Suus1 maybe_update_lr lr: 8e-05
2022-08-02 22:16:51.601788: saving best epoch checkpoint...
2022-08-02 22:16:52.225097: saving checkpoint...
2022-08-02 22:16:59.354790: done, saving took 7.74 seconds
2022-08-02 22:16:59.368968: This epoch took 125.838790 s

2022-08-02 22:16:59.371288: 
epoch:  108
2022-08-02 22:18:50.589206: train loss : -0.7167
2022-08-02 22:18:59.902843: validation loss: -0.6887
2022-08-02 22:18:59.912669: Average global foreground Dice: [0.764]
2022-08-02 22:18:59.921057: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:19:00.862832: Suus1 maybe_update_lr lr: 8e-05
2022-08-02 22:19:00.891747: saving best epoch checkpoint...
2022-08-02 22:19:01.366631: saving checkpoint...
2022-08-02 22:19:08.594345: done, saving took 7.69 seconds
2022-08-02 22:19:08.607677: This epoch took 129.233439 s

2022-08-02 22:19:08.610398: 
epoch:  109
2022-08-02 22:20:56.027683: train loss : -0.7176
2022-08-02 22:21:07.867229: validation loss: -0.6637
2022-08-02 22:21:07.903274: Average global foreground Dice: [0.7487]
2022-08-02 22:21:07.929688: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:21:09.245424: Suus1 maybe_update_lr lr: 8e-05
2022-08-02 22:21:09.274983: This epoch took 120.662730 s

2022-08-02 22:21:09.297689: 
epoch:  110
2022-08-02 22:23:03.639221: train loss : -0.7160
2022-08-02 22:23:15.212570: validation loss: -0.7136
2022-08-02 22:23:15.248453: Average global foreground Dice: [0.791]
2022-08-02 22:23:15.280232: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:23:16.559616: Suus1 maybe_update_lr lr: 8e-05
2022-08-02 22:23:16.592835: saving best epoch checkpoint...
2022-08-02 22:23:17.164765: saving checkpoint...
2022-08-02 22:23:23.440315: done, saving took 6.82 seconds
2022-08-02 22:23:23.455006: This epoch took 134.136848 s

2022-08-02 22:23:23.457333: 
epoch:  111
2022-08-02 22:25:12.043917: train loss : -0.7302
2022-08-02 22:25:24.283017: validation loss: -0.7063
2022-08-02 22:25:24.317391: Average global foreground Dice: [0.7805]
2022-08-02 22:25:24.338726: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:25:25.620187: Suus1 maybe_update_lr lr: 8e-05
2022-08-02 22:25:25.648798: saving best epoch checkpoint...
2022-08-02 22:25:25.886207: saving checkpoint...
2022-08-02 22:25:32.891371: done, saving took 7.21 seconds
2022-08-02 22:25:32.933726: This epoch took 129.474194 s

2022-08-02 22:25:32.956958: 
epoch:  112
2022-08-02 22:27:21.970316: train loss : -0.7214
2022-08-02 22:27:34.829277: validation loss: -0.6653
2022-08-02 22:27:34.861184: Average global foreground Dice: [0.7418]
2022-08-02 22:27:34.875747: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:27:36.100126: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-02 22:27:36.118819: This epoch took 123.136126 s

2022-08-02 22:27:36.138710: 
epoch:  113
2022-08-02 22:29:35.405858: train loss : -0.7244
2022-08-02 22:29:45.292175: validation loss: -0.6780
2022-08-02 22:29:45.296225: Average global foreground Dice: [0.7546]
2022-08-02 22:29:45.318739: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:29:46.495755: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-02 22:29:46.519236: This epoch took 130.343511 s

2022-08-02 22:29:46.539709: 
epoch:  114
2022-08-02 22:31:44.246183: train loss : -0.7245
2022-08-02 22:31:57.343004: validation loss: -0.6593
2022-08-02 22:31:57.383516: Average global foreground Dice: [0.7414]
2022-08-02 22:31:57.423729: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:31:59.058475: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-02 22:31:59.074865: This epoch took 132.506868 s

2022-08-02 22:31:59.106721: 
epoch:  115
2022-08-02 22:33:48.757504: train loss : -0.7104
2022-08-02 22:34:01.815041: validation loss: -0.7076
2022-08-02 22:34:01.880518: Average global foreground Dice: [0.7904]
2022-08-02 22:34:01.902703: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:34:03.240637: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-02 22:34:03.260792: This epoch took 124.121056 s

2022-08-02 22:34:03.282706: 
epoch:  116
2022-08-02 22:35:55.341331: train loss : -0.7406
2022-08-02 22:36:05.019758: validation loss: -0.6910
2022-08-02 22:36:05.051654: Average global foreground Dice: [0.7738]
2022-08-02 22:36:05.064197: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:36:06.033847: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-02 22:36:06.059731: saving best epoch checkpoint...
2022-08-02 22:36:06.424532: saving checkpoint...
2022-08-02 22:36:12.757487: done, saving took 6.67 seconds
2022-08-02 22:36:12.770241: This epoch took 129.456544 s

2022-08-02 22:36:12.772611: 
epoch:  117
2022-08-02 22:37:59.480670: train loss : -0.7210
2022-08-02 22:38:10.051885: validation loss: -0.7037
2022-08-02 22:38:10.071937: Average global foreground Dice: [0.7765]
2022-08-02 22:38:10.114747: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:38:11.353125: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-02 22:38:11.389760: saving best epoch checkpoint...
2022-08-02 22:38:11.799956: saving checkpoint...
2022-08-02 22:38:18.340265: done, saving took 6.94 seconds
2022-08-02 22:38:18.352319: This epoch took 125.577443 s

2022-08-02 22:38:18.354875: 
epoch:  118
2022-08-02 22:40:11.175482: train loss : -0.7187
2022-08-02 22:40:23.467942: validation loss: -0.6771
2022-08-02 22:40:23.498448: Average global foreground Dice: [0.7551]
2022-08-02 22:40:23.521708: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:40:24.594371: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-02 22:40:24.624857: This epoch took 126.267492 s

2022-08-02 22:40:24.657710: 
epoch:  119
2022-08-02 22:42:14.377805: train loss : -0.7327
2022-08-02 22:42:26.338358: validation loss: -0.7050
2022-08-02 22:42:26.369330: Average global foreground Dice: [0.7791]
2022-08-02 22:42:26.402034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:42:27.537437: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-02 22:42:27.576784: saving best epoch checkpoint...
2022-08-02 22:42:27.923824: saving checkpoint...
2022-08-02 22:42:35.528867: done, saving took 7.92 seconds
2022-08-02 22:42:35.551848: This epoch took 130.861111 s

2022-08-02 22:42:35.554235: 
epoch:  120
2022-08-02 22:44:26.971022: train loss : -0.7299
2022-08-02 22:44:38.467883: validation loss: -0.6853
2022-08-02 22:44:38.500839: Average global foreground Dice: [0.7549]
2022-08-02 22:44:38.530746: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:44:39.411186: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-02 22:44:39.429152: This epoch took 123.872603 s

2022-08-02 22:44:39.449955: 
epoch:  121
2022-08-02 22:46:34.615034: train loss : -0.7305
2022-08-02 22:46:44.864737: validation loss: -0.6999
2022-08-02 22:46:44.896925: Average global foreground Dice: [0.7639]
2022-08-02 22:46:44.916404: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:46:46.720277: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-02 22:46:46.754795: This epoch took 127.274346 s

2022-08-02 22:46:46.776706: 
epoch:  122
2022-08-02 22:48:47.775999: train loss : -0.7300
2022-08-02 22:48:59.434044: validation loss: -0.7014
2022-08-02 22:48:59.465760: Average global foreground Dice: [0.7692]
2022-08-02 22:48:59.495719: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:49:01.297857: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-02 22:49:01.329778: This epoch took 134.539422 s

2022-08-02 22:49:01.356699: 
epoch:  123
2022-08-02 22:51:03.326658: train loss : -0.7047
2022-08-02 22:51:13.400798: validation loss: -0.6861
2022-08-02 22:51:13.456269: Average global foreground Dice: [0.7569]
2022-08-02 22:51:13.469840: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:51:14.761215: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-02 22:51:14.782752: This epoch took 133.389060 s

2022-08-02 22:51:14.804689: 
epoch:  124
2022-08-02 22:53:09.746221: train loss : -0.7365
2022-08-02 22:53:21.010098: validation loss: -0.6928
2022-08-02 22:53:21.051485: Average global foreground Dice: [0.7605]
2022-08-02 22:53:21.069701: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:53:22.587445: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-02 22:53:22.629783: This epoch took 127.803076 s

2022-08-02 22:53:22.673693: 
epoch:  125
2022-08-02 22:55:19.067516: train loss : -0.7264
2022-08-02 22:55:29.590222: validation loss: -0.6721
2022-08-02 22:55:29.622507: Average global foreground Dice: [0.7452]
2022-08-02 22:55:29.637766: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:55:30.961891: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-02 22:55:30.978671: This epoch took 128.269941 s

2022-08-02 22:55:30.995468: 
epoch:  126
2022-08-02 22:57:33.803930: train loss : -0.7381
2022-08-02 22:57:44.376579: validation loss: -0.7231
2022-08-02 22:57:44.413451: Average global foreground Dice: [0.7864]
2022-08-02 22:57:44.442861: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:57:45.597521: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-02 22:57:45.618087: This epoch took 134.613555 s

2022-08-02 22:57:45.641732: 
epoch:  127
2022-08-02 22:59:36.123325: train loss : -0.7452
2022-08-02 22:59:47.053917: validation loss: -0.6959
2022-08-02 22:59:47.065147: Average global foreground Dice: [0.7679]
2022-08-02 22:59:47.067456: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 22:59:48.572042: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-02 22:59:48.603797: saving best epoch checkpoint...
2022-08-02 22:59:48.910680: saving checkpoint...
2022-08-02 22:59:56.012733: done, saving took 7.38 seconds
2022-08-02 22:59:56.026968: This epoch took 130.354829 s

2022-08-02 22:59:56.029334: 
epoch:  128
2022-08-02 23:01:46.864573: train loss : -0.7358
2022-08-02 23:01:57.399924: validation loss: -0.7016
2022-08-02 23:01:57.432635: Average global foreground Dice: [0.7654]
2022-08-02 23:01:57.447742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:01:58.690872: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-02 23:01:58.720821: saving best epoch checkpoint...
2022-08-02 23:01:59.248249: saving checkpoint...
2022-08-02 23:02:06.558514: done, saving took 7.82 seconds
2022-08-02 23:02:06.573450: This epoch took 130.541637 s

2022-08-02 23:02:06.576358: 
epoch:  129
2022-08-02 23:03:56.917277: train loss : -0.7368
2022-08-02 23:04:07.807967: validation loss: -0.7046
2022-08-02 23:04:07.837126: Average global foreground Dice: [0.7779]
2022-08-02 23:04:07.867719: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:04:08.742497: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-02 23:04:08.764758: saving best epoch checkpoint...
2022-08-02 23:04:09.063790: saving checkpoint...
2022-08-02 23:04:16.896653: done, saving took 8.10 seconds
2022-08-02 23:04:16.913619: This epoch took 130.335038 s

2022-08-02 23:04:16.916159: 
epoch:  130
2022-08-02 23:06:16.145636: train loss : -0.7315
2022-08-02 23:06:26.437654: validation loss: -0.6837
2022-08-02 23:06:26.463625: Average global foreground Dice: [0.7584]
2022-08-02 23:06:26.487734: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:06:27.954133: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-02 23:06:27.985798: This epoch took 131.066885 s

2022-08-02 23:06:28.007685: 
epoch:  131
2022-08-02 23:08:20.340395: train loss : -0.7480
2022-08-02 23:08:31.246137: validation loss: -0.7041
2022-08-02 23:08:31.279479: Average global foreground Dice: [0.7812]
2022-08-02 23:08:31.308459: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:08:32.911918: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-02 23:08:32.943771: saving best epoch checkpoint...
2022-08-02 23:08:33.157139: saving checkpoint...
2022-08-02 23:08:39.957146: done, saving took 6.99 seconds
2022-08-02 23:08:39.973939: This epoch took 131.932204 s

2022-08-02 23:08:39.976604: 
epoch:  132
2022-08-02 23:10:28.275654: train loss : -0.7404
2022-08-02 23:10:40.869179: validation loss: -0.6712
2022-08-02 23:10:40.901768: Average global foreground Dice: [0.7383]
2022-08-02 23:10:40.924737: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:10:42.501727: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-02 23:10:42.528792: This epoch took 122.549506 s

2022-08-02 23:10:42.562680: 
epoch:  133
2022-08-02 23:12:45.665321: train loss : -0.7418
2022-08-02 23:12:56.787657: validation loss: -0.6780
2022-08-02 23:12:56.805580: Average global foreground Dice: [0.753]
2022-08-02 23:12:56.830951: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:12:57.984307: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-02 23:12:58.015917: This epoch took 135.420187 s

2022-08-02 23:12:58.040745: 
epoch:  134
2022-08-02 23:14:47.483203: train loss : -0.7295
2022-08-02 23:14:55.839696: validation loss: -0.7132
2022-08-02 23:14:55.859200: Average global foreground Dice: [0.7831]
2022-08-02 23:14:55.887802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:14:56.938265: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-02 23:14:56.966757: This epoch took 118.894479 s

2022-08-02 23:14:56.996749: 
epoch:  135
2022-08-02 23:16:48.333539: train loss : -0.7443
2022-08-02 23:17:01.748199: validation loss: -0.7175
2022-08-02 23:17:01.778436: Average global foreground Dice: [0.7879]
2022-08-02 23:17:01.804730: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:17:02.984302: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-02 23:17:03.009794: saving best epoch checkpoint...
2022-08-02 23:17:03.491844: saving checkpoint...
2022-08-02 23:17:10.127019: done, saving took 7.08 seconds
2022-08-02 23:17:10.137641: This epoch took 133.116933 s

2022-08-02 23:17:10.141169: 
epoch:  136
2022-08-02 23:18:57.716705: train loss : -0.7294
2022-08-02 23:19:10.755918: validation loss: -0.7012
2022-08-02 23:19:10.787537: Average global foreground Dice: [0.7688]
2022-08-02 23:19:10.810703: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:19:12.107213: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-02 23:19:12.126749: saving best epoch checkpoint...
2022-08-02 23:19:12.413911: saving checkpoint...
2022-08-02 23:19:19.855291: done, saving took 7.70 seconds
2022-08-02 23:19:19.865238: This epoch took 129.721135 s

2022-08-02 23:19:19.867570: 
epoch:  137
2022-08-02 23:21:08.625710: train loss : -0.7410
2022-08-02 23:21:18.632041: validation loss: -0.7176
2022-08-02 23:21:18.665452: Average global foreground Dice: [0.7843]
2022-08-02 23:21:18.699733: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:21:19.762809: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-02 23:21:19.782742: saving best epoch checkpoint...
2022-08-02 23:21:20.090257: saving checkpoint...
2022-08-02 23:21:27.343743: done, saving took 7.54 seconds
2022-08-02 23:21:27.354139: This epoch took 127.484446 s

2022-08-02 23:21:27.356413: 
epoch:  138
2022-08-02 23:23:23.510595: train loss : -0.7502
2022-08-02 23:23:33.288103: validation loss: -0.7249
2022-08-02 23:23:33.327273: Average global foreground Dice: [0.7999]
2022-08-02 23:23:33.352934: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:23:34.584837: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-02 23:23:34.611562: saving best epoch checkpoint...
2022-08-02 23:23:34.900432: saving checkpoint...
2022-08-02 23:23:41.188632: done, saving took 6.56 seconds
2022-08-02 23:23:41.202539: This epoch took 133.844049 s

2022-08-02 23:23:41.204700: 
epoch:  139
2022-08-02 23:25:25.763525: train loss : -0.7545
2022-08-02 23:25:37.510467: validation loss: -0.7246
2022-08-02 23:25:37.562424: Average global foreground Dice: [0.796]
2022-08-02 23:25:37.591764: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:25:39.092725: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-02 23:25:39.123780: saving best epoch checkpoint...
2022-08-02 23:25:39.459900: saving checkpoint...
2022-08-02 23:25:45.793299: done, saving took 6.65 seconds
2022-08-02 23:25:45.809048: This epoch took 124.602327 s

2022-08-02 23:25:45.811286: 
epoch:  140
2022-08-02 23:27:36.742418: train loss : -0.7424
2022-08-02 23:27:46.475112: validation loss: -0.7076
2022-08-02 23:27:46.512546: Average global foreground Dice: [0.7858]
2022-08-02 23:27:46.533928: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:27:48.496179: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-02 23:27:48.527768: saving best epoch checkpoint...
2022-08-02 23:27:48.781388: saving checkpoint...
2022-08-02 23:27:55.801242: done, saving took 7.25 seconds
2022-08-02 23:27:55.814651: This epoch took 130.001012 s

2022-08-02 23:27:55.816966: 
epoch:  141
2022-08-02 23:29:47.262129: train loss : -0.7480
2022-08-02 23:29:58.985934: validation loss: -0.7059
2022-08-02 23:29:59.017208: Average global foreground Dice: [0.7757]
2022-08-02 23:29:59.023460: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:30:00.465240: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-02 23:30:00.493755: saving best epoch checkpoint...
2022-08-02 23:30:00.802371: saving checkpoint...
2022-08-02 23:30:06.835014: done, saving took 6.32 seconds
2022-08-02 23:30:06.847679: This epoch took 131.028665 s

2022-08-02 23:30:06.849940: 
epoch:  142
2022-08-02 23:31:57.017948: train loss : -0.7487
2022-08-02 23:32:08.936374: validation loss: -0.6609
2022-08-02 23:32:08.962169: Average global foreground Dice: [0.7371]
2022-08-02 23:32:08.974804: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:32:10.560714: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-02 23:32:10.579815: This epoch took 123.727650 s

2022-08-02 23:32:10.598041: 
epoch:  143
2022-08-02 23:34:09.810400: train loss : -0.7418
2022-08-02 23:34:21.750876: validation loss: -0.7050
2022-08-02 23:34:21.780669: Average global foreground Dice: [0.7791]
2022-08-02 23:34:21.810851: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:34:22.970442: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-02 23:34:22.999767: This epoch took 132.375221 s

2022-08-02 23:34:23.019967: 
epoch:  144
2022-08-02 23:36:20.380843: train loss : -0.7377
2022-08-02 23:36:32.628640: validation loss: -0.7181
2022-08-02 23:36:32.655531: Average global foreground Dice: [0.7901]
2022-08-02 23:36:32.672700: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:36:34.465180: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-02 23:36:34.508797: This epoch took 131.460502 s

2022-08-02 23:36:34.541718: 
epoch:  145
2022-08-02 23:38:26.193053: train loss : -0.7501
2022-08-02 23:38:38.591150: validation loss: -0.6863
2022-08-02 23:38:38.616733: Average global foreground Dice: [0.7632]
2022-08-02 23:38:38.637689: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:38:39.867368: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-02 23:38:39.899796: This epoch took 125.333690 s

2022-08-02 23:38:39.921697: 
epoch:  146
2022-08-02 23:40:35.634961: train loss : -0.7506
2022-08-02 23:40:48.587781: validation loss: -0.6867
2022-08-02 23:40:48.625655: Average global foreground Dice: [0.763]
2022-08-02 23:40:48.640711: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:40:50.194700: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-02 23:40:50.226764: This epoch took 130.272058 s

2022-08-02 23:40:50.259658: 
epoch:  147
2022-08-02 23:42:41.091627: train loss : -0.7373
2022-08-02 23:42:52.736507: validation loss: -0.6855
2022-08-02 23:42:52.773405: Average global foreground Dice: [0.7516]
2022-08-02 23:42:52.802723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:42:53.793329: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-02 23:42:53.824216: This epoch took 123.530532 s

2022-08-02 23:42:53.853721: 
epoch:  148
2022-08-02 23:45:00.846173: train loss : -0.7383
2022-08-02 23:45:11.207030: validation loss: -0.7188
2022-08-02 23:45:11.246541: Average global foreground Dice: [0.79]
2022-08-02 23:45:11.279752: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:45:12.502707: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-02 23:45:12.523749: This epoch took 138.648025 s

2022-08-02 23:45:12.566394: 
epoch:  149
2022-08-02 23:47:05.058594: train loss : -0.7598
2022-08-02 23:47:19.691644: validation loss: -0.7122
2022-08-02 23:47:19.726166: Average global foreground Dice: [0.7774]
2022-08-02 23:47:19.756712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:47:21.115687: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-02 23:47:21.148699: saving scheduled checkpoint file...
2022-08-02 23:47:21.469006: saving checkpoint...
2022-08-02 23:47:27.297846: done, saving took 6.12 seconds
2022-08-02 23:47:27.313083: done
2022-08-02 23:47:27.315208: This epoch took 134.727662 s

2022-08-02 23:47:27.317258: 
epoch:  150
2022-08-02 23:49:14.747838: train loss : -0.7575
2022-08-02 23:49:25.526887: validation loss: -0.6946
2022-08-02 23:49:25.545605: Average global foreground Dice: [0.7624]
2022-08-02 23:49:25.566708: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:49:27.523200: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-02 23:49:27.566834: This epoch took 120.247300 s

2022-08-02 23:49:27.592711: 
epoch:  151
2022-08-02 23:51:19.014208: train loss : -0.7515
2022-08-02 23:51:32.931222: validation loss: -0.7026
2022-08-02 23:51:32.960246: Average global foreground Dice: [0.7764]
2022-08-02 23:51:33.013712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:51:34.554401: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-02 23:51:34.576862: This epoch took 126.952143 s

2022-08-02 23:51:34.598724: 
epoch:  152
2022-08-02 23:53:31.213847: train loss : -0.7447
2022-08-02 23:53:43.057447: validation loss: -0.7040
2022-08-02 23:53:43.084358: Average global foreground Dice: [0.7778]
2022-08-02 23:53:43.104726: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:53:44.430446: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-02 23:53:44.475514: This epoch took 129.859805 s

2022-08-02 23:53:44.507686: 
epoch:  153
2022-08-02 23:55:35.265364: train loss : -0.7481
2022-08-02 23:55:44.840901: validation loss: -0.7273
2022-08-02 23:55:44.867265: Average global foreground Dice: [0.8018]
2022-08-02 23:55:44.890710: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:55:46.526584: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-02 23:55:46.573782: saving best epoch checkpoint...
2022-08-02 23:55:46.927046: saving checkpoint...
2022-08-02 23:55:54.113264: done, saving took 7.50 seconds
2022-08-02 23:55:54.129680: This epoch took 129.583975 s

2022-08-02 23:55:54.131965: 
epoch:  154
2022-08-02 23:57:39.090222: train loss : -0.7582
2022-08-02 23:57:51.054016: validation loss: -0.7077
2022-08-02 23:57:51.083308: Average global foreground Dice: [0.7754]
2022-08-02 23:57:51.119705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 23:57:52.526851: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-02 23:57:52.554885: This epoch took 118.420473 s

2022-08-02 23:57:52.577721: 
epoch:  155
2022-08-02 23:59:48.624803: train loss : -0.7588
2022-08-02 23:59:59.998788: validation loss: -0.7400
2022-08-03 00:00:00.030485: Average global foreground Dice: [0.8056]
2022-08-03 00:00:00.051753: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:00:01.873468: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-03 00:00:01.906869: saving best epoch checkpoint...
2022-08-03 00:00:02.276365: saving checkpoint...
2022-08-03 00:00:10.016669: done, saving took 8.08 seconds
2022-08-03 00:00:10.030825: This epoch took 137.426104 s

2022-08-03 00:00:10.033105: 
epoch:  156
2022-08-03 00:02:02.813423: train loss : -0.7540
2022-08-03 00:02:13.862816: validation loss: -0.7283
2022-08-03 00:02:13.895263: Average global foreground Dice: [0.797]
2022-08-03 00:02:13.916700: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:02:15.449727: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-03 00:02:15.471940: saving best epoch checkpoint...
2022-08-03 00:02:15.989616: saving checkpoint...
2022-08-03 00:02:22.650843: done, saving took 7.15 seconds
2022-08-03 00:02:22.672204: This epoch took 132.636955 s

2022-08-03 00:02:22.675159: 
epoch:  157
2022-08-03 00:04:10.723293: train loss : -0.7501
2022-08-03 00:04:22.323738: validation loss: -0.7163
2022-08-03 00:04:22.355378: Average global foreground Dice: [0.7816]
2022-08-03 00:04:22.387783: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:04:23.751346: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-03 00:04:23.765972: saving best epoch checkpoint...
2022-08-03 00:04:24.011798: saving checkpoint...
2022-08-03 00:04:30.135695: done, saving took 6.33 seconds
2022-08-03 00:04:30.146267: This epoch took 127.468657 s

2022-08-03 00:04:30.148560: 
epoch:  158
2022-08-03 00:06:20.924408: train loss : -0.7510
2022-08-03 00:06:31.552919: validation loss: -0.7124
2022-08-03 00:06:31.556867: Average global foreground Dice: [0.7802]
2022-08-03 00:06:31.559523: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:06:32.466521: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-03 00:06:32.469915: This epoch took 122.319130 s

2022-08-03 00:06:32.472688: 
epoch:  159
2022-08-03 00:08:32.817131: train loss : -0.7647
2022-08-03 00:08:43.847545: validation loss: -0.7285
2022-08-03 00:08:43.881349: Average global foreground Dice: [0.7983]
2022-08-03 00:08:43.909637: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:08:46.026663: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-03 00:08:46.047733: saving best epoch checkpoint...
2022-08-03 00:08:46.304578: saving checkpoint...
2022-08-03 00:08:52.520954: done, saving took 6.45 seconds
2022-08-03 00:08:52.532004: This epoch took 140.056717 s

2022-08-03 00:08:52.534540: 
epoch:  160
2022-08-03 00:10:42.610867: train loss : -0.7441
2022-08-03 00:10:52.228023: validation loss: -0.7113
2022-08-03 00:10:52.278205: Average global foreground Dice: [0.7796]
2022-08-03 00:10:52.300867: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:10:53.620098: Suus1 maybe_update_lr lr: 7e-05
2022-08-03 00:10:53.632788: This epoch took 121.095913 s

2022-08-03 00:10:53.649673: 
epoch:  161
2022-08-03 00:12:41.467836: train loss : -0.7567
2022-08-03 00:12:52.985623: validation loss: -0.7241
2022-08-03 00:12:53.019347: Average global foreground Dice: [0.7852]
2022-08-03 00:12:53.040774: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:12:54.727417: Suus1 maybe_update_lr lr: 7e-05
2022-08-03 00:12:54.769790: saving best epoch checkpoint...
2022-08-03 00:12:55.088021: saving checkpoint...
2022-08-03 00:13:02.514057: done, saving took 7.71 seconds
2022-08-03 00:13:02.541778: This epoch took 128.866055 s

2022-08-03 00:13:02.544694: 
epoch:  162
2022-08-03 00:14:58.321928: train loss : -0.7628
2022-08-03 00:15:07.295047: validation loss: -0.7272
2022-08-03 00:15:07.299315: Average global foreground Dice: [0.7964]
2022-08-03 00:15:07.302111: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:15:08.073431: Suus1 maybe_update_lr lr: 7e-05
2022-08-03 00:15:08.076512: saving best epoch checkpoint...
2022-08-03 00:15:08.394242: saving checkpoint...
2022-08-03 00:15:15.500883: done, saving took 7.42 seconds
2022-08-03 00:15:15.520777: This epoch took 132.972728 s

2022-08-03 00:15:15.523117: 
epoch:  163
2022-08-03 00:17:03.967220: train loss : -0.7497
2022-08-03 00:17:14.631314: validation loss: -0.7220
2022-08-03 00:17:14.666409: Average global foreground Dice: [0.7881]
2022-08-03 00:17:14.695707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:17:15.836907: Suus1 maybe_update_lr lr: 7e-05
2022-08-03 00:17:15.878793: saving best epoch checkpoint...
2022-08-03 00:17:16.301372: saving checkpoint...
2022-08-03 00:17:24.110974: done, saving took 8.19 seconds
2022-08-03 00:17:24.126111: This epoch took 128.600609 s

2022-08-03 00:17:24.129266: 
epoch:  164
2022-08-03 00:19:16.222421: train loss : -0.7547
2022-08-03 00:19:28.314802: validation loss: -0.7209
2022-08-03 00:19:28.318580: Average global foreground Dice: [0.7855]
2022-08-03 00:19:28.321058: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:19:30.272027: Suus1 maybe_update_lr lr: 7e-05
2022-08-03 00:19:30.300783: saving best epoch checkpoint...
2022-08-03 00:19:30.982022: saving checkpoint...
2022-08-03 00:19:37.160296: done, saving took 6.83 seconds
2022-08-03 00:19:37.175180: This epoch took 133.043342 s

2022-08-03 00:19:37.177492: 
epoch:  165
2022-08-03 00:21:35.779448: train loss : -0.7620
2022-08-03 00:21:44.402794: validation loss: -0.7540
2022-08-03 00:21:44.437402: Average global foreground Dice: [0.8196]
2022-08-03 00:21:44.457712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:21:46.089859: Suus1 maybe_update_lr lr: 7e-05
2022-08-03 00:21:46.132716: saving best epoch checkpoint...
2022-08-03 00:21:46.517527: saving checkpoint...
2022-08-03 00:21:52.712560: done, saving took 6.56 seconds
2022-08-03 00:21:52.725882: This epoch took 135.546288 s

2022-08-03 00:21:52.728210: 
epoch:  166
2022-08-03 00:23:38.661053: train loss : -0.7488
2022-08-03 00:23:49.447504: validation loss: -0.6971
2022-08-03 00:23:49.483490: Average global foreground Dice: [0.7684]
2022-08-03 00:23:49.516773: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:23:50.470990: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-03 00:23:50.502823: This epoch took 117.772555 s

2022-08-03 00:23:50.522707: 
epoch:  167
2022-08-03 00:25:40.541164: train loss : -0.7622
2022-08-03 00:25:51.179139: validation loss: -0.7126
2022-08-03 00:25:51.207309: Average global foreground Dice: [0.7841]
2022-08-03 00:25:51.240703: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:25:52.241924: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-03 00:25:52.275910: This epoch took 121.721720 s

2022-08-03 00:25:52.319709: 
epoch:  168
2022-08-03 00:27:48.697206: train loss : -0.7641
2022-08-03 00:27:56.859519: validation loss: -0.7207
2022-08-03 00:27:56.910154: Average global foreground Dice: [0.7884]
2022-08-03 00:27:56.941131: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:27:58.062826: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-03 00:27:58.088789: This epoch took 125.731060 s

2022-08-03 00:27:58.104716: 
epoch:  169
2022-08-03 00:29:59.018382: train loss : -0.7524
2022-08-03 00:30:09.482961: validation loss: -0.7060
2022-08-03 00:30:09.508231: Average global foreground Dice: [0.7805]
2022-08-03 00:30:09.516634: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:30:12.023496: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-03 00:30:12.055789: This epoch took 133.921028 s

2022-08-03 00:30:12.088692: 
epoch:  170
2022-08-03 00:32:14.828880: train loss : -0.7631
2022-08-03 00:32:24.977653: validation loss: -0.7054
2022-08-03 00:32:25.013731: Average global foreground Dice: [0.7719]
2022-08-03 00:32:25.043721: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:32:26.455026: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-03 00:32:26.483393: This epoch took 134.369694 s

2022-08-03 00:32:26.510747: 
epoch:  171
2022-08-03 00:34:21.324899: train loss : -0.7568
2022-08-03 00:34:32.473168: validation loss: -0.7135
2022-08-03 00:34:32.486315: Average global foreground Dice: [0.7837]
2022-08-03 00:34:32.489131: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:34:33.599558: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-03 00:34:33.612416: This epoch took 127.079688 s

2022-08-03 00:34:33.615043: 
epoch:  172
2022-08-03 00:36:31.587815: train loss : -0.7620
2022-08-03 00:36:40.386708: validation loss: -0.7156
2022-08-03 00:36:40.420617: Average global foreground Dice: [0.7808]
2022-08-03 00:36:40.452757: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:36:41.872572: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-03 00:36:41.882397: This epoch took 128.264516 s

2022-08-03 00:36:41.916470: 
epoch:  173
2022-08-03 00:38:36.875276: train loss : -0.7608
2022-08-03 00:38:47.333303: validation loss: -0.7130
2022-08-03 00:38:47.366436: Average global foreground Dice: [0.7762]
2022-08-03 00:38:47.398976: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:38:49.217469: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-03 00:38:49.235951: This epoch took 127.288265 s

2022-08-03 00:38:49.252743: 
epoch:  174
2022-08-03 00:40:43.416472: train loss : -0.7515
2022-08-03 00:40:53.007462: validation loss: -0.6993
2022-08-03 00:40:53.039077: Average global foreground Dice: [0.7682]
2022-08-03 00:40:53.069755: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:40:54.424264: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-03 00:40:54.456790: This epoch took 125.178055 s

2022-08-03 00:40:54.535702: 
epoch:  175
2022-08-03 00:42:48.729066: train loss : -0.7634
2022-08-03 00:42:59.952578: validation loss: -0.7232
2022-08-03 00:42:59.982807: Average global foreground Dice: [0.7863]
2022-08-03 00:43:00.015716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:43:01.133079: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-03 00:43:01.147474: This epoch took 126.580766 s

2022-08-03 00:43:01.186720: 
epoch:  176
2022-08-03 00:44:54.756193: train loss : -0.7615
2022-08-03 00:45:06.261637: validation loss: -0.7183
2022-08-03 00:45:06.305386: Average global foreground Dice: [0.7883]
2022-08-03 00:45:06.328736: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:45:07.280333: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-03 00:45:07.308814: This epoch took 126.098664 s

2022-08-03 00:45:07.341725: 
epoch:  177
2022-08-03 00:47:00.376734: train loss : -0.7617
2022-08-03 00:47:11.544826: validation loss: -0.7252
2022-08-03 00:47:11.567159: Average global foreground Dice: [0.7898]
2022-08-03 00:47:11.569938: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:47:12.788970: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-03 00:47:12.820781: This epoch took 125.457060 s

2022-08-03 00:47:12.837533: 
epoch:  178
2022-08-03 00:49:01.776347: train loss : -0.7677
2022-08-03 00:49:16.734581: validation loss: -0.6893
2022-08-03 00:49:16.772790: Average global foreground Dice: [0.7627]
2022-08-03 00:49:16.785719: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:49:18.333503: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-03 00:49:18.367247: This epoch took 125.486523 s

2022-08-03 00:49:18.388689: 
epoch:  179
2022-08-03 00:51:18.824736: train loss : -0.7646
2022-08-03 00:51:28.510195: validation loss: -0.7296
2022-08-03 00:51:28.535392: Average global foreground Dice: [0.7903]
2022-08-03 00:51:28.566187: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:51:29.503406: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-03 00:51:29.538132: This epoch took 131.120394 s

2022-08-03 00:51:29.557823: 
epoch:  180
2022-08-03 00:53:28.274219: train loss : -0.7606
2022-08-03 00:53:39.701937: validation loss: -0.6936
2022-08-03 00:53:39.739687: Average global foreground Dice: [0.7641]
2022-08-03 00:53:39.758913: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:53:40.931710: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-03 00:53:40.964774: This epoch took 131.385077 s

2022-08-03 00:53:40.997715: 
epoch:  181
2022-08-03 00:55:35.135441: train loss : -0.7519
2022-08-03 00:55:46.722514: validation loss: -0.7344
2022-08-03 00:55:46.765325: Average global foreground Dice: [0.8006]
2022-08-03 00:55:46.785707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:55:48.709554: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-03 00:55:48.733778: This epoch took 127.686072 s

2022-08-03 00:55:48.760707: 
epoch:  182
2022-08-03 00:57:39.881103: train loss : -0.7671
2022-08-03 00:57:51.089518: validation loss: -0.7281
2022-08-03 00:57:51.119289: Average global foreground Dice: [0.7967]
2022-08-03 00:57:51.142740: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 00:57:52.092900: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-03 00:57:52.128372: This epoch took 123.338666 s

2022-08-03 00:57:52.143122: 
epoch:  183
2022-08-03 00:59:57.170342: train loss : -0.7697
2022-08-03 01:00:10.807452: validation loss: -0.7156
2022-08-03 01:00:10.841592: Average global foreground Dice: [0.7857]
2022-08-03 01:00:10.889780: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:00:12.003499: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-03 01:00:12.025838: This epoch took 139.853130 s

2022-08-03 01:00:12.058684: 
epoch:  184
2022-08-03 01:02:04.892536: train loss : -0.7580
2022-08-03 01:02:17.268055: validation loss: -0.7129
2022-08-03 01:02:17.301194: Average global foreground Dice: [0.7755]
2022-08-03 01:02:17.333703: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:02:18.867687: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-03 01:02:18.910785: This epoch took 126.819068 s

2022-08-03 01:02:18.943685: 
epoch:  185
2022-08-03 01:04:18.038414: train loss : -0.7652
2022-08-03 01:04:29.573501: validation loss: -0.7028
2022-08-03 01:04:29.608265: Average global foreground Dice: [0.7692]
2022-08-03 01:04:29.628841: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:04:31.108740: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-03 01:04:31.129797: This epoch took 132.159076 s

2022-08-03 01:04:31.157131: 
epoch:  186
2022-08-03 01:06:29.497310: train loss : -0.7703
2022-08-03 01:06:40.621462: validation loss: -0.7364
2022-08-03 01:06:40.651660: Average global foreground Dice: [0.8047]
2022-08-03 01:06:40.656147: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:06:41.663621: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-03 01:06:41.684786: This epoch took 130.507520 s

2022-08-03 01:06:41.710955: 
epoch:  187
2022-08-03 01:08:33.823730: train loss : -0.7739
2022-08-03 01:08:45.832921: validation loss: -0.7398
2022-08-03 01:08:45.862214: Average global foreground Dice: [0.8058]
2022-08-03 01:08:45.892787: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:08:47.189161: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-03 01:08:47.222778: This epoch took 125.490063 s

2022-08-03 01:08:47.255671: 
epoch:  188
2022-08-03 01:10:45.642223: train loss : -0.7759
2022-08-03 01:10:56.714436: validation loss: -0.7507
2022-08-03 01:10:56.741456: Average global foreground Dice: [0.8125]
2022-08-03 01:10:56.761168: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:10:58.137477: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-03 01:10:58.169763: saving best epoch checkpoint...
2022-08-03 01:10:58.642810: saving checkpoint...
2022-08-03 01:11:04.917138: done, saving took 6.71 seconds
2022-08-03 01:11:04.931940: This epoch took 137.643267 s

2022-08-03 01:11:04.934380: 
epoch:  189
2022-08-03 01:12:52.814324: train loss : -0.7712
2022-08-03 01:13:06.123683: validation loss: -0.7181
2022-08-03 01:13:06.155343: Average global foreground Dice: [0.784]
2022-08-03 01:13:06.167765: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:13:07.402778: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-03 01:13:07.421956: This epoch took 122.485350 s

2022-08-03 01:13:07.445687: 
epoch:  190
2022-08-03 01:15:03.246794: train loss : -0.7697
2022-08-03 01:15:17.026375: validation loss: -0.6995
2022-08-03 01:15:17.071519: Average global foreground Dice: [0.777]
2022-08-03 01:15:17.099742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:15:18.541714: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-03 01:15:18.573827: This epoch took 131.104058 s

2022-08-03 01:15:18.595713: 
epoch:  191
2022-08-03 01:17:14.172548: train loss : -0.7746
2022-08-03 01:17:27.190403: validation loss: -0.7219
2022-08-03 01:17:27.209617: Average global foreground Dice: [0.7868]
2022-08-03 01:17:27.234724: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:17:28.427120: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-03 01:17:28.443806: This epoch took 129.804083 s

2022-08-03 01:17:28.481715: 
epoch:  192
2022-08-03 01:19:26.434894: train loss : -0.7642
2022-08-03 01:19:34.975131: validation loss: -0.6873
2022-08-03 01:19:35.011472: Average global foreground Dice: [0.7659]
2022-08-03 01:19:35.032264: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:19:36.157544: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-03 01:19:36.173122: This epoch took 127.670263 s

2022-08-03 01:19:36.192509: 
epoch:  193
2022-08-03 01:21:28.751471: train loss : -0.7674
2022-08-03 01:21:38.625485: validation loss: -0.7443
2022-08-03 01:21:38.657439: Average global foreground Dice: [0.8115]
2022-08-03 01:21:38.680717: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:21:39.848523: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-03 01:21:39.872763: This epoch took 123.648865 s

2022-08-03 01:21:39.875828: 
epoch:  194
2022-08-03 01:23:38.245639: train loss : -0.7699
2022-08-03 01:23:49.751342: validation loss: -0.7124
2022-08-03 01:23:49.791311: Average global foreground Dice: [0.7766]
2022-08-03 01:23:49.823742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:23:51.619905: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-03 01:23:51.659881: This epoch took 131.769647 s

2022-08-03 01:23:51.692732: 
epoch:  195
2022-08-03 01:25:50.035455: train loss : -0.7678
2022-08-03 01:26:00.260848: validation loss: -0.7222
2022-08-03 01:26:00.289328: Average global foreground Dice: [0.7819]
2022-08-03 01:26:00.321748: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:26:01.973437: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-03 01:26:01.994770: This epoch took 130.273045 s

2022-08-03 01:26:02.008700: 
epoch:  196
2022-08-03 01:27:57.262351: train loss : -0.7786
2022-08-03 01:28:08.751494: validation loss: -0.7322
2022-08-03 01:28:08.794281: Average global foreground Dice: [0.7932]
2022-08-03 01:28:08.815736: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:28:10.379260: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-03 01:28:10.396895: This epoch took 128.360703 s

2022-08-03 01:28:10.417739: 
epoch:  197
2022-08-03 01:30:12.450323: train loss : -0.7732
2022-08-03 01:30:20.048770: validation loss: -0.7152
2022-08-03 01:30:20.059715: Average global foreground Dice: [0.7754]
2022-08-03 01:30:20.092078: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:30:20.943988: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-03 01:30:20.966612: This epoch took 130.534472 s

2022-08-03 01:30:20.972676: 
epoch:  198
2022-08-03 01:32:15.709597: train loss : -0.7742
2022-08-03 01:32:26.387883: validation loss: -0.7185
2022-08-03 01:32:26.402678: Average global foreground Dice: [0.7794]
2022-08-03 01:32:26.438707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:32:27.655382: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-03 01:32:27.675843: This epoch took 126.687497 s

2022-08-03 01:32:27.678774: 
epoch:  199
2022-08-03 01:34:28.137532: train loss : -0.7802
2022-08-03 01:34:40.054999: validation loss: -0.7205
2022-08-03 01:34:40.083414: Average global foreground Dice: [0.7872]
2022-08-03 01:34:40.124742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:34:42.705316: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-03 01:34:42.736760: saving scheduled checkpoint file...
2022-08-03 01:34:43.020312: saving checkpoint...
2022-08-03 01:34:49.450567: done, saving took 6.68 seconds
2022-08-03 01:34:49.464622: done
2022-08-03 01:34:49.467082: This epoch took 141.767359 s

2022-08-03 01:34:49.469441: 
epoch:  200
2022-08-03 01:36:43.708100: train loss : -0.7727
2022-08-03 01:36:56.700052: validation loss: -0.7221
2022-08-03 01:36:56.742993: Average global foreground Dice: [0.7826]
2022-08-03 01:36:56.767731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:36:58.092318: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-03 01:36:58.112126: This epoch took 128.640221 s

2022-08-03 01:36:58.129681: 
epoch:  201
2022-08-03 01:38:54.006878: train loss : -0.7647
2022-08-03 01:39:04.233896: validation loss: -0.7126
2022-08-03 01:39:04.263407: Average global foreground Dice: [0.7795]
2022-08-03 01:39:04.284724: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:39:05.599662: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-03 01:39:05.610240: This epoch took 127.435553 s

2022-08-03 01:39:05.612948: 
epoch:  202
2022-08-03 01:41:03.246673: train loss : -0.7663
2022-08-03 01:41:14.607872: validation loss: -0.7127
2022-08-03 01:41:14.638210: Average global foreground Dice: [0.7833]
2022-08-03 01:41:14.657728: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:41:16.271669: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-03 01:41:16.302763: This epoch took 130.687493 s

2022-08-03 01:41:16.317719: 
epoch:  203
2022-08-03 01:43:19.169916: train loss : -0.7756
2022-08-03 01:43:28.996094: validation loss: -0.7233
2022-08-03 01:43:29.030394: Average global foreground Dice: [0.7918]
2022-08-03 01:43:29.062726: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:43:30.712631: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-03 01:43:30.725743: This epoch took 134.374045 s

2022-08-03 01:43:30.750689: 
epoch:  204
2022-08-03 01:45:30.832077: train loss : -0.7805
2022-08-03 01:45:43.611559: validation loss: -0.7089
2022-08-03 01:45:43.643430: Average global foreground Dice: [0.7745]
2022-08-03 01:45:43.662723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:45:44.926498: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-03 01:45:44.945817: This epoch took 134.170113 s

2022-08-03 01:45:44.964681: 
epoch:  205
2022-08-03 01:47:40.125439: train loss : -0.7731
2022-08-03 01:47:50.794294: validation loss: -0.7099
2022-08-03 01:47:50.826550: Average global foreground Dice: [0.7699]
2022-08-03 01:47:50.848699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:47:52.299067: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-03 01:47:52.320774: This epoch took 127.335924 s

2022-08-03 01:47:52.342696: 
epoch:  206
2022-08-03 01:49:47.985955: train loss : -0.7711
2022-08-03 01:49:58.956669: validation loss: -0.7330
2022-08-03 01:49:58.990252: Average global foreground Dice: [0.7973]
2022-08-03 01:49:59.000209: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:50:00.279647: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-03 01:50:00.293252: This epoch took 127.931549 s

2022-08-03 01:50:00.298360: 
epoch:  207
2022-08-03 01:52:04.873047: train loss : -0.7756
2022-08-03 01:52:16.158161: validation loss: -0.7235
2022-08-03 01:52:16.190542: Average global foreground Dice: [0.791]
2022-08-03 01:52:16.212619: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:52:17.229890: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-03 01:52:17.250821: This epoch took 136.929695 s

2022-08-03 01:52:17.272719: 
epoch:  208
2022-08-03 01:54:07.799453: train loss : -0.7849
2022-08-03 01:54:18.800482: validation loss: -0.7255
2022-08-03 01:54:18.820142: Average global foreground Dice: [0.7866]
2022-08-03 01:54:18.822803: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:54:20.350197: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-03 01:54:20.382846: This epoch took 123.078538 s

2022-08-03 01:54:20.402683: 
epoch:  209
2022-08-03 01:56:17.937667: train loss : -0.7698
2022-08-03 01:56:30.305943: validation loss: -0.7188
2022-08-03 01:56:30.339373: Average global foreground Dice: [0.7829]
2022-08-03 01:56:30.369732: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:56:32.537538: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-03 01:56:32.568780: This epoch took 132.152081 s

2022-08-03 01:56:32.587979: 
epoch:  210
2022-08-03 01:58:25.415016: train loss : -0.7824
2022-08-03 01:58:38.424212: validation loss: -0.7076
2022-08-03 01:58:38.468458: Average global foreground Dice: [0.7796]
2022-08-03 01:58:38.494687: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 01:58:39.858618: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-03 01:58:39.890769: This epoch took 127.287387 s

2022-08-03 01:58:39.929704: 
epoch:  211
2022-08-03 02:00:40.491861: train loss : -0.7777
2022-08-03 02:00:51.506808: validation loss: -0.7085
2022-08-03 02:00:51.542169: Average global foreground Dice: [0.7757]
2022-08-03 02:00:51.602743: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:00:52.801632: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-03 02:00:52.835813: This epoch took 132.878081 s

2022-08-03 02:00:52.865638: 
epoch:  212
2022-08-03 02:02:42.854359: train loss : -0.7829
2022-08-03 02:02:55.164086: validation loss: -0.7303
2022-08-03 02:02:55.196250: Average global foreground Dice: [0.7872]
2022-08-03 02:02:55.218726: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:02:56.687250: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-03 02:02:56.719771: This epoch took 123.832943 s

2022-08-03 02:02:56.763695: 
epoch:  213
2022-08-03 02:04:53.401779: train loss : -0.7814
2022-08-03 02:05:06.260206: validation loss: -0.7312
2022-08-03 02:05:06.285582: Average global foreground Dice: [0.7959]
2022-08-03 02:05:06.307792: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:05:07.726200: Suus1 maybe_update_lr lr: 6e-05
2022-08-03 02:05:07.757817: This epoch took 130.986160 s

2022-08-03 02:05:07.790740: 
epoch:  214
2022-08-03 02:07:02.764705: train loss : -0.7826
2022-08-03 02:07:17.390621: validation loss: -0.7202
2022-08-03 02:07:17.416070: Average global foreground Dice: [0.7818]
2022-08-03 02:07:17.448702: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:07:19.224716: Suus1 maybe_update_lr lr: 6e-05
2022-08-03 02:07:19.245813: This epoch took 131.432886 s

2022-08-03 02:07:19.268689: 
epoch:  215
2022-08-03 02:09:13.288922: train loss : -0.7887
2022-08-03 02:09:24.072338: validation loss: -0.7071
2022-08-03 02:09:24.133966: Average global foreground Dice: [0.7713]
2022-08-03 02:09:24.153104: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:09:25.397554: Suus1 maybe_update_lr lr: 6e-05
2022-08-03 02:09:25.419797: This epoch took 126.129066 s

2022-08-03 02:09:25.425629: 
epoch:  216
2022-08-03 02:11:17.769074: train loss : -0.7848
2022-08-03 02:11:28.519157: validation loss: -0.7339
2022-08-03 02:11:28.553486: Average global foreground Dice: [0.7934]
2022-08-03 02:11:28.579691: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:11:30.245693: Suus1 maybe_update_lr lr: 6e-05
2022-08-03 02:11:30.277781: This epoch took 124.849720 s

2022-08-03 02:11:30.297690: 
epoch:  217
2022-08-03 02:13:20.324766: train loss : -0.7804
2022-08-03 02:13:30.476844: validation loss: -0.7131
2022-08-03 02:13:30.518696: Average global foreground Dice: [0.7793]
2022-08-03 02:13:30.541717: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:13:31.842126: Suus1 maybe_update_lr lr: 6e-05
2022-08-03 02:13:31.874745: This epoch took 121.553041 s

2022-08-03 02:13:31.930695: 
epoch:  218
2022-08-03 02:15:28.253398: train loss : -0.7810
2022-08-03 02:15:40.771196: validation loss: -0.6894
2022-08-03 02:15:40.818867: Average global foreground Dice: [0.7579]
2022-08-03 02:15:40.836472: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:15:41.891826: Suus1 maybe_update_lr lr: 6e-05
2022-08-03 02:15:41.909270: This epoch took 129.945557 s

2022-08-03 02:15:41.919741: 
epoch:  219
2022-08-03 02:17:38.099702: train loss : -0.7812
2022-08-03 02:17:50.470592: validation loss: -0.7270
2022-08-03 02:17:50.503399: Average global foreground Dice: [0.7895]
2022-08-03 02:17:50.525712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:17:52.196506: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-03 02:17:52.229749: This epoch took 130.282055 s

2022-08-03 02:17:52.261697: 
epoch:  220
2022-08-03 02:19:47.600843: train loss : -0.7831
2022-08-03 02:19:57.494474: validation loss: -0.6780
2022-08-03 02:19:57.529152: Average global foreground Dice: [0.7439]
2022-08-03 02:19:57.552110: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:19:59.043906: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-03 02:19:59.064825: This epoch took 126.770124 s

2022-08-03 02:19:59.086715: 
epoch:  221
2022-08-03 02:21:55.270488: train loss : -0.7683
2022-08-03 02:22:05.336627: validation loss: -0.7057
2022-08-03 02:22:05.366483: Average global foreground Dice: [0.7678]
2022-08-03 02:22:05.387701: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:22:06.733202: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-03 02:22:06.765747: This epoch took 127.655030 s

2022-08-03 02:22:06.776311: 
epoch:  222
2022-08-03 02:24:01.265453: train loss : -0.7624
2022-08-03 02:24:13.797135: validation loss: -0.7163
2022-08-03 02:24:13.829358: Average global foreground Dice: [0.7886]
2022-08-03 02:24:13.850732: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:24:15.091619: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-03 02:24:15.110773: This epoch took 128.301083 s

2022-08-03 02:24:15.140671: 
epoch:  223
2022-08-03 02:26:10.390278: train loss : -0.7751
2022-08-03 02:26:20.068287: validation loss: -0.7226
2022-08-03 02:26:20.094117: Average global foreground Dice: [0.7818]
2022-08-03 02:26:20.114748: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:26:21.456470: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-03 02:26:21.484483: This epoch took 126.310724 s

2022-08-03 02:26:21.504737: 
epoch:  224
2022-08-03 02:28:17.916237: train loss : -0.7853
2022-08-03 02:28:30.153974: validation loss: -0.7437
2022-08-03 02:28:30.187367: Average global foreground Dice: [0.8049]
2022-08-03 02:28:30.219744: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:28:31.569797: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-03 02:28:31.602754: This epoch took 130.077659 s

2022-08-03 02:28:31.634810: 
epoch:  225
2022-08-03 02:30:33.331827: train loss : -0.7798
2022-08-03 02:30:44.395452: validation loss: -0.7345
2022-08-03 02:30:44.427845: Average global foreground Dice: [0.7968]
2022-08-03 02:30:44.447359: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:30:45.783475: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-03 02:30:45.816028: This epoch took 134.165220 s

2022-08-03 02:30:45.837677: 
epoch:  226
2022-08-03 02:32:39.385508: train loss : -0.7937
2022-08-03 02:32:50.279180: validation loss: -0.7418
2022-08-03 02:32:50.333083: Average global foreground Dice: [0.8024]
2022-08-03 02:32:50.353678: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:32:51.949319: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-03 02:32:51.995824: This epoch took 126.128087 s

2022-08-03 02:32:52.013700: 
epoch:  227
2022-08-03 02:34:47.006019: train loss : -0.7886
2022-08-03 02:34:56.870218: validation loss: -0.7432
2022-08-03 02:34:56.911679: Average global foreground Dice: [0.794]
2022-08-03 02:34:56.927765: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:34:58.159260: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-03 02:34:58.190845: This epoch took 126.148141 s

2022-08-03 02:34:58.215219: 
epoch:  228
2022-08-03 02:36:49.318871: train loss : -0.7870
2022-08-03 02:37:01.044064: validation loss: -0.7472
2022-08-03 02:37:01.092533: Average global foreground Dice: [0.8006]
2022-08-03 02:37:01.123689: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:37:02.309023: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-03 02:37:02.329759: This epoch took 124.091056 s

2022-08-03 02:37:02.351666: 
epoch:  229
2022-08-03 02:38:51.130968: train loss : -0.7920
2022-08-03 02:39:02.254605: validation loss: -0.7398
2022-08-03 02:39:02.288384: Average global foreground Dice: [0.7978]
2022-08-03 02:39:02.312702: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:39:04.986593: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-03 02:39:04.999591: This epoch took 122.615886 s

2022-08-03 02:39:05.008105: 
epoch:  230
2022-08-03 02:41:00.364233: train loss : -0.7902
2022-08-03 02:41:11.967725: validation loss: -0.7380
2022-08-03 02:41:11.996112: Average global foreground Dice: [0.7991]
2022-08-03 02:41:12.025738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:41:13.285284: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-03 02:41:13.305830: saving best epoch checkpoint...
2022-08-03 02:41:13.559677: saving checkpoint...
2022-08-03 02:41:20.818668: done, saving took 7.49 seconds
2022-08-03 02:41:20.832028: This epoch took 135.810216 s

2022-08-03 02:41:20.834315: 
epoch:  231
2022-08-03 02:43:09.464725: train loss : -0.7886
2022-08-03 02:43:25.108119: validation loss: -0.7177
2022-08-03 02:43:25.139285: Average global foreground Dice: [0.7805]
2022-08-03 02:43:25.171727: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:43:26.620337: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-03 02:43:26.644853: This epoch took 125.808389 s

2022-08-03 02:43:26.667742: 
epoch:  232
2022-08-03 02:45:23.772452: train loss : -0.7848
2022-08-03 02:45:36.887212: validation loss: -0.7388
2022-08-03 02:45:36.929744: Average global foreground Dice: [0.8014]
2022-08-03 02:45:36.938433: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:45:38.462734: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-03 02:45:38.495772: saving best epoch checkpoint...
2022-08-03 02:45:38.843059: saving checkpoint...
2022-08-03 02:45:44.657473: done, saving took 6.13 seconds
2022-08-03 02:45:44.669188: This epoch took 137.969263 s

2022-08-03 02:45:44.671314: 
epoch:  233
2022-08-03 02:47:33.962933: train loss : -0.7861
2022-08-03 02:47:45.970961: validation loss: -0.7346
2022-08-03 02:47:46.012811: Average global foreground Dice: [0.7918]
2022-08-03 02:47:46.029706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:47:47.828601: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-03 02:47:47.859755: saving best epoch checkpoint...
2022-08-03 02:47:48.156956: saving checkpoint...
2022-08-03 02:47:54.898518: done, saving took 7.02 seconds
2022-08-03 02:47:54.914244: This epoch took 130.241007 s

2022-08-03 02:47:54.917026: 
epoch:  234
2022-08-03 02:49:54.892266: train loss : -0.7822
2022-08-03 02:50:05.005857: validation loss: -0.7485
2022-08-03 02:50:05.041353: Average global foreground Dice: [0.811]
2022-08-03 02:50:05.073714: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:50:06.464035: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-03 02:50:06.478993: saving best epoch checkpoint...
2022-08-03 02:50:06.916534: saving checkpoint...
2022-08-03 02:50:13.846350: done, saving took 7.34 seconds
2022-08-03 02:50:13.861902: This epoch took 138.942446 s

2022-08-03 02:50:13.864298: 
epoch:  235
2022-08-03 02:52:01.914687: train loss : -0.7874
2022-08-03 02:52:12.659172: validation loss: -0.7392
2022-08-03 02:52:12.689424: Average global foreground Dice: [0.7985]
2022-08-03 02:52:12.697695: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:52:13.817629: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-03 02:52:13.837726: saving best epoch checkpoint...
2022-08-03 02:52:14.207347: saving checkpoint...
2022-08-03 02:52:20.920868: done, saving took 7.07 seconds
2022-08-03 02:52:20.935623: This epoch took 127.069229 s

2022-08-03 02:52:20.938169: 
epoch:  236
2022-08-03 02:54:12.535383: train loss : -0.7830
2022-08-03 02:54:24.149945: validation loss: -0.7010
2022-08-03 02:54:24.194234: Average global foreground Dice: [0.7704]
2022-08-03 02:54:24.215694: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:54:25.538233: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-03 02:54:25.569738: This epoch took 124.629097 s

2022-08-03 02:54:25.591664: 
epoch:  237
2022-08-03 02:56:16.961219: train loss : -0.7922
2022-08-03 02:56:26.384728: validation loss: -0.7337
2022-08-03 02:56:26.422767: Average global foreground Dice: [0.7877]
2022-08-03 02:56:26.452916: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:56:27.605838: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-03 02:56:27.626778: This epoch took 122.003084 s

2022-08-03 02:56:27.659683: 
epoch:  238
2022-08-03 02:58:35.761643: train loss : -0.7898
2022-08-03 02:58:44.595985: validation loss: -0.7231
2022-08-03 02:58:44.628114: Average global foreground Dice: [0.7828]
2022-08-03 02:58:44.643747: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 02:58:45.775398: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-03 02:58:45.800851: This epoch took 138.113086 s

2022-08-03 02:58:45.822711: 
epoch:  239
2022-08-03 03:00:43.305995: train loss : -0.7903
2022-08-03 03:00:57.668710: validation loss: -0.7224
2022-08-03 03:00:57.713107: Average global foreground Dice: [0.7917]
2022-08-03 03:00:57.727726: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:00:59.131388: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-03 03:00:59.173857: This epoch took 133.318124 s

2022-08-03 03:00:59.207684: 
epoch:  240
2022-08-03 03:02:49.863380: train loss : -0.7885
2022-08-03 03:03:01.475253: validation loss: -0.7199
2022-08-03 03:03:01.507310: Average global foreground Dice: [0.7796]
2022-08-03 03:03:01.547700: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:03:03.158789: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-03 03:03:03.185785: This epoch took 123.956077 s

2022-08-03 03:03:03.202696: 
epoch:  241
2022-08-03 03:05:07.059333: train loss : -0.7854
2022-08-03 03:05:18.331295: validation loss: -0.7247
2022-08-03 03:05:18.369498: Average global foreground Dice: [0.7906]
2022-08-03 03:05:18.384705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:05:19.346512: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-03 03:05:19.379809: This epoch took 136.151275 s

2022-08-03 03:05:19.402717: 
epoch:  242
2022-08-03 03:07:08.873313: train loss : -0.7844
2022-08-03 03:07:20.200803: validation loss: -0.7022
2022-08-03 03:07:20.240899: Average global foreground Dice: [0.7697]
2022-08-03 03:07:20.264755: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:07:21.870713: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-03 03:07:21.874053: This epoch took 122.443758 s

2022-08-03 03:07:21.877008: 
epoch:  243
2022-08-03 03:09:20.529723: train loss : -0.7933
2022-08-03 03:09:32.358811: validation loss: -0.7334
2022-08-03 03:09:32.400323: Average global foreground Dice: [0.7946]
2022-08-03 03:09:32.420252: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:09:33.773616: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-03 03:09:33.808756: This epoch took 131.928827 s

2022-08-03 03:09:33.838711: 
epoch:  244
2022-08-03 03:11:29.008098: train loss : -0.7859
2022-08-03 03:11:40.821850: validation loss: -0.7502
2022-08-03 03:11:40.853575: Average global foreground Dice: [0.8132]
2022-08-03 03:11:40.867714: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:11:42.179536: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-03 03:11:42.206045: This epoch took 128.334334 s

2022-08-03 03:11:42.225686: 
epoch:  245
2022-08-03 03:13:42.757969: train loss : -0.7946
2022-08-03 03:13:52.899111: validation loss: -0.7286
2022-08-03 03:13:52.933269: Average global foreground Dice: [0.7948]
2022-08-03 03:13:52.958615: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:13:54.094459: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-03 03:13:54.126737: This epoch took 131.887038 s

2022-08-03 03:13:54.149672: 
epoch:  246
2022-08-03 03:15:49.992551: train loss : -0.7867
2022-08-03 03:16:02.492791: validation loss: -0.7071
2022-08-03 03:16:02.526293: Average global foreground Dice: [0.7676]
2022-08-03 03:16:02.546864: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:16:04.381815: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-03 03:16:04.424876: This epoch took 130.245171 s

2022-08-03 03:16:04.438623: 
epoch:  247
2022-08-03 03:18:03.137667: train loss : -0.7889
2022-08-03 03:18:14.108000: validation loss: -0.7338
2022-08-03 03:18:14.151195: Average global foreground Dice: [0.7961]
2022-08-03 03:18:14.168282: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:18:15.654627: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-03 03:18:15.682796: This epoch took 131.201668 s

2022-08-03 03:18:15.688297: 
epoch:  248
2022-08-03 03:20:09.490857: train loss : -0.8022
2022-08-03 03:20:22.273547: validation loss: -0.7390
2022-08-03 03:20:22.320271: Average global foreground Dice: [0.7977]
2022-08-03 03:20:22.352726: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:20:23.972559: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-03 03:20:24.007824: This epoch took 128.294924 s

2022-08-03 03:20:24.033685: 
epoch:  249
2022-08-03 03:22:22.378391: train loss : -0.7948
2022-08-03 03:22:34.973582: validation loss: -0.7516
2022-08-03 03:22:35.006689: Average global foreground Dice: [0.8045]
2022-08-03 03:22:35.036796: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:22:36.271740: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-03 03:22:36.293730: saving scheduled checkpoint file...
2022-08-03 03:22:36.595989: saving checkpoint...
2022-08-03 03:22:43.097033: done, saving took 6.78 seconds
2022-08-03 03:22:43.115074: done
2022-08-03 03:22:43.117856: This epoch took 139.051161 s

2022-08-03 03:22:43.120383: 
epoch:  250
2022-08-03 03:24:34.612915: train loss : -0.7988
2022-08-03 03:24:48.275916: validation loss: -0.7313
2022-08-03 03:24:48.308769: Average global foreground Dice: [0.7993]
2022-08-03 03:24:48.338687: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:24:49.613529: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-03 03:24:49.641763: This epoch took 126.519273 s

2022-08-03 03:24:49.659438: 
epoch:  251
2022-08-03 03:26:42.785506: train loss : -0.7866
2022-08-03 03:26:54.062295: validation loss: -0.7162
2022-08-03 03:26:54.094107: Average global foreground Dice: [0.7808]
2022-08-03 03:26:54.124716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:26:55.721869: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-03 03:26:55.754818: This epoch took 126.066471 s

2022-08-03 03:26:55.779224: 
epoch:  252
2022-08-03 03:28:46.491498: train loss : -0.7906
2022-08-03 03:28:59.276467: validation loss: -0.7023
2022-08-03 03:28:59.318765: Average global foreground Dice: [0.7554]
2022-08-03 03:28:59.350707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:29:00.381011: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-03 03:29:00.402751: This epoch took 124.601045 s

2022-08-03 03:29:00.420693: 
epoch:  253
2022-08-03 03:31:04.445349: train loss : -0.7851
2022-08-03 03:31:13.663898: validation loss: -0.7541
2022-08-03 03:31:13.697185: Average global foreground Dice: [0.8127]
2022-08-03 03:31:13.725699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:31:15.086049: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-03 03:31:15.115855: This epoch took 134.671968 s

2022-08-03 03:31:15.148631: 
epoch:  254
2022-08-03 03:33:08.755692: train loss : -0.7908
2022-08-03 03:33:18.934473: validation loss: -0.7308
2022-08-03 03:33:18.962214: Average global foreground Dice: [0.7928]
2022-08-03 03:33:18.981876: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:33:20.186488: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-03 03:33:20.207898: This epoch took 125.026152 s

2022-08-03 03:33:20.221697: 
epoch:  255
2022-08-03 03:35:19.934442: train loss : -0.8006
2022-08-03 03:35:32.690909: validation loss: -0.7138
2022-08-03 03:35:32.730346: Average global foreground Dice: [0.7858]
2022-08-03 03:35:32.748727: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:35:33.462349: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-03 03:35:33.488764: This epoch took 133.245044 s

2022-08-03 03:35:33.507705: 
epoch:  256
2022-08-03 03:37:34.795494: train loss : -0.7955
2022-08-03 03:37:45.917195: validation loss: -0.7663
2022-08-03 03:37:45.930434: Average global foreground Dice: [0.8235]
2022-08-03 03:37:45.973723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:37:47.438463: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-03 03:37:47.469801: saving best epoch checkpoint...
2022-08-03 03:37:47.934202: saving checkpoint...
2022-08-03 03:37:54.848927: done, saving took 7.35 seconds
2022-08-03 03:37:54.864276: This epoch took 141.330328 s

2022-08-03 03:37:54.866519: 
epoch:  257
2022-08-03 03:39:47.801199: train loss : -0.7910
2022-08-03 03:39:57.143358: validation loss: -0.7500
2022-08-03 03:39:57.209312: Average global foreground Dice: [0.8095]
2022-08-03 03:39:57.231729: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:39:58.450723: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-03 03:39:58.479782: saving best epoch checkpoint...
2022-08-03 03:39:59.017946: saving checkpoint...
2022-08-03 03:40:06.888507: done, saving took 8.39 seconds
2022-08-03 03:40:06.903988: This epoch took 132.035362 s

2022-08-03 03:40:06.906343: 
epoch:  258
2022-08-03 03:42:00.967311: train loss : -0.7861
2022-08-03 03:42:10.820226: validation loss: -0.7468
2022-08-03 03:42:10.863464: Average global foreground Dice: [0.8068]
2022-08-03 03:42:10.889396: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:42:11.971639: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-03 03:42:11.996737: saving best epoch checkpoint...
2022-08-03 03:42:12.565814: saving checkpoint...
2022-08-03 03:42:19.667973: done, saving took 7.64 seconds
2022-08-03 03:42:19.686763: This epoch took 132.778085 s

2022-08-03 03:42:19.689212: 
epoch:  259
2022-08-03 03:44:10.562322: train loss : -0.7857
2022-08-03 03:44:22.479130: validation loss: -0.7431
2022-08-03 03:44:22.519336: Average global foreground Dice: [0.8085]
2022-08-03 03:44:22.548452: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:44:23.620077: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-03 03:44:23.648605: saving best epoch checkpoint...
2022-08-03 03:44:23.922075: saving checkpoint...
2022-08-03 03:44:31.186370: done, saving took 7.51 seconds
2022-08-03 03:44:31.201427: This epoch took 131.509944 s

2022-08-03 03:44:31.203601: 
epoch:  260
2022-08-03 03:46:24.966188: train loss : -0.7878
2022-08-03 03:46:38.400915: validation loss: -0.7202
2022-08-03 03:46:38.434520: Average global foreground Dice: [0.7829]
2022-08-03 03:46:38.473765: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:46:39.690473: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-03 03:46:39.706785: This epoch took 128.500800 s

2022-08-03 03:46:39.713413: 
epoch:  261
2022-08-03 03:48:33.636185: train loss : -0.7949
2022-08-03 03:48:46.101084: validation loss: -0.7465
2022-08-03 03:48:46.130220: Average global foreground Dice: [0.8039]
2022-08-03 03:48:46.150837: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:48:47.669434: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-03 03:48:47.711167: This epoch took 127.974394 s

2022-08-03 03:48:47.732693: 
epoch:  262
2022-08-03 03:50:46.995104: train loss : -0.7990
2022-08-03 03:50:57.270188: validation loss: -0.7650
2022-08-03 03:50:57.310491: Average global foreground Dice: [0.8209]
2022-08-03 03:50:57.338757: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:50:58.677541: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-03 03:50:58.709752: saving best epoch checkpoint...
2022-08-03 03:50:59.060354: saving checkpoint...
2022-08-03 03:51:06.440645: done, saving took 7.71 seconds
2022-08-03 03:51:06.450383: This epoch took 138.705898 s

2022-08-03 03:51:06.452637: 
epoch:  263
2022-08-03 03:52:56.710517: train loss : -0.7930
2022-08-03 03:53:08.506624: validation loss: -0.7388
2022-08-03 03:53:08.537015: Average global foreground Dice: [0.801]
2022-08-03 03:53:08.553723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:53:09.679535: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-03 03:53:09.710768: saving best epoch checkpoint...
2022-08-03 03:53:10.099926: saving checkpoint...
2022-08-03 03:53:16.647794: done, saving took 6.91 seconds
2022-08-03 03:53:16.661305: This epoch took 130.206626 s

2022-08-03 03:53:16.663649: 
epoch:  264
2022-08-03 03:54:58.310451: train loss : -0.8006
2022-08-03 03:55:10.587409: validation loss: -0.7349
2022-08-03 03:55:10.616278: Average global foreground Dice: [0.7943]
2022-08-03 03:55:10.619229: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:55:12.089528: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-03 03:55:12.107786: This epoch took 115.441765 s

2022-08-03 03:55:12.132717: 
epoch:  265
2022-08-03 03:57:07.082984: train loss : -0.7986
2022-08-03 03:57:18.907916: validation loss: -0.7058
2022-08-03 03:57:18.941516: Average global foreground Dice: [0.7688]
2022-08-03 03:57:18.973736: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:57:20.608799: Suus1 maybe_update_lr lr: 5e-05
2022-08-03 03:57:20.641839: This epoch took 128.486139 s

2022-08-03 03:57:20.686713: 
epoch:  266
2022-08-03 03:59:13.937352: train loss : -0.7981
2022-08-03 03:59:24.601425: validation loss: -0.7529
2022-08-03 03:59:24.629324: Average global foreground Dice: [0.8089]
2022-08-03 03:59:24.655709: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 03:59:25.743570: Suus1 maybe_update_lr lr: 5e-05
2022-08-03 03:59:25.785783: This epoch took 125.061903 s

2022-08-03 03:59:25.829682: 
epoch:  267
2022-08-03 04:01:25.539073: train loss : -0.8066
2022-08-03 04:01:37.011138: validation loss: -0.7434
2022-08-03 04:01:37.036441: Average global foreground Dice: [0.801]
2022-08-03 04:01:37.043465: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:01:38.567394: Suus1 maybe_update_lr lr: 5e-05
2022-08-03 04:01:38.599748: This epoch took 132.749326 s

2022-08-03 04:01:38.632676: 
epoch:  268
2022-08-03 04:03:26.389186: train loss : -0.8038
2022-08-03 04:03:38.374829: validation loss: -0.7376
2022-08-03 04:03:38.392904: Average global foreground Dice: [0.8004]
2022-08-03 04:03:38.412125: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:03:39.642878: Suus1 maybe_update_lr lr: 5e-05
2022-08-03 04:03:39.673760: This epoch took 121.011055 s

2022-08-03 04:03:39.695686: 
epoch:  269
2022-08-03 04:05:37.565650: train loss : -0.7962
2022-08-03 04:05:49.970541: validation loss: -0.7261
2022-08-03 04:05:50.002510: Average global foreground Dice: [0.7839]
2022-08-03 04:05:50.014712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:05:51.600687: Suus1 maybe_update_lr lr: 5e-05
2022-08-03 04:05:51.621767: This epoch took 131.897054 s

2022-08-03 04:05:51.654972: 
epoch:  270
2022-08-03 04:07:45.922373: train loss : -0.7928
2022-08-03 04:07:57.766295: validation loss: -0.7438
2022-08-03 04:07:57.794102: Average global foreground Dice: [0.8026]
2022-08-03 04:07:57.824921: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:07:59.586137: Suus1 maybe_update_lr lr: 5e-05
2022-08-03 04:07:59.634606: This epoch took 127.957693 s

2022-08-03 04:07:59.668699: 
epoch:  271
2022-08-03 04:09:58.936538: train loss : -0.7907
2022-08-03 04:10:09.170190: validation loss: -0.7531
2022-08-03 04:10:09.214601: Average global foreground Dice: [0.8091]
2022-08-03 04:10:09.246746: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:10:10.767897: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-03 04:10:10.778271: This epoch took 131.092367 s

2022-08-03 04:10:10.783103: 
epoch:  272
2022-08-03 04:12:07.879453: train loss : -0.8058
2022-08-03 04:12:19.635864: validation loss: -0.7377
2022-08-03 04:12:19.663504: Average global foreground Dice: [0.7937]
2022-08-03 04:12:19.683281: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:12:20.710426: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-03 04:12:20.742785: This epoch took 129.946966 s

2022-08-03 04:12:20.767704: 
epoch:  273
2022-08-03 04:14:12.300216: train loss : -0.7940
2022-08-03 04:14:24.449909: validation loss: -0.7341
2022-08-03 04:14:24.482319: Average global foreground Dice: [0.7972]
2022-08-03 04:14:24.517704: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:14:26.192614: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-03 04:14:26.258472: This epoch took 125.468760 s

2022-08-03 04:14:26.270567: 
epoch:  274
2022-08-03 04:16:35.159654: train loss : -0.7912
2022-08-03 04:16:47.264484: validation loss: -0.7472
2022-08-03 04:16:47.305602: Average global foreground Dice: [0.8074]
2022-08-03 04:16:47.336705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:16:48.732723: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-03 04:16:48.775691: This epoch took 142.502556 s

2022-08-03 04:16:48.807707: 
epoch:  275
2022-08-03 04:18:51.379978: train loss : -0.7940
2022-08-03 04:19:01.350628: validation loss: -0.7420
2022-08-03 04:19:01.355079: Average global foreground Dice: [0.8022]
2022-08-03 04:19:01.357691: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:19:02.209913: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-03 04:19:02.222021: This epoch took 133.377323 s

2022-08-03 04:19:02.228054: 
epoch:  276
2022-08-03 04:21:02.081318: train loss : -0.7913
2022-08-03 04:21:11.664493: validation loss: -0.6851
2022-08-03 04:21:11.698383: Average global foreground Dice: [0.7572]
2022-08-03 04:21:11.725794: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:21:12.695065: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-03 04:21:12.719782: This epoch took 130.480222 s

2022-08-03 04:21:12.743697: 
epoch:  277
2022-08-03 04:23:08.364594: train loss : -0.7873
2022-08-03 04:23:20.050911: validation loss: -0.7405
2022-08-03 04:23:20.133623: Average global foreground Dice: [0.8014]
2022-08-03 04:23:20.165728: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:23:21.530173: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-03 04:23:21.534708: This epoch took 128.748672 s

2022-08-03 04:23:21.556026: 
epoch:  278
2022-08-03 04:25:15.912663: train loss : -0.7981
2022-08-03 04:25:26.503940: validation loss: -0.7365
2022-08-03 04:25:26.526192: Average global foreground Dice: [0.8026]
2022-08-03 04:25:26.535845: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:25:27.959749: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-03 04:25:27.991788: This epoch took 126.411072 s

2022-08-03 04:25:28.004715: 
epoch:  279
2022-08-03 04:27:29.124045: train loss : -0.8025
2022-08-03 04:27:41.912493: validation loss: -0.7500
2022-08-03 04:27:41.944902: Average global foreground Dice: [0.8067]
2022-08-03 04:27:41.974484: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:27:43.471072: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-03 04:27:43.494900: This epoch took 135.462178 s

2022-08-03 04:27:43.535740: 
epoch:  280
2022-08-03 04:29:37.832095: train loss : -0.8059
2022-08-03 04:29:47.906567: validation loss: -0.7425
2022-08-03 04:29:47.938370: Average global foreground Dice: [0.8073]
2022-08-03 04:29:47.969692: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:29:49.343939: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-03 04:29:49.361736: This epoch took 125.805029 s

2022-08-03 04:29:49.383680: 
epoch:  281
2022-08-03 04:31:38.799235: train loss : -0.7988
2022-08-03 04:31:50.306928: validation loss: -0.7519
2022-08-03 04:31:50.338743: Average global foreground Dice: [0.8141]
2022-08-03 04:31:50.360692: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:31:51.711822: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-03 04:31:51.743802: saving best epoch checkpoint...
2022-08-03 04:31:52.051638: saving checkpoint...
2022-08-03 04:31:58.619154: done, saving took 6.85 seconds
2022-08-03 04:31:58.632079: This epoch took 129.217382 s

2022-08-03 04:31:58.634557: 
epoch:  282
2022-08-03 04:33:39.575125: train loss : -0.8037
2022-08-03 04:33:51.331591: validation loss: -0.7217
2022-08-03 04:33:51.386207: Average global foreground Dice: [0.7766]
2022-08-03 04:33:51.412722: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:33:52.789123: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-03 04:33:52.809786: This epoch took 114.172932 s

2022-08-03 04:33:52.826649: 
epoch:  283
2022-08-03 04:35:52.416185: train loss : -0.8051
2022-08-03 04:36:04.153599: validation loss: -0.7440
2022-08-03 04:36:04.185420: Average global foreground Dice: [0.8081]
2022-08-03 04:36:04.200732: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:36:05.648084: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-03 04:36:05.676803: This epoch took 132.834081 s

2022-08-03 04:36:05.704704: 
epoch:  284
2022-08-03 04:38:05.771345: train loss : -0.7979
2022-08-03 04:38:18.804496: validation loss: -0.7475
2022-08-03 04:38:18.844407: Average global foreground Dice: [0.8083]
2022-08-03 04:38:18.860450: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:38:20.647878: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-03 04:38:20.679356: This epoch took 134.937619 s

2022-08-03 04:38:20.696722: 
epoch:  285
2022-08-03 04:40:11.868524: train loss : -0.7995
2022-08-03 04:40:24.203941: validation loss: -0.7340
2022-08-03 04:40:24.231669: Average global foreground Dice: [0.7901]
2022-08-03 04:40:24.265728: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:40:25.623921: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-03 04:40:25.655870: This epoch took 124.935123 s

2022-08-03 04:40:25.665451: 
epoch:  286
2022-08-03 04:42:19.375090: train loss : -0.8135
2022-08-03 04:42:30.924508: validation loss: -0.7317
2022-08-03 04:42:30.955255: Average global foreground Dice: [0.7914]
2022-08-03 04:42:30.970696: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:42:32.518113: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-03 04:42:32.538242: This epoch took 126.869922 s

2022-08-03 04:42:32.561696: 
epoch:  287
2022-08-03 04:44:30.805401: train loss : -0.8056
2022-08-03 04:44:42.828831: validation loss: -0.7398
2022-08-03 04:44:42.860397: Average global foreground Dice: [0.7976]
2022-08-03 04:44:42.881733: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:44:43.951995: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-03 04:44:43.954819: This epoch took 131.362105 s

2022-08-03 04:44:43.957036: 
epoch:  288
2022-08-03 04:46:45.120830: train loss : -0.8057
2022-08-03 04:46:56.901725: validation loss: -0.7521
2022-08-03 04:46:56.924499: Average global foreground Dice: [0.8104]
2022-08-03 04:46:56.955737: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:46:58.283857: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-03 04:46:58.304808: This epoch took 134.345557 s

2022-08-03 04:46:58.331733: 
epoch:  289
2022-08-03 04:48:56.496872: train loss : -0.8054
2022-08-03 04:49:08.907333: validation loss: -0.7608
2022-08-03 04:49:08.937284: Average global foreground Dice: [0.8195]
2022-08-03 04:49:08.959693: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:49:10.409671: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-03 04:49:10.441775: saving best epoch checkpoint...
2022-08-03 04:49:10.765788: saving checkpoint...
2022-08-03 04:49:16.471792: done, saving took 6.00 seconds
2022-08-03 04:49:16.483783: This epoch took 138.115282 s

2022-08-03 04:49:16.486097: 
epoch:  290
2022-08-03 04:51:04.108483: train loss : -0.8012
2022-08-03 04:51:15.855566: validation loss: -0.7702
2022-08-03 04:51:15.887281: Average global foreground Dice: [0.8228]
2022-08-03 04:51:15.916426: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:51:17.174428: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-03 04:51:17.194772: saving best epoch checkpoint...
2022-08-03 04:51:17.362977: saving checkpoint...
2022-08-03 04:51:23.822490: done, saving took 6.60 seconds
2022-08-03 04:51:23.836673: This epoch took 127.348424 s

2022-08-03 04:51:23.838980: 
epoch:  291
2022-08-03 04:53:12.015924: train loss : -0.8105
2022-08-03 04:53:24.122001: validation loss: -0.7368
2022-08-03 04:53:24.151571: Average global foreground Dice: [0.7947]
2022-08-03 04:53:24.183740: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:53:25.921627: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-03 04:53:25.951766: This epoch took 122.110700 s

2022-08-03 04:53:25.984679: 
epoch:  292
2022-08-03 04:55:17.029649: train loss : -0.8139
2022-08-03 04:55:29.433860: validation loss: -0.7493
2022-08-03 04:55:29.472517: Average global foreground Dice: [0.8044]
2022-08-03 04:55:29.528763: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:55:30.803471: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-03 04:55:30.806726: This epoch took 124.795015 s

2022-08-03 04:55:30.824907: 
epoch:  293
2022-08-03 04:57:33.147275: train loss : -0.8091
2022-08-03 04:57:44.120229: validation loss: -0.7322
2022-08-03 04:57:44.162028: Average global foreground Dice: [0.7944]
2022-08-03 04:57:44.181553: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:57:45.113044: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-03 04:57:45.133768: This epoch took 134.276085 s

2022-08-03 04:57:45.156710: 
epoch:  294
2022-08-03 04:59:42.913179: train loss : -0.8013
2022-08-03 04:59:55.599593: validation loss: -0.7524
2022-08-03 04:59:55.644316: Average global foreground Dice: [0.8119]
2022-08-03 04:59:55.676709: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 04:59:56.996500: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-03 04:59:57.029909: This epoch took 131.842373 s

2022-08-03 04:59:57.054741: 
epoch:  295
2022-08-03 05:01:50.823713: train loss : -0.8067
2022-08-03 05:02:02.736224: validation loss: -0.7356
2022-08-03 05:02:02.770246: Average global foreground Dice: [0.788]
2022-08-03 05:02:02.795709: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:02:04.112449: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-03 05:02:04.137796: This epoch took 127.050052 s

2022-08-03 05:02:04.170707: 
epoch:  296
2022-08-03 05:04:00.963784: train loss : -0.8010
2022-08-03 05:04:11.830122: validation loss: -0.7515
2022-08-03 05:04:11.862002: Average global foreground Dice: [0.8089]
2022-08-03 05:04:11.889703: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:04:12.962070: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-03 05:04:12.966408: This epoch took 128.773700 s

2022-08-03 05:04:12.970726: 
epoch:  297
2022-08-03 05:06:10.425272: train loss : -0.8102
2022-08-03 05:06:22.054427: validation loss: -0.7332
2022-08-03 05:06:22.083315: Average global foreground Dice: [0.7963]
2022-08-03 05:06:22.104702: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:06:23.096560: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-03 05:06:23.123356: This epoch took 130.137405 s

2022-08-03 05:06:23.130940: 
epoch:  298
2022-08-03 05:08:26.508595: train loss : -0.8105
2022-08-03 05:08:36.933148: validation loss: -0.7377
2022-08-03 05:08:36.977599: Average global foreground Dice: [0.797]
2022-08-03 05:08:36.999733: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:08:38.721541: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-03 05:08:38.752778: This epoch took 135.619393 s

2022-08-03 05:08:38.785701: 
epoch:  299
2022-08-03 05:10:25.815973: train loss : -0.8046
2022-08-03 05:10:38.238164: validation loss: -0.7395
2022-08-03 05:10:38.278566: Average global foreground Dice: [0.8004]
2022-08-03 05:10:38.296741: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:10:39.714853: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-03 05:10:39.744297: saving scheduled checkpoint file...
2022-08-03 05:10:40.142665: saving checkpoint...
2022-08-03 05:10:48.232991: done, saving took 8.46 seconds
2022-08-03 05:10:48.254330: done
2022-08-03 05:10:48.256533: This epoch took 129.437838 s

2022-08-03 05:10:48.258604: 
epoch:  300
2022-08-03 05:12:39.827626: train loss : -0.8055
2022-08-03 05:12:51.843312: validation loss: -0.7310
2022-08-03 05:12:51.875275: Average global foreground Dice: [0.7972]
2022-08-03 05:12:51.900722: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:12:53.296519: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-03 05:12:53.327789: This epoch took 125.066852 s

2022-08-03 05:12:53.357689: 
epoch:  301
2022-08-03 05:14:47.631517: train loss : -0.8088
2022-08-03 05:14:59.143398: validation loss: -0.7500
2022-08-03 05:14:59.175323: Average global foreground Dice: [0.8089]
2022-08-03 05:14:59.194703: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:15:00.683348: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-03 05:15:00.715738: This epoch took 127.325054 s

2022-08-03 05:15:00.738671: 
epoch:  302
2022-08-03 05:16:53.290326: train loss : -0.7986
2022-08-03 05:17:05.065928: validation loss: -0.7350
2022-08-03 05:17:05.106225: Average global foreground Dice: [0.7927]
2022-08-03 05:17:05.133698: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:17:06.164603: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-03 05:17:06.194075: This epoch took 125.428373 s

2022-08-03 05:17:06.221747: 
epoch:  303
2022-08-03 05:18:58.085940: train loss : -0.8115
2022-08-03 05:19:08.928204: validation loss: -0.7289
2022-08-03 05:19:08.961282: Average global foreground Dice: [0.7968]
2022-08-03 05:19:08.979580: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:19:10.233508: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-03 05:19:10.254757: This epoch took 124.020028 s

2022-08-03 05:19:10.268531: 
epoch:  304
2022-08-03 05:21:04.584728: train loss : -0.8117
2022-08-03 05:21:16.425534: validation loss: -0.7606
2022-08-03 05:21:16.461965: Average global foreground Dice: [0.8179]
2022-08-03 05:21:16.485737: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:21:17.503136: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-03 05:21:17.536760: This epoch took 127.263657 s

2022-08-03 05:21:17.552411: 
epoch:  305
2022-08-03 05:23:12.208796: train loss : -0.8033
2022-08-03 05:23:22.795760: validation loss: -0.7515
2022-08-03 05:23:22.827392: Average global foreground Dice: [0.8079]
2022-08-03 05:23:22.846713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:23:24.024437: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-03 05:23:24.042996: This epoch took 126.453904 s

2022-08-03 05:23:24.063745: 
epoch:  306
2022-08-03 05:25:20.691432: train loss : -0.8115
2022-08-03 05:25:31.245428: validation loss: -0.7615
2022-08-03 05:25:31.276706: Average global foreground Dice: [0.8138]
2022-08-03 05:25:31.296226: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:25:32.469478: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-03 05:25:32.497998: saving best epoch checkpoint...
2022-08-03 05:25:32.826355: saving checkpoint...
2022-08-03 05:25:39.449106: done, saving took 6.93 seconds
2022-08-03 05:25:39.459660: This epoch took 135.373835 s

2022-08-03 05:25:39.462463: 
epoch:  307
2022-08-03 05:27:29.006869: train loss : -0.8094
2022-08-03 05:27:41.629866: validation loss: -0.7532
2022-08-03 05:27:41.658445: Average global foreground Dice: [0.8127]
2022-08-03 05:27:41.680686: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:27:43.318766: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-03 05:27:43.332965: saving best epoch checkpoint...
2022-08-03 05:27:44.015592: saving checkpoint...
2022-08-03 05:27:51.970640: done, saving took 8.62 seconds
2022-08-03 05:27:51.986432: This epoch took 132.521352 s

2022-08-03 05:27:51.988807: 
epoch:  308
2022-08-03 05:29:41.879130: train loss : -0.8065
2022-08-03 05:29:52.963498: validation loss: -0.7452
2022-08-03 05:29:52.995452: Average global foreground Dice: [0.809]
2022-08-03 05:29:53.021749: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:29:54.714986: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-03 05:29:54.745214: saving best epoch checkpoint...
2022-08-03 05:29:55.128265: saving checkpoint...
2022-08-03 05:30:02.079393: done, saving took 7.31 seconds
2022-08-03 05:30:02.091990: This epoch took 130.101122 s

2022-08-03 05:30:02.094573: 
epoch:  309
2022-08-03 05:32:00.975511: train loss : -0.8154
2022-08-03 05:32:13.775244: validation loss: -0.7461
2022-08-03 05:32:13.818166: Average global foreground Dice: [0.7991]
2022-08-03 05:32:13.832715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:32:15.384770: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-03 05:32:15.404784: This epoch took 133.308141 s

2022-08-03 05:32:15.430688: 
epoch:  310
2022-08-03 05:34:06.846553: train loss : -0.8036
2022-08-03 05:34:16.445838: validation loss: -0.7560
2022-08-03 05:34:16.479110: Average global foreground Dice: [0.8134]
2022-08-03 05:34:16.497107: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:34:17.870409: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-03 05:34:17.874960: saving best epoch checkpoint...
2022-08-03 05:34:18.231892: saving checkpoint...
2022-08-03 05:34:24.524134: done, saving took 6.63 seconds
2022-08-03 05:34:24.538881: This epoch took 129.101363 s

2022-08-03 05:34:24.542274: 
epoch:  311
2022-08-03 05:36:16.879498: train loss : -0.8112
2022-08-03 05:36:28.574880: validation loss: -0.7320
2022-08-03 05:36:28.608248: Average global foreground Dice: [0.7917]
2022-08-03 05:36:28.640745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:36:29.551134: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-03 05:36:29.589824: This epoch took 125.045160 s

2022-08-03 05:36:29.630723: 
epoch:  312
2022-08-03 05:38:35.725114: train loss : -0.8106
2022-08-03 05:38:45.664420: validation loss: -0.7280
2022-08-03 05:38:45.696542: Average global foreground Dice: [0.7874]
2022-08-03 05:38:45.721706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:38:46.669874: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-03 05:38:46.689797: This epoch took 137.015046 s

2022-08-03 05:38:46.730715: 
epoch:  313
2022-08-03 05:40:42.638422: train loss : -0.8114
2022-08-03 05:40:54.041318: validation loss: -0.7655
2022-08-03 05:40:54.074112: Average global foreground Dice: [0.8186]
2022-08-03 05:40:54.093457: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:40:55.138816: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-03 05:40:55.172783: This epoch took 128.417031 s

2022-08-03 05:40:55.192176: 
epoch:  314
2022-08-03 05:42:57.065500: train loss : -0.8130
2022-08-03 05:43:08.849234: validation loss: -0.7503
2022-08-03 05:43:08.881785: Average global foreground Dice: [0.8075]
2022-08-03 05:43:08.897763: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:43:10.875043: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-03 05:43:10.886754: This epoch took 135.679701 s

2022-08-03 05:43:10.897663: 
epoch:  315
2022-08-03 05:45:12.921768: train loss : -0.8074
2022-08-03 05:45:25.194557: validation loss: -0.7432
2022-08-03 05:45:25.232910: Average global foreground Dice: [0.7917]
2022-08-03 05:45:25.236382: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:45:26.132419: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-03 05:45:26.135026: This epoch took 135.231502 s

2022-08-03 05:45:26.153686: 
epoch:  316
2022-08-03 05:47:17.060833: train loss : -0.8129
2022-08-03 05:47:30.136361: validation loss: -0.7456
2022-08-03 05:47:30.168220: Average global foreground Dice: [0.8075]
2022-08-03 05:47:30.198085: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:47:31.707713: Suus1 maybe_update_lr lr: 4e-05
2022-08-03 05:47:31.721423: This epoch took 125.548782 s

2022-08-03 05:47:31.743696: 
epoch:  317
2022-08-03 05:49:25.584951: train loss : -0.8142
2022-08-03 05:49:37.166291: validation loss: -0.7514
2022-08-03 05:49:37.172542: Average global foreground Dice: [0.804]
2022-08-03 05:49:37.201763: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:49:39.516798: Suus1 maybe_update_lr lr: 4e-05
2022-08-03 05:49:39.545750: This epoch took 127.769008 s

2022-08-03 05:49:39.568716: 
epoch:  318
2022-08-03 05:51:35.227373: train loss : -0.8112
2022-08-03 05:51:45.232500: validation loss: -0.7395
2022-08-03 05:51:45.264384: Average global foreground Dice: [0.8]
2022-08-03 05:51:45.292726: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:51:46.589483: Suus1 maybe_update_lr lr: 4e-05
2022-08-03 05:51:46.610808: This epoch took 127.001942 s

2022-08-03 05:51:46.632707: 
epoch:  319
2022-08-03 05:53:46.231313: train loss : -0.8149
2022-08-03 05:53:58.533714: validation loss: -0.7576
2022-08-03 05:53:58.565203: Average global foreground Dice: [0.8191]
2022-08-03 05:53:58.597712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:53:59.810801: Suus1 maybe_update_lr lr: 4e-05
2022-08-03 05:53:59.845788: This epoch took 133.191069 s

2022-08-03 05:53:59.877221: 
epoch:  320
2022-08-03 05:55:50.104131: train loss : -0.8164
2022-08-03 05:56:01.742937: validation loss: -0.7491
2022-08-03 05:56:01.786361: Average global foreground Dice: [0.8058]
2022-08-03 05:56:01.807698: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:56:03.168451: Suus1 maybe_update_lr lr: 4e-05
2022-08-03 05:56:03.220637: This epoch took 123.326913 s

2022-08-03 05:56:03.253722: 
epoch:  321
2022-08-03 05:58:10.436995: train loss : -0.8093
2022-08-03 05:58:21.584685: validation loss: -0.7426
2022-08-03 05:58:21.617465: Average global foreground Dice: [0.8039]
2022-08-03 05:58:21.624766: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 05:58:23.034138: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-03 05:58:23.071821: This epoch took 139.806108 s

2022-08-03 05:58:23.104707: 
epoch:  322
2022-08-03 06:00:18.010170: train loss : -0.8118
2022-08-03 06:00:28.983261: validation loss: -0.7512
2022-08-03 06:00:29.018639: Average global foreground Dice: [0.8061]
2022-08-03 06:00:29.043715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:00:30.322364: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-03 06:00:30.365759: This epoch took 127.238027 s

2022-08-03 06:00:30.400697: 
epoch:  323
2022-08-03 06:02:27.796701: train loss : -0.8150
2022-08-03 06:02:38.716386: validation loss: -0.7605
2022-08-03 06:02:38.746343: Average global foreground Dice: [0.8128]
2022-08-03 06:02:38.772855: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:02:39.570210: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-03 06:02:39.601745: saving best epoch checkpoint...
2022-08-03 06:02:40.075033: saving checkpoint...
2022-08-03 06:02:46.100623: done, saving took 6.50 seconds
2022-08-03 06:02:46.110765: This epoch took 135.707431 s

2022-08-03 06:02:46.112823: 
epoch:  324
2022-08-03 06:04:30.005219: train loss : -0.8171
2022-08-03 06:04:40.226190: validation loss: -0.7533
2022-08-03 06:04:40.254575: Average global foreground Dice: [0.8132]
2022-08-03 06:04:40.281739: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:04:41.715717: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-03 06:04:41.737052: saving best epoch checkpoint...
2022-08-03 06:04:42.135099: saving checkpoint...
2022-08-03 06:04:48.925650: done, saving took 7.16 seconds
2022-08-03 06:04:48.935783: This epoch took 122.820701 s

2022-08-03 06:04:48.937980: 
epoch:  325
2022-08-03 06:06:34.300931: train loss : -0.8152
2022-08-03 06:06:45.139961: validation loss: -0.7434
2022-08-03 06:06:45.165328: Average global foreground Dice: [0.8025]
2022-08-03 06:06:45.197747: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:06:47.024226: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-03 06:06:47.057777: This epoch took 118.117627 s

2022-08-03 06:06:47.071197: 
epoch:  326
2022-08-03 06:08:40.768007: train loss : -0.8156
2022-08-03 06:08:54.507868: validation loss: -0.7570
2022-08-03 06:08:54.541378: Average global foreground Dice: [0.8114]
2022-08-03 06:08:54.573732: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:08:56.047890: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-03 06:08:56.088883: saving best epoch checkpoint...
2022-08-03 06:08:56.602015: saving checkpoint...
2022-08-03 06:09:03.013344: done, saving took 6.90 seconds
2022-08-03 06:09:03.028751: This epoch took 135.955202 s

2022-08-03 06:09:03.031139: 
epoch:  327
2022-08-03 06:10:49.202969: train loss : -0.8136
2022-08-03 06:11:01.351544: validation loss: -0.7509
2022-08-03 06:11:01.382262: Average global foreground Dice: [0.8076]
2022-08-03 06:11:01.410716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:11:02.663344: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-03 06:11:02.684776: saving best epoch checkpoint...
2022-08-03 06:11:03.230739: saving checkpoint...
2022-08-03 06:11:09.417654: done, saving took 6.70 seconds
2022-08-03 06:11:09.447203: This epoch took 126.413897 s

2022-08-03 06:11:09.449403: 
epoch:  328
2022-08-03 06:12:56.492435: train loss : -0.8160
2022-08-03 06:13:07.797767: validation loss: -0.7255
2022-08-03 06:13:07.837389: Average global foreground Dice: [0.7874]
2022-08-03 06:13:07.871690: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:13:09.332038: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-03 06:13:09.360778: This epoch took 119.909294 s

2022-08-03 06:13:09.399700: 
epoch:  329
2022-08-03 06:15:08.238726: train loss : -0.8137
2022-08-03 06:15:20.627112: validation loss: -0.7716
2022-08-03 06:15:20.660629: Average global foreground Dice: [0.8249]
2022-08-03 06:15:20.674701: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:15:21.648095: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-03 06:15:21.679766: saving best epoch checkpoint...
2022-08-03 06:15:22.117845: saving checkpoint...
2022-08-03 06:15:29.084845: done, saving took 7.37 seconds
2022-08-03 06:15:29.095804: This epoch took 139.663060 s

2022-08-03 06:15:29.098056: 
epoch:  330
2022-08-03 06:17:23.773529: train loss : -0.8047
2022-08-03 06:17:36.201763: validation loss: -0.7516
2022-08-03 06:17:36.223258: Average global foreground Dice: [0.7994]
2022-08-03 06:17:36.236455: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:17:37.055301: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-03 06:17:37.080804: This epoch took 127.980665 s

2022-08-03 06:17:37.108702: 
epoch:  331
2022-08-03 06:19:23.594637: train loss : -0.8193
2022-08-03 06:19:34.576588: validation loss: -0.7375
2022-08-03 06:19:34.617906: Average global foreground Dice: [0.8007]
2022-08-03 06:19:34.641709: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:19:36.139431: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-03 06:19:36.170783: This epoch took 119.040066 s

2022-08-03 06:19:36.202717: 
epoch:  332
2022-08-03 06:21:38.574652: train loss : -0.8130
2022-08-03 06:21:51.302924: validation loss: -0.7529
2022-08-03 06:21:51.335447: Average global foreground Dice: [0.8054]
2022-08-03 06:21:51.354701: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:21:52.658508: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-03 06:21:52.690825: This epoch took 136.467077 s

2022-08-03 06:21:52.723713: 
epoch:  333
2022-08-03 06:23:47.909177: train loss : -0.8150
2022-08-03 06:23:58.335829: validation loss: -0.7386
2022-08-03 06:23:58.373373: Average global foreground Dice: [0.7983]
2022-08-03 06:23:58.395683: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:23:59.470347: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-03 06:23:59.497783: This epoch took 126.743077 s

2022-08-03 06:23:59.530709: 
epoch:  334
2022-08-03 06:25:58.485827: train loss : -0.8148
2022-08-03 06:26:11.967270: validation loss: -0.7489
2022-08-03 06:26:12.002016: Average global foreground Dice: [0.803]
2022-08-03 06:26:12.026791: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:26:13.504931: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-03 06:26:13.543849: This epoch took 133.980115 s

2022-08-03 06:26:13.556698: 
epoch:  335
2022-08-03 06:28:09.066476: train loss : -0.8158
2022-08-03 06:28:21.162182: validation loss: -0.7517
2022-08-03 06:28:21.193168: Average global foreground Dice: [0.8081]
2022-08-03 06:28:21.225709: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:28:22.466835: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-03 06:28:22.488763: This epoch took 128.911710 s

2022-08-03 06:28:22.511689: 
epoch:  336
2022-08-03 06:30:15.661560: train loss : -0.8140
2022-08-03 06:30:27.544539: validation loss: -0.7583
2022-08-03 06:30:27.584371: Average global foreground Dice: [0.8082]
2022-08-03 06:30:27.627742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:30:29.179521: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-03 06:30:29.203290: This epoch took 126.673496 s

2022-08-03 06:30:29.205739: 
epoch:  337
2022-08-03 06:32:24.591833: train loss : -0.8115
2022-08-03 06:32:32.333784: validation loss: -0.7476
2022-08-03 06:32:32.377722: Average global foreground Dice: [0.8045]
2022-08-03 06:32:32.411746: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:32:34.424786: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-03 06:32:34.450037: This epoch took 125.241909 s

2022-08-03 06:32:34.456432: 
epoch:  338
2022-08-03 06:34:34.009776: train loss : -0.8158
2022-08-03 06:34:46.841682: validation loss: -0.7400
2022-08-03 06:34:46.856522: Average global foreground Dice: [0.8019]
2022-08-03 06:34:46.859130: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:34:48.049976: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-03 06:34:48.090705: This epoch took 133.611994 s

2022-08-03 06:34:48.101690: 
epoch:  339
2022-08-03 06:36:39.632525: train loss : -0.8176
2022-08-03 06:36:51.619831: validation loss: -0.7403
2022-08-03 06:36:51.679896: Average global foreground Dice: [0.7974]
2022-08-03 06:36:51.701209: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:36:53.206963: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-03 06:36:53.228818: This epoch took 125.113538 s

2022-08-03 06:36:53.263802: 
epoch:  340
2022-08-03 06:38:44.359874: train loss : -0.8173
2022-08-03 06:38:57.566502: validation loss: -0.7670
2022-08-03 06:38:57.598452: Average global foreground Dice: [0.8207]
2022-08-03 06:38:57.618854: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:38:59.383892: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-03 06:38:59.420821: This epoch took 126.124123 s

2022-08-03 06:38:59.448748: 
epoch:  341
2022-08-03 06:41:00.126763: train loss : -0.8144
2022-08-03 06:41:11.443076: validation loss: -0.7505
2022-08-03 06:41:11.471574: Average global foreground Dice: [0.8067]
2022-08-03 06:41:11.474384: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:41:13.141464: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-03 06:41:13.173784: This epoch took 133.704070 s

2022-08-03 06:41:13.206688: 
epoch:  342
2022-08-03 06:43:09.947705: train loss : -0.8162
2022-08-03 06:43:19.707862: validation loss: -0.7497
2022-08-03 06:43:19.741280: Average global foreground Dice: [0.8071]
2022-08-03 06:43:19.773713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:43:20.992962: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-03 06:43:21.023557: This epoch took 127.793824 s

2022-08-03 06:43:21.056683: 
epoch:  343
2022-08-03 06:45:21.883653: train loss : -0.8145
2022-08-03 06:45:33.649827: validation loss: -0.7390
2022-08-03 06:45:33.681031: Average global foreground Dice: [0.7991]
2022-08-03 06:45:33.710702: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:45:35.122994: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-03 06:45:35.154768: This epoch took 134.078635 s

2022-08-03 06:45:35.187681: 
epoch:  344
2022-08-03 06:47:31.449459: train loss : -0.8164
2022-08-03 06:47:42.404430: validation loss: -0.7669
2022-08-03 06:47:42.422076: Average global foreground Dice: [0.8207]
2022-08-03 06:47:42.454693: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:47:43.638544: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-03 06:47:43.671751: saving best epoch checkpoint...
2022-08-03 06:47:43.980596: saving checkpoint...
2022-08-03 06:47:50.446348: done, saving took 6.75 seconds
2022-08-03 06:47:50.460494: This epoch took 135.250814 s

2022-08-03 06:47:50.463369: 
epoch:  345
2022-08-03 06:49:44.029585: train loss : -0.8142
2022-08-03 06:49:55.819304: validation loss: -0.7188
2022-08-03 06:49:55.848625: Average global foreground Dice: [0.7812]
2022-08-03 06:49:55.880745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:49:56.973381: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-03 06:49:56.991618: This epoch took 126.525968 s

2022-08-03 06:49:56.994550: 
epoch:  346
2022-08-03 06:51:49.401399: train loss : -0.8186
2022-08-03 06:52:00.312398: validation loss: -0.7643
2022-08-03 06:52:00.353976: Average global foreground Dice: [0.8177]
2022-08-03 06:52:00.369907: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:52:01.749628: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-03 06:52:01.763671: This epoch took 124.766665 s

2022-08-03 06:52:01.770686: 
epoch:  347
2022-08-03 06:53:57.549103: train loss : -0.8115
2022-08-03 06:54:10.317779: validation loss: -0.7468
2022-08-03 06:54:10.346348: Average global foreground Dice: [0.8068]
2022-08-03 06:54:10.374719: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:54:11.509648: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-03 06:54:11.531747: This epoch took 129.739034 s

2022-08-03 06:54:11.552670: 
epoch:  348
2022-08-03 06:56:09.492099: train loss : -0.8157
2022-08-03 06:56:21.357847: validation loss: -0.7546
2022-08-03 06:56:21.390227: Average global foreground Dice: [0.809]
2022-08-03 06:56:21.408699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:56:22.504715: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-03 06:56:22.535773: This epoch took 130.960094 s

2022-08-03 06:56:22.548713: 
epoch:  349
2022-08-03 06:58:18.337266: train loss : -0.8188
2022-08-03 06:58:30.918727: validation loss: -0.7621
2022-08-03 06:58:30.952321: Average global foreground Dice: [0.822]
2022-08-03 06:58:30.984715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 06:58:32.729641: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-03 06:58:32.762736: saving scheduled checkpoint file...
2022-08-03 06:58:33.117882: saving checkpoint...
2022-08-03 06:58:39.545795: done, saving took 6.76 seconds
2022-08-03 06:58:39.557538: done
2022-08-03 06:58:39.560440: saving best epoch checkpoint...
2022-08-03 06:58:39.644686: saving checkpoint...
2022-08-03 06:58:44.178499: done, saving took 4.62 seconds
2022-08-03 06:58:44.187847: This epoch took 141.616945 s

2022-08-03 06:58:44.191209: 
epoch:  350
2022-08-03 07:00:32.833005: train loss : -0.8157
2022-08-03 07:00:43.303565: validation loss: -0.7322
2022-08-03 07:00:43.324810: Average global foreground Dice: [0.7931]
2022-08-03 07:00:43.345902: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:00:44.285461: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-03 07:00:44.315474: This epoch took 120.122049 s

2022-08-03 07:00:44.318567: 
epoch:  351
2022-08-03 07:02:32.916192: train loss : -0.8120
2022-08-03 07:02:40.044530: validation loss: -0.7410
2022-08-03 07:02:40.048817: Average global foreground Dice: [0.7887]
2022-08-03 07:02:40.051271: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:02:40.829492: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-03 07:02:40.832639: This epoch took 116.500861 s

2022-08-03 07:02:40.834960: 
epoch:  352
2022-08-03 07:04:40.017379: train loss : -0.8172
2022-08-03 07:04:53.077785: validation loss: -0.7343
2022-08-03 07:04:53.134558: Average global foreground Dice: [0.7898]
2022-08-03 07:04:53.159591: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:04:54.097870: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-03 07:04:54.125796: This epoch took 133.288480 s

2022-08-03 07:04:54.153672: 
epoch:  353
2022-08-03 07:06:46.772570: train loss : -0.8189
2022-08-03 07:07:00.820877: validation loss: -0.7275
2022-08-03 07:07:00.902749: Average global foreground Dice: [0.7865]
2022-08-03 07:07:00.905990: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:07:02.667453: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-03 07:07:02.696081: This epoch took 128.515368 s

2022-08-03 07:07:02.716138: 
epoch:  354
2022-08-03 07:08:57.325682: train loss : -0.8222
2022-08-03 07:09:09.741542: validation loss: -0.7430
2022-08-03 07:09:09.784440: Average global foreground Dice: [0.7952]
2022-08-03 07:09:09.832711: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:09:11.073843: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-03 07:09:11.117788: This epoch took 128.380082 s

2022-08-03 07:09:11.169693: 
epoch:  355
2022-08-03 07:11:00.796988: train loss : -0.8181
2022-08-03 07:11:13.971151: validation loss: -0.7459
2022-08-03 07:11:14.002708: Average global foreground Dice: [0.7999]
2022-08-03 07:11:14.037717: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:11:15.148480: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-03 07:11:15.212814: This epoch took 124.009081 s

2022-08-03 07:11:15.252803: 
epoch:  356
2022-08-03 07:13:03.364004: train loss : -0.8207
2022-08-03 07:13:15.246943: validation loss: -0.7673
2022-08-03 07:13:15.280461: Average global foreground Dice: [0.8202]
2022-08-03 07:13:15.313731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:13:16.816564: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-03 07:13:16.848807: This epoch took 121.558074 s

2022-08-03 07:13:16.881718: 
epoch:  357
2022-08-03 07:15:14.032511: train loss : -0.8256
2022-08-03 07:15:26.050997: validation loss: -0.7524
2022-08-03 07:15:26.084282: Average global foreground Dice: [0.8061]
2022-08-03 07:15:26.092944: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:15:27.192319: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-03 07:15:27.222001: This epoch took 130.307208 s

2022-08-03 07:15:27.254677: 
epoch:  358
2022-08-03 07:17:19.043940: train loss : -0.8192
2022-08-03 07:17:29.548350: validation loss: -0.7767
2022-08-03 07:17:29.572531: Average global foreground Dice: [0.8275]
2022-08-03 07:17:29.597807: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:17:30.802399: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-03 07:17:30.827827: This epoch took 123.543396 s

2022-08-03 07:17:30.858688: 
epoch:  359
2022-08-03 07:19:29.843586: train loss : -0.8278
2022-08-03 07:19:44.498636: validation loss: -0.7402
2022-08-03 07:19:44.526475: Average global foreground Dice: [0.8026]
2022-08-03 07:19:44.550292: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:19:45.769259: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-03 07:19:45.800829: This epoch took 134.909127 s

2022-08-03 07:19:45.833779: 
epoch:  360
2022-08-03 07:21:45.710867: train loss : -0.8231
2022-08-03 07:21:56.667400: validation loss: -0.7277
2022-08-03 07:21:56.700436: Average global foreground Dice: [0.7845]
2022-08-03 07:21:56.728797: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:21:57.589350: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-03 07:21:57.639749: This epoch took 131.773011 s

2022-08-03 07:21:57.672708: 
epoch:  361
2022-08-03 07:23:56.067868: train loss : -0.8264
2022-08-03 07:24:06.837483: validation loss: -0.7500
2022-08-03 07:24:06.882285: Average global foreground Dice: [0.8058]
2022-08-03 07:24:06.894478: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:24:08.393210: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-03 07:24:08.413883: This epoch took 130.719169 s

2022-08-03 07:24:08.430694: 
epoch:  362
2022-08-03 07:26:08.870586: train loss : -0.8201
2022-08-03 07:26:22.641983: validation loss: -0.7504
2022-08-03 07:26:22.681343: Average global foreground Dice: [0.8104]
2022-08-03 07:26:22.701662: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:26:24.569103: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-03 07:26:24.600839: This epoch took 136.136891 s

2022-08-03 07:26:24.633754: 
epoch:  363
2022-08-03 07:28:21.998930: train loss : -0.8208
2022-08-03 07:28:30.888586: validation loss: -0.7670
2022-08-03 07:28:30.926539: Average global foreground Dice: [0.8217]
2022-08-03 07:28:30.940876: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:28:32.110943: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-03 07:28:32.146724: This epoch took 127.479987 s

2022-08-03 07:28:32.160412: 
epoch:  364
2022-08-03 07:30:26.748354: train loss : -0.8245
2022-08-03 07:30:37.696307: validation loss: -0.7444
2022-08-03 07:30:37.718138: Average global foreground Dice: [0.7981]
2022-08-03 07:30:37.737488: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:30:39.097860: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-03 07:30:39.121742: This epoch took 126.918053 s

2022-08-03 07:30:39.145067: 
epoch:  365
2022-08-03 07:32:35.823702: train loss : -0.8179
2022-08-03 07:32:46.324307: validation loss: -0.7413
2022-08-03 07:32:46.359440: Average global foreground Dice: [0.7997]
2022-08-03 07:32:46.380725: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:32:47.300194: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-03 07:32:47.302692: This epoch took 128.137469 s

2022-08-03 07:32:47.304996: 
epoch:  366
2022-08-03 07:34:43.064181: train loss : -0.8182
2022-08-03 07:34:55.402057: validation loss: -0.7468
2022-08-03 07:34:55.436689: Average global foreground Dice: [0.8076]
2022-08-03 07:34:55.465736: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:34:57.942785: Suus1 maybe_update_lr lr: 3e-05
2022-08-03 07:34:57.974680: This epoch took 130.667466 s

2022-08-03 07:34:57.994799: 
epoch:  367
2022-08-03 07:36:51.514268: train loss : -0.8199
2022-08-03 07:37:04.090539: validation loss: -0.7412
2022-08-03 07:37:04.124435: Average global foreground Dice: [0.7959]
2022-08-03 07:37:04.148697: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:37:05.455338: Suus1 maybe_update_lr lr: 3e-05
2022-08-03 07:37:05.491785: This epoch took 127.463066 s

2022-08-03 07:37:05.533689: 
epoch:  368
2022-08-03 07:39:02.969554: train loss : -0.8220
2022-08-03 07:39:13.716014: validation loss: -0.7502
2022-08-03 07:39:13.758452: Average global foreground Dice: [0.8041]
2022-08-03 07:39:13.774705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:39:15.189327: Suus1 maybe_update_lr lr: 3e-05
2022-08-03 07:39:15.207561: This epoch took 129.649851 s

2022-08-03 07:39:15.226669: 
epoch:  369
2022-08-03 07:41:18.187276: train loss : -0.8093
2022-08-03 07:41:28.776144: validation loss: -0.7593
2022-08-03 07:41:28.811465: Average global foreground Dice: [0.8111]
2022-08-03 07:41:28.837742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:41:30.797904: Suus1 maybe_update_lr lr: 3e-05
2022-08-03 07:41:30.829796: This epoch took 135.600312 s

2022-08-03 07:41:30.862708: 
epoch:  370
2022-08-03 07:43:31.986943: train loss : -0.8204
2022-08-03 07:43:44.654342: validation loss: -0.7577
2022-08-03 07:43:44.675439: Average global foreground Dice: [0.8064]
2022-08-03 07:43:44.698323: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:43:46.089846: Suus1 maybe_update_lr lr: 3e-05
2022-08-03 07:43:46.105241: This epoch took 135.182523 s

2022-08-03 07:43:46.118719: 
epoch:  371
2022-08-03 07:45:37.789085: train loss : -0.8224
2022-08-03 07:45:46.462120: validation loss: -0.7541
2022-08-03 07:45:46.484378: Average global foreground Dice: [0.8096]
2022-08-03 07:45:46.513658: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:45:47.316426: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-03 07:45:47.319767: This epoch took 121.178049 s

2022-08-03 07:45:47.322540: 
epoch:  372
2022-08-03 07:47:40.843898: train loss : -0.8168
2022-08-03 07:47:50.964424: validation loss: -0.7507
2022-08-03 07:47:51.017314: Average global foreground Dice: [0.8174]
2022-08-03 07:47:51.059881: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:47:53.000013: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-03 07:47:53.032844: This epoch took 125.707602 s

2022-08-03 07:47:53.065706: 
epoch:  373
2022-08-03 07:49:55.655510: train loss : -0.8178
2022-08-03 07:50:06.421088: validation loss: -0.7738
2022-08-03 07:50:06.448643: Average global foreground Dice: [0.827]
2022-08-03 07:50:06.469743: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:50:07.390506: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-03 07:50:07.393947: saving best epoch checkpoint...
2022-08-03 07:50:07.651763: saving checkpoint...
2022-08-03 07:50:13.557427: done, saving took 6.16 seconds
2022-08-03 07:50:13.574101: This epoch took 140.471398 s

2022-08-03 07:50:13.576719: 
epoch:  374
2022-08-03 07:52:05.072116: train loss : -0.8216
2022-08-03 07:52:16.371314: validation loss: -0.7526
2022-08-03 07:52:16.405323: Average global foreground Dice: [0.8142]
2022-08-03 07:52:16.428742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:52:17.933975: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-03 07:52:17.973773: saving best epoch checkpoint...
2022-08-03 07:52:18.570919: saving checkpoint...
2022-08-03 07:52:25.291973: done, saving took 7.29 seconds
2022-08-03 07:52:25.323583: This epoch took 131.744544 s

2022-08-03 07:52:25.326536: 
epoch:  375
2022-08-03 07:54:19.655603: train loss : -0.8194
2022-08-03 07:54:30.885098: validation loss: -0.7537
2022-08-03 07:54:30.931680: Average global foreground Dice: [0.8078]
2022-08-03 07:54:30.963745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:54:32.496272: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-03 07:54:32.525772: This epoch took 127.196683 s

2022-08-03 07:54:32.539402: 
epoch:  376
2022-08-03 07:56:29.406041: train loss : -0.8245
2022-08-03 07:56:40.301625: validation loss: -0.7546
2022-08-03 07:56:40.344176: Average global foreground Dice: [0.8088]
2022-08-03 07:56:40.389717: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:56:42.735447: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-03 07:56:42.761175: This epoch took 130.219252 s

2022-08-03 07:56:42.786679: 
epoch:  377
2022-08-03 07:58:48.118246: train loss : -0.8230
2022-08-03 07:58:58.839219: validation loss: -0.7710
2022-08-03 07:58:58.860464: Average global foreground Dice: [0.8234]
2022-08-03 07:58:58.879811: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 07:59:00.338129: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-03 07:59:00.362848: saving best epoch checkpoint...
2022-08-03 07:59:00.830824: saving checkpoint...
2022-08-03 07:59:07.714123: done, saving took 7.33 seconds
2022-08-03 07:59:07.726484: This epoch took 144.920906 s

2022-08-03 07:59:07.728755: 
epoch:  378
2022-08-03 08:00:59.148201: train loss : -0.8214
2022-08-03 08:01:10.075930: validation loss: -0.7616
2022-08-03 08:01:10.120167: Average global foreground Dice: [0.8148]
2022-08-03 08:01:10.143695: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:01:11.253823: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-03 08:01:11.285807: saving best epoch checkpoint...
2022-08-03 08:01:11.753987: saving checkpoint...
2022-08-03 08:01:18.915991: done, saving took 7.60 seconds
2022-08-03 08:01:18.929145: This epoch took 131.198257 s

2022-08-03 08:01:18.931408: 
epoch:  379
2022-08-03 08:03:11.367524: train loss : -0.8182
2022-08-03 08:03:21.815246: validation loss: -0.7575
2022-08-03 08:03:21.846626: Average global foreground Dice: [0.8113]
2022-08-03 08:03:21.876799: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:03:23.132116: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-03 08:03:23.161782: saving best epoch checkpoint...
2022-08-03 08:03:23.497685: saving checkpoint...
2022-08-03 08:03:31.151117: done, saving took 7.96 seconds
2022-08-03 08:03:31.161531: This epoch took 132.228034 s

2022-08-03 08:03:31.163817: 
epoch:  380
2022-08-03 08:05:20.936342: train loss : -0.8137
2022-08-03 08:05:32.946973: validation loss: -0.7336
2022-08-03 08:05:32.976253: Average global foreground Dice: [0.7955]
2022-08-03 08:05:33.008732: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:05:34.035431: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-03 08:05:34.055735: This epoch took 122.889789 s

2022-08-03 08:05:34.073062: 
epoch:  381
2022-08-03 08:07:37.207167: train loss : -0.8185
2022-08-03 08:07:48.310270: validation loss: -0.7345
2022-08-03 08:07:48.333453: Average global foreground Dice: [0.793]
2022-08-03 08:07:48.362698: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:07:49.586385: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-03 08:07:49.618756: This epoch took 135.520917 s

2022-08-03 08:07:49.646694: 
epoch:  382
2022-08-03 08:09:45.654996: train loss : -0.8217
2022-08-03 08:09:56.616636: validation loss: -0.7643
2022-08-03 08:09:56.660155: Average global foreground Dice: [0.8162]
2022-08-03 08:09:56.672625: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:09:58.053993: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-03 08:09:58.082540: This epoch took 128.413824 s

2022-08-03 08:09:58.105518: 
epoch:  383
2022-08-03 08:11:51.548110: train loss : -0.8235
2022-08-03 08:12:03.957281: validation loss: -0.7691
2022-08-03 08:12:04.001419: Average global foreground Dice: [0.8186]
2022-08-03 08:12:04.029710: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:12:05.130753: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-03 08:12:05.150808: This epoch took 127.012103 s

2022-08-03 08:12:05.173721: 
epoch:  384
2022-08-03 08:14:14.645968: train loss : -0.8287
2022-08-03 08:14:26.400716: validation loss: -0.7646
2022-08-03 08:14:26.422086: Average global foreground Dice: [0.8172]
2022-08-03 08:14:26.441641: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:14:27.469563: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-03 08:14:27.482201: This epoch took 142.290484 s

2022-08-03 08:14:27.488449: 
epoch:  385
2022-08-03 08:16:17.227958: train loss : -0.8259
2022-08-03 08:16:27.261013: validation loss: -0.7591
2022-08-03 08:16:27.290929: Average global foreground Dice: [0.811]
2022-08-03 08:16:27.320770: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:16:30.362434: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-03 08:16:30.394752: This epoch took 122.898904 s

2022-08-03 08:16:30.427653: 
epoch:  386
2022-08-03 08:18:29.420432: train loss : -0.8228
2022-08-03 08:18:40.396650: validation loss: -0.7659
2022-08-03 08:18:40.413399: Average global foreground Dice: [0.8124]
2022-08-03 08:18:40.463712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:18:41.862005: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-03 08:18:41.882820: This epoch took 131.421116 s

2022-08-03 08:18:41.904704: 
epoch:  387
2022-08-03 08:20:37.968680: train loss : -0.8281
2022-08-03 08:20:47.325298: validation loss: -0.7726
2022-08-03 08:20:47.365525: Average global foreground Dice: [0.8258]
2022-08-03 08:20:47.397727: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:20:48.834368: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-03 08:20:48.865796: saving best epoch checkpoint...
2022-08-03 08:20:49.364501: saving checkpoint...
2022-08-03 08:20:57.781203: done, saving took 8.88 seconds
2022-08-03 08:20:57.795307: This epoch took 135.870165 s

2022-08-03 08:20:57.798321: 
epoch:  388
2022-08-03 08:22:42.117499: train loss : -0.8228
2022-08-03 08:22:54.404964: validation loss: -0.7608
2022-08-03 08:22:54.438439: Average global foreground Dice: [0.8153]
2022-08-03 08:22:54.467913: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:22:55.749911: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-03 08:22:55.771767: saving best epoch checkpoint...
2022-08-03 08:22:56.138807: saving checkpoint...
2022-08-03 08:23:02.617661: done, saving took 6.83 seconds
2022-08-03 08:23:02.627200: This epoch took 124.826593 s

2022-08-03 08:23:02.629431: 
epoch:  389
2022-08-03 08:24:56.393034: train loss : -0.8245
2022-08-03 08:25:06.279981: validation loss: -0.7416
2022-08-03 08:25:06.319355: Average global foreground Dice: [0.7943]
2022-08-03 08:25:06.337714: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:25:07.835840: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-03 08:25:07.880778: This epoch took 125.248945 s

2022-08-03 08:25:07.913687: 
epoch:  390
2022-08-03 08:27:03.328196: train loss : -0.8275
2022-08-03 08:27:16.219407: validation loss: -0.7556
2022-08-03 08:27:16.253408: Average global foreground Dice: [0.8042]
2022-08-03 08:27:16.286727: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:27:17.170597: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-03 08:27:17.203789: This epoch took 129.268086 s

2022-08-03 08:27:17.232049: 
epoch:  391
2022-08-03 08:29:10.549258: train loss : -0.8267
2022-08-03 08:29:21.720079: validation loss: -0.7538
2022-08-03 08:29:21.754473: Average global foreground Dice: [0.8142]
2022-08-03 08:29:21.776720: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:29:23.011456: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-03 08:29:23.042180: This epoch took 125.790491 s

2022-08-03 08:29:23.068786: 
epoch:  392
2022-08-03 08:31:19.619215: train loss : -0.8267
2022-08-03 08:31:30.477273: validation loss: -0.7581
2022-08-03 08:31:30.494039: Average global foreground Dice: [0.8103]
2022-08-03 08:31:30.499254: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:31:31.401449: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-03 08:31:31.404850: This epoch took 128.290157 s

2022-08-03 08:31:31.407480: 
epoch:  393
2022-08-03 08:33:16.413754: train loss : -0.8251
2022-08-03 08:33:27.734979: validation loss: -0.7598
2022-08-03 08:33:27.766789: Average global foreground Dice: [0.8156]
2022-08-03 08:33:27.792720: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:33:29.065316: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-03 08:33:29.075610: This epoch took 117.665555 s

2022-08-03 08:33:29.078096: 
epoch:  394
2022-08-03 08:35:33.284313: train loss : -0.8180
2022-08-03 08:35:41.446369: validation loss: -0.7572
2022-08-03 08:35:41.462788: Average global foreground Dice: [0.8137]
2022-08-03 08:35:41.470680: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:35:43.063103: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-03 08:35:43.083857: This epoch took 134.003465 s

2022-08-03 08:35:43.108032: 
epoch:  395
2022-08-03 08:37:38.721553: train loss : -0.8254
2022-08-03 08:37:48.763575: validation loss: -0.7455
2022-08-03 08:37:48.803738: Average global foreground Dice: [0.7992]
2022-08-03 08:37:48.823201: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:37:49.932351: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-03 08:37:49.974838: This epoch took 126.850098 s

2022-08-03 08:37:49.997688: 
epoch:  396
2022-08-03 08:39:44.241940: train loss : -0.8224
2022-08-03 08:39:54.173052: validation loss: -0.7717
2022-08-03 08:39:54.219394: Average global foreground Dice: [0.8253]
2022-08-03 08:39:54.240706: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:39:55.188483: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-03 08:39:55.219745: This epoch took 125.190630 s

2022-08-03 08:39:55.245926: 
epoch:  397
2022-08-03 08:41:50.408535: train loss : -0.8275
2022-08-03 08:42:01.783669: validation loss: -0.7619
2022-08-03 08:42:01.824719: Average global foreground Dice: [0.8142]
2022-08-03 08:42:01.856737: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:42:03.163011: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-03 08:42:03.194808: This epoch took 127.928763 s

2022-08-03 08:42:03.227729: 
epoch:  398
2022-08-03 08:43:56.667568: train loss : -0.8271
2022-08-03 08:44:07.939935: validation loss: -0.7747
2022-08-03 08:44:07.987809: Average global foreground Dice: [0.8294]
2022-08-03 08:44:08.019731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:44:09.735967: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-03 08:44:09.739021: saving best epoch checkpoint...
2022-08-03 08:44:09.990172: saving checkpoint...
2022-08-03 08:44:17.079749: done, saving took 7.34 seconds
2022-08-03 08:44:17.095887: This epoch took 133.835174 s

2022-08-03 08:44:17.098205: 
epoch:  399
2022-08-03 08:46:14.505331: train loss : -0.8273
2022-08-03 08:46:26.402843: validation loss: -0.7685
2022-08-03 08:46:26.436292: Average global foreground Dice: [0.8239]
2022-08-03 08:46:26.457688: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:46:27.603682: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-03 08:46:27.636730: saving scheduled checkpoint file...
2022-08-03 08:46:28.125460: saving checkpoint...
2022-08-03 08:46:35.716927: done, saving took 8.05 seconds
2022-08-03 08:46:35.731874: done
2022-08-03 08:46:35.734293: saving best epoch checkpoint...
2022-08-03 08:46:35.824439: saving checkpoint...
2022-08-03 08:46:41.221784: done, saving took 5.48 seconds
2022-08-03 08:46:41.231414: This epoch took 144.130971 s

2022-08-03 08:46:41.233901: 
epoch:  400
2022-08-03 08:48:35.433630: train loss : -0.8266
2022-08-03 08:48:46.400775: validation loss: -0.7498
2022-08-03 08:48:46.434419: Average global foreground Dice: [0.8035]
2022-08-03 08:48:46.466738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:48:48.033976: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-03 08:48:48.065758: This epoch took 126.829281 s

2022-08-03 08:48:48.088427: 
epoch:  401
2022-08-03 08:50:41.774939: train loss : -0.8265
2022-08-03 08:50:54.326555: validation loss: -0.7662
2022-08-03 08:50:54.362299: Average global foreground Dice: [0.8204]
2022-08-03 08:50:54.386700: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:50:55.536941: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-03 08:50:55.567786: This epoch took 127.436051 s

2022-08-03 08:50:55.600801: 
epoch:  402
2022-08-03 08:52:43.487377: train loss : -0.8346
2022-08-03 08:52:55.291455: validation loss: -0.7573
2022-08-03 08:52:55.330192: Average global foreground Dice: [0.8137]
2022-08-03 08:52:55.350373: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:52:56.543734: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-03 08:52:56.571859: This epoch took 120.945142 s

2022-08-03 08:52:56.595337: 
epoch:  403
2022-08-03 08:54:57.455146: train loss : -0.8280
2022-08-03 08:55:08.166288: validation loss: -0.7487
2022-08-03 08:55:08.198685: Average global foreground Dice: [0.81]
2022-08-03 08:55:08.221731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:55:09.314480: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-03 08:55:09.352830: This epoch took 132.725120 s

2022-08-03 08:55:09.377737: 
epoch:  404
2022-08-03 08:56:55.428194: train loss : -0.8303
2022-08-03 08:57:08.752160: validation loss: -0.7519
2022-08-03 08:57:08.784468: Average global foreground Dice: [0.8041]
2022-08-03 08:57:08.805713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:57:10.458230: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-03 08:57:10.489812: This epoch took 121.083863 s

2022-08-03 08:57:10.511711: 
epoch:  405
2022-08-03 08:59:14.445836: train loss : -0.8253
2022-08-03 08:59:26.828327: validation loss: -0.7345
2022-08-03 08:59:26.858169: Average global foreground Dice: [0.7952]
2022-08-03 08:59:26.890826: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 08:59:28.004815: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-03 08:59:28.036804: This epoch took 137.502105 s

2022-08-03 08:59:28.067738: 
epoch:  406
2022-08-03 09:01:23.921330: train loss : -0.8227
2022-08-03 09:01:35.589929: validation loss: -0.7324
2022-08-03 09:01:35.605677: Average global foreground Dice: [0.7911]
2022-08-03 09:01:35.608962: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:01:36.827015: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-03 09:01:36.847755: This epoch took 128.750011 s

2022-08-03 09:01:36.879714: 
epoch:  407
2022-08-03 09:03:28.145514: train loss : -0.8310
2022-08-03 09:03:39.084554: validation loss: -0.7604
2022-08-03 09:03:39.129457: Average global foreground Dice: [0.8173]
2022-08-03 09:03:39.148381: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:03:40.372220: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-03 09:03:40.392706: This epoch took 123.488990 s

2022-08-03 09:03:40.420267: 
epoch:  408
2022-08-03 09:05:41.997956: train loss : -0.8264
2022-08-03 09:05:53.874170: validation loss: -0.7593
2022-08-03 09:05:53.905601: Average global foreground Dice: [0.8087]
2022-08-03 09:05:53.932753: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:05:54.991534: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-03 09:05:55.012786: This epoch took 134.564048 s

2022-08-03 09:05:55.033174: 
epoch:  409
2022-08-03 09:07:40.581577: train loss : -0.8296
2022-08-03 09:07:52.084939: validation loss: -0.7665
2022-08-03 09:07:52.167554: Average global foreground Dice: [0.8147]
2022-08-03 09:07:52.201542: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:07:54.434445: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-03 09:07:54.468761: This epoch took 119.417002 s

2022-08-03 09:07:54.490190: 
epoch:  410
2022-08-03 09:09:47.234229: train loss : -0.8258
2022-08-03 09:09:58.790939: validation loss: -0.7410
2022-08-03 09:09:58.822373: Average global foreground Dice: [0.7919]
2022-08-03 09:09:58.834755: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:10:00.273906: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-03 09:10:00.295810: This epoch took 125.776450 s

2022-08-03 09:10:00.323707: 
epoch:  411
2022-08-03 09:11:58.214840: train loss : -0.8322
2022-08-03 09:12:09.269942: validation loss: -0.7642
2022-08-03 09:12:09.303210: Average global foreground Dice: [0.8163]
2022-08-03 09:12:09.341196: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:12:10.394588: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-03 09:12:10.408910: This epoch took 130.060185 s

2022-08-03 09:12:10.421674: 
epoch:  412
2022-08-03 09:14:05.762397: train loss : -0.8263
2022-08-03 09:14:16.304307: validation loss: -0.7741
2022-08-03 09:14:16.336139: Average global foreground Dice: [0.8246]
2022-08-03 09:14:16.358715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:14:17.570782: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-03 09:14:17.592783: This epoch took 127.165115 s

2022-08-03 09:14:17.614749: 
epoch:  413
2022-08-03 09:16:19.041460: train loss : -0.8242
2022-08-03 09:16:28.403451: validation loss: -0.7725
2022-08-03 09:16:28.434376: Average global foreground Dice: [0.8264]
2022-08-03 09:16:28.474743: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:16:29.440890: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-03 09:16:29.457754: This epoch took 131.819034 s

2022-08-03 09:16:29.499327: 
epoch:  414
2022-08-03 09:18:20.489126: train loss : -0.8244
2022-08-03 09:18:30.033646: validation loss: -0.7727
2022-08-03 09:18:30.062420: Average global foreground Dice: [0.8256]
2022-08-03 09:18:30.085723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:18:31.257273: Suus1 maybe_update_lr lr: 2e-05
2022-08-03 09:18:31.289738: This epoch took 121.758036 s

2022-08-03 09:18:31.322665: 
epoch:  415
2022-08-03 09:20:26.818530: train loss : -0.8293
2022-08-03 09:20:37.771249: validation loss: -0.7660
2022-08-03 09:20:37.814043: Average global foreground Dice: [0.8135]
2022-08-03 09:20:37.835705: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:20:39.565096: Suus1 maybe_update_lr lr: 2e-05
2022-08-03 09:20:39.592781: This epoch took 128.237102 s

2022-08-03 09:20:39.616692: 
epoch:  416
2022-08-03 09:22:36.032842: train loss : -0.8245
2022-08-03 09:22:45.166708: validation loss: -0.7707
2022-08-03 09:22:45.197683: Average global foreground Dice: [0.8226]
2022-08-03 09:22:45.211815: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:22:46.838450: Suus1 maybe_update_lr lr: 2e-05
2022-08-03 09:22:46.877754: This epoch took 127.228040 s

2022-08-03 09:22:46.910712: 
epoch:  417
2022-08-03 09:24:44.421173: train loss : -0.8294
2022-08-03 09:24:55.176474: validation loss: -0.7699
2022-08-03 09:24:55.203641: Average global foreground Dice: [0.8201]
2022-08-03 09:24:55.223322: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:24:56.761682: Suus1 maybe_update_lr lr: 2e-05
2022-08-03 09:24:56.774482: saving best epoch checkpoint...
2022-08-03 09:24:57.282979: saving checkpoint...
2022-08-03 09:25:05.500658: done, saving took 8.71 seconds
2022-08-03 09:25:05.516149: This epoch took 138.577426 s

2022-08-03 09:25:05.518465: 
epoch:  418
2022-08-03 09:26:56.746796: train loss : -0.8293
2022-08-03 09:27:08.826328: validation loss: -0.7594
2022-08-03 09:27:08.855631: Average global foreground Dice: [0.8118]
2022-08-03 09:27:08.887725: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:27:10.109821: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-03 09:27:10.149031: This epoch took 124.628411 s

2022-08-03 09:27:10.169055: 
epoch:  419
2022-08-03 09:29:07.313261: train loss : -0.8339
2022-08-03 09:29:17.356916: validation loss: -0.7513
2022-08-03 09:29:17.387246: Average global foreground Dice: [0.8099]
2022-08-03 09:29:17.417721: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:29:18.561728: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-03 09:29:18.587780: This epoch took 128.398573 s

2022-08-03 09:29:18.620728: 
epoch:  420
2022-08-03 09:31:08.792208: train loss : -0.8299
2022-08-03 09:31:20.967665: validation loss: -0.7531
2022-08-03 09:31:21.009560: Average global foreground Dice: [0.8076]
2022-08-03 09:31:21.041485: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:31:22.322958: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-03 09:31:22.354769: This epoch took 123.701046 s

2022-08-03 09:31:22.387696: 
epoch:  421
2022-08-03 09:33:16.906410: train loss : -0.8306
2022-08-03 09:33:27.474325: validation loss: -0.7599
2022-08-03 09:33:27.505849: Average global foreground Dice: [0.8197]
2022-08-03 09:33:27.525444: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:33:28.587287: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-03 09:33:28.615741: This epoch took 126.205503 s

2022-08-03 09:33:28.637715: 
epoch:  422
2022-08-03 09:35:20.622863: train loss : -0.8292
2022-08-03 09:35:33.046233: validation loss: -0.7440
2022-08-03 09:35:33.086379: Average global foreground Dice: [0.8056]
2022-08-03 09:35:33.129713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:35:34.223686: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-03 09:35:34.245801: This epoch took 125.579887 s

2022-08-03 09:35:34.277711: 
epoch:  423
2022-08-03 09:37:31.380678: train loss : -0.8333
2022-08-03 09:37:44.619407: validation loss: -0.7824
2022-08-03 09:37:44.645205: Average global foreground Dice: [0.8332]
2022-08-03 09:37:44.668421: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:37:45.749372: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-03 09:37:45.778774: saving best epoch checkpoint...
2022-08-03 09:37:46.050957: saving checkpoint...
2022-08-03 09:37:52.056362: done, saving took 6.26 seconds
2022-08-03 09:37:52.068176: This epoch took 137.768295 s

2022-08-03 09:37:52.070485: 
epoch:  424
2022-08-03 09:39:45.449244: train loss : -0.8298
2022-08-03 09:39:58.319173: validation loss: -0.7424
2022-08-03 09:39:58.351511: Average global foreground Dice: [0.7944]
2022-08-03 09:39:58.372719: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:40:00.187178: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-03 09:40:00.218800: This epoch took 128.146137 s

2022-08-03 09:40:00.251734: 
epoch:  425
2022-08-03 09:41:52.148322: train loss : -0.8277
2022-08-03 09:42:04.061727: validation loss: -0.7484
2022-08-03 09:42:04.088234: Average global foreground Dice: [0.8019]
2022-08-03 09:42:04.135966: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:42:05.406738: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-03 09:42:05.432960: This epoch took 125.148238 s

2022-08-03 09:42:05.448459: 
epoch:  426
2022-08-03 09:43:58.511431: train loss : -0.8317
2022-08-03 09:44:10.409013: validation loss: -0.7656
2022-08-03 09:44:10.438991: Average global foreground Dice: [0.818]
2022-08-03 09:44:10.457412: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:44:11.820260: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-03 09:44:11.867771: This epoch took 126.375074 s

2022-08-03 09:44:11.900667: 
epoch:  427
2022-08-03 09:46:15.943973: train loss : -0.8347
2022-08-03 09:46:27.177272: validation loss: -0.7635
2022-08-03 09:46:27.219241: Average global foreground Dice: [0.8142]
2022-08-03 09:46:27.235955: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:46:28.430092: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-03 09:46:28.451772: This epoch took 136.518093 s

2022-08-03 09:46:28.493670: 
epoch:  428
2022-08-03 09:48:28.260160: train loss : -0.8315
2022-08-03 09:48:41.412029: validation loss: -0.7719
2022-08-03 09:48:41.445307: Average global foreground Dice: [0.8238]
2022-08-03 09:48:41.463997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:48:42.738938: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-03 09:48:42.770567: This epoch took 134.243885 s

2022-08-03 09:48:42.791729: 
epoch:  429
2022-08-03 09:50:34.677955: train loss : -0.8297
2022-08-03 09:50:45.800222: validation loss: -0.7690
2022-08-03 09:50:45.815313: Average global foreground Dice: [0.8157]
2022-08-03 09:50:45.836714: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:50:47.436034: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-03 09:50:47.456742: This epoch took 124.632045 s

2022-08-03 09:50:47.475976: 
epoch:  430
2022-08-03 09:52:43.948100: train loss : -0.8313
2022-08-03 09:52:57.097819: validation loss: -0.7282
2022-08-03 09:52:57.127231: Average global foreground Dice: [0.7842]
2022-08-03 09:52:57.159711: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:52:58.240779: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-03 09:52:58.273762: This epoch took 130.770775 s

2022-08-03 09:52:58.305022: 
epoch:  431
2022-08-03 09:54:53.703485: train loss : -0.8318
2022-08-03 09:55:05.212411: validation loss: -0.7687
2022-08-03 09:55:05.248261: Average global foreground Dice: [0.82]
2022-08-03 09:55:05.274727: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:55:06.521120: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-03 09:55:06.553400: This epoch took 128.215669 s

2022-08-03 09:55:06.583539: 
epoch:  432
2022-08-03 09:57:12.114815: train loss : -0.8291
2022-08-03 09:57:20.994133: validation loss: -0.7745
2022-08-03 09:57:21.025493: Average global foreground Dice: [0.8245]
2022-08-03 09:57:21.037741: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:57:21.917269: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-03 09:57:21.939680: This epoch took 135.338985 s

2022-08-03 09:57:21.959525: 
epoch:  433
2022-08-03 09:59:23.181463: train loss : -0.8312
2022-08-03 09:59:33.190041: validation loss: -0.7559
2022-08-03 09:59:33.240169: Average global foreground Dice: [0.8063]
2022-08-03 09:59:33.271506: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 09:59:34.871817: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-03 09:59:34.903751: This epoch took 132.920837 s

2022-08-03 09:59:34.936706: 
epoch:  434
2022-08-03 10:01:33.997492: train loss : -0.8310
2022-08-03 10:01:45.495317: validation loss: -0.7795
2022-08-03 10:01:45.541163: Average global foreground Dice: [0.8324]
2022-08-03 10:01:45.573718: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:01:47.015188: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-03 10:01:47.052802: This epoch took 132.078902 s

2022-08-03 10:01:47.083953: 
epoch:  435
2022-08-03 10:03:44.555718: train loss : -0.8298
2022-08-03 10:03:54.596812: validation loss: -0.7829
2022-08-03 10:03:54.634806: Average global foreground Dice: [0.8294]
2022-08-03 10:03:54.654312: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:03:55.858724: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-03 10:03:55.887763: saving best epoch checkpoint...
2022-08-03 10:03:56.248784: saving checkpoint...
2022-08-03 10:04:03.510164: done, saving took 7.59 seconds
2022-08-03 10:04:03.520514: This epoch took 136.410088 s

2022-08-03 10:04:03.522739: 
epoch:  436
2022-08-03 10:05:52.288390: train loss : -0.8320
2022-08-03 10:06:02.944749: validation loss: -0.7654
2022-08-03 10:06:02.984270: Average global foreground Dice: [0.8207]
2022-08-03 10:06:02.990589: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:06:04.697668: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-03 10:06:04.728786: saving best epoch checkpoint...
2022-08-03 10:06:05.126330: saving checkpoint...
2022-08-03 10:06:11.977685: done, saving took 7.22 seconds
2022-08-03 10:06:11.990639: This epoch took 128.465626 s

2022-08-03 10:06:11.993158: 
epoch:  437
2022-08-03 10:08:00.423118: train loss : -0.8349
2022-08-03 10:08:12.107007: validation loss: -0.7484
2022-08-03 10:08:12.128897: Average global foreground Dice: [0.7996]
2022-08-03 10:08:12.131453: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:08:13.274615: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-03 10:08:13.299855: This epoch took 121.304308 s

2022-08-03 10:08:13.327882: 
epoch:  438
2022-08-03 10:10:10.346512: train loss : -0.8333
2022-08-03 10:10:20.758043: validation loss: -0.7611
2022-08-03 10:10:20.798339: Average global foreground Dice: [0.8163]
2022-08-03 10:10:20.826715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:10:22.069928: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-03 10:10:22.107790: This epoch took 128.757879 s

2022-08-03 10:10:22.133707: 
epoch:  439
2022-08-03 10:12:16.343213: train loss : -0.8335
2022-08-03 10:12:28.285349: validation loss: -0.7649
2022-08-03 10:12:28.323404: Average global foreground Dice: [0.8142]
2022-08-03 10:12:28.349731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:12:29.639683: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-03 10:12:29.664791: This epoch took 127.508073 s

2022-08-03 10:12:29.688701: 
epoch:  440
2022-08-03 10:14:25.029322: train loss : -0.8310
2022-08-03 10:14:35.448823: validation loss: -0.7681
2022-08-03 10:14:35.481344: Average global foreground Dice: [0.8159]
2022-08-03 10:14:35.502738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:14:36.709682: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-03 10:14:36.727822: This epoch took 127.013108 s

2022-08-03 10:14:36.748725: 
epoch:  441
2022-08-03 10:16:40.608439: train loss : -0.8266
2022-08-03 10:16:50.909320: validation loss: -0.7699
2022-08-03 10:16:50.932429: Average global foreground Dice: [0.8194]
2022-08-03 10:16:50.953971: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:16:51.938186: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-03 10:16:51.958847: This epoch took 135.188136 s

2022-08-03 10:16:51.980725: 
epoch:  442
2022-08-03 10:18:49.378175: train loss : -0.8303
2022-08-03 10:18:57.937190: validation loss: -0.7498
2022-08-03 10:18:57.962446: Average global foreground Dice: [0.8041]
2022-08-03 10:18:57.981733: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:18:58.875488: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-03 10:18:58.888846: This epoch took 126.886104 s

2022-08-03 10:18:58.911309: 
epoch:  443
2022-08-03 10:21:02.200690: train loss : -0.8300
2022-08-03 10:21:12.882541: validation loss: -0.7588
2022-08-03 10:21:12.920906: Average global foreground Dice: [0.809]
2022-08-03 10:21:12.953638: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:21:14.611503: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-03 10:21:14.643784: This epoch took 135.701945 s

2022-08-03 10:21:14.676700: 
epoch:  444
2022-08-03 10:23:06.130555: train loss : -0.8375
2022-08-03 10:23:18.485161: validation loss: -0.7689
2022-08-03 10:23:18.518235: Average global foreground Dice: [0.8205]
2022-08-03 10:23:18.548866: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:23:19.642855: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-03 10:23:19.663830: This epoch took 124.944124 s

2022-08-03 10:23:19.697890: 
epoch:  445
2022-08-03 10:25:11.670426: train loss : -0.8313
2022-08-03 10:25:24.345031: validation loss: -0.7721
2022-08-03 10:25:24.414218: Average global foreground Dice: [0.8193]
2022-08-03 10:25:24.445716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:25:25.490991: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-03 10:25:25.512768: This epoch took 125.792072 s

2022-08-03 10:25:25.534675: 
epoch:  446
2022-08-03 10:27:14.853047: train loss : -0.8369
2022-08-03 10:27:27.355336: validation loss: -0.7672
2022-08-03 10:27:27.386010: Average global foreground Dice: [0.8231]
2022-08-03 10:27:27.405576: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:27:28.785996: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-03 10:27:28.818803: This epoch took 123.263587 s

2022-08-03 10:27:28.841731: 
epoch:  447
2022-08-03 10:29:26.720019: train loss : -0.8290
2022-08-03 10:29:37.925037: validation loss: -0.7461
2022-08-03 10:29:37.958767: Average global foreground Dice: [0.7969]
2022-08-03 10:29:37.989715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:29:39.334740: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-03 10:29:39.354795: This epoch took 130.481270 s

2022-08-03 10:29:39.376977: 
epoch:  448
2022-08-03 10:31:35.500736: train loss : -0.8312
2022-08-03 10:31:47.174880: validation loss: -0.7411
2022-08-03 10:31:47.205070: Average global foreground Dice: [0.806]
2022-08-03 10:31:47.228205: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:31:48.443078: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-03 10:31:48.463809: This epoch took 129.065102 s

2022-08-03 10:31:48.509705: 
epoch:  449
2022-08-03 10:33:40.153676: train loss : -0.8358
2022-08-03 10:33:51.317500: validation loss: -0.7645
2022-08-03 10:33:51.333304: Average global foreground Dice: [0.8177]
2022-08-03 10:33:51.354795: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:33:52.676575: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-03 10:33:52.705628: saving scheduled checkpoint file...
2022-08-03 10:33:53.076766: saving checkpoint...
2022-08-03 10:33:59.464170: done, saving took 6.74 seconds
2022-08-03 10:33:59.482626: done
2022-08-03 10:33:59.485250: This epoch took 130.942522 s

2022-08-03 10:33:59.487715: 
epoch:  450
2022-08-03 10:35:52.132313: train loss : -0.8327
2022-08-03 10:36:03.643786: validation loss: -0.7597
2022-08-03 10:36:03.676394: Average global foreground Dice: [0.8126]
2022-08-03 10:36:03.707180: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:36:05.009660: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-03 10:36:05.047129: This epoch took 125.556924 s

2022-08-03 10:36:05.079710: 
epoch:  451
2022-08-03 10:37:55.759026: train loss : -0.8363
2022-08-03 10:38:05.873103: validation loss: -0.7431
2022-08-03 10:38:05.898359: Average global foreground Dice: [0.7967]
2022-08-03 10:38:05.918631: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:38:07.562208: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-03 10:38:07.620772: This epoch took 122.521001 s

2022-08-03 10:38:07.638702: 
epoch:  452
2022-08-03 10:40:01.524577: train loss : -0.8311
2022-08-03 10:40:12.605942: validation loss: -0.7538
2022-08-03 10:40:12.643658: Average global foreground Dice: [0.8029]
2022-08-03 10:40:12.674722: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:40:14.138293: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-03 10:40:14.170732: This epoch took 126.510025 s

2022-08-03 10:40:14.193665: 
epoch:  453
2022-08-03 10:42:09.801043: train loss : -0.8358
2022-08-03 10:42:21.684205: validation loss: -0.7566
2022-08-03 10:42:21.713317: Average global foreground Dice: [0.8067]
2022-08-03 10:42:21.744740: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:42:23.133040: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-03 10:42:23.162407: This epoch took 128.944710 s

2022-08-03 10:42:23.183701: 
epoch:  454
2022-08-03 10:44:19.330876: train loss : -0.8357
2022-08-03 10:44:31.198046: validation loss: -0.7473
2022-08-03 10:44:31.229274: Average global foreground Dice: [0.8021]
2022-08-03 10:44:31.246734: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:44:32.137100: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-03 10:44:32.165019: This epoch took 128.948280 s

2022-08-03 10:44:32.185170: 
epoch:  455
2022-08-03 10:46:25.017440: train loss : -0.8326
2022-08-03 10:46:36.911410: validation loss: -0.7627
2022-08-03 10:46:36.945558: Average global foreground Dice: [0.815]
2022-08-03 10:46:36.963104: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:46:38.052357: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-03 10:46:38.055304: This epoch took 125.849885 s

2022-08-03 10:46:38.072469: 
epoch:  456
2022-08-03 10:48:27.845227: train loss : -0.8353
2022-08-03 10:48:38.459869: validation loss: -0.7678
2022-08-03 10:48:38.488070: Average global foreground Dice: [0.8214]
2022-08-03 10:48:38.508709: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:48:39.979191: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-03 10:48:39.999772: This epoch took 121.907606 s

2022-08-03 10:48:40.025736: 
epoch:  457
2022-08-03 10:50:44.303978: train loss : -0.8283
2022-08-03 10:50:56.400194: validation loss: -0.7617
2022-08-03 10:50:56.428846: Average global foreground Dice: [0.8183]
2022-08-03 10:50:56.436738: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:50:57.402401: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-03 10:50:57.445767: This epoch took 137.394031 s

2022-08-03 10:50:57.489685: 
epoch:  458
2022-08-03 10:52:51.070239: train loss : -0.8336
2022-08-03 10:53:00.661764: validation loss: -0.7526
2022-08-03 10:53:00.701554: Average global foreground Dice: [0.803]
2022-08-03 10:53:00.706228: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:53:01.976571: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-03 10:53:02.003802: This epoch took 124.492102 s

2022-08-03 10:53:02.026704: 
epoch:  459
2022-08-03 10:55:04.886244: train loss : -0.8325
2022-08-03 10:55:14.846627: validation loss: -0.7587
2022-08-03 10:55:14.875431: Average global foreground Dice: [0.8139]
2022-08-03 10:55:14.891165: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:55:16.054035: Suus1 maybe_update_lr lr: 1e-05
2022-08-03 10:55:16.075767: This epoch took 134.022573 s

2022-08-03 10:55:16.108689: 
epoch:  460
2022-08-03 10:57:11.006164: train loss : -0.8361
2022-08-03 10:57:22.304827: validation loss: -0.7550
2022-08-03 10:57:22.336526: Average global foreground Dice: [0.8052]
2022-08-03 10:57:22.356123: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:57:23.696720: Suus1 maybe_update_lr lr: 1e-05
2022-08-03 10:57:23.729831: This epoch took 127.592081 s

2022-08-03 10:57:23.760685: 
epoch:  461
2022-08-03 10:59:18.834486: train loss : -0.8351
2022-08-03 10:59:29.044050: validation loss: -0.7537
2022-08-03 10:59:29.075143: Average global foreground Dice: [0.811]
2022-08-03 10:59:29.087686: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 10:59:30.239351: Suus1 maybe_update_lr lr: 1e-05
2022-08-03 10:59:30.265770: This epoch took 126.480970 s

2022-08-03 10:59:30.289709: 
epoch:  462
2022-08-03 11:01:32.823969: train loss : -0.8252
2022-08-03 11:01:42.879045: validation loss: -0.7570
2022-08-03 11:01:42.904160: Average global foreground Dice: [0.8085]
2022-08-03 11:01:42.920695: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:01:44.447872: Suus1 maybe_update_lr lr: 1e-05
2022-08-03 11:01:44.479829: This epoch took 134.168079 s

2022-08-03 11:01:44.512700: 
epoch:  463
2022-08-03 11:03:30.939735: train loss : -0.8326
2022-08-03 11:03:43.138134: validation loss: -0.7542
2022-08-03 11:03:43.170219: Average global foreground Dice: [0.808]
2022-08-03 11:03:43.182674: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:03:44.516842: Suus1 maybe_update_lr lr: 9e-06
2022-08-03 11:03:44.545824: This epoch took 120.010139 s

2022-08-03 11:03:44.579437: 
epoch:  464
2022-08-03 11:05:43.838280: train loss : -0.8294
2022-08-03 11:05:55.136083: validation loss: -0.7587
2022-08-03 11:05:55.168113: Average global foreground Dice: [0.8156]
2022-08-03 11:05:55.172041: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:05:56.253872: Suus1 maybe_update_lr lr: 9e-06
2022-08-03 11:05:56.274770: This epoch took 131.640805 s

2022-08-03 11:05:56.300709: 
epoch:  465
2022-08-03 11:07:54.595773: train loss : -0.8344
2022-08-03 11:08:08.737086: validation loss: -0.7658
2022-08-03 11:08:08.769253: Average global foreground Dice: [0.821]
2022-08-03 11:08:08.799716: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:08:10.022404: Suus1 maybe_update_lr lr: 9e-06
2022-08-03 11:08:10.050842: This epoch took 133.717118 s

2022-08-03 11:08:10.081457: 
epoch:  466
2022-08-03 11:10:04.005551: train loss : -0.8371
2022-08-03 11:10:13.590561: validation loss: -0.7767
2022-08-03 11:10:13.607785: Average global foreground Dice: [0.8236]
2022-08-03 11:10:13.627542: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:10:14.552153: Suus1 maybe_update_lr lr: 9e-06
2022-08-03 11:10:14.583363: This epoch took 124.479871 s

2022-08-03 11:10:14.605671: 
epoch:  467
2022-08-03 11:12:17.062320: train loss : -0.8287
2022-08-03 11:12:27.739563: validation loss: -0.7727
2022-08-03 11:12:27.771311: Average global foreground Dice: [0.8237]
2022-08-03 11:12:27.782146: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:12:29.435839: Suus1 maybe_update_lr lr: 8e-06
2022-08-03 11:12:29.467810: This epoch took 134.839122 s

2022-08-03 11:12:29.479098: 
epoch:  468
2022-08-03 11:14:22.363115: train loss : -0.8405
2022-08-03 11:14:34.321592: validation loss: -0.7617
2022-08-03 11:14:34.347080: Average global foreground Dice: [0.8106]
2022-08-03 11:14:34.356574: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:14:35.694123: Suus1 maybe_update_lr lr: 8e-06
2022-08-03 11:14:35.713769: This epoch took 126.192979 s

2022-08-03 11:14:35.735713: 
epoch:  469
2022-08-03 11:16:34.695334: train loss : -0.8362
2022-08-03 11:16:45.166918: validation loss: -0.7696
2022-08-03 11:16:45.183380: Average global foreground Dice: [0.8224]
2022-08-03 11:16:45.201181: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:16:46.829659: Suus1 maybe_update_lr lr: 8e-06
2022-08-03 11:16:46.861737: This epoch took 131.104187 s

2022-08-03 11:16:46.905677: 
epoch:  470
2022-08-03 11:18:40.215594: train loss : -0.8384
2022-08-03 11:18:52.628888: validation loss: -0.7510
2022-08-03 11:18:52.661415: Average global foreground Dice: [0.8062]
2022-08-03 11:18:52.690077: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:18:53.828441: Suus1 maybe_update_lr lr: 8e-06
2022-08-03 11:18:53.863786: This epoch took 126.931068 s

2022-08-03 11:18:53.896672: 
epoch:  471
2022-08-03 11:20:51.747294: train loss : -0.8348
2022-08-03 11:21:02.801793: validation loss: -0.7592
2022-08-03 11:21:02.846277: Average global foreground Dice: [0.8153]
2022-08-03 11:21:02.878724: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:21:04.173393: Suus1 maybe_update_lr lr: 7e-06
2022-08-03 11:21:04.211773: This epoch took 130.272071 s

2022-08-03 11:21:04.241714: 
epoch:  472
2022-08-03 11:22:56.149308: train loss : -0.8369
2022-08-03 11:23:08.512068: validation loss: -0.7407
2022-08-03 11:23:08.544470: Average global foreground Dice: [0.8002]
2022-08-03 11:23:08.558707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:23:10.404823: Suus1 maybe_update_lr lr: 7e-06
2022-08-03 11:23:10.432521: This epoch took 126.157595 s

2022-08-03 11:23:10.439000: 
epoch:  473
2022-08-03 11:25:05.585766: train loss : -0.8380
2022-08-03 11:25:18.170436: validation loss: -0.7831
2022-08-03 11:25:18.206523: Average global foreground Dice: [0.8353]
2022-08-03 11:25:18.216319: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:25:19.370554: Suus1 maybe_update_lr lr: 7e-06
2022-08-03 11:25:19.402751: This epoch took 128.949017 s

2022-08-03 11:25:19.424690: 
epoch:  474
2022-08-03 11:27:08.719658: train loss : -0.8378
2022-08-03 11:27:20.569973: validation loss: -0.7746
2022-08-03 11:27:20.573919: Average global foreground Dice: [0.8285]
2022-08-03 11:27:20.605717: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:27:21.998705: Suus1 maybe_update_lr lr: 7e-06
2022-08-03 11:27:22.019776: This epoch took 122.577904 s

2022-08-03 11:27:22.037377: 
epoch:  475
2022-08-03 11:29:22.857921: train loss : -0.8386
2022-08-03 11:29:34.742941: validation loss: -0.7849
2022-08-03 11:29:34.760023: Average global foreground Dice: [0.8311]
2022-08-03 11:29:34.784016: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:29:35.875854: Suus1 maybe_update_lr lr: 7e-06
2022-08-03 11:29:35.896882: saving best epoch checkpoint...
2022-08-03 11:29:36.258691: saving checkpoint...
2022-08-03 11:29:42.595141: done, saving took 6.68 seconds
2022-08-03 11:29:42.606355: This epoch took 140.566203 s

2022-08-03 11:29:42.608882: 
epoch:  476
2022-08-03 11:31:31.009603: train loss : -0.8370
2022-08-03 11:31:41.608614: validation loss: -0.7793
2022-08-03 11:31:41.645480: Average global foreground Dice: [0.8289]
2022-08-03 11:31:41.677529: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:31:43.222384: Suus1 maybe_update_lr lr: 6e-06
2022-08-03 11:31:43.234194: saving best epoch checkpoint...
2022-08-03 11:31:43.760465: saving checkpoint...
2022-08-03 11:31:50.315363: done, saving took 7.05 seconds
2022-08-03 11:31:50.329496: This epoch took 127.718254 s

2022-08-03 11:31:50.331690: 
epoch:  477
2022-08-03 11:33:30.295128: train loss : -0.8342
2022-08-03 11:33:41.762151: validation loss: -0.7672
2022-08-03 11:33:41.806359: Average global foreground Dice: [0.8215]
2022-08-03 11:33:41.825778: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:33:43.905737: Suus1 maybe_update_lr lr: 6e-06
2022-08-03 11:33:43.921889: saving best epoch checkpoint...
2022-08-03 11:33:44.275464: saving checkpoint...
2022-08-03 11:33:50.699213: done, saving took 6.77 seconds
2022-08-03 11:33:50.709070: This epoch took 120.374919 s

2022-08-03 11:33:50.711362: 
epoch:  478
2022-08-03 11:35:40.971839: train loss : -0.8349
2022-08-03 11:35:52.918974: validation loss: -0.7788
2022-08-03 11:35:52.947463: Average global foreground Dice: [0.8277]
2022-08-03 11:35:52.968672: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:35:54.091112: Suus1 maybe_update_lr lr: 6e-06
2022-08-03 11:35:54.117057: saving best epoch checkpoint...
2022-08-03 11:35:54.470829: saving checkpoint...
2022-08-03 11:36:01.476265: done, saving took 7.33 seconds
2022-08-03 11:36:01.491056: This epoch took 130.777486 s

2022-08-03 11:36:01.493407: 
epoch:  479
2022-08-03 11:37:53.527863: train loss : -0.8301
2022-08-03 11:38:06.674977: validation loss: -0.7352
2022-08-03 11:38:06.706692: Average global foreground Dice: [0.7958]
2022-08-03 11:38:06.744752: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:38:08.201279: Suus1 maybe_update_lr lr: 6e-06
2022-08-03 11:38:08.241762: This epoch took 126.746146 s

2022-08-03 11:38:08.274695: 
epoch:  480
2022-08-03 11:40:05.493867: train loss : -0.8376
2022-08-03 11:40:17.662373: validation loss: -0.7585
2022-08-03 11:40:17.695484: Average global foreground Dice: [0.8128]
2022-08-03 11:40:17.716713: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:40:19.297411: Suus1 maybe_update_lr lr: 5e-06
2022-08-03 11:40:19.340933: This epoch took 131.033209 s

2022-08-03 11:40:19.373729: 
epoch:  481
2022-08-03 11:42:12.783565: train loss : -0.8346
2022-08-03 11:42:24.611966: validation loss: -0.7729
2022-08-03 11:42:24.632976: Average global foreground Dice: [0.8224]
2022-08-03 11:42:24.635516: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:42:25.746639: Suus1 maybe_update_lr lr: 5e-06
2022-08-03 11:42:25.764776: This epoch took 126.373220 s

2022-08-03 11:42:25.770616: 
epoch:  482
2022-08-03 11:44:21.269225: train loss : -0.8418
2022-08-03 11:44:31.286539: validation loss: -0.7353
2022-08-03 11:44:31.318586: Average global foreground Dice: [0.7897]
2022-08-03 11:44:31.338775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:44:32.520800: Suus1 maybe_update_lr lr: 5e-06
2022-08-03 11:44:32.541804: This epoch took 126.768265 s

2022-08-03 11:44:32.564415: 
epoch:  483
2022-08-03 11:46:24.445151: train loss : -0.8399
2022-08-03 11:46:37.278553: validation loss: -0.7476
2022-08-03 11:46:37.307243: Average global foreground Dice: [0.805]
2022-08-03 11:46:37.327821: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:46:38.373437: Suus1 maybe_update_lr lr: 5e-06
2022-08-03 11:46:38.403625: This epoch took 125.795892 s

2022-08-03 11:46:38.434713: 
epoch:  484
2022-08-03 11:48:31.641454: train loss : -0.8361
2022-08-03 11:48:41.856951: validation loss: -0.7545
2022-08-03 11:48:41.891223: Average global foreground Dice: [0.815]
2022-08-03 11:48:41.917818: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:48:43.308126: Suus1 maybe_update_lr lr: 4e-06
2022-08-03 11:48:43.330750: This epoch took 124.876023 s

2022-08-03 11:48:43.354691: 
epoch:  485
2022-08-03 11:50:44.363175: train loss : -0.8344
2022-08-03 11:50:56.206801: validation loss: -0.7620
2022-08-03 11:50:56.241354: Average global foreground Dice: [0.8154]
2022-08-03 11:50:56.267699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:50:57.538078: Suus1 maybe_update_lr lr: 4e-06
2022-08-03 11:50:57.563822: This epoch took 134.185121 s

2022-08-03 11:50:57.591691: 
epoch:  486
2022-08-03 11:52:52.926389: train loss : -0.8346
2022-08-03 11:53:04.347327: validation loss: -0.7746
2022-08-03 11:53:04.388480: Average global foreground Dice: [0.8253]
2022-08-03 11:53:04.402724: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:53:05.822143: Suus1 maybe_update_lr lr: 4e-06
2022-08-03 11:53:05.854832: This epoch took 128.240110 s

2022-08-03 11:53:05.889683: 
epoch:  487
2022-08-03 11:55:03.353640: train loss : -0.8338
2022-08-03 11:55:13.515694: validation loss: -0.7616
2022-08-03 11:55:13.549744: Average global foreground Dice: [0.8135]
2022-08-03 11:55:13.568708: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:55:14.595513: Suus1 maybe_update_lr lr: 3e-06
2022-08-03 11:55:14.632595: This epoch took 128.711885 s

2022-08-03 11:55:14.650643: 
epoch:  488
2022-08-03 11:57:12.030251: train loss : -0.8399
2022-08-03 11:57:24.851479: validation loss: -0.7849
2022-08-03 11:57:24.862344: Average global foreground Dice: [0.8323]
2022-08-03 11:57:24.883815: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:57:26.115360: Suus1 maybe_update_lr lr: 3e-06
2022-08-03 11:57:26.145257: This epoch took 131.452559 s

2022-08-03 11:57:26.166714: 
epoch:  489
2022-08-03 11:59:18.172864: train loss : -0.8378
2022-08-03 11:59:27.796747: validation loss: -0.7449
2022-08-03 11:59:27.828300: Average global foreground Dice: [0.7963]
2022-08-03 11:59:27.842709: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 11:59:28.997062: Suus1 maybe_update_lr lr: 3e-06
2022-08-03 11:59:29.027760: This epoch took 122.831053 s

2022-08-03 11:59:29.059309: 
epoch:  490
2022-08-03 12:01:24.786859: train loss : -0.8366
2022-08-03 12:01:37.866425: validation loss: -0.7687
2022-08-03 12:01:37.898524: Average global foreground Dice: [0.8169]
2022-08-03 12:01:37.910732: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:01:39.721424: Suus1 maybe_update_lr lr: 3e-06
2022-08-03 12:01:39.753750: This epoch took 130.672040 s

2022-08-03 12:01:39.775697: 
epoch:  491
2022-08-03 12:03:30.356769: train loss : -0.8408
2022-08-03 12:03:42.602533: validation loss: -0.7660
2022-08-03 12:03:42.638385: Average global foreground Dice: [0.8189]
2022-08-03 12:03:42.681699: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:03:43.877508: Suus1 maybe_update_lr lr: 2e-06
2022-08-03 12:03:43.896437: This epoch took 124.087729 s

2022-08-03 12:03:43.898890: 
epoch:  492
2022-08-03 12:05:39.778232: train loss : -0.8352
2022-08-03 12:05:51.928360: validation loss: -0.7540
2022-08-03 12:05:51.974229: Average global foreground Dice: [0.808]
2022-08-03 12:05:52.001079: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:05:53.461030: Suus1 maybe_update_lr lr: 2e-06
2022-08-03 12:05:53.492836: This epoch took 129.591450 s

2022-08-03 12:05:53.516894: 
epoch:  493
2022-08-03 12:07:58.041176: train loss : -0.8382
2022-08-03 12:08:08.196205: validation loss: -0.7680
2022-08-03 12:08:08.227680: Average global foreground Dice: [0.8204]
2022-08-03 12:08:08.248865: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:08:09.834923: Suus1 maybe_update_lr lr: 2e-06
2022-08-03 12:08:09.865814: This epoch took 136.311086 s

2022-08-03 12:08:09.895850: 
epoch:  494
2022-08-03 12:09:56.903359: train loss : -0.8360
2022-08-03 12:10:07.816058: validation loss: -0.7769
2022-08-03 12:10:07.847076: Average global foreground Dice: [0.8285]
2022-08-03 12:10:07.870700: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:10:09.665528: Suus1 maybe_update_lr lr: 2e-06
2022-08-03 12:10:09.697764: This epoch took 119.779033 s

2022-08-03 12:10:09.719707: 
epoch:  495
2022-08-03 12:12:13.490530: train loss : -0.8386
2022-08-03 12:12:22.560491: validation loss: -0.7719
2022-08-03 12:12:22.602058: Average global foreground Dice: [0.8237]
2022-08-03 12:12:22.621324: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:12:23.811844: Suus1 maybe_update_lr lr: 1e-06
2022-08-03 12:12:23.843845: This epoch took 134.096916 s

2022-08-03 12:12:23.860515: 
epoch:  496
2022-08-03 12:14:14.471745: train loss : -0.8366
2022-08-03 12:14:26.387843: validation loss: -0.7589
2022-08-03 12:14:26.414318: Average global foreground Dice: [0.8096]
2022-08-03 12:14:26.445715: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:14:27.503034: Suus1 maybe_update_lr lr: 1e-06
2022-08-03 12:14:27.524768: This epoch took 123.649762 s

2022-08-03 12:14:27.550428: 
epoch:  497
2022-08-03 12:16:25.704350: train loss : -0.8444
2022-08-03 12:16:37.256957: validation loss: -0.7580
2022-08-03 12:16:37.290029: Average global foreground Dice: [0.8144]
2022-08-03 12:16:37.309501: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:16:38.830027: Suus1 maybe_update_lr lr: 1e-06
2022-08-03 12:16:38.861833: This epoch took 131.290095 s

2022-08-03 12:16:38.874724: 
epoch:  498
2022-08-03 12:18:28.452568: train loss : -0.8397
2022-08-03 12:18:40.381270: validation loss: -0.7501
2022-08-03 12:18:40.414285: Average global foreground Dice: [0.8038]
2022-08-03 12:18:40.446881: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:18:42.069865: Suus1 maybe_update_lr lr: 0.0
2022-08-03 12:18:42.113764: This epoch took 123.209021 s

2022-08-03 12:18:42.150684: 
epoch:  499
2022-08-03 12:20:35.578745: train loss : -0.8379
2022-08-03 12:20:46.504285: validation loss: -0.7841
2022-08-03 12:20:46.537375: Average global foreground Dice: [0.8314]
2022-08-03 12:20:46.611170: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-03 12:20:48.575077: Suus1 maybe_update_lr lr: 0.0
2022-08-03 12:20:48.607709: saving scheduled checkpoint file...
2022-08-03 12:20:49.294708: saving checkpoint...
2022-08-03 12:20:55.843174: done, saving took 7.20 seconds
2022-08-03 12:20:55.872419: done
2022-08-03 12:20:55.875071: This epoch took 133.694114 s

2022-08-03 12:20:56.053935: saving checkpoint...
2022-08-03 12:21:00.976352: done, saving took 5.10 seconds
panc_0003 (2, 216, 396, 396)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 80
computing Gaussian
done
prediction done
suus panc_0003 transposed
suus panc_0003 not saving softmax
suus panc_0003 voeg toe aan pred_gt tuples voor later
panc_0018 (2, 223, 582, 582)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
suus panc_0018 transposed
suus panc_0018 not saving softmax
suus panc_0018 voeg toe aan pred_gt tuples voor later
panc_0036 (2, 223, 489, 489)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0036 transposed
suus panc_0036 not saving softmax
suus panc_0036 voeg toe aan pred_gt tuples voor later
panc_0042 (2, 310, 407, 407)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 140
using precomputed Gaussian
prediction done
suus panc_0042 transposed
suus panc_0042 not saving softmax
suus panc_0042 voeg toe aan pred_gt tuples voor later
panc_0044 (2, 233, 564, 564)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 564, 564)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 93, 186, 279, 372], [0, 67, 135, 202, 269, 337, 404]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0044 transposed
suus panc_0044 not saving softmax
suus panc_0044 voeg toe aan pred_gt tuples voor later
panc_0045 (2, 216, 559, 559)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0045 transposed
suus panc_0045 not saving softmax
suus panc_0045 voeg toe aan pred_gt tuples voor later
panc_0050 (2, 310, 419, 419)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
suus panc_0050 transposed
suus panc_0050 not saving softmax
suus panc_0050 voeg toe aan pred_gt tuples voor later
panc_0059 (2, 203, 560, 560)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0059 transposed
suus panc_0059 not saving softmax
suus panc_0059 voeg toe aan pred_gt tuples voor later
panc_0064 (2, 213, 535, 535)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 213, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 100, 133], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0064 transposed
suus panc_0064 not saving softmax
suus panc_0064 voeg toe aan pred_gt tuples voor later
panc_0075 (2, 224, 535, 535)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 108, 144], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0075 transposed
suus panc_0075 not saving softmax
suus panc_0075 voeg toe aan pred_gt tuples voor later
panc_0076 (2, 210, 535, 535)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0076 transposed
suus panc_0076 not saving softmax
suus panc_0076 voeg toe aan pred_gt tuples voor later
panc_0079 (2, 251, 582, 582)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0079 transposed
suus panc_0079 not saving softmax
suus panc_0079 voeg toe aan pred_gt tuples voor later
panc_0080 (2, 181, 582, 582)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 181, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 101], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 168
using precomputed Gaussian
prediction done
suus panc_0080 transposed
suus panc_0080 not saving softmax
suus panc_0080 voeg toe aan pred_gt tuples voor later
2022-08-03 12:35:26.334764: finished prediction
2022-08-03 12:35:26.338918: evaluation of raw predictions
2022-08-03 12:35:50.350879: determining postprocessing
Foreground vs background
before: 0.8186463098679807
after:  0.825917478698886
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0045.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0050.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0059.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0003.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0064.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0018.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0075.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0036.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/validation_raw/panc_0076.nii.gz
done
Done training all the folds! Now start the same command but with continue option, to generate log files


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='501', fold='0', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-03 12:36:31.846106: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-03 12:36:31.858950: The split file contains 5 splits.
2022-08-03 12:36:31.861438: Desired fold for training: 0
2022-08-03 12:36:31.863617: This split has 54 training and 14 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-03 12:36:34.069535: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-03 12:37:16.515713: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-03 12:37:42.665070: Unable to plot network architecture:
2022-08-03 12:37:42.670000: local variable 'g' referenced before assignment
2022-08-03 12:37:42.672434: 
printing the network instead:

2022-08-03 12:37:42.674797: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-03 12:37:42.683091: 

2022-08-03 12:37:42.964566: saving checkpoint...
2022-08-03 12:37:48.013596: done, saving took 5.33 seconds
suus panc_0006 voeg toe aan pred_gt tuples voor later
suus panc_0008 voeg toe aan pred_gt tuples voor later
suus panc_0028 voeg toe aan pred_gt tuples voor later
suus panc_0039 voeg toe aan pred_gt tuples voor later
suus panc_0043 voeg toe aan pred_gt tuples voor later
suus panc_0051 voeg toe aan pred_gt tuples voor later
suus panc_0052 voeg toe aan pred_gt tuples voor later
suus panc_0056 voeg toe aan pred_gt tuples voor later
suus panc_0058 voeg toe aan pred_gt tuples voor later
suus panc_0061 voeg toe aan pred_gt tuples voor later
suus panc_0062 voeg toe aan pred_gt tuples voor later
suus panc_0069 voeg toe aan pred_gt tuples voor later
suus panc_0072 voeg toe aan pred_gt tuples voor later
suus panc_0074 voeg toe aan pred_gt tuples voor later
2022-08-03 12:37:48.582277: finished prediction
2022-08-03 12:37:48.585210: evaluation of raw predictions
2022-08-03 12:37:54.588384: determining postprocessing
Foreground vs background
before: 0.860920298381398
after:  0.8567740719503181
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='501', fold='1', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-03 12:38:17.767066: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-03 12:38:17.780640: The split file contains 5 splits.
2022-08-03 12:38:17.783060: Desired fold for training: 1
2022-08-03 12:38:17.785224: This split has 54 training and 14 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-03 12:38:19.766988: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-03 12:39:05.585936: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-03 12:39:25.985917: Unable to plot network architecture:
2022-08-03 12:39:25.990248: local variable 'g' referenced before assignment
2022-08-03 12:39:25.992841: 
printing the network instead:

2022-08-03 12:39:25.995134: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-03 12:39:26.003640: 

2022-08-03 12:39:26.296528: saving checkpoint...
2022-08-03 12:39:32.157793: done, saving took 6.15 seconds
suus panc_0004 voeg toe aan pred_gt tuples voor later
suus panc_0011 voeg toe aan pred_gt tuples voor later
suus panc_0015 voeg toe aan pred_gt tuples voor later
suus panc_0024 voeg toe aan pred_gt tuples voor later
suus panc_0027 voeg toe aan pred_gt tuples voor later
suus panc_0030 voeg toe aan pred_gt tuples voor later
suus panc_0031 voeg toe aan pred_gt tuples voor later
suus panc_0032 voeg toe aan pred_gt tuples voor later
suus panc_0035 voeg toe aan pred_gt tuples voor later
suus panc_0037 voeg toe aan pred_gt tuples voor later
suus panc_0048 voeg toe aan pred_gt tuples voor later
suus panc_0053 voeg toe aan pred_gt tuples voor later
suus panc_0054 voeg toe aan pred_gt tuples voor later
suus panc_0055 voeg toe aan pred_gt tuples voor later
2022-08-03 12:39:32.745111: finished prediction
2022-08-03 12:39:32.748076: evaluation of raw predictions
2022-08-03 12:39:39.474253: determining postprocessing
Foreground vs background
before: 0.8177461676565987
after:  0.8125428449565671
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='501', fold='2', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-03 12:40:05.045081: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-03 12:40:05.056042: The split file contains 5 splits.
2022-08-03 12:40:05.058877: Desired fold for training: 2
2022-08-03 12:40:05.061051: This split has 54 training and 14 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-03 12:40:07.033622: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-03 12:40:55.281659: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-03 12:41:16.685379: Unable to plot network architecture:
2022-08-03 12:41:16.689367: local variable 'g' referenced before assignment
2022-08-03 12:41:16.691425: 
printing the network instead:

2022-08-03 12:41:16.693839: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-03 12:41:16.701512: 

2022-08-03 12:41:17.046082: saving checkpoint...
2022-08-03 12:41:22.635392: done, saving took 5.93 seconds
suus panc_0002 voeg toe aan pred_gt tuples voor later
suus panc_0005 voeg toe aan pred_gt tuples voor later
suus panc_0007 voeg toe aan pred_gt tuples voor later
suus panc_0019 voeg toe aan pred_gt tuples voor later
suus panc_0023 voeg toe aan pred_gt tuples voor later
suus panc_0029 voeg toe aan pred_gt tuples voor later
suus panc_0038 voeg toe aan pred_gt tuples voor later
suus panc_0040 voeg toe aan pred_gt tuples voor later
suus panc_0063 voeg toe aan pred_gt tuples voor later
suus panc_0066 voeg toe aan pred_gt tuples voor later
suus panc_0067 voeg toe aan pred_gt tuples voor later
suus panc_0068 voeg toe aan pred_gt tuples voor later
suus panc_0071 voeg toe aan pred_gt tuples voor later
suus panc_0078 voeg toe aan pred_gt tuples voor later
2022-08-03 12:41:23.163790: finished prediction
2022-08-03 12:41:23.165993: evaluation of raw predictions
2022-08-03 12:41:29.675381: determining postprocessing
Foreground vs background
before: 0.804674222889048
after:  0.7879643174446106
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='501', fold='3', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-03 12:41:54.765083: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-03 12:41:54.775086: The split file contains 5 splits.
2022-08-03 12:41:54.777483: Desired fold for training: 3
2022-08-03 12:41:54.779800: This split has 55 training and 13 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-03 12:41:56.759249: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-03 12:42:46.206124: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-03 12:43:03.419766: Unable to plot network architecture:
2022-08-03 12:43:03.443841: local variable 'g' referenced before assignment
2022-08-03 12:43:03.472689: 
printing the network instead:

2022-08-03 12:43:03.505708: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-03 12:43:03.530694: 

2022-08-03 12:43:03.953880: saving checkpoint...
2022-08-03 12:43:10.048454: done, saving took 6.49 seconds
suus panc_0010 voeg toe aan pred_gt tuples voor later
suus panc_0012 voeg toe aan pred_gt tuples voor later
suus panc_0013 voeg toe aan pred_gt tuples voor later
suus panc_0014 voeg toe aan pred_gt tuples voor later
suus panc_0016 voeg toe aan pred_gt tuples voor later
suus panc_0020 voeg toe aan pred_gt tuples voor later
suus panc_0021 voeg toe aan pred_gt tuples voor later
suus panc_0022 voeg toe aan pred_gt tuples voor later
suus panc_0034 voeg toe aan pred_gt tuples voor later
suus panc_0046 voeg toe aan pred_gt tuples voor later
suus panc_0047 voeg toe aan pred_gt tuples voor later
suus panc_0060 voeg toe aan pred_gt tuples voor later
suus panc_0077 voeg toe aan pred_gt tuples voor later
2022-08-03 12:43:10.725771: finished prediction
2022-08-03 12:43:10.728913: evaluation of raw predictions
2022-08-03 12:43:17.704039: determining postprocessing
Foreground vs background
before: 0.8393840992157207
after:  0.8397681619977218
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2LR', task='501', fold='4', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2LR.nnUNetTrainerV2_Hybrid2LR'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-03 12:43:44.087625: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-03 12:43:44.098089: The split file contains 5 splits.
2022-08-03 12:43:44.100777: Desired fold for training: 4
2022-08-03 12:43:44.102988: This split has 55 training and 13 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-03 12:43:46.089006: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-03 12:44:21.472862: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-03 12:44:40.889247: Unable to plot network architecture:
2022-08-03 12:44:40.900541: local variable 'g' referenced before assignment
2022-08-03 12:44:40.918703: 
printing the network instead:

2022-08-03 12:44:40.921468: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-03 12:44:40.930511: 

2022-08-03 12:44:41.397868: saving checkpoint...
2022-08-03 12:44:47.820951: done, saving took 6.89 seconds
suus panc_0003 voeg toe aan pred_gt tuples voor later
suus panc_0018 voeg toe aan pred_gt tuples voor later
suus panc_0036 voeg toe aan pred_gt tuples voor later
suus panc_0042 voeg toe aan pred_gt tuples voor later
suus panc_0044 voeg toe aan pred_gt tuples voor later
suus panc_0045 voeg toe aan pred_gt tuples voor later
suus panc_0050 voeg toe aan pred_gt tuples voor later
suus panc_0059 voeg toe aan pred_gt tuples voor later
suus panc_0064 voeg toe aan pred_gt tuples voor later
suus panc_0075 voeg toe aan pred_gt tuples voor later
suus panc_0076 voeg toe aan pred_gt tuples voor later
suus panc_0079 voeg toe aan pred_gt tuples voor later
suus panc_0080 voeg toe aan pred_gt tuples voor later
2022-08-03 12:44:48.386590: finished prediction
2022-08-03 12:44:48.389132: evaluation of raw predictions
2022-08-03 12:44:55.560422: determining postprocessing
Foreground vs background
before: 0.8186463098679807
after:  0.825917478698886
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
done
Start postprocessing..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Foreground vs background
before: 0.8282524259274521
after:  0.8243507442643654
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done
Done postprocessing! Now start inferencing its own train and test files.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

using model stored in  /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1
This model expects 1 input modalities for each image
Found 12 unique case ids, here are some examples: ['panc_0082' 'panc_0001' 'panc_0026' 'panc_0049' 'panc_0009' 'panc_0041'
 'panc_0041' 'panc_0082' 'panc_0033' 'panc_0073']
If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc
number of cases: 12
number of cases that still need to be predicted: 12
emptying cuda cache
loading parameters for folds, None
folds is None so we will automatically look for output folders (not using 'all'!)
found the following folds:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_0', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_1', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_2', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_3', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4']
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus5 - zet de plans properties
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
using the following model files:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2LR__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']
starting preprocessing generator
preprocessing took 1.811981201171875e-05 seconds
starting prediction...
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0026.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.       0.859375 0.859375] 

no resampling necessary
no resampling necessary
before: {'spacing': array([1.      , 0.859375, 0.859375]), 'spacing_transposed': array([1.      , 0.859375, 0.859375]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 512, 512)} 

(1, 310, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0073.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.       0.703125 0.703125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.      , 0.703125, 0.703125]), 'spacing_transposed': array([1.      , 0.703125, 0.703125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 419, 419)} 

(1, 310, 419, 419)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0001.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 240, 512, 512) after crop: (1, 240, 512, 512) spacing: [1.       0.859375 0.859375] 

no resampling necessary
no resampling necessary
before: {'spacing': array([1.      , 0.859375, 0.859375]), 'spacing_transposed': array([1.      , 0.859375, 0.859375]), 'data.shape (data is transposed)': (1, 240, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 240, 512, 512)} 

(1, 240, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0049.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 222, 512, 512) after crop: (1, 222, 512, 512) spacing: [1.        0.8984375 0.8984375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([1.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 222, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 222, 535, 535)} 

(1, 222, 535, 535)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0009.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 196, 512, 512) after crop: (1, 196, 512, 512) spacing: [1.         0.85799998 0.85799998] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.85799998, 0.85799998]), 'spacing_transposed': array([1.        , 0.85799998, 0.85799998]), 'data.shape (data is transposed)': (1, 196, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 196, 511, 511)} 

(1, 196, 511, 511)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0057.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 228, 512, 512) after crop: (1, 228, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 228, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 228, 582, 582)} 

(1, 228, 582, 582)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0041.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 237, 512, 512) after crop: (1, 237, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 237, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 237, 559, 559)} 

(1, 237, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0082.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 226, 512, 512) after crop: (1, 226, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 226, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 226, 559, 559)} 

(1, 226, 559, 559)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0017.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.00000012 0.78125    0.78125   ] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.00000012, 0.78125   , 0.78125   ]), 'spacing_transposed': array([1.00000012, 0.78125   , 0.78125   ]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 465, 465)} 

(1, 310, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0065.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 216, 512, 512) after crop: (1, 216, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 216, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 216, 582, 582)} 

(1, 216, 582, 582)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0033.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.00000012 0.7421875  0.7421875 ] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.00000012, 0.7421875 , 0.7421875 ]), 'spacing_transposed': array([1.00000012, 0.7421875 , 0.7421875 ]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 442, 442)} 

(1, 310, 442, 442)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0081.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 209, 512, 512) after crop: (1, 209, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 209, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 209, 582, 582)} 

(1, 209, 582, 582)
This worker has ended successfully, no errors to report
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0026.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0009.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0017.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0073.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0057.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0065.nii.gz
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0001.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0041.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0033.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0049.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0082.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0081.nii.gz
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0026.nii.gz
loading took 0.16428041458129883 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
computing Gaussian
done
prediction done
fold 0 prediction took 88.36822056770325 seconds
loading another fold took 0.10323095321655273 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.3612654209137 seconds
loading another fold took 0.10204195976257324 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.84184908866882 seconds
loading another fold took 0.10336446762084961 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.94015550613403 seconds
loading another fold took 0.10397076606750488 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 82.00685715675354 seconds
full prediction took 0.05870556831359863 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0001.nii.gz
loading took 0.10885977745056152 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
fold 0 prediction took 59.727142333984375 seconds
loading another fold took 0.10277462005615234 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.744038343429565 seconds
loading another fold took 0.10298585891723633 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.685317277908325 seconds
loading another fold took 0.10295891761779785 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.63710284233093 seconds
loading another fold took 0.10300326347351074 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.694196701049805 seconds
full prediction took 0.045044660568237305 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0009.nii.gz
loading took 0.10781216621398926 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
fold 0 prediction took 47.9824492931366 seconds
loading another fold took 0.10228395462036133 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
another fold took 46.91133451461792 seconds
loading another fold took 0.10278129577636719 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
another fold took 46.88074994087219 seconds
loading another fold took 0.10283017158508301 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
another fold took 46.86129069328308 seconds
loading another fold took 0.10270428657531738 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
another fold took 46.88696599006653 seconds
full prediction took 0.036539316177368164 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0041.nii.gz
loading took 0.11062836647033691 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
fold 0 prediction took 60.011505365371704 seconds
loading another fold took 0.10293030738830566 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.8344304561615 seconds
loading another fold took 0.10284066200256348 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.99599361419678 seconds
loading another fold took 0.10290384292602539 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.91352367401123 seconds
loading another fold took 0.10295724868774414 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.84429359436035 seconds
full prediction took 0.053164005279541016 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0017.nii.gz
loading took 0.10690474510192871 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
fold 0 prediction took 56.08832550048828 seconds
loading another fold took 0.1027064323425293 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.953513383865356 seconds
loading another fold took 0.10285043716430664 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.91454339027405 seconds
loading another fold took 0.10292553901672363 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.805363178253174 seconds
loading another fold took 0.10306406021118164 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.93512034416199 seconds
full prediction took 0.04809451103210449 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0033.nii.gz
loading took 0.10729646682739258 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
fold 0 prediction took 54.88476800918579 seconds
loading another fold took 0.10362792015075684 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.68377757072449 seconds
loading another fold took 0.10393404960632324 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.63885283470154 seconds
loading another fold took 0.1039285659790039 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.79206967353821 seconds
loading another fold took 0.10398292541503906 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.728174448013306 seconds
full prediction took 0.04335498809814453 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0073.nii.gz
loading took 0.10631990432739258 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
fold 0 prediction took 54.69795608520508 seconds
loading another fold took 0.1022500991821289 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.58096146583557 seconds
loading another fold took 0.10300612449645996 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.53163003921509 seconds
loading another fold took 0.10302567481994629 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.45955467224121 seconds
loading another fold took 0.10271883010864258 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
another fold took 54.57540822029114 seconds
full prediction took 0.03942155838012695 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0049.nii.gz
loading took 0.106689453125 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
fold 0 prediction took 58.76382231712341 seconds
loading another fold took 0.10227537155151367 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.61406993865967 seconds
loading another fold took 0.10251736640930176 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.6895489692688 seconds
loading another fold took 0.1026771068572998 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.545294761657715 seconds
loading another fold took 0.10260629653930664 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.574663162231445 seconds
full prediction took 0.04526329040527344 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0057.nii.gz
loading took 0.11645936965942383 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
fold 0 prediction took 81.81504726409912 seconds
loading another fold took 0.10267877578735352 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.74184679985046 seconds
loading another fold took 0.10243964195251465 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.74824666976929 seconds
loading another fold took 0.10278105735778809 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.84725666046143 seconds
loading another fold took 0.10308980941772461 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.79984188079834 seconds
full prediction took 0.055268287658691406 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0082.nii.gz
loading took 0.10755395889282227 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
fold 0 prediction took 58.84233736991882 seconds
loading another fold took 0.10237431526184082 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.76901984214783 seconds
loading another fold took 0.10222959518432617 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.76291060447693 seconds
loading another fold took 0.10252737998962402 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.7066433429718 seconds
loading another fold took 0.10282611846923828 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
another fold took 58.75569224357605 seconds
full prediction took 0.04993581771850586 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0065.nii.gz
loading took 0.10779786109924316 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
fold 0 prediction took 81.68278694152832 seconds
loading another fold took 0.10148262977600098 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.52157092094421 seconds
loading another fold took 0.10124325752258301 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.69441223144531 seconds
loading another fold took 0.10131049156188965 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.59716558456421 seconds
loading another fold took 0.10224246978759766 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.70660424232483 seconds
full prediction took 0.051697731018066406 seconds
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2LR/501/imagesTs/panc_0081.nii.gz
loading took 0.10544991493225098 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
fold 0 prediction took 81.84449481964111 seconds
loading another fold took 0.10355472564697266 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.78377103805542 seconds
loading another fold took 0.10349440574645996 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.96766066551208 seconds
loading another fold took 0.10351252555847168 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.99538826942444 seconds
loading another fold took 0.10378885269165039 seconds
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
another fold took 81.9185802936554 seconds
full prediction took 0.05070805549621582 seconds
inference done. Now waiting for the segmentation export to finish...
postprocessing...
postprocessing took 10.928559303283691 seconds
Done inferencing! Now start the evaluation.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Program finished with exit code 0 at: Thu Jul 28 18:37:05 CEST 2022
