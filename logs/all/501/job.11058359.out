Starting at Thu Jul 28 18:36:53 CEST 2022
Running on hosts: res-hpc-lkeb05
Running on 1 nodes.
Running 1 tasks.
CPUs on node: 8.
Account: div2-lkeb
Job ID: 11058359
Job name: PancreasAll
Node running script: res-hpc-lkeb05
Submit host: res-hpc-lo02.researchlumc.nl
GPUS: 0 or 
Mon Aug  1 12:59:01 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro RTX 6000     On   | 00000000:AF:00.0 Off |                  Off |
| 34%   41C    P8    16W / 260W |      0MiB / 24220MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
Current working directory is /home/smaijer
Load all modules..
Done with loading all modules. Modules:
Activate conda env nnunet..
Verifying environment variables:
Installing hidden layer and nnUnet..
Collecting hiddenlayer
  Cloning https://github.com/FabianIsensee/hiddenlayer.git (to revision more_plotted_details) to /tmp/pip-install-p21g1du4/hiddenlayer_a1e48184b629484d9dd9d8a5f75fd0f6
  Resolved https://github.com/FabianIsensee/hiddenlayer.git to commit 4b98f9e5cccebac67368f02b95f4700b522345b1
Using legacy 'setup.py install' for hiddenlayer, since package 'wheel' is not installed.
Installing collected packages: hiddenlayer
    Running setup.py install for hiddenlayer: started
    Running setup.py install for hiddenlayer: finished with status 'done'
Successfully installed hiddenlayer-0.2
Start preprocessing..
Done preprocessing! Start training all the folds..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='501', fold='3', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-01 12:59:20.544330: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-01 12:59:20.554506: The split file contains 5 splits.
2022-08-01 12:59:20.556643: Desired fold for training: 3
2022-08-01 12:59:20.558506: This split has 55 training and 13 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-01 12:59:22.962561: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/model_latest.model train= True
SuusB run_training - zet learning rate als  
2022-08-01 13:00:00.090017: Suus1 maybe_update_lr lr: 3.4e-05
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-01 13:00:19.960317: Unable to plot network architecture:
2022-08-01 13:00:19.963773: local variable 'g' referenced before assignment
2022-08-01 13:00:19.966168: 
printing the network instead:

2022-08-01 13:00:19.978776: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-01 13:00:20.019732: 

2022-08-01 13:00:20.040857: 
epoch:  350
2022-08-01 13:02:26.631826: train loss : -0.8133
2022-08-01 13:02:37.497210: validation loss: -0.7693
2022-08-01 13:02:37.547732: Average global foreground Dice: [0.8321]
2022-08-01 13:02:37.589416: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:02:39.009222: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-01 13:02:39.039798: saving best epoch checkpoint...
2022-08-01 13:02:39.518029: saving checkpoint...
2022-08-01 13:02:45.036100: done, saving took 5.98 seconds
2022-08-01 13:02:45.046124: This epoch took 144.967343 s

2022-08-01 13:02:45.048358: 
epoch:  351
2022-08-01 13:04:35.265285: train loss : -0.8197
2022-08-01 13:04:43.601337: validation loss: -0.7758
2022-08-01 13:04:43.604806: Average global foreground Dice: [0.8399]
2022-08-01 13:04:43.634013: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:04:44.365196: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-01 13:04:44.388839: saving best epoch checkpoint...
2022-08-01 13:04:44.687713: saving checkpoint...
2022-08-01 13:04:50.131155: done, saving took 5.70 seconds
2022-08-01 13:04:50.140374: This epoch took 125.090026 s

2022-08-01 13:04:50.142498: 
epoch:  352
2022-08-01 13:06:41.443269: train loss : -0.8158
2022-08-01 13:06:49.817398: validation loss: -0.7555
2022-08-01 13:06:49.862036: Average global foreground Dice: [0.8193]
2022-08-01 13:06:49.923232: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:06:50.658430: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-01 13:06:50.699875: This epoch took 120.555320 s

2022-08-01 13:06:50.731102: 
epoch:  353
2022-08-01 13:08:47.228649: train loss : -0.8102
2022-08-01 13:08:57.464062: validation loss: -0.7827
2022-08-01 13:08:57.485571: Average global foreground Dice: [0.8423]
2022-08-01 13:08:57.517802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:08:58.247622: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-01 13:08:58.279808: This epoch took 127.528433 s

2022-08-01 13:08:58.288541: 
epoch:  354
2022-08-01 13:10:48.602085: train loss : -0.8155
2022-08-01 13:11:01.427547: validation loss: -0.7691
2022-08-01 13:11:01.469560: Average global foreground Dice: [0.8273]
2022-08-01 13:11:01.501307: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:11:02.315416: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-01 13:11:02.347844: This epoch took 124.045065 s

2022-08-01 13:11:02.398775: 
epoch:  355
2022-08-01 13:12:53.164972: train loss : -0.8181
2022-08-01 13:13:04.276901: validation loss: -0.7622
2022-08-01 13:13:04.315817: Average global foreground Dice: [0.8224]
2022-08-01 13:13:04.333883: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:13:05.190310: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-01 13:13:05.225174: This epoch took 122.766383 s

2022-08-01 13:13:05.256216: 
epoch:  356
2022-08-01 13:14:55.239847: train loss : -0.8175
2022-08-01 13:15:05.606441: validation loss: -0.7755
2022-08-01 13:15:05.658046: Average global foreground Dice: [0.8298]
2022-08-01 13:15:05.689890: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:15:06.343047: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-01 13:15:06.355996: This epoch took 121.078589 s

2022-08-01 13:15:06.401257: 
epoch:  357
2022-08-01 13:17:03.032207: train loss : -0.8130
2022-08-01 13:17:12.176488: validation loss: -0.7680
2022-08-01 13:17:12.179680: Average global foreground Dice: [0.831]
2022-08-01 13:17:12.181914: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:17:12.842353: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-01 13:17:12.870627: This epoch took 126.443670 s

2022-08-01 13:17:12.911978: 
epoch:  358
2022-08-01 13:19:03.150644: train loss : -0.8202
2022-08-01 13:19:12.451145: validation loss: -0.7889
2022-08-01 13:19:12.469127: Average global foreground Dice: [0.8437]
2022-08-01 13:19:12.471936: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:19:13.068748: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-01 13:19:13.110705: This epoch took 120.181816 s

2022-08-01 13:19:13.152788: 
epoch:  359
2022-08-01 13:21:03.468307: train loss : -0.8102
2022-08-01 13:21:13.484465: validation loss: -0.7808
2022-08-01 13:21:13.513408: Average global foreground Dice: [0.8431]
2022-08-01 13:21:13.564778: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:21:14.742284: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-01 13:21:14.774786: saving best epoch checkpoint...
2022-08-01 13:21:15.088834: saving checkpoint...
2022-08-01 13:21:20.895332: done, saving took 6.08 seconds
2022-08-01 13:21:20.906060: This epoch took 127.739689 s

2022-08-01 13:21:20.908098: 
epoch:  360
2022-08-01 13:23:10.165748: train loss : -0.8146
2022-08-01 13:23:20.485043: validation loss: -0.7759
2022-08-01 13:23:20.516215: Average global foreground Dice: [0.8367]
2022-08-01 13:23:20.565809: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:23:22.120258: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-01 13:23:22.141802: saving best epoch checkpoint...
2022-08-01 13:23:22.623040: saving checkpoint...
2022-08-01 13:23:28.786429: done, saving took 6.62 seconds
2022-08-01 13:23:28.795289: This epoch took 127.885034 s

2022-08-01 13:23:28.797222: 
epoch:  361
2022-08-01 13:25:19.540315: train loss : -0.8179
2022-08-01 13:25:30.385413: validation loss: -0.7605
2022-08-01 13:25:30.425560: Average global foreground Dice: [0.8253]
2022-08-01 13:25:30.436049: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:25:31.495653: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-01 13:25:31.548868: This epoch took 122.749718 s

2022-08-01 13:25:31.575787: 
epoch:  362
2022-08-01 13:27:27.374903: train loss : -0.8148
2022-08-01 13:27:39.111840: validation loss: -0.7690
2022-08-01 13:27:39.159317: Average global foreground Dice: [0.831]
2022-08-01 13:27:39.199937: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:27:40.427510: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-01 13:27:40.461530: This epoch took 128.856750 s

2022-08-01 13:27:40.503309: 
epoch:  363
2022-08-01 13:29:31.426268: train loss : -0.8115
2022-08-01 13:29:43.666070: validation loss: -0.7702
2022-08-01 13:29:43.708333: Average global foreground Dice: [0.8326]
2022-08-01 13:29:43.743799: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:29:44.610964: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-01 13:29:44.630166: This epoch took 124.092377 s

2022-08-01 13:29:44.646485: 
epoch:  364
2022-08-01 13:31:36.412588: train loss : -0.8161
2022-08-01 13:31:47.889713: validation loss: -0.7413
2022-08-01 13:31:47.921520: Average global foreground Dice: [0.8075]
2022-08-01 13:31:47.964995: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:31:49.086952: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-01 13:31:49.116850: This epoch took 124.428956 s

2022-08-01 13:31:49.150151: 
epoch:  365
2022-08-01 13:33:39.215988: train loss : -0.8100
2022-08-01 13:33:48.670707: validation loss: -0.7679
2022-08-01 13:33:48.730386: Average global foreground Dice: [0.8323]
2022-08-01 13:33:48.771033: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:33:49.934526: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-01 13:33:49.986015: This epoch took 120.792440 s

2022-08-01 13:33:50.011316: 
epoch:  366
2022-08-01 13:35:47.839262: train loss : -0.8175
2022-08-01 13:35:57.354218: validation loss: -0.7760
2022-08-01 13:35:57.394642: Average global foreground Dice: [0.8316]
2022-08-01 13:35:57.420796: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:35:58.445470: Suus1 maybe_update_lr lr: 3e-05
2022-08-01 13:35:58.454436: This epoch took 128.420556 s

2022-08-01 13:35:58.459080: 
epoch:  367
2022-08-01 13:37:49.478579: train loss : -0.8232
2022-08-01 13:37:56.888959: validation loss: -0.7681
2022-08-01 13:37:56.911471: Average global foreground Dice: [0.8341]
2022-08-01 13:37:56.943786: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:37:57.915365: Suus1 maybe_update_lr lr: 3e-05
2022-08-01 13:37:57.947868: This epoch took 119.475833 s

2022-08-01 13:37:57.980777: 
epoch:  368
2022-08-01 13:39:48.219479: train loss : -0.8183
2022-08-01 13:39:57.729712: validation loss: -0.7927
2022-08-01 13:39:57.765257: Average global foreground Dice: [0.8483]
2022-08-01 13:39:57.790159: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:39:58.628244: Suus1 maybe_update_lr lr: 3e-05
2022-08-01 13:39:58.705892: This epoch took 120.694118 s

2022-08-01 13:39:58.740715: 
epoch:  369
2022-08-01 13:41:59.451224: train loss : -0.8224
2022-08-01 13:42:08.223500: validation loss: -0.7799
2022-08-01 13:42:08.250260: Average global foreground Dice: [0.8328]
2022-08-01 13:42:08.276778: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:42:09.243458: Suus1 maybe_update_lr lr: 3e-05
2022-08-01 13:42:09.286911: This epoch took 130.515590 s

2022-08-01 13:42:09.328760: 
epoch:  370
2022-08-01 13:44:09.581049: train loss : -0.8093
2022-08-01 13:44:22.637013: validation loss: -0.7723
2022-08-01 13:44:22.677806: Average global foreground Dice: [0.8317]
2022-08-01 13:44:22.711807: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:44:23.637523: Suus1 maybe_update_lr lr: 3e-05
2022-08-01 13:44:23.667843: This epoch took 134.311066 s

2022-08-01 13:44:23.670510: 
epoch:  371
2022-08-01 13:46:21.901063: train loss : -0.8133
2022-08-01 13:46:32.386640: validation loss: -0.7614
2022-08-01 13:46:32.437526: Average global foreground Dice: [0.8183]
2022-08-01 13:46:32.484783: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:46:33.324534: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-01 13:46:33.333588: This epoch took 129.660792 s

2022-08-01 13:46:33.335921: 
epoch:  372
2022-08-01 13:48:21.756248: train loss : -0.8208
2022-08-01 13:48:30.388761: validation loss: -0.7770
2022-08-01 13:48:30.410446: Average global foreground Dice: [0.8303]
2022-08-01 13:48:30.423218: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:48:31.067237: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-01 13:48:31.107012: This epoch took 117.764637 s

2022-08-01 13:48:31.147921: 
epoch:  373
2022-08-01 13:50:20.821783: train loss : -0.8178
2022-08-01 13:50:31.354126: validation loss: -0.7774
2022-08-01 13:50:31.398163: Average global foreground Dice: [0.8373]
2022-08-01 13:50:31.418804: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:50:32.187832: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-01 13:50:32.221905: This epoch took 121.042555 s

2022-08-01 13:50:32.254758: 
epoch:  374
2022-08-01 13:52:23.029492: train loss : -0.8155
2022-08-01 13:52:31.554627: validation loss: -0.7664
2022-08-01 13:52:31.566967: Average global foreground Dice: [0.8251]
2022-08-01 13:52:31.585802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:52:32.221112: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-01 13:52:32.223538: This epoch took 119.926778 s

2022-08-01 13:52:32.225772: 
epoch:  375
2022-08-01 13:54:23.008342: train loss : -0.8177
2022-08-01 13:54:34.362712: validation loss: -0.7851
2022-08-01 13:54:34.398716: Average global foreground Dice: [0.8348]
2022-08-01 13:54:34.430816: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:54:35.185819: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-01 13:54:35.188052: This epoch took 122.960200 s

2022-08-01 13:54:35.190163: 
epoch:  376
2022-08-01 13:56:40.770622: train loss : -0.8179
2022-08-01 13:56:49.783961: validation loss: -0.7765
2022-08-01 13:56:49.840465: Average global foreground Dice: [0.8313]
2022-08-01 13:56:49.917763: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:56:51.300637: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-01 13:56:51.426842: This epoch took 136.234676 s

2022-08-01 13:56:51.512902: 
epoch:  377
2022-08-01 13:58:46.586945: train loss : -0.8138
2022-08-01 13:59:00.417431: validation loss: -0.7929
2022-08-01 13:59:00.448767: Average global foreground Dice: [0.8468]
2022-08-01 13:59:00.474863: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 13:59:01.114544: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-01 13:59:01.120805: This epoch took 129.562968 s

2022-08-01 13:59:01.123080: 
epoch:  378
2022-08-01 14:00:55.973859: train loss : -0.8220
2022-08-01 14:01:05.950628: validation loss: -0.7849
2022-08-01 14:01:05.988709: Average global foreground Dice: [0.8389]
2022-08-01 14:01:06.021853: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:01:07.182583: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-01 14:01:07.192817: This epoch took 126.067628 s

2022-08-01 14:01:07.219182: 
epoch:  379
2022-08-01 14:02:57.896450: train loss : -0.8178
2022-08-01 14:03:05.425250: validation loss: -0.7979
2022-08-01 14:03:05.465340: Average global foreground Dice: [0.8517]
2022-08-01 14:03:05.505707: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:03:06.260523: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-01 14:03:06.307741: saving best epoch checkpoint...
2022-08-01 14:03:06.816368: saving checkpoint...
2022-08-01 14:03:12.814752: done, saving took 6.48 seconds
2022-08-01 14:03:12.827810: This epoch took 125.599603 s

2022-08-01 14:03:12.829962: 
epoch:  380
2022-08-01 14:05:05.122957: train loss : -0.8262
2022-08-01 14:05:16.107607: validation loss: -0.7808
2022-08-01 14:05:16.159257: Average global foreground Dice: [0.8396]
2022-08-01 14:05:16.197285: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:05:17.545751: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-01 14:05:17.566832: saving best epoch checkpoint...
2022-08-01 14:05:18.089321: saving checkpoint...
2022-08-01 14:05:23.580823: done, saving took 6.01 seconds
2022-08-01 14:05:23.591430: This epoch took 130.759418 s

2022-08-01 14:05:23.594092: 
epoch:  381
2022-08-01 14:07:19.879109: train loss : -0.8188
2022-08-01 14:07:30.991175: validation loss: -0.7721
2022-08-01 14:07:31.015558: Average global foreground Dice: [0.8389]
2022-08-01 14:07:31.057854: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:07:32.049044: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-01 14:07:32.052627: saving best epoch checkpoint...
2022-08-01 14:07:32.340963: saving checkpoint...
2022-08-01 14:07:37.712045: done, saving took 5.65 seconds
2022-08-01 14:07:37.722285: This epoch took 134.125787 s

2022-08-01 14:07:37.724507: 
epoch:  382
2022-08-01 14:09:27.694075: train loss : -0.8185
2022-08-01 14:09:37.209263: validation loss: -0.7867
2022-08-01 14:09:37.237248: Average global foreground Dice: [0.8408]
2022-08-01 14:09:37.259820: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:09:37.926876: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-01 14:09:37.966949: saving best epoch checkpoint...
2022-08-01 14:09:38.262263: saving checkpoint...
2022-08-01 14:09:43.659885: done, saving took 5.67 seconds
2022-08-01 14:09:43.670753: This epoch took 125.943712 s

2022-08-01 14:09:43.673301: 
epoch:  383
2022-08-01 14:11:32.888112: train loss : -0.8205
2022-08-01 14:11:45.281980: validation loss: -0.7618
2022-08-01 14:11:45.334055: Average global foreground Dice: [0.8253]
2022-08-01 14:11:45.429478: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:11:46.032157: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-01 14:11:46.035648: This epoch took 122.360148 s

2022-08-01 14:11:46.041370: 
epoch:  384
2022-08-01 14:13:35.601141: train loss : -0.8167
2022-08-01 14:13:45.564944: validation loss: -0.7806
2022-08-01 14:13:45.603345: Average global foreground Dice: [0.8369]
2022-08-01 14:13:45.629782: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:13:46.461601: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-01 14:13:46.493861: This epoch took 120.450224 s

2022-08-01 14:13:46.540777: 
epoch:  385
2022-08-01 14:15:36.877490: train loss : -0.8178
2022-08-01 14:15:46.497132: validation loss: -0.7927
2022-08-01 14:15:46.523375: Average global foreground Dice: [0.8462]
2022-08-01 14:15:46.565788: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:15:47.360534: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-01 14:15:47.398827: saving best epoch checkpoint...
2022-08-01 14:15:47.874931: saving checkpoint...
2022-08-01 14:15:53.847804: done, saving took 6.42 seconds
2022-08-01 14:15:53.857486: This epoch took 127.283713 s

2022-08-01 14:15:53.859453: 
epoch:  386
2022-08-01 14:17:43.697714: train loss : -0.8226
2022-08-01 14:17:54.774573: validation loss: -0.7796
2022-08-01 14:17:54.805472: Average global foreground Dice: [0.837]
2022-08-01 14:17:54.838780: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:17:55.504215: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-01 14:17:55.547849: saving best epoch checkpoint...
2022-08-01 14:17:55.881966: saving checkpoint...
2022-08-01 14:18:01.874460: done, saving took 6.28 seconds
2022-08-01 14:18:01.883894: This epoch took 128.022426 s

2022-08-01 14:18:01.886210: 
epoch:  387
2022-08-01 14:19:48.065777: train loss : -0.8210
2022-08-01 14:19:57.862827: validation loss: -0.7665
2022-08-01 14:19:57.915142: Average global foreground Dice: [0.8274]
2022-08-01 14:19:57.937836: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:19:59.287212: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-01 14:19:59.325865: This epoch took 117.437487 s

2022-08-01 14:19:59.348704: 
epoch:  388
2022-08-01 14:21:45.631163: train loss : -0.8185
2022-08-01 14:21:53.798200: validation loss: -0.7901
2022-08-01 14:21:53.805623: Average global foreground Dice: [0.8458]
2022-08-01 14:21:53.824541: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:21:54.489297: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-01 14:21:54.492364: saving best epoch checkpoint...
2022-08-01 14:21:54.821042: saving checkpoint...
2022-08-01 14:22:00.772113: done, saving took 6.28 seconds
2022-08-01 14:22:00.783200: This epoch took 121.406769 s

2022-08-01 14:22:00.785893: 
epoch:  389
2022-08-01 14:23:47.912225: train loss : -0.8229
2022-08-01 14:23:57.580912: validation loss: -0.7875
2022-08-01 14:23:57.618607: Average global foreground Dice: [0.8434]
2022-08-01 14:23:57.653794: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:23:58.526302: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-01 14:23:58.569077: saving best epoch checkpoint...
2022-08-01 14:23:58.945207: saving checkpoint...
2022-08-01 14:24:04.529946: done, saving took 5.93 seconds
2022-08-01 14:24:04.539916: This epoch took 123.751666 s

2022-08-01 14:24:04.542439: 
epoch:  390
2022-08-01 14:25:53.348338: train loss : -0.8224
2022-08-01 14:26:02.287827: validation loss: -0.7745
2022-08-01 14:26:02.326296: Average global foreground Dice: [0.8281]
2022-08-01 14:26:02.367787: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:26:03.077187: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-01 14:26:03.147416: This epoch took 118.602733 s

2022-08-01 14:26:03.180758: 
epoch:  391
2022-08-01 14:27:55.272691: train loss : -0.8239
2022-08-01 14:28:05.544427: validation loss: -0.7787
2022-08-01 14:28:05.592051: Average global foreground Dice: [0.8408]
2022-08-01 14:28:05.618837: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:28:06.327606: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-01 14:28:06.372379: This epoch took 123.165549 s

2022-08-01 14:28:06.434813: 
epoch:  392
2022-08-01 14:29:55.975297: train loss : -0.8234
2022-08-01 14:30:06.033395: validation loss: -0.7769
2022-08-01 14:30:06.064155: Average global foreground Dice: [0.8307]
2022-08-01 14:30:06.105023: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:30:06.756381: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-01 14:30:06.758807: This epoch took 120.271007 s

2022-08-01 14:30:06.760980: 
epoch:  393
2022-08-01 14:31:54.840308: train loss : -0.8234
2022-08-01 14:32:04.526176: validation loss: -0.7809
2022-08-01 14:32:04.573053: Average global foreground Dice: [0.835]
2022-08-01 14:32:04.607812: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:32:05.302110: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-01 14:32:05.304629: This epoch took 118.541472 s

2022-08-01 14:32:05.313353: 
epoch:  394
2022-08-01 14:33:54.902351: train loss : -0.8180
2022-08-01 14:34:04.336787: validation loss: -0.7457
2022-08-01 14:34:04.366185: Average global foreground Dice: [0.8103]
2022-08-01 14:34:04.368679: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:34:05.075120: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-01 14:34:05.078418: This epoch took 119.761693 s

2022-08-01 14:34:05.080796: 
epoch:  395
2022-08-01 14:35:55.361188: train loss : -0.8249
2022-08-01 14:36:04.790272: validation loss: -0.7916
2022-08-01 14:36:04.814922: Average global foreground Dice: [0.8472]
2022-08-01 14:36:04.825508: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:36:05.485632: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-01 14:36:05.519148: This epoch took 120.436076 s

2022-08-01 14:36:05.521625: 
epoch:  396
2022-08-01 14:37:55.465530: train loss : -0.8231
2022-08-01 14:38:04.971338: validation loss: -0.7787
2022-08-01 14:38:04.998301: Average global foreground Dice: [0.8365]
2022-08-01 14:38:05.011134: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:38:06.057271: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-01 14:38:06.088835: This epoch took 120.564804 s

2022-08-01 14:38:06.103752: 
epoch:  397
2022-08-01 14:39:55.881463: train loss : -0.8240
2022-08-01 14:40:06.357965: validation loss: -0.7688
2022-08-01 14:40:06.400511: Average global foreground Dice: [0.828]
2022-08-01 14:40:06.411611: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:40:07.313974: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-01 14:40:07.333262: This epoch took 121.226869 s

2022-08-01 14:40:07.352538: 
epoch:  398
2022-08-01 14:41:56.978090: train loss : -0.8227
2022-08-01 14:42:05.847290: validation loss: -0.7687
2022-08-01 14:42:05.886708: Average global foreground Dice: [0.8284]
2022-08-01 14:42:05.915935: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:42:06.482327: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-01 14:42:06.518708: This epoch took 119.144958 s

2022-08-01 14:42:06.570771: 
epoch:  399
2022-08-01 14:44:02.118995: train loss : -0.8246
2022-08-01 14:44:11.547624: validation loss: -0.7899
2022-08-01 14:44:11.573332: Average global foreground Dice: [0.8437]
2022-08-01 14:44:11.602935: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:44:12.366877: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-01 14:44:12.398839: saving scheduled checkpoint file...
2022-08-01 14:44:12.896537: saving checkpoint...
2022-08-01 14:44:18.741481: done, saving took 6.31 seconds
2022-08-01 14:44:18.752728: done
2022-08-01 14:44:18.754739: This epoch took 132.144979 s

2022-08-01 14:44:18.756596: 
epoch:  400
2022-08-01 14:46:09.104061: train loss : -0.8213
2022-08-01 14:46:20.254439: validation loss: -0.7501
2022-08-01 14:46:20.279084: Average global foreground Dice: [0.8105]
2022-08-01 14:46:20.302632: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:46:20.993626: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-01 14:46:21.022366: This epoch took 122.263739 s

2022-08-01 14:46:21.062456: 
epoch:  401
2022-08-01 14:48:11.240421: train loss : -0.8184
2022-08-01 14:48:19.854200: validation loss: -0.7714
2022-08-01 14:48:19.879526: Average global foreground Dice: [0.8329]
2022-08-01 14:48:19.921904: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:48:20.842298: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-01 14:48:20.874850: This epoch took 119.776390 s

2022-08-01 14:48:20.904040: 
epoch:  402
2022-08-01 14:50:13.041906: train loss : -0.8178
2022-08-01 14:50:22.118194: validation loss: -0.7685
2022-08-01 14:50:22.141820: Average global foreground Dice: [0.8308]
2022-08-01 14:50:22.166890: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:50:22.732156: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-01 14:50:22.749678: This epoch took 121.839260 s

2022-08-01 14:50:22.788720: 
epoch:  403
2022-08-01 14:52:15.380209: train loss : -0.8267
2022-08-01 14:52:25.700387: validation loss: -0.7652
2022-08-01 14:52:25.749789: Average global foreground Dice: [0.8285]
2022-08-01 14:52:25.789802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:52:26.463653: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-01 14:52:26.516815: This epoch took 123.721229 s

2022-08-01 14:52:26.538509: 
epoch:  404
2022-08-01 14:54:16.732411: train loss : -0.8279
2022-08-01 14:54:24.279535: validation loss: -0.7606
2022-08-01 14:54:24.309200: Average global foreground Dice: [0.8237]
2022-08-01 14:54:24.311652: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:54:24.923872: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-01 14:54:24.947367: This epoch took 118.400398 s

2022-08-01 14:54:24.967775: 
epoch:  405
2022-08-01 14:56:18.522852: train loss : -0.8237
2022-08-01 14:56:26.735747: validation loss: -0.7700
2022-08-01 14:56:26.768371: Average global foreground Dice: [0.8233]
2022-08-01 14:56:26.801844: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:56:27.461323: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-01 14:56:27.497035: This epoch took 122.508244 s

2022-08-01 14:56:27.537704: 
epoch:  406
2022-08-01 14:58:23.103622: train loss : -0.8209
2022-08-01 14:58:33.018500: validation loss: -0.7745
2022-08-01 14:58:33.049847: Average global foreground Dice: [0.8338]
2022-08-01 14:58:33.078802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 14:58:34.238634: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-01 14:58:34.270196: This epoch took 126.695404 s

2022-08-01 14:58:34.294962: 
epoch:  407
2022-08-01 15:00:23.696649: train loss : -0.8288
2022-08-01 15:00:33.731574: validation loss: -0.7879
2022-08-01 15:00:33.787563: Average global foreground Dice: [0.8443]
2022-08-01 15:00:33.823775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:00:34.924374: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-01 15:00:34.959599: This epoch took 120.642690 s

2022-08-01 15:00:35.003749: 
epoch:  408
2022-08-01 15:02:29.293977: train loss : -0.8204
2022-08-01 15:02:37.273799: validation loss: -0.7766
2022-08-01 15:02:37.320364: Average global foreground Dice: [0.8379]
2022-08-01 15:02:37.347786: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:02:38.055574: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-01 15:02:38.101998: This epoch took 123.064119 s

2022-08-01 15:02:38.125990: 
epoch:  409
2022-08-01 15:04:33.159559: train loss : -0.8229
2022-08-01 15:04:41.784833: validation loss: -0.7869
2022-08-01 15:04:41.821359: Average global foreground Dice: [0.8399]
2022-08-01 15:04:41.853892: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:04:42.536040: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-01 15:04:42.574739: This epoch took 124.421931 s

2022-08-01 15:04:42.590012: 
epoch:  410
2022-08-01 15:06:34.148265: train loss : -0.8179
2022-08-01 15:06:46.678538: validation loss: -0.7906
2022-08-01 15:06:46.727317: Average global foreground Dice: [0.8445]
2022-08-01 15:06:46.751777: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:06:47.575097: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-01 15:06:47.585120: This epoch took 124.992882 s

2022-08-01 15:06:47.619776: 
epoch:  411
2022-08-01 15:08:36.648067: train loss : -0.8259
2022-08-01 15:08:48.063482: validation loss: -0.7369
2022-08-01 15:08:48.082954: Average global foreground Dice: [0.8062]
2022-08-01 15:08:48.100930: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:08:48.879335: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-01 15:08:48.902844: This epoch took 121.255035 s

2022-08-01 15:08:48.925920: 
epoch:  412
2022-08-01 15:10:36.683653: train loss : -0.8292
2022-08-01 15:10:44.551581: validation loss: -0.7653
2022-08-01 15:10:44.580250: Average global foreground Dice: [0.8309]
2022-08-01 15:10:44.618807: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:10:45.435806: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-01 15:10:45.438315: This epoch took 116.483543 s

2022-08-01 15:10:45.446864: 
epoch:  413
2022-08-01 15:12:38.189329: train loss : -0.8239
2022-08-01 15:12:46.598558: validation loss: -0.7878
2022-08-01 15:12:46.622696: Average global foreground Dice: [0.8496]
2022-08-01 15:12:46.642764: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:12:47.522718: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-01 15:12:47.550409: This epoch took 122.083883 s

2022-08-01 15:12:47.585769: 
epoch:  414
2022-08-01 15:14:38.421336: train loss : -0.8212
2022-08-01 15:14:48.434491: validation loss: -0.7577
2022-08-01 15:14:48.465842: Average global foreground Dice: [0.8187]
2022-08-01 15:14:48.497786: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:14:49.138752: Suus1 maybe_update_lr lr: 2e-05
2022-08-01 15:14:49.171891: This epoch took 121.553096 s

2022-08-01 15:14:49.205753: 
epoch:  415
2022-08-01 15:16:46.656055: train loss : -0.8271
2022-08-01 15:16:56.397790: validation loss: -0.7929
2022-08-01 15:16:56.438668: Average global foreground Dice: [0.8481]
2022-08-01 15:16:56.471810: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:16:57.684860: Suus1 maybe_update_lr lr: 2e-05
2022-08-01 15:16:57.718878: This epoch took 128.479098 s

2022-08-01 15:16:57.740819: 
epoch:  416
2022-08-01 15:18:55.372089: train loss : -0.8205
2022-08-01 15:19:02.489417: validation loss: -0.7778
2022-08-01 15:19:02.501704: Average global foreground Dice: [0.8366]
2022-08-01 15:19:02.515442: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:19:03.112764: Suus1 maybe_update_lr lr: 2e-05
2022-08-01 15:19:03.132848: This epoch took 125.355693 s

2022-08-01 15:19:03.135384: 
epoch:  417
2022-08-01 15:20:51.204186: train loss : -0.8235
2022-08-01 15:20:57.732616: validation loss: -0.7981
2022-08-01 15:20:57.735852: Average global foreground Dice: [0.8522]
2022-08-01 15:20:57.737892: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:20:58.214726: Suus1 maybe_update_lr lr: 2e-05
2022-08-01 15:20:58.216983: This epoch took 115.079456 s

2022-08-01 15:20:58.218822: 
epoch:  418
2022-08-01 15:22:41.821516: train loss : -0.8251
2022-08-01 15:22:48.402267: validation loss: -0.7720
2022-08-01 15:22:48.405408: Average global foreground Dice: [0.8324]
2022-08-01 15:22:48.407414: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:22:48.885876: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-01 15:22:48.888234: This epoch took 110.667608 s

2022-08-01 15:22:48.890249: 
epoch:  419
2022-08-01 15:24:33.440567: train loss : -0.8239
2022-08-01 15:24:40.555989: validation loss: -0.7836
2022-08-01 15:24:40.578157: Average global foreground Dice: [0.8435]
2022-08-01 15:24:40.610795: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:24:41.357485: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-01 15:24:41.383819: This epoch took 112.491624 s

2022-08-01 15:24:41.398087: 
epoch:  420
2022-08-01 15:26:30.489832: train loss : -0.8235
2022-08-01 15:26:40.966757: validation loss: -0.7637
2022-08-01 15:26:40.997612: Average global foreground Dice: [0.8253]
2022-08-01 15:26:41.005207: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:26:41.665311: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-01 15:26:41.703849: This epoch took 120.273994 s

2022-08-01 15:26:41.748744: 
epoch:  421
2022-08-01 15:28:31.376614: train loss : -0.8230
2022-08-01 15:28:41.189465: validation loss: -0.7701
2022-08-01 15:28:41.201348: Average global foreground Dice: [0.8265]
2022-08-01 15:28:41.203617: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:28:41.777412: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-01 15:28:41.792127: This epoch took 120.020298 s

2022-08-01 15:28:41.818822: 
epoch:  422
2022-08-01 15:30:38.114507: train loss : -0.8286
2022-08-01 15:30:49.072563: validation loss: -0.7801
2022-08-01 15:30:49.104360: Average global foreground Dice: [0.8354]
2022-08-01 15:30:49.121801: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:30:49.940637: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-01 15:30:49.977887: This epoch took 128.131067 s

2022-08-01 15:30:50.000749: 
epoch:  423
2022-08-01 15:32:39.385315: train loss : -0.8278
2022-08-01 15:32:48.160626: validation loss: -0.7871
2022-08-01 15:32:48.186291: Average global foreground Dice: [0.8446]
2022-08-01 15:32:48.256745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:32:49.052443: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-01 15:32:49.092854: This epoch took 119.064034 s

2022-08-01 15:32:49.126789: 
epoch:  424
2022-08-01 15:34:42.774709: train loss : -0.8255
2022-08-01 15:34:52.550296: validation loss: -0.7665
2022-08-01 15:34:52.574375: Average global foreground Dice: [0.8298]
2022-08-01 15:34:52.608824: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:34:53.268677: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-01 15:34:53.272062: This epoch took 124.113105 s

2022-08-01 15:34:53.276167: 
epoch:  425
2022-08-01 15:36:41.717172: train loss : -0.8238
2022-08-01 15:36:50.512347: validation loss: -0.7920
2022-08-01 15:36:50.556419: Average global foreground Dice: [0.8447]
2022-08-01 15:36:50.585089: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:36:51.657275: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-01 15:36:51.660019: This epoch took 118.381245 s

2022-08-01 15:36:51.662365: 
epoch:  426
2022-08-01 15:38:49.571817: train loss : -0.8172
2022-08-01 15:39:00.002018: validation loss: -0.7802
2022-08-01 15:39:00.061248: Average global foreground Dice: [0.8444]
2022-08-01 15:39:00.086755: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:39:00.759780: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-01 15:39:00.762556: This epoch took 129.097972 s

2022-08-01 15:39:00.765064: 
epoch:  427
2022-08-01 15:40:50.745448: train loss : -0.8242
2022-08-01 15:41:01.167382: validation loss: -0.7738
2022-08-01 15:41:01.181921: Average global foreground Dice: [0.8366]
2022-08-01 15:41:01.210787: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:41:01.932102: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-01 15:41:01.935135: This epoch took 121.167785 s

2022-08-01 15:41:01.937521: 
epoch:  428
2022-08-01 15:42:59.710059: train loss : -0.8268
2022-08-01 15:43:08.221481: validation loss: -0.7839
2022-08-01 15:43:08.265476: Average global foreground Dice: [0.8457]
2022-08-01 15:43:08.282028: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:43:09.218912: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-01 15:43:09.240473: saving best epoch checkpoint...
2022-08-01 15:43:10.025257: saving checkpoint...
2022-08-01 15:43:16.101638: done, saving took 6.82 seconds
2022-08-01 15:43:16.114567: This epoch took 134.174869 s

2022-08-01 15:43:16.117806: 
epoch:  429
2022-08-01 15:45:07.717673: train loss : -0.8258
2022-08-01 15:45:21.101786: validation loss: -0.7845
2022-08-01 15:45:21.139863: Average global foreground Dice: [0.8432]
2022-08-01 15:45:21.168593: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:45:22.028662: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-01 15:45:22.055855: saving best epoch checkpoint...
2022-08-01 15:45:22.543008: saving checkpoint...
2022-08-01 15:45:28.528784: done, saving took 6.45 seconds
2022-08-01 15:45:28.538220: This epoch took 132.417742 s

2022-08-01 15:45:28.540457: 
epoch:  430
2022-08-01 15:47:17.073175: train loss : -0.8288
2022-08-01 15:47:26.473308: validation loss: -0.7778
2022-08-01 15:47:26.519358: Average global foreground Dice: [0.8395]
2022-08-01 15:47:26.540759: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:47:27.343059: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-01 15:47:27.389786: saving best epoch checkpoint...
2022-08-01 15:47:27.774170: saving checkpoint...
2022-08-01 15:47:34.125835: done, saving took 6.70 seconds
2022-08-01 15:47:34.136261: This epoch took 125.593705 s

2022-08-01 15:47:34.138739: 
epoch:  431
2022-08-01 15:49:19.610965: train loss : -0.8322
2022-08-01 15:49:27.223135: validation loss: -0.7693
2022-08-01 15:49:27.239346: Average global foreground Dice: [0.8266]
2022-08-01 15:49:27.260769: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:49:27.811812: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-01 15:49:27.815206: This epoch took 113.673936 s

2022-08-01 15:49:27.817679: 
epoch:  432
2022-08-01 15:51:18.317973: train loss : -0.8298
2022-08-01 15:51:28.070309: validation loss: -0.7235
2022-08-01 15:51:28.101846: Average global foreground Dice: [0.7936]
2022-08-01 15:51:28.134809: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:51:28.870501: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-01 15:51:28.922576: This epoch took 121.102111 s

2022-08-01 15:51:28.962657: 
epoch:  433
2022-08-01 15:53:18.761784: train loss : -0.8223
2022-08-01 15:53:30.064947: validation loss: -0.7756
2022-08-01 15:53:30.096556: Average global foreground Dice: [0.8343]
2022-08-01 15:53:30.125671: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:53:30.842654: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-01 15:53:30.845132: This epoch took 121.847540 s

2022-08-01 15:53:30.847330: 
epoch:  434
2022-08-01 15:55:23.956395: train loss : -0.8319
2022-08-01 15:55:38.757589: validation loss: -0.7885
2022-08-01 15:55:38.818295: Average global foreground Dice: [0.8446]
2022-08-01 15:55:38.864793: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:55:39.757148: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-01 15:55:39.784831: This epoch took 128.935523 s

2022-08-01 15:55:39.816142: 
epoch:  435
2022-08-01 15:57:33.902045: train loss : -0.8284
2022-08-01 15:57:43.112297: validation loss: -0.7801
2022-08-01 15:57:43.152183: Average global foreground Dice: [0.8383]
2022-08-01 15:57:43.173787: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:57:44.208290: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-01 15:57:44.220196: This epoch took 124.380431 s

2022-08-01 15:57:44.257732: 
epoch:  436
2022-08-01 15:59:32.368727: train loss : -0.8288
2022-08-01 15:59:43.664695: validation loss: -0.7575
2022-08-01 15:59:43.694435: Average global foreground Dice: [0.8182]
2022-08-01 15:59:43.714792: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 15:59:44.351430: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-01 15:59:44.391982: This epoch took 120.101817 s

2022-08-01 15:59:44.435767: 
epoch:  437
2022-08-01 16:01:35.695100: train loss : -0.8282
2022-08-01 16:01:45.888796: validation loss: -0.7844
2022-08-01 16:01:45.930950: Average global foreground Dice: [0.8376]
2022-08-01 16:01:45.952451: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:01:46.912620: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-01 16:01:46.945876: This epoch took 122.468901 s

2022-08-01 16:01:46.980771: 
epoch:  438
2022-08-01 16:03:45.015129: train loss : -0.8323
2022-08-01 16:03:53.110705: validation loss: -0.7910
2022-08-01 16:03:53.134133: Average global foreground Dice: [0.8451]
2022-08-01 16:03:53.169777: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:03:53.959649: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-01 16:03:54.002395: This epoch took 126.990136 s

2022-08-01 16:03:54.028010: 
epoch:  439
2022-08-01 16:05:44.018304: train loss : -0.8297
2022-08-01 16:05:53.119961: validation loss: -0.7650
2022-08-01 16:05:53.161117: Average global foreground Dice: [0.8239]
2022-08-01 16:05:53.188533: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:05:54.498822: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-01 16:05:54.519840: This epoch took 120.459026 s

2022-08-01 16:05:54.541771: 
epoch:  440
2022-08-01 16:07:42.680543: train loss : -0.8279
2022-08-01 16:07:57.785213: validation loss: -0.7779
2022-08-01 16:07:57.789150: Average global foreground Dice: [0.8351]
2022-08-01 16:07:57.809810: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:07:58.460953: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-01 16:07:58.479692: This epoch took 123.903898 s

2022-08-01 16:07:58.490591: 
epoch:  441
2022-08-01 16:09:47.977988: train loss : -0.8301
2022-08-01 16:09:57.197470: validation loss: -0.7800
2022-08-01 16:09:57.228853: Average global foreground Dice: [0.8382]
2022-08-01 16:09:57.247292: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:09:57.833772: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-01 16:09:57.837572: This epoch took 119.331016 s

2022-08-01 16:09:57.839839: 
epoch:  442
2022-08-01 16:11:46.689675: train loss : -0.8276
2022-08-01 16:11:53.652660: validation loss: -0.7796
2022-08-01 16:11:53.658782: Average global foreground Dice: [0.8358]
2022-08-01 16:11:53.665376: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:11:54.162520: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-01 16:11:54.169482: This epoch took 116.327150 s

2022-08-01 16:11:54.173087: 
epoch:  443
2022-08-01 16:13:42.606097: train loss : -0.8342
2022-08-01 16:13:51.685555: validation loss: -0.8014
2022-08-01 16:13:51.733298: Average global foreground Dice: [0.8504]
2022-08-01 16:13:51.742949: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:13:52.328495: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-01 16:13:52.350066: This epoch took 118.174052 s

2022-08-01 16:13:52.382771: 
epoch:  444
2022-08-01 16:15:42.743218: train loss : -0.8281
2022-08-01 16:15:55.691431: validation loss: -0.7850
2022-08-01 16:15:55.752401: Average global foreground Dice: [0.8397]
2022-08-01 16:15:55.776771: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:15:57.356040: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-01 16:15:57.359230: This epoch took 124.956424 s

2022-08-01 16:15:57.364073: 
epoch:  445
2022-08-01 16:17:55.286099: train loss : -0.8239
2022-08-01 16:18:04.934198: validation loss: -0.7894
2022-08-01 16:18:04.966228: Average global foreground Dice: [0.8476]
2022-08-01 16:18:04.969764: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:18:05.731362: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-01 16:18:05.744983: This epoch took 128.372786 s

2022-08-01 16:18:05.761868: 
epoch:  446
2022-08-01 16:20:06.176700: train loss : -0.8273
2022-08-01 16:20:16.088156: validation loss: -0.7964
2022-08-01 16:20:16.197328: Average global foreground Dice: [0.8503]
2022-08-01 16:20:16.233916: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:20:17.139285: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-01 16:20:17.151356: saving best epoch checkpoint...
2022-08-01 16:20:17.422048: saving checkpoint...
2022-08-01 16:20:23.496343: done, saving took 6.34 seconds
2022-08-01 16:20:23.508262: This epoch took 137.735513 s

2022-08-01 16:20:23.511186: 
epoch:  447
2022-08-01 16:22:12.204967: train loss : -0.8325
2022-08-01 16:22:21.537596: validation loss: -0.7769
2022-08-01 16:22:21.560765: Average global foreground Dice: [0.8351]
2022-08-01 16:22:21.576790: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:22:22.427018: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-01 16:22:22.445794: This epoch took 118.931985 s

2022-08-01 16:22:22.465491: 
epoch:  448
2022-08-01 16:24:13.242654: train loss : -0.8294
2022-08-01 16:24:21.616249: validation loss: -0.7829
2022-08-01 16:24:21.634189: Average global foreground Dice: [0.8434]
2022-08-01 16:24:21.659786: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:24:22.356947: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-01 16:24:22.384811: saving best epoch checkpoint...
2022-08-01 16:24:22.894163: saving checkpoint...
2022-08-01 16:24:28.608371: done, saving took 6.19 seconds
2022-08-01 16:24:28.619134: This epoch took 126.139370 s

2022-08-01 16:24:28.621409: 
epoch:  449
2022-08-01 16:26:19.832619: train loss : -0.8296
2022-08-01 16:26:29.781878: validation loss: -0.7735
2022-08-01 16:26:29.813693: Average global foreground Dice: [0.8381]
2022-08-01 16:26:29.857162: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:26:30.478499: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-01 16:26:30.481825: saving scheduled checkpoint file...
2022-08-01 16:26:30.804501: saving checkpoint...
2022-08-01 16:26:36.453333: done, saving took 5.94 seconds
2022-08-01 16:26:36.465524: done
2022-08-01 16:26:36.468355: This epoch took 127.844588 s

2022-08-01 16:26:36.470515: 
epoch:  450
2022-08-01 16:28:24.416993: train loss : -0.8275
2022-08-01 16:28:33.408721: validation loss: -0.7721
2022-08-01 16:28:33.429869: Average global foreground Dice: [0.8305]
2022-08-01 16:28:33.465771: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:28:34.064116: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-01 16:28:34.088502: This epoch took 117.615933 s

2022-08-01 16:28:34.110937: 
epoch:  451
2022-08-01 16:30:28.658149: train loss : -0.8285
2022-08-01 16:30:37.456955: validation loss: -0.7963
2022-08-01 16:30:37.495272: Average global foreground Dice: [0.8473]
2022-08-01 16:30:37.525430: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:30:38.251338: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-01 16:30:38.290411: saving best epoch checkpoint...
2022-08-01 16:30:38.670938: saving checkpoint...
2022-08-01 16:30:44.032984: done, saving took 5.69 seconds
2022-08-01 16:30:44.042311: This epoch took 129.928359 s

2022-08-01 16:30:44.044286: 
epoch:  452
2022-08-01 16:32:40.805713: train loss : -0.8363
2022-08-01 16:32:50.306093: validation loss: -0.7998
2022-08-01 16:32:50.309815: Average global foreground Dice: [0.8533]
2022-08-01 16:32:50.326694: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:32:50.908531: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-01 16:32:50.914140: saving best epoch checkpoint...
2022-08-01 16:32:51.182545: saving checkpoint...
2022-08-01 16:32:57.001906: done, saving took 6.08 seconds
2022-08-01 16:32:57.012772: This epoch took 132.966568 s

2022-08-01 16:32:57.014937: 
epoch:  453
2022-08-01 16:34:47.095414: train loss : -0.8276
2022-08-01 16:34:56.814499: validation loss: -0.7915
2022-08-01 16:34:56.867393: Average global foreground Dice: [0.8424]
2022-08-01 16:34:56.889088: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:34:57.940112: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-01 16:34:58.003800: saving best epoch checkpoint...
2022-08-01 16:34:58.750480: saving checkpoint...
2022-08-01 16:35:04.593740: done, saving took 6.55 seconds
2022-08-01 16:35:04.606335: This epoch took 127.589362 s

2022-08-01 16:35:04.609129: 
epoch:  454
2022-08-01 16:36:55.439023: train loss : -0.8294
2022-08-01 16:37:06.573867: validation loss: -0.7811
2022-08-01 16:37:06.600157: Average global foreground Dice: [0.8385]
2022-08-01 16:37:06.620425: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:37:07.716043: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-01 16:37:07.720486: This epoch took 123.109068 s

2022-08-01 16:37:07.723613: 
epoch:  455
2022-08-01 16:39:05.007031: train loss : -0.8302
2022-08-01 16:39:12.314400: validation loss: -0.7911
2022-08-01 16:39:12.348550: Average global foreground Dice: [0.8437]
2022-08-01 16:39:12.368827: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:39:13.334706: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-01 16:39:13.358618: saving best epoch checkpoint...
2022-08-01 16:39:13.820247: saving checkpoint...
2022-08-01 16:39:20.435669: done, saving took 7.04 seconds
2022-08-01 16:39:20.447037: This epoch took 132.718019 s

2022-08-01 16:39:20.449752: 
epoch:  456
2022-08-01 16:41:16.920550: train loss : -0.8295
2022-08-01 16:41:28.348846: validation loss: -0.7690
2022-08-01 16:41:28.377869: Average global foreground Dice: [0.8338]
2022-08-01 16:41:28.419325: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:41:29.162117: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-01 16:41:29.190676: This epoch took 128.738123 s

2022-08-01 16:41:29.229769: 
epoch:  457
2022-08-01 16:43:18.073739: train loss : -0.8324
2022-08-01 16:43:29.518754: validation loss: -0.7495
2022-08-01 16:43:29.562567: Average global foreground Dice: [0.8236]
2022-08-01 16:43:29.621843: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:43:30.465568: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-01 16:43:30.509282: This epoch took 121.214506 s

2022-08-01 16:43:30.555803: 
epoch:  458
2022-08-01 16:45:20.695405: train loss : -0.8294
2022-08-01 16:45:29.970113: validation loss: -0.8046
2022-08-01 16:45:29.974025: Average global foreground Dice: [0.8581]
2022-08-01 16:45:29.976672: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:45:30.516531: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-01 16:45:30.523698: This epoch took 119.927686 s

2022-08-01 16:45:30.526265: 
epoch:  459
2022-08-01 16:47:17.494256: train loss : -0.8266
2022-08-01 16:47:29.211731: validation loss: -0.7709
2022-08-01 16:47:29.235894: Average global foreground Dice: [0.8314]
2022-08-01 16:47:29.242366: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:47:29.908738: Suus1 maybe_update_lr lr: 1e-05
2022-08-01 16:47:29.939869: This epoch took 119.411242 s

2022-08-01 16:47:29.968778: 
epoch:  460
2022-08-01 16:49:20.746465: train loss : -0.8292
2022-08-01 16:49:32.899153: validation loss: -0.7765
2022-08-01 16:49:32.929367: Average global foreground Dice: [0.8341]
2022-08-01 16:49:32.946658: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:49:33.578087: Suus1 maybe_update_lr lr: 1e-05
2022-08-01 16:49:33.599823: This epoch took 123.597049 s

2022-08-01 16:49:33.644783: 
epoch:  461
2022-08-01 16:51:24.344867: train loss : -0.8329
2022-08-01 16:51:31.030768: validation loss: -0.7935
2022-08-01 16:51:31.034456: Average global foreground Dice: [0.8493]
2022-08-01 16:51:31.037190: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:51:31.568300: Suus1 maybe_update_lr lr: 1e-05
2022-08-01 16:51:31.570924: This epoch took 117.892057 s

2022-08-01 16:51:31.573515: 
epoch:  462
2022-08-01 16:53:26.114804: train loss : -0.8269
2022-08-01 16:53:36.008991: validation loss: -0.7799
2022-08-01 16:53:36.049484: Average global foreground Dice: [0.8311]
2022-08-01 16:53:36.076813: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:53:36.714205: Suus1 maybe_update_lr lr: 1e-05
2022-08-01 16:53:36.717012: This epoch took 125.141389 s

2022-08-01 16:53:36.719079: 
epoch:  463
2022-08-01 16:55:26.037658: train loss : -0.8315
2022-08-01 16:55:35.647720: validation loss: -0.7971
2022-08-01 16:55:35.675423: Average global foreground Dice: [0.8459]
2022-08-01 16:55:35.705325: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:55:36.398547: Suus1 maybe_update_lr lr: 9e-06
2022-08-01 16:55:36.401081: This epoch took 119.679909 s

2022-08-01 16:55:36.403091: 
epoch:  464
2022-08-01 16:57:23.775343: train loss : -0.8352
2022-08-01 16:57:33.919847: validation loss: -0.7950
2022-08-01 16:57:33.942645: Average global foreground Dice: [0.8513]
2022-08-01 16:57:33.978583: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:57:35.036200: Suus1 maybe_update_lr lr: 9e-06
2022-08-01 16:57:35.056783: saving best epoch checkpoint...
2022-08-01 16:57:35.325360: saving checkpoint...
2022-08-01 16:57:40.898527: done, saving took 5.81 seconds
2022-08-01 16:57:40.908837: This epoch took 124.503753 s

2022-08-01 16:57:40.911519: 
epoch:  465
2022-08-01 16:59:30.930427: train loss : -0.8318
2022-08-01 16:59:41.224294: validation loss: -0.7981
2022-08-01 16:59:41.259812: Average global foreground Dice: [0.846]
2022-08-01 16:59:41.294804: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 16:59:41.969942: Suus1 maybe_update_lr lr: 9e-06
2022-08-01 16:59:41.994891: saving best epoch checkpoint...
2022-08-01 16:59:42.254797: saving checkpoint...
2022-08-01 16:59:48.906358: done, saving took 6.89 seconds
2022-08-01 16:59:48.918262: This epoch took 128.004706 s

2022-08-01 16:59:48.920721: 
epoch:  466
2022-08-01 17:01:35.380819: train loss : -0.8322
2022-08-01 17:01:43.009119: validation loss: -0.8008
2022-08-01 17:01:43.012648: Average global foreground Dice: [0.851]
2022-08-01 17:01:43.015272: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:01:43.534041: Suus1 maybe_update_lr lr: 9e-06
2022-08-01 17:01:43.537974: saving best epoch checkpoint...
2022-08-01 17:01:43.752444: saving checkpoint...
2022-08-01 17:01:50.232504: done, saving took 6.69 seconds
2022-08-01 17:01:50.243117: This epoch took 121.320210 s

2022-08-01 17:01:50.245552: 
epoch:  467
2022-08-01 17:03:39.555336: train loss : -0.8303
2022-08-01 17:03:49.400257: validation loss: -0.7846
2022-08-01 17:03:49.418670: Average global foreground Dice: [0.837]
2022-08-01 17:03:49.445122: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:03:50.042330: Suus1 maybe_update_lr lr: 8e-06
2022-08-01 17:03:50.087793: This epoch took 119.839678 s

2022-08-01 17:03:50.099803: 
epoch:  468
2022-08-01 17:05:41.137559: train loss : -0.8348
2022-08-01 17:05:49.264136: validation loss: -0.7853
2022-08-01 17:05:49.312379: Average global foreground Dice: [0.8428]
2022-08-01 17:05:49.332739: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:05:50.108756: Suus1 maybe_update_lr lr: 8e-06
2022-08-01 17:05:50.125870: This epoch took 119.996073 s

2022-08-01 17:05:50.143896: 
epoch:  469
2022-08-01 17:07:41.559635: train loss : -0.8365
2022-08-01 17:07:52.410245: validation loss: -0.7708
2022-08-01 17:07:52.440635: Average global foreground Dice: [0.833]
2022-08-01 17:07:52.469103: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:07:53.203691: Suus1 maybe_update_lr lr: 8e-06
2022-08-01 17:07:53.234828: This epoch took 123.087372 s

2022-08-01 17:07:53.271756: 
epoch:  470
2022-08-01 17:09:44.241412: train loss : -0.8262
2022-08-01 17:09:53.177312: validation loss: -0.7837
2022-08-01 17:09:53.216946: Average global foreground Dice: [0.8359]
2022-08-01 17:09:53.233778: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:09:54.069536: Suus1 maybe_update_lr lr: 8e-06
2022-08-01 17:09:54.103856: This epoch took 120.807065 s

2022-08-01 17:09:54.165854: 
epoch:  471
2022-08-01 17:11:46.300505: train loss : -0.8306
2022-08-01 17:11:58.417194: validation loss: -0.7887
2022-08-01 17:11:58.438868: Average global foreground Dice: [0.845]
2022-08-01 17:11:58.441914: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:11:59.294638: Suus1 maybe_update_lr lr: 7e-06
2022-08-01 17:11:59.316801: This epoch took 125.113032 s

2022-08-01 17:11:59.327775: 
epoch:  472
2022-08-01 17:13:49.691736: train loss : -0.8341
2022-08-01 17:13:59.953669: validation loss: -0.7744
2022-08-01 17:13:59.987265: Average global foreground Dice: [0.8363]
2022-08-01 17:14:00.013847: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:14:01.233818: Suus1 maybe_update_lr lr: 7e-06
2022-08-01 17:14:01.244070: This epoch took 121.913466 s

2022-08-01 17:14:01.263023: 
epoch:  473
2022-08-01 17:15:49.142020: train loss : -0.8327
2022-08-01 17:15:56.502761: validation loss: -0.7816
2022-08-01 17:15:56.506665: Average global foreground Dice: [0.8345]
2022-08-01 17:15:56.509139: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:15:57.021423: Suus1 maybe_update_lr lr: 7e-06
2022-08-01 17:15:57.024278: This epoch took 115.754761 s

2022-08-01 17:15:57.026716: 
epoch:  474
2022-08-01 17:17:44.548138: train loss : -0.8319
2022-08-01 17:17:53.864091: validation loss: -0.7911
2022-08-01 17:17:53.868682: Average global foreground Dice: [0.8445]
2022-08-01 17:17:53.871522: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:17:54.452087: Suus1 maybe_update_lr lr: 7e-06
2022-08-01 17:17:54.456156: This epoch took 117.426714 s

2022-08-01 17:17:54.458856: 
epoch:  475
2022-08-01 17:19:41.045975: train loss : -0.8354
2022-08-01 17:19:48.125386: validation loss: -0.7742
2022-08-01 17:19:48.158782: Average global foreground Dice: [0.8345]
2022-08-01 17:19:48.162014: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:19:48.726831: Suus1 maybe_update_lr lr: 7e-06
2022-08-01 17:19:48.730882: This epoch took 114.268271 s

2022-08-01 17:19:48.733275: 
epoch:  476
2022-08-01 17:21:38.623855: train loss : -0.8364
2022-08-01 17:21:47.147831: validation loss: -0.7749
2022-08-01 17:21:47.170470: Average global foreground Dice: [0.833]
2022-08-01 17:21:47.194471: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:21:47.826198: Suus1 maybe_update_lr lr: 6e-06
2022-08-01 17:21:47.843867: This epoch took 119.106371 s

2022-08-01 17:21:47.880761: 
epoch:  477
2022-08-01 17:23:35.032327: train loss : -0.8380
2022-08-01 17:23:43.386613: validation loss: -0.7935
2022-08-01 17:23:43.399322: Average global foreground Dice: [0.8475]
2022-08-01 17:23:43.401672: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:23:44.006735: Suus1 maybe_update_lr lr: 6e-06
2022-08-01 17:23:44.009001: This epoch took 116.091253 s

2022-08-01 17:23:44.011082: 
epoch:  478
2022-08-01 17:25:30.944976: train loss : -0.8350
2022-08-01 17:25:40.539290: validation loss: -0.7869
2022-08-01 17:25:40.546951: Average global foreground Dice: [0.8402]
2022-08-01 17:25:40.560785: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:25:41.159254: Suus1 maybe_update_lr lr: 6e-06
2022-08-01 17:25:41.161948: This epoch took 117.148489 s

2022-08-01 17:25:41.173611: 
epoch:  479
2022-08-01 17:27:30.765700: train loss : -0.8259
2022-08-01 17:27:38.112989: validation loss: -0.7673
2022-08-01 17:27:38.126418: Average global foreground Dice: [0.8331]
2022-08-01 17:27:38.147796: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:27:38.661710: Suus1 maybe_update_lr lr: 6e-06
2022-08-01 17:27:38.664019: This epoch took 117.487203 s

2022-08-01 17:27:38.666242: 
epoch:  480
2022-08-01 17:29:27.214567: train loss : -0.8346
2022-08-01 17:29:38.632267: validation loss: -0.7778
2022-08-01 17:29:38.673446: Average global foreground Dice: [0.839]
2022-08-01 17:29:38.703227: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:29:39.496359: Suus1 maybe_update_lr lr: 5e-06
2022-08-01 17:29:39.499010: This epoch took 120.830219 s

2022-08-01 17:29:39.501200: 
epoch:  481
2022-08-01 17:31:28.729177: train loss : -0.8321
2022-08-01 17:31:36.579059: validation loss: -0.7856
2022-08-01 17:31:36.648340: Average global foreground Dice: [0.8428]
2022-08-01 17:31:36.702118: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:31:37.581137: Suus1 maybe_update_lr lr: 5e-06
2022-08-01 17:31:37.633880: This epoch took 118.130439 s

2022-08-01 17:31:37.666776: 
epoch:  482
2022-08-01 17:33:34.350730: train loss : -0.8264
2022-08-01 17:33:43.533505: validation loss: -0.7854
2022-08-01 17:33:43.571507: Average global foreground Dice: [0.8374]
2022-08-01 17:33:43.600956: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:33:44.576356: Suus1 maybe_update_lr lr: 5e-06
2022-08-01 17:33:44.596828: This epoch took 126.880065 s

2022-08-01 17:33:44.623775: 
epoch:  483
2022-08-01 17:35:35.071604: train loss : -0.8384
2022-08-01 17:35:43.593133: validation loss: -0.7776
2022-08-01 17:35:43.601391: Average global foreground Dice: [0.8333]
2022-08-01 17:35:43.623697: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:35:44.297034: Suus1 maybe_update_lr lr: 5e-06
2022-08-01 17:35:44.317887: This epoch took 119.666831 s

2022-08-01 17:35:44.341793: 
epoch:  484
2022-08-01 17:37:34.708953: train loss : -0.8370
2022-08-01 17:37:44.708902: validation loss: -0.7754
2022-08-01 17:37:44.772218: Average global foreground Dice: [0.8304]
2022-08-01 17:37:44.802958: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:37:45.679796: Suus1 maybe_update_lr lr: 4e-06
2022-08-01 17:37:45.723808: This epoch took 121.359396 s

2022-08-01 17:37:45.756741: 
epoch:  485
2022-08-01 17:39:38.155133: train loss : -0.8289
2022-08-01 17:39:46.267946: validation loss: -0.7700
2022-08-01 17:39:46.288938: Average global foreground Dice: [0.8285]
2022-08-01 17:39:46.307138: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:39:46.889235: Suus1 maybe_update_lr lr: 4e-06
2022-08-01 17:39:46.892042: This epoch took 121.097228 s

2022-08-01 17:39:46.902666: 
epoch:  486
2022-08-01 17:41:36.119802: train loss : -0.8339
2022-08-01 17:41:44.687819: validation loss: -0.7936
2022-08-01 17:41:44.692000: Average global foreground Dice: [0.8479]
2022-08-01 17:41:44.694731: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:41:45.222388: Suus1 maybe_update_lr lr: 4e-06
2022-08-01 17:41:45.225429: This epoch took 118.318986 s

2022-08-01 17:41:45.227721: 
epoch:  487
2022-08-01 17:43:38.132797: train loss : -0.8341
2022-08-01 17:43:47.345207: validation loss: -0.7973
2022-08-01 17:43:47.396380: Average global foreground Dice: [0.8475]
2022-08-01 17:43:47.429771: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:43:48.879294: Suus1 maybe_update_lr lr: 3e-06
2022-08-01 17:43:48.898057: This epoch took 123.667934 s

2022-08-01 17:43:48.906914: 
epoch:  488
2022-08-01 17:45:34.940993: train loss : -0.8376
2022-08-01 17:45:42.646521: validation loss: -0.7433
2022-08-01 17:45:42.671519: Average global foreground Dice: [0.8144]
2022-08-01 17:45:42.690784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:45:43.400025: Suus1 maybe_update_lr lr: 3e-06
2022-08-01 17:45:43.413858: This epoch took 114.496352 s

2022-08-01 17:45:43.421739: 
epoch:  489
2022-08-01 17:47:32.635367: train loss : -0.8306
2022-08-01 17:47:43.759567: validation loss: -0.7831
2022-08-01 17:47:43.789798: Average global foreground Dice: [0.8363]
2022-08-01 17:47:43.824609: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:47:44.472250: Suus1 maybe_update_lr lr: 3e-06
2022-08-01 17:47:44.496860: This epoch took 121.072501 s

2022-08-01 17:47:44.518084: 
epoch:  490
2022-08-01 17:49:36.805300: train loss : -0.8299
2022-08-01 17:49:47.534790: validation loss: -0.7998
2022-08-01 17:49:47.618477: Average global foreground Dice: [0.851]
2022-08-01 17:49:47.662842: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:49:48.419893: Suus1 maybe_update_lr lr: 3e-06
2022-08-01 17:49:48.483858: This epoch took 123.955300 s

2022-08-01 17:49:48.509756: 
epoch:  491
2022-08-01 17:51:45.808595: train loss : -0.8312
2022-08-01 17:51:54.835960: validation loss: -0.8091
2022-08-01 17:51:54.868593: Average global foreground Dice: [0.8555]
2022-08-01 17:51:54.892771: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:51:55.595426: Suus1 maybe_update_lr lr: 2e-06
2022-08-01 17:51:55.638855: This epoch took 127.101526 s

2022-08-01 17:51:55.672771: 
epoch:  492
2022-08-01 17:53:45.806981: train loss : -0.8350
2022-08-01 17:53:54.082054: validation loss: -0.7947
2022-08-01 17:53:54.147497: Average global foreground Dice: [0.8471]
2022-08-01 17:53:54.208817: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:53:55.551970: Suus1 maybe_update_lr lr: 2e-06
2022-08-01 17:53:55.554667: This epoch took 119.850137 s

2022-08-01 17:53:55.557064: 
epoch:  493
2022-08-01 17:55:44.947590: train loss : -0.8317
2022-08-01 17:55:56.958020: validation loss: -0.7717
2022-08-01 17:55:57.002467: Average global foreground Dice: [0.838]
2022-08-01 17:55:57.024780: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:55:57.834571: Suus1 maybe_update_lr lr: 2e-06
2022-08-01 17:55:57.837732: This epoch took 122.278417 s

2022-08-01 17:55:57.840115: 
epoch:  494
2022-08-01 17:57:48.362133: train loss : -0.8330
2022-08-01 17:57:58.275986: validation loss: -0.7826
2022-08-01 17:57:58.309318: Average global foreground Dice: [0.8376]
2022-08-01 17:57:58.340203: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:57:59.151948: Suus1 maybe_update_lr lr: 2e-06
2022-08-01 17:57:59.172856: This epoch took 121.330399 s

2022-08-01 17:57:59.178273: 
epoch:  495
2022-08-01 17:59:46.043697: train loss : -0.8345
2022-08-01 17:59:53.133272: validation loss: -0.8004
2022-08-01 17:59:53.136413: Average global foreground Dice: [0.8504]
2022-08-01 17:59:53.138981: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 17:59:53.659359: Suus1 maybe_update_lr lr: 1e-06
2022-08-01 17:59:53.668349: This epoch took 114.476515 s

2022-08-01 17:59:53.681458: 
epoch:  496
2022-08-01 18:01:43.772543: train loss : -0.8342
2022-08-01 18:01:52.044662: validation loss: -0.7792
2022-08-01 18:01:52.058422: Average global foreground Dice: [0.8372]
2022-08-01 18:01:52.067450: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:01:52.726224: Suus1 maybe_update_lr lr: 1e-06
2022-08-01 18:01:52.756819: This epoch took 119.071362 s

2022-08-01 18:01:52.790745: 
epoch:  497
2022-08-01 18:03:42.547201: train loss : -0.8361
2022-08-01 18:03:50.879619: validation loss: -0.7789
2022-08-01 18:03:50.883774: Average global foreground Dice: [0.8406]
2022-08-01 18:03:50.896697: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:03:51.646961: Suus1 maybe_update_lr lr: 1e-06
2022-08-01 18:03:51.649909: This epoch took 118.840064 s

2022-08-01 18:03:51.653197: 
epoch:  498
2022-08-01 18:05:43.676312: train loss : -0.8372
2022-08-01 18:05:52.586072: validation loss: -0.7924
2022-08-01 18:05:52.620476: Average global foreground Dice: [0.8467]
2022-08-01 18:05:52.667776: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:05:53.334286: Suus1 maybe_update_lr lr: 0.0
2022-08-01 18:05:53.360821: This epoch took 121.705086 s

2022-08-01 18:05:53.383929: 
epoch:  499
2022-08-01 18:07:44.312250: train loss : -0.8339
2022-08-01 18:07:54.023690: validation loss: -0.8015
2022-08-01 18:07:54.087249: Average global foreground Dice: [0.853]
2022-08-01 18:07:54.118775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:07:55.115428: Suus1 maybe_update_lr lr: 0.0
2022-08-01 18:07:55.147809: saving scheduled checkpoint file...
2022-08-01 18:07:56.068806: saving checkpoint...
2022-08-01 18:08:01.565034: done, saving took 6.38 seconds
2022-08-01 18:08:01.578338: done
2022-08-01 18:08:01.580469: saving best epoch checkpoint...
2022-08-01 18:08:01.667392: saving checkpoint...
2022-08-01 18:08:06.969961: done, saving took 5.39 seconds
2022-08-01 18:08:06.980649: This epoch took 133.576696 s

2022-08-01 18:08:07.075970: saving checkpoint...
2022-08-01 18:08:12.323801: done, saving took 5.34 seconds
panc_0010 (2, 206, 553, 553)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 553, 553)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 90, 180, 271, 361], [0, 79, 157, 236, 314, 393]]
number of tiles: 150
computing Gaussian
done
prediction done
suus panc_0010 transposed
suus panc_0010 not saving softmax
suus panc_0010 voeg toe aan pred_gt tuples voor later
panc_0012 (2, 214, 582, 582)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 214, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 100, 134], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
suus panc_0012 transposed
suus panc_0012 not saving softmax
suus panc_0012 voeg toe aan pred_gt tuples voor later
panc_0013 (2, 186, 442, 442)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 186, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 71, 106], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
suus panc_0013 transposed
suus panc_0013 not saving softmax
suus panc_0013 voeg toe aan pred_gt tuples voor later
panc_0014 (2, 231, 559, 559)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 231, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 113, 151], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0014 transposed
suus panc_0014 not saving softmax
suus panc_0014 voeg toe aan pred_gt tuples voor later
panc_0016 (2, 310, 465, 465)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
suus panc_0016 transposed
suus panc_0016 not saving softmax
suus panc_0016 voeg toe aan pred_gt tuples voor later
panc_0020 (2, 218, 511, 511)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0020 transposed
suus panc_0020 not saving softmax
suus panc_0020 voeg toe aan pred_gt tuples voor later
panc_0021 (2, 223, 570, 570)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 570, 570)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 94, 189, 284, 378], [0, 68, 137, 205, 273, 342, 410]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0021 transposed
suus panc_0021 not saving softmax
suus panc_0021 voeg toe aan pred_gt tuples voor later
panc_0022 (2, 310, 396, 396)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 112
using precomputed Gaussian
prediction done
suus panc_0022 transposed
suus panc_0022 not saving softmax
suus panc_0022 voeg toe aan pred_gt tuples voor later
panc_0034 (2, 205, 466, 466)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
suus panc_0034 transposed
suus panc_0034 not saving softmax
suus panc_0034 voeg toe aan pred_gt tuples voor later
panc_0046 (2, 198, 466, 466)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 80
using precomputed Gaussian
prediction done
suus panc_0046 transposed
suus panc_0046 not saving softmax
suus panc_0046 voeg toe aan pred_gt tuples voor later
panc_0047 (2, 201, 489, 489)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0047 transposed
suus panc_0047 not saving softmax
suus panc_0047 voeg toe aan pred_gt tuples voor later
panc_0060 (2, 310, 465, 465)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
suus panc_0060 transposed
suus panc_0060 not saving softmax
suus panc_0060 voeg toe aan pred_gt tuples voor later
panc_0077 (2, 233, 512, 512)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0077 transposed
suus panc_0077 not saving softmax
suus panc_0077 voeg toe aan pred_gt tuples voor later
2022-08-01 18:23:36.260603: finished prediction
2022-08-01 18:23:36.265478: evaluation of raw predictions
2022-08-01 18:23:48.035053: determining postprocessing
Foreground vs background
before: 0.839060037651967
after:  0.8405487532950637
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0020.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0012.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0046.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0013.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0047.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0021.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0022.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0010.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0034.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0014.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0060.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0016.nii.gz
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/validation_raw/panc_0077.nii.gz
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='501', fold='4', validation_only=False, continue_training=False, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=True, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-01 18:24:17.993942: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-01 18:24:18.003712: The split file contains 5 splits.
2022-08-01 18:24:18.005865: Desired fold for training: 4
2022-08-01 18:24:18.007811: This split has 55 training and 13 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusB run_training - zet learning rate als  
2022-08-01 18:24:20.337056: Suus1 maybe_update_lr lr: 0.0001
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-01 18:24:42.173302: Unable to plot network architecture:
2022-08-01 18:24:42.178721: local variable 'g' referenced before assignment
2022-08-01 18:24:42.181584: 
printing the network instead:

2022-08-01 18:24:42.184072: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-01 18:24:42.190715: 

2022-08-01 18:24:42.193686: 
epoch:  0
2022-08-01 18:26:44.841761: train loss : 0.1390
2022-08-01 18:26:52.363918: validation loss: 0.0584
2022-08-01 18:26:52.400571: Average global foreground Dice: [0.1095]
2022-08-01 18:26:52.428664: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:26:53.293653: Suus1 maybe_update_lr lr: 0.0001
2022-08-01 18:26:53.297510: This epoch took 131.101250 s

2022-08-01 18:26:53.300318: 
epoch:  1
2022-08-01 18:28:40.653264: train loss : 0.0300
2022-08-01 18:28:50.830692: validation loss: 0.0095
2022-08-01 18:28:50.843927: Average global foreground Dice: [0.1947]
2022-08-01 18:28:50.848306: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:28:51.420213: Suus1 maybe_update_lr lr: 0.0001
2022-08-01 18:28:51.450595: saving best epoch checkpoint...
2022-08-01 18:28:51.762470: saving checkpoint...
2022-08-01 18:28:56.848202: done, saving took 5.39 seconds
2022-08-01 18:28:56.862120: This epoch took 123.559041 s

2022-08-01 18:28:56.865746: 
epoch:  2
2022-08-01 18:30:44.201167: train loss : -0.0366
2022-08-01 18:30:52.966626: validation loss: -0.0598
2022-08-01 18:30:52.997783: Average global foreground Dice: [0.2647]
2022-08-01 18:30:53.024740: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:30:53.604759: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-01 18:30:53.607511: saving best epoch checkpoint...
2022-08-01 18:30:53.760281: saving checkpoint...
2022-08-01 18:30:58.996882: done, saving took 5.39 seconds
2022-08-01 18:30:59.006360: This epoch took 122.137944 s

2022-08-01 18:30:59.008620: 
epoch:  3
2022-08-01 18:32:47.081021: train loss : -0.1277
2022-08-01 18:32:54.517265: validation loss: -0.1466
2022-08-01 18:32:54.547530: Average global foreground Dice: [0.339]
2022-08-01 18:32:54.594060: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:32:55.452722: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-01 18:32:55.484029: saving best epoch checkpoint...
2022-08-01 18:32:55.737262: saving checkpoint...
2022-08-01 18:33:01.556250: done, saving took 6.05 seconds
2022-08-01 18:33:01.566169: This epoch took 122.555291 s

2022-08-01 18:33:01.569974: 
epoch:  4
2022-08-01 18:34:47.330232: train loss : -0.1866
2022-08-01 18:34:55.350580: validation loss: -0.1919
2022-08-01 18:34:55.369299: Average global foreground Dice: [0.3486]
2022-08-01 18:34:55.387126: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:34:56.098028: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-01 18:34:56.104479: saving best epoch checkpoint...
2022-08-01 18:34:56.365635: saving checkpoint...
2022-08-01 18:35:01.836219: done, saving took 5.71 seconds
2022-08-01 18:35:01.845898: This epoch took 120.273502 s

2022-08-01 18:35:01.848178: 
epoch:  5
2022-08-01 18:36:49.183491: train loss : -0.2385
2022-08-01 18:36:57.890161: validation loss: -0.2190
2022-08-01 18:36:57.893993: Average global foreground Dice: [0.3584]
2022-08-01 18:36:57.896322: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:36:58.392256: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-01 18:36:58.399336: saving best epoch checkpoint...
2022-08-01 18:36:58.686763: saving checkpoint...
2022-08-01 18:37:04.158217: done, saving took 5.76 seconds
2022-08-01 18:37:04.168120: This epoch took 122.316618 s

2022-08-01 18:37:04.171141: 
epoch:  6
2022-08-01 18:38:52.630353: train loss : -0.2586
2022-08-01 18:39:01.699730: validation loss: -0.2341
2022-08-01 18:39:01.710770: Average global foreground Dice: [0.398]
2022-08-01 18:39:01.757880: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:39:02.390172: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-01 18:39:02.411136: saving best epoch checkpoint...
2022-08-01 18:39:02.668051: saving checkpoint...
2022-08-01 18:39:08.076675: done, saving took 5.64 seconds
2022-08-01 18:39:08.091623: This epoch took 123.917921 s

2022-08-01 18:39:08.093916: 
epoch:  7
2022-08-01 18:40:58.964114: train loss : -0.2848
2022-08-01 18:41:09.326677: validation loss: -0.2924
2022-08-01 18:41:09.365047: Average global foreground Dice: [0.4186]
2022-08-01 18:41:09.392952: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:41:10.040866: Suus1 maybe_update_lr lr: 9.9e-05
2022-08-01 18:41:10.062895: saving best epoch checkpoint...
2022-08-01 18:41:10.436336: saving checkpoint...
2022-08-01 18:41:16.053767: done, saving took 5.97 seconds
2022-08-01 18:41:16.069098: This epoch took 127.973068 s

2022-08-01 18:41:16.071070: 
epoch:  8
2022-08-01 18:43:04.854743: train loss : -0.3174
2022-08-01 18:43:11.992733: validation loss: -0.3688
2022-08-01 18:43:12.028567: Average global foreground Dice: [0.5008]
2022-08-01 18:43:12.059033: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:43:12.853786: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-01 18:43:12.870663: saving best epoch checkpoint...
2022-08-01 18:43:13.206036: saving checkpoint...
2022-08-01 18:43:18.736778: done, saving took 5.86 seconds
2022-08-01 18:43:18.750882: This epoch took 122.677665 s

2022-08-01 18:43:18.753295: 
epoch:  9
2022-08-01 18:45:09.056859: train loss : -0.3191
2022-08-01 18:45:17.662986: validation loss: -0.3204
2022-08-01 18:45:17.709534: Average global foreground Dice: [0.4627]
2022-08-01 18:45:17.772807: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:45:18.424727: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-01 18:45:18.440199: saving best epoch checkpoint...
2022-08-01 18:45:18.634776: saving checkpoint...
2022-08-01 18:45:24.062290: done, saving took 5.62 seconds
2022-08-01 18:45:24.078029: This epoch took 125.322724 s

2022-08-01 18:45:24.081331: 
epoch:  10
2022-08-01 18:47:12.257309: train loss : -0.3346
2022-08-01 18:47:22.292769: validation loss: -0.3320
2022-08-01 18:47:22.303470: Average global foreground Dice: [0.4589]
2022-08-01 18:47:22.305833: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:47:23.334243: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-01 18:47:23.336582: saving best epoch checkpoint...
2022-08-01 18:47:23.477674: saving checkpoint...
2022-08-01 18:47:29.263673: done, saving took 5.93 seconds
2022-08-01 18:47:29.276681: This epoch took 125.192674 s

2022-08-01 18:47:29.279080: 
epoch:  11
2022-08-01 18:49:17.818330: train loss : -0.3605
2022-08-01 18:49:26.677009: validation loss: -0.3856
2022-08-01 18:49:26.710306: Average global foreground Dice: [0.5116]
2022-08-01 18:49:26.739074: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:49:27.654148: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-01 18:49:27.675810: saving best epoch checkpoint...
2022-08-01 18:49:27.905252: saving checkpoint...
2022-08-01 18:49:33.604583: done, saving took 5.90 seconds
2022-08-01 18:49:33.618006: This epoch took 124.336903 s

2022-08-01 18:49:33.620123: 
epoch:  12
2022-08-01 18:51:24.044983: train loss : -0.3682
2022-08-01 18:51:32.630018: validation loss: -0.3808
2022-08-01 18:51:32.671409: Average global foreground Dice: [0.4978]
2022-08-01 18:51:32.723722: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:51:33.505756: Suus1 maybe_update_lr lr: 9.8e-05
2022-08-01 18:51:33.516645: saving best epoch checkpoint...
2022-08-01 18:51:33.788270: saving checkpoint...
2022-08-01 18:51:39.567304: done, saving took 6.02 seconds
2022-08-01 18:51:39.580074: This epoch took 125.957440 s

2022-08-01 18:51:39.582299: 
epoch:  13
2022-08-01 18:53:28.139469: train loss : -0.3746
2022-08-01 18:53:35.337997: validation loss: -0.3809
2022-08-01 18:53:35.341554: Average global foreground Dice: [0.5087]
2022-08-01 18:53:35.344100: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:53:35.811998: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-01 18:53:35.814587: saving best epoch checkpoint...
2022-08-01 18:53:36.003691: saving checkpoint...
2022-08-01 18:53:41.143194: done, saving took 5.33 seconds
2022-08-01 18:53:41.156643: This epoch took 121.572116 s

2022-08-01 18:53:41.158945: 
epoch:  14
2022-08-01 18:55:28.645105: train loss : -0.3762
2022-08-01 18:55:36.326562: validation loss: -0.3941
2022-08-01 18:55:36.331080: Average global foreground Dice: [0.5134]
2022-08-01 18:55:36.333461: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:55:36.918509: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-01 18:55:36.922287: saving best epoch checkpoint...
2022-08-01 18:55:37.123720: saving checkpoint...
2022-08-01 18:55:42.254542: done, saving took 5.33 seconds
2022-08-01 18:55:42.270674: This epoch took 121.109125 s

2022-08-01 18:55:42.273583: 
epoch:  15
2022-08-01 18:57:31.791147: train loss : -0.3820
2022-08-01 18:57:41.498698: validation loss: -0.3990
2022-08-01 18:57:41.536546: Average global foreground Dice: [0.5228]
2022-08-01 18:57:41.565804: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:57:42.171325: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-01 18:57:42.216751: saving best epoch checkpoint...
2022-08-01 18:57:42.646907: saving checkpoint...
2022-08-01 18:57:48.622340: done, saving took 6.36 seconds
2022-08-01 18:57:48.637777: This epoch took 126.361787 s

2022-08-01 18:57:48.640109: 
epoch:  16
2022-08-01 18:59:36.692469: train loss : -0.4046
2022-08-01 18:59:43.617725: validation loss: -0.3577
2022-08-01 18:59:43.621497: Average global foreground Dice: [0.4916]
2022-08-01 18:59:43.624213: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 18:59:44.112999: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-01 18:59:44.115609: saving best epoch checkpoint...
2022-08-01 18:59:44.366680: saving checkpoint...
2022-08-01 18:59:49.849103: done, saving took 5.73 seconds
2022-08-01 18:59:49.868385: This epoch took 121.226092 s

2022-08-01 18:59:49.870652: 
epoch:  17
2022-08-01 19:01:41.090751: train loss : -0.4147
2022-08-01 19:01:51.635787: validation loss: -0.3990
2022-08-01 19:01:51.719055: Average global foreground Dice: [0.5277]
2022-08-01 19:01:51.788105: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:01:52.549389: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-01 19:01:52.552222: saving best epoch checkpoint...
2022-08-01 19:01:52.758150: saving checkpoint...
2022-08-01 19:01:58.602243: done, saving took 6.05 seconds
2022-08-01 19:01:58.620817: This epoch took 128.747844 s

2022-08-01 19:01:58.623049: 
epoch:  18
2022-08-01 19:03:47.362356: train loss : -0.4267
2022-08-01 19:03:56.312863: validation loss: -0.4441
2022-08-01 19:03:56.375396: Average global foreground Dice: [0.554]
2022-08-01 19:03:56.409874: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:03:57.214293: Suus1 maybe_update_lr lr: 9.7e-05
2022-08-01 19:03:57.251334: saving best epoch checkpoint...
2022-08-01 19:03:57.705763: saving checkpoint...
2022-08-01 19:04:03.252744: done, saving took 5.96 seconds
2022-08-01 19:04:03.267614: This epoch took 124.642198 s

2022-08-01 19:04:03.269611: 
epoch:  19
2022-08-01 19:05:52.438721: train loss : -0.4426
2022-08-01 19:06:01.541336: validation loss: -0.4194
2022-08-01 19:06:01.567730: Average global foreground Dice: [0.5419]
2022-08-01 19:06:01.599324: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:06:02.321449: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-01 19:06:02.362810: saving best epoch checkpoint...
2022-08-01 19:06:02.928377: saving checkpoint...
2022-08-01 19:06:08.721940: done, saving took 6.33 seconds
2022-08-01 19:06:08.740098: This epoch took 125.468376 s

2022-08-01 19:06:08.742518: 
epoch:  20
2022-08-01 19:08:00.916176: train loss : -0.4525
2022-08-01 19:08:09.977135: validation loss: -0.4475
2022-08-01 19:08:10.018295: Average global foreground Dice: [0.5657]
2022-08-01 19:08:10.044775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:08:11.090662: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-01 19:08:11.110799: saving best epoch checkpoint...
2022-08-01 19:08:11.373371: saving checkpoint...
2022-08-01 19:08:17.046532: done, saving took 5.92 seconds
2022-08-01 19:08:17.058416: This epoch took 128.313778 s

2022-08-01 19:08:17.061800: 
epoch:  21
2022-08-01 19:10:07.100471: train loss : -0.4244
2022-08-01 19:10:14.317416: validation loss: -0.4623
2022-08-01 19:10:14.352143: Average global foreground Dice: [0.5772]
2022-08-01 19:10:14.388153: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:10:15.429416: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-01 19:10:15.466815: saving best epoch checkpoint...
2022-08-01 19:10:15.763765: saving checkpoint...
2022-08-01 19:10:21.841274: done, saving took 6.33 seconds
2022-08-01 19:10:21.852624: This epoch took 124.788539 s

2022-08-01 19:10:21.854848: 
epoch:  22
2022-08-01 19:12:08.813228: train loss : -0.4676
2022-08-01 19:12:16.557321: validation loss: -0.4570
2022-08-01 19:12:16.576216: Average global foreground Dice: [0.5693]
2022-08-01 19:12:16.588480: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:12:17.095452: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-01 19:12:17.100115: saving best epoch checkpoint...
2022-08-01 19:12:17.331294: saving checkpoint...
2022-08-01 19:12:23.042264: done, saving took 5.93 seconds
2022-08-01 19:12:23.055810: This epoch took 121.198830 s

2022-08-01 19:12:23.057904: 
epoch:  23
2022-08-01 19:14:09.438581: train loss : -0.4612
2022-08-01 19:14:17.262228: validation loss: -0.4853
2022-08-01 19:14:17.266184: Average global foreground Dice: [0.5883]
2022-08-01 19:14:17.268851: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:14:17.766615: Suus1 maybe_update_lr lr: 9.6e-05
2022-08-01 19:14:17.769266: saving best epoch checkpoint...
2022-08-01 19:14:18.050949: saving checkpoint...
2022-08-01 19:14:23.253669: done, saving took 5.48 seconds
2022-08-01 19:14:23.267111: This epoch took 120.207217 s

2022-08-01 19:14:23.269641: 
epoch:  24
2022-08-01 19:16:12.316518: train loss : -0.4562
2022-08-01 19:16:21.298972: validation loss: -0.4725
2022-08-01 19:16:21.333632: Average global foreground Dice: [0.5982]
2022-08-01 19:16:21.366943: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:16:22.038134: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-01 19:16:22.080819: saving best epoch checkpoint...
2022-08-01 19:16:22.358531: saving checkpoint...
2022-08-01 19:16:27.808599: done, saving took 5.69 seconds
2022-08-01 19:16:27.823055: This epoch took 124.551175 s

2022-08-01 19:16:27.825526: 
epoch:  25
2022-08-01 19:18:15.764629: train loss : -0.4807
2022-08-01 19:18:26.076971: validation loss: -0.4643
2022-08-01 19:18:26.119149: Average global foreground Dice: [0.5767]
2022-08-01 19:18:26.139783: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:18:26.815585: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-01 19:18:26.842135: saving best epoch checkpoint...
2022-08-01 19:18:27.109421: saving checkpoint...
2022-08-01 19:18:32.778218: done, saving took 5.91 seconds
2022-08-01 19:18:32.792227: This epoch took 124.964502 s

2022-08-01 19:18:32.794729: 
epoch:  26
2022-08-01 19:20:22.017110: train loss : -0.4925
2022-08-01 19:20:29.539955: validation loss: -0.4760
2022-08-01 19:20:29.571889: Average global foreground Dice: [0.5863]
2022-08-01 19:20:29.601227: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:20:30.260530: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-01 19:20:30.289811: saving best epoch checkpoint...
2022-08-01 19:20:30.820526: saving checkpoint...
2022-08-01 19:20:36.499219: done, saving took 6.16 seconds
2022-08-01 19:20:36.513548: This epoch took 123.716619 s

2022-08-01 19:20:36.515643: 
epoch:  27
2022-08-01 19:22:25.645230: train loss : -0.5211
2022-08-01 19:22:33.624550: validation loss: -0.5309
2022-08-01 19:22:33.658476: Average global foreground Dice: [0.6396]
2022-08-01 19:22:33.687926: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:22:34.392497: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-01 19:22:34.423813: saving best epoch checkpoint...
2022-08-01 19:22:34.658911: saving checkpoint...
2022-08-01 19:22:40.473192: done, saving took 6.03 seconds
2022-08-01 19:22:40.489078: This epoch took 123.971421 s

2022-08-01 19:22:40.491578: 
epoch:  28
2022-08-01 19:24:29.146710: train loss : -0.5254
2022-08-01 19:24:36.605344: validation loss: -0.5202
2022-08-01 19:24:36.609047: Average global foreground Dice: [0.6271]
2022-08-01 19:24:36.611808: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:24:37.110463: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-01 19:24:37.112882: saving best epoch checkpoint...
2022-08-01 19:24:37.363797: saving checkpoint...
2022-08-01 19:24:42.782986: done, saving took 5.67 seconds
2022-08-01 19:24:42.801605: This epoch took 122.307525 s

2022-08-01 19:24:42.804799: 
epoch:  29
2022-08-01 19:26:33.473244: train loss : -0.5118
2022-08-01 19:26:41.308184: validation loss: -0.5503
2022-08-01 19:26:41.360433: Average global foreground Dice: [0.652]
2022-08-01 19:26:41.382903: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:26:42.392055: Suus1 maybe_update_lr lr: 9.5e-05
2022-08-01 19:26:42.423835: saving best epoch checkpoint...
2022-08-01 19:26:42.873109: saving checkpoint...
2022-08-01 19:26:48.648223: done, saving took 6.20 seconds
2022-08-01 19:26:48.664298: This epoch took 125.857314 s

2022-08-01 19:26:48.666733: 
epoch:  30
2022-08-01 19:28:38.561310: train loss : -0.5016
2022-08-01 19:28:46.839630: validation loss: -0.5127
2022-08-01 19:28:46.862867: Average global foreground Dice: [0.6206]
2022-08-01 19:28:46.884753: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:28:47.730928: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-01 19:28:47.740173: saving best epoch checkpoint...
2022-08-01 19:28:47.947236: saving checkpoint...
2022-08-01 19:28:52.888194: done, saving took 5.14 seconds
2022-08-01 19:28:52.900735: This epoch took 124.231686 s

2022-08-01 19:28:52.903429: 
epoch:  31
2022-08-01 19:30:41.610876: train loss : -0.5341
2022-08-01 19:30:50.191490: validation loss: -0.5402
2022-08-01 19:30:50.224778: Average global foreground Dice: [0.6552]
2022-08-01 19:30:50.255486: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:30:51.020576: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-01 19:30:51.044657: saving best epoch checkpoint...
2022-08-01 19:30:51.256501: saving checkpoint...
2022-08-01 19:30:57.017713: done, saving took 5.96 seconds
2022-08-01 19:30:57.027590: This epoch took 124.121874 s

2022-08-01 19:30:57.029911: 
epoch:  32
2022-08-01 19:32:46.299351: train loss : -0.5303
2022-08-01 19:32:57.126257: validation loss: -0.4609
2022-08-01 19:32:57.184154: Average global foreground Dice: [0.5872]
2022-08-01 19:32:57.210774: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:32:57.901408: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-01 19:32:57.962895: saving best epoch checkpoint...
2022-08-01 19:32:58.263481: saving checkpoint...
2022-08-01 19:33:04.105653: done, saving took 6.10 seconds
2022-08-01 19:33:04.116013: This epoch took 127.084015 s

2022-08-01 19:33:04.119004: 
epoch:  33
2022-08-01 19:34:52.475171: train loss : -0.5225
2022-08-01 19:35:03.536253: validation loss: -0.5148
2022-08-01 19:35:03.562296: Average global foreground Dice: [0.6182]
2022-08-01 19:35:03.587049: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:35:04.230948: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-01 19:35:04.253807: saving best epoch checkpoint...
2022-08-01 19:35:04.746581: saving checkpoint...
2022-08-01 19:35:10.620866: done, saving took 6.33 seconds
2022-08-01 19:35:10.635731: This epoch took 126.514497 s

2022-08-01 19:35:10.639495: 
epoch:  34
2022-08-01 19:36:58.714525: train loss : -0.5258
2022-08-01 19:37:07.763039: validation loss: -0.5433
2022-08-01 19:37:07.815638: Average global foreground Dice: [0.6423]
2022-08-01 19:37:07.841800: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:37:08.562892: Suus1 maybe_update_lr lr: 9.4e-05
2022-08-01 19:37:08.612885: saving best epoch checkpoint...
2022-08-01 19:37:08.901659: saving checkpoint...
2022-08-01 19:37:14.264590: done, saving took 5.62 seconds
2022-08-01 19:37:14.278879: This epoch took 123.636777 s

2022-08-01 19:37:14.281343: 
epoch:  35
2022-08-01 19:39:01.485840: train loss : -0.5472
2022-08-01 19:39:10.093741: validation loss: -0.5886
2022-08-01 19:39:10.101799: Average global foreground Dice: [0.68]
2022-08-01 19:39:10.104624: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:39:10.644341: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-01 19:39:10.647135: saving best epoch checkpoint...
2022-08-01 19:39:10.893012: saving checkpoint...
2022-08-01 19:39:16.875024: done, saving took 6.23 seconds
2022-08-01 19:39:16.889132: This epoch took 122.605672 s

2022-08-01 19:39:16.891348: 
epoch:  36
2022-08-01 19:41:03.249299: train loss : -0.5535
2022-08-01 19:41:11.026168: validation loss: -0.5983
2022-08-01 19:41:11.029739: Average global foreground Dice: [0.6978]
2022-08-01 19:41:11.032286: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:41:11.650808: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-01 19:41:11.662313: saving best epoch checkpoint...
2022-08-01 19:41:11.967815: saving checkpoint...
2022-08-01 19:41:17.850668: done, saving took 6.16 seconds
2022-08-01 19:41:17.866070: This epoch took 120.972751 s

2022-08-01 19:41:17.868217: 
epoch:  37
2022-08-01 19:43:08.687382: train loss : -0.5635
2022-08-01 19:43:16.465186: validation loss: -0.6142
2022-08-01 19:43:16.498957: Average global foreground Dice: [0.7161]
2022-08-01 19:43:16.581096: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:43:17.287544: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-01 19:43:17.290111: saving best epoch checkpoint...
2022-08-01 19:43:17.547030: saving checkpoint...
2022-08-01 19:43:22.560035: done, saving took 5.27 seconds
2022-08-01 19:43:22.576014: This epoch took 124.705688 s

2022-08-01 19:43:22.579087: 
epoch:  38
2022-08-01 19:45:11.362814: train loss : -0.5564
2022-08-01 19:45:18.741436: validation loss: -0.4949
2022-08-01 19:45:18.745337: Average global foreground Dice: [0.6112]
2022-08-01 19:45:18.747599: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:45:19.245578: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-01 19:45:19.248126: This epoch took 116.666818 s

2022-08-01 19:45:19.250216: 
epoch:  39
2022-08-01 19:47:07.536961: train loss : -0.5516
2022-08-01 19:47:14.825248: validation loss: -0.5844
2022-08-01 19:47:14.830082: Average global foreground Dice: [0.6843]
2022-08-01 19:47:14.833423: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:47:15.434047: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-01 19:47:15.436575: saving best epoch checkpoint...
2022-08-01 19:47:15.694521: saving checkpoint...
2022-08-01 19:47:21.132002: done, saving took 5.69 seconds
2022-08-01 19:47:21.148476: This epoch took 121.895951 s

2022-08-01 19:47:21.150756: 
epoch:  40
2022-08-01 19:49:09.466280: train loss : -0.5711
2022-08-01 19:49:17.268202: validation loss: -0.6047
2022-08-01 19:49:17.302740: Average global foreground Dice: [0.7042]
2022-08-01 19:49:17.326721: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:49:18.443997: Suus1 maybe_update_lr lr: 9.3e-05
2022-08-01 19:49:18.446905: saving best epoch checkpoint...
2022-08-01 19:49:18.648890: saving checkpoint...
2022-08-01 19:49:24.266219: done, saving took 5.82 seconds
2022-08-01 19:49:24.279060: This epoch took 123.126221 s

2022-08-01 19:49:24.281224: 
epoch:  41
2022-08-01 19:51:14.949333: train loss : -0.5635
2022-08-01 19:51:25.559176: validation loss: -0.5599
2022-08-01 19:51:25.585392: Average global foreground Dice: [0.6685]
2022-08-01 19:51:25.625807: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:51:26.374443: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-01 19:51:26.419366: saving best epoch checkpoint...
2022-08-01 19:51:26.694272: saving checkpoint...
2022-08-01 19:51:33.060189: done, saving took 6.61 seconds
2022-08-01 19:51:33.073417: This epoch took 128.790060 s

2022-08-01 19:51:33.075670: 
epoch:  42
2022-08-01 19:53:22.047647: train loss : -0.5699
2022-08-01 19:53:30.243794: validation loss: -0.5448
2022-08-01 19:53:30.292445: Average global foreground Dice: [0.6506]
2022-08-01 19:53:30.324818: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:53:31.109568: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-01 19:53:31.156810: saving best epoch checkpoint...
2022-08-01 19:53:31.495872: saving checkpoint...
2022-08-01 19:53:37.958632: done, saving took 6.76 seconds
2022-08-01 19:53:37.972661: This epoch took 124.894684 s

2022-08-01 19:53:37.974911: 
epoch:  43
2022-08-01 19:55:24.138023: train loss : -0.5924
2022-08-01 19:55:30.754987: validation loss: -0.5910
2022-08-01 19:55:30.763455: Average global foreground Dice: [0.6849]
2022-08-01 19:55:30.767323: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:55:31.300334: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-01 19:55:31.303199: saving best epoch checkpoint...
2022-08-01 19:55:31.509780: saving checkpoint...
2022-08-01 19:55:36.478311: done, saving took 5.16 seconds
2022-08-01 19:55:36.492278: This epoch took 118.515277 s

2022-08-01 19:55:36.494742: 
epoch:  44
2022-08-01 19:57:23.648832: train loss : -0.5833
2022-08-01 19:57:32.486164: validation loss: -0.5780
2022-08-01 19:57:32.489884: Average global foreground Dice: [0.6794]
2022-08-01 19:57:32.492534: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:57:32.988077: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-01 19:57:32.990659: saving best epoch checkpoint...
2022-08-01 19:57:33.238648: saving checkpoint...
2022-08-01 19:57:38.616562: done, saving took 5.62 seconds
2022-08-01 19:57:38.631742: This epoch took 122.134394 s

2022-08-01 19:57:38.634056: 
epoch:  45
2022-08-01 19:59:25.066717: train loss : -0.6026
2022-08-01 19:59:32.037444: validation loss: -0.5696
2022-08-01 19:59:32.073331: Average global foreground Dice: [0.6709]
2022-08-01 19:59:32.107947: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 19:59:32.683649: Suus1 maybe_update_lr lr: 9.2e-05
2022-08-01 19:59:32.700188: saving best epoch checkpoint...
2022-08-01 19:59:33.001460: saving checkpoint...
2022-08-01 19:59:38.466110: done, saving took 5.75 seconds
2022-08-01 19:59:38.481404: This epoch took 119.845157 s

2022-08-01 19:59:38.483701: 
epoch:  46
2022-08-01 20:01:26.926303: train loss : -0.5855
2022-08-01 20:01:34.247204: validation loss: -0.5851
2022-08-01 20:01:34.251000: Average global foreground Dice: [0.6861]
2022-08-01 20:01:34.254290: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:01:34.784014: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-01 20:01:34.788775: saving best epoch checkpoint...
2022-08-01 20:01:35.042779: saving checkpoint...
2022-08-01 20:01:40.752150: done, saving took 5.96 seconds
2022-08-01 20:01:40.767838: This epoch took 122.281511 s

2022-08-01 20:01:40.770071: 
epoch:  47
2022-08-01 20:03:28.970973: train loss : -0.6048
2022-08-01 20:03:39.612889: validation loss: -0.6046
2022-08-01 20:03:39.625991: Average global foreground Dice: [0.6926]
2022-08-01 20:03:39.644976: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:03:40.314302: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-01 20:03:40.337970: saving best epoch checkpoint...
2022-08-01 20:03:40.804106: saving checkpoint...
2022-08-01 20:03:46.609385: done, saving took 6.26 seconds
2022-08-01 20:03:46.626095: This epoch took 125.853758 s

2022-08-01 20:03:46.628427: 
epoch:  48
2022-08-01 20:05:32.411561: train loss : -0.6043
2022-08-01 20:05:39.074545: validation loss: -0.5938
2022-08-01 20:05:39.086440: Average global foreground Dice: [0.6893]
2022-08-01 20:05:39.091049: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:05:39.581793: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-01 20:05:39.584654: saving best epoch checkpoint...
2022-08-01 20:05:39.857016: saving checkpoint...
2022-08-01 20:05:44.938595: done, saving took 5.35 seconds
2022-08-01 20:05:44.954940: This epoch took 118.324466 s

2022-08-01 20:05:44.957281: 
epoch:  49
2022-08-01 20:07:33.803717: train loss : -0.5982
2022-08-01 20:07:42.645911: validation loss: -0.6172
2022-08-01 20:07:42.664918: Average global foreground Dice: [0.7144]
2022-08-01 20:07:42.667361: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:07:43.272457: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-01 20:07:43.276058: saving scheduled checkpoint file...
2022-08-01 20:07:43.766867: saving checkpoint...
2022-08-01 20:07:48.843275: done, saving took 5.56 seconds
2022-08-01 20:07:48.861918: done
2022-08-01 20:07:48.865522: saving best epoch checkpoint...
2022-08-01 20:07:49.025605: saving checkpoint...
2022-08-01 20:07:54.148603: done, saving took 5.28 seconds
2022-08-01 20:07:54.164034: This epoch took 129.204524 s

2022-08-01 20:07:54.166435: 
epoch:  50
2022-08-01 20:09:41.397821: train loss : -0.6071
2022-08-01 20:09:49.631599: validation loss: -0.6092
2022-08-01 20:09:49.666360: Average global foreground Dice: [0.7058]
2022-08-01 20:09:49.702767: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:09:50.448704: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-01 20:09:50.481230: saving best epoch checkpoint...
2022-08-01 20:09:50.942322: saving checkpoint...
2022-08-01 20:09:57.026668: done, saving took 6.51 seconds
2022-08-01 20:09:57.043969: This epoch took 122.874759 s

2022-08-01 20:09:57.047030: 
epoch:  51
2022-08-01 20:11:46.350327: train loss : -0.6149
2022-08-01 20:11:52.981399: validation loss: -0.6207
2022-08-01 20:11:52.987336: Average global foreground Dice: [0.7158]
2022-08-01 20:11:52.993087: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:11:53.484336: Suus1 maybe_update_lr lr: 9.1e-05
2022-08-01 20:11:53.486899: saving best epoch checkpoint...
2022-08-01 20:11:53.668027: saving checkpoint...
2022-08-01 20:11:58.995942: done, saving took 5.51 seconds
2022-08-01 20:11:59.007326: This epoch took 121.958095 s

2022-08-01 20:11:59.009677: 
epoch:  52
2022-08-01 20:13:51.233004: train loss : -0.6211
2022-08-01 20:13:59.874305: validation loss: -0.6078
2022-08-01 20:13:59.897312: Average global foreground Dice: [0.6983]
2022-08-01 20:13:59.918784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:14:00.659937: Suus1 maybe_update_lr lr: 9e-05
2022-08-01 20:14:00.682643: saving best epoch checkpoint...
2022-08-01 20:14:01.046314: saving checkpoint...
2022-08-01 20:14:06.464638: done, saving took 5.75 seconds
2022-08-01 20:14:06.476999: This epoch took 127.465176 s

2022-08-01 20:14:06.479932: 
epoch:  53
2022-08-01 20:15:55.497831: train loss : -0.6020
2022-08-01 20:16:04.018269: validation loss: -0.5582
2022-08-01 20:16:04.049509: Average global foreground Dice: [0.6562]
2022-08-01 20:16:04.069060: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:16:05.031522: Suus1 maybe_update_lr lr: 9e-05
2022-08-01 20:16:05.062852: This epoch took 118.580448 s

2022-08-01 20:16:05.095775: 
epoch:  54
2022-08-01 20:17:53.971485: train loss : -0.6241
2022-08-01 20:18:00.775866: validation loss: -0.6176
2022-08-01 20:18:00.779718: Average global foreground Dice: [0.704]
2022-08-01 20:18:00.782167: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:18:01.264509: Suus1 maybe_update_lr lr: 9e-05
2022-08-01 20:18:01.266862: saving best epoch checkpoint...
2022-08-01 20:18:01.444582: saving checkpoint...
2022-08-01 20:18:07.440355: done, saving took 6.17 seconds
2022-08-01 20:18:07.461287: This epoch took 122.332459 s

2022-08-01 20:18:07.463582: 
epoch:  55
2022-08-01 20:19:57.394468: train loss : -0.6349
2022-08-01 20:20:04.924708: validation loss: -0.5964
2022-08-01 20:20:04.951281: Average global foreground Dice: [0.6884]
2022-08-01 20:20:04.981783: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:20:05.749887: Suus1 maybe_update_lr lr: 9e-05
2022-08-01 20:20:05.752926: saving best epoch checkpoint...
2022-08-01 20:20:05.928304: saving checkpoint...
2022-08-01 20:20:11.009647: done, saving took 5.25 seconds
2022-08-01 20:20:11.023740: This epoch took 123.557984 s

2022-08-01 20:20:11.026467: 
epoch:  56
2022-08-01 20:21:56.506448: train loss : -0.6057
2022-08-01 20:22:02.861223: validation loss: -0.6059
2022-08-01 20:22:02.864366: Average global foreground Dice: [0.6903]
2022-08-01 20:22:02.866522: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:22:03.303944: Suus1 maybe_update_lr lr: 9e-05
2022-08-01 20:22:03.306793: saving best epoch checkpoint...
2022-08-01 20:22:03.458092: saving checkpoint...
2022-08-01 20:22:08.903334: done, saving took 5.59 seconds
2022-08-01 20:22:08.916737: This epoch took 117.887781 s

2022-08-01 20:22:08.919187: 
epoch:  57
2022-08-01 20:23:53.894554: train loss : -0.6427
2022-08-01 20:24:00.382212: validation loss: -0.5956
2022-08-01 20:24:00.385171: Average global foreground Dice: [0.6915]
2022-08-01 20:24:00.387271: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:24:00.829131: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-01 20:24:00.831607: saving best epoch checkpoint...
2022-08-01 20:24:00.991184: saving checkpoint...
2022-08-01 20:24:06.489644: done, saving took 5.66 seconds
2022-08-01 20:24:06.506820: This epoch took 117.584958 s

2022-08-01 20:24:06.509565: 
epoch:  58
2022-08-01 20:25:51.346572: train loss : -0.6190
2022-08-01 20:25:57.953138: validation loss: -0.6020
2022-08-01 20:25:57.957351: Average global foreground Dice: [0.6992]
2022-08-01 20:25:57.960377: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:25:58.429866: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-01 20:25:58.432914: saving best epoch checkpoint...
2022-08-01 20:25:58.611309: saving checkpoint...
2022-08-01 20:26:03.737204: done, saving took 5.30 seconds
2022-08-01 20:26:03.754116: This epoch took 117.242115 s

2022-08-01 20:26:03.756621: 
epoch:  59
2022-08-01 20:27:52.590126: train loss : -0.6437
2022-08-01 20:28:00.468356: validation loss: -0.5683
2022-08-01 20:28:00.500152: Average global foreground Dice: [0.6609]
2022-08-01 20:28:00.520775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:28:01.055055: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-01 20:28:01.057929: This epoch took 117.298945 s

2022-08-01 20:28:01.060845: 
epoch:  60
2022-08-01 20:29:49.376749: train loss : -0.6210
2022-08-01 20:29:57.852603: validation loss: -0.6107
2022-08-01 20:29:57.856742: Average global foreground Dice: [0.7067]
2022-08-01 20:29:57.859184: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:29:58.414018: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-01 20:29:58.416873: saving best epoch checkpoint...
2022-08-01 20:29:58.643873: saving checkpoint...
2022-08-01 20:30:04.134416: done, saving took 5.72 seconds
2022-08-01 20:30:04.153592: This epoch took 123.088692 s

2022-08-01 20:30:04.156609: 
epoch:  61
2022-08-01 20:31:49.705520: train loss : -0.6245
2022-08-01 20:31:56.610991: validation loss: -0.6079
2022-08-01 20:31:56.620357: Average global foreground Dice: [0.6972]
2022-08-01 20:31:56.630126: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:31:57.164794: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-01 20:31:57.167630: saving best epoch checkpoint...
2022-08-01 20:31:57.361041: saving checkpoint...
2022-08-01 20:32:02.927140: done, saving took 5.76 seconds
2022-08-01 20:32:02.937943: This epoch took 118.779084 s

2022-08-01 20:32:02.940704: 
epoch:  62
2022-08-01 20:33:51.129407: train loss : -0.6347
2022-08-01 20:33:58.760943: validation loss: -0.6117
2022-08-01 20:33:58.765388: Average global foreground Dice: [0.7092]
2022-08-01 20:33:58.768171: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:33:59.317731: Suus1 maybe_update_lr lr: 8.9e-05
2022-08-01 20:33:59.328735: saving best epoch checkpoint...
2022-08-01 20:33:59.607368: saving checkpoint...
2022-08-01 20:34:05.269053: done, saving took 5.94 seconds
2022-08-01 20:34:05.281322: This epoch took 122.338468 s

2022-08-01 20:34:05.284335: 
epoch:  63
2022-08-01 20:35:53.700925: train loss : -0.6332
2022-08-01 20:36:02.329968: validation loss: -0.6365
2022-08-01 20:36:02.373309: Average global foreground Dice: [0.7236]
2022-08-01 20:36:02.396626: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:36:03.150483: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-01 20:36:03.186840: saving best epoch checkpoint...
2022-08-01 20:36:03.576546: saving checkpoint...
2022-08-01 20:36:08.945578: done, saving took 5.71 seconds
2022-08-01 20:36:08.960521: This epoch took 123.673884 s

2022-08-01 20:36:08.963193: 
epoch:  64
2022-08-01 20:37:58.091583: train loss : -0.6385
2022-08-01 20:38:05.586125: validation loss: -0.6193
2022-08-01 20:38:05.590457: Average global foreground Dice: [0.7106]
2022-08-01 20:38:05.593181: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:38:06.158504: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-01 20:38:06.171866: saving best epoch checkpoint...
2022-08-01 20:38:06.419765: saving checkpoint...
2022-08-01 20:38:11.982621: done, saving took 5.79 seconds
2022-08-01 20:38:11.997043: This epoch took 123.031451 s

2022-08-01 20:38:11.999645: 
epoch:  65
2022-08-01 20:39:58.306418: train loss : -0.6524
2022-08-01 20:40:06.327587: validation loss: -0.6397
2022-08-01 20:40:06.331847: Average global foreground Dice: [0.7302]
2022-08-01 20:40:06.334569: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:40:06.813639: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-01 20:40:06.816726: saving best epoch checkpoint...
2022-08-01 20:40:07.103377: saving checkpoint...
2022-08-01 20:40:12.130536: done, saving took 5.31 seconds
2022-08-01 20:40:12.144357: This epoch took 120.142287 s

2022-08-01 20:40:12.146544: 
epoch:  66
2022-08-01 20:42:00.436074: train loss : -0.6424
2022-08-01 20:42:07.552996: validation loss: -0.5822
2022-08-01 20:42:07.558065: Average global foreground Dice: [0.67]
2022-08-01 20:42:07.560647: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:42:08.107157: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-01 20:42:08.110067: This epoch took 115.961148 s

2022-08-01 20:42:08.112629: 
epoch:  67
2022-08-01 20:43:55.924699: train loss : -0.6545
2022-08-01 20:44:03.560123: validation loss: -0.6514
2022-08-01 20:44:03.590004: Average global foreground Dice: [0.7358]
2022-08-01 20:44:03.611780: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:44:04.166789: Suus1 maybe_update_lr lr: 8.8e-05
2022-08-01 20:44:04.169163: saving best epoch checkpoint...
2022-08-01 20:44:04.363681: saving checkpoint...
2022-08-01 20:44:09.560673: done, saving took 5.39 seconds
2022-08-01 20:44:09.575193: This epoch took 121.459961 s

2022-08-01 20:44:09.577487: 
epoch:  68
2022-08-01 20:45:55.852478: train loss : -0.6652
2022-08-01 20:46:03.096913: validation loss: -0.6316
2022-08-01 20:46:03.100823: Average global foreground Dice: [0.7246]
2022-08-01 20:46:03.103460: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:46:03.598918: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-01 20:46:03.601830: saving best epoch checkpoint...
2022-08-01 20:46:03.801197: saving checkpoint...
2022-08-01 20:46:09.131799: done, saving took 5.53 seconds
2022-08-01 20:46:09.148212: This epoch took 119.568439 s

2022-08-01 20:46:09.150510: 
epoch:  69
2022-08-01 20:47:56.583696: train loss : -0.6665
2022-08-01 20:48:04.736391: validation loss: -0.6358
2022-08-01 20:48:04.766470: Average global foreground Dice: [0.7215]
2022-08-01 20:48:04.794809: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:48:05.704310: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-01 20:48:05.712531: saving best epoch checkpoint...
2022-08-01 20:48:05.983876: saving checkpoint...
2022-08-01 20:48:11.575039: done, saving took 5.86 seconds
2022-08-01 20:48:11.590802: This epoch took 122.437876 s

2022-08-01 20:48:11.593288: 
epoch:  70
2022-08-01 20:49:59.831082: train loss : -0.6700
2022-08-01 20:50:07.453625: validation loss: -0.6324
2022-08-01 20:50:07.475678: Average global foreground Dice: [0.7242]
2022-08-01 20:50:07.497881: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:50:08.393494: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-01 20:50:08.395659: saving best epoch checkpoint...
2022-08-01 20:50:08.536550: saving checkpoint...
2022-08-01 20:50:13.720635: done, saving took 5.32 seconds
2022-08-01 20:50:13.732090: This epoch took 122.136630 s

2022-08-01 20:50:13.734124: 
epoch:  71
2022-08-01 20:52:02.854706: train loss : -0.6570
2022-08-01 20:52:11.743291: validation loss: -0.6593
2022-08-01 20:52:11.765932: Average global foreground Dice: [0.7397]
2022-08-01 20:52:11.789450: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:52:12.425101: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-01 20:52:12.437603: saving best epoch checkpoint...
2022-08-01 20:52:12.635375: saving checkpoint...
2022-08-01 20:52:18.307913: done, saving took 5.87 seconds
2022-08-01 20:52:18.322402: This epoch took 124.586322 s

2022-08-01 20:52:18.325276: 
epoch:  72
2022-08-01 20:54:04.699760: train loss : -0.6691
2022-08-01 20:54:11.494352: validation loss: -0.6463
2022-08-01 20:54:11.497307: Average global foreground Dice: [0.7326]
2022-08-01 20:54:11.500254: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:54:11.976667: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-01 20:54:11.979429: saving best epoch checkpoint...
2022-08-01 20:54:12.138752: saving checkpoint...
2022-08-01 20:54:18.066725: done, saving took 6.08 seconds
2022-08-01 20:54:18.079986: This epoch took 119.752504 s

2022-08-01 20:54:18.083214: 
epoch:  73
2022-08-01 20:56:06.773675: train loss : -0.6586
2022-08-01 20:56:14.324606: validation loss: -0.6610
2022-08-01 20:56:14.328211: Average global foreground Dice: [0.7451]
2022-08-01 20:56:14.341803: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:56:14.851635: Suus1 maybe_update_lr lr: 8.7e-05
2022-08-01 20:56:14.854252: saving best epoch checkpoint...
2022-08-01 20:56:14.995714: saving checkpoint...
2022-08-01 20:56:19.856342: done, saving took 5.00 seconds
2022-08-01 20:56:19.870591: This epoch took 121.784937 s

2022-08-01 20:56:19.873101: 
epoch:  74
2022-08-01 20:58:06.897376: train loss : -0.6471
2022-08-01 20:58:13.404355: validation loss: -0.6212
2022-08-01 20:58:13.408834: Average global foreground Dice: [0.6917]
2022-08-01 20:58:13.411629: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 20:58:13.963228: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-01 20:58:13.965774: This epoch took 114.090093 s

2022-08-01 20:58:13.969195: 
epoch:  75
2022-08-01 21:00:00.732443: train loss : -0.6586
2022-08-01 21:00:07.542822: validation loss: -0.6185
2022-08-01 21:00:07.554017: Average global foreground Dice: [0.7074]
2022-08-01 21:00:07.556286: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:00:08.018405: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-01 21:00:08.021351: This epoch took 114.049532 s

2022-08-01 21:00:08.023544: 
epoch:  76
2022-08-01 21:01:56.974061: train loss : -0.6687
2022-08-01 21:02:04.396256: validation loss: -0.6436
2022-08-01 21:02:04.421134: Average global foreground Dice: [0.7266]
2022-08-01 21:02:04.436039: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:02:04.951213: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-01 21:02:04.953822: This epoch took 116.928082 s

2022-08-01 21:02:04.956457: 
epoch:  77
2022-08-01 21:03:54.640675: train loss : -0.6719
2022-08-01 21:04:04.994117: validation loss: -0.6407
2022-08-01 21:04:05.041156: Average global foreground Dice: [0.7335]
2022-08-01 21:04:05.064586: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:04:05.903129: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-01 21:04:05.905907: saving best epoch checkpoint...
2022-08-01 21:04:06.138679: saving checkpoint...
2022-08-01 21:04:11.694388: done, saving took 5.79 seconds
2022-08-01 21:04:11.710540: This epoch took 126.750918 s

2022-08-01 21:04:11.712938: 
epoch:  78
2022-08-01 21:06:01.308454: train loss : -0.6668
2022-08-01 21:06:11.308731: validation loss: -0.6745
2022-08-01 21:06:11.351433: Average global foreground Dice: [0.7616]
2022-08-01 21:06:11.390798: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:06:12.146874: Suus1 maybe_update_lr lr: 8.6e-05
2022-08-01 21:06:12.170693: saving best epoch checkpoint...
2022-08-01 21:06:12.456302: saving checkpoint...
2022-08-01 21:06:18.268432: done, saving took 6.07 seconds
2022-08-01 21:06:18.284961: This epoch took 126.569781 s

2022-08-01 21:06:18.287667: 
epoch:  79
2022-08-01 21:08:09.173450: train loss : -0.6592
2022-08-01 21:08:17.550001: validation loss: -0.6153
2022-08-01 21:08:17.580349: Average global foreground Dice: [0.7118]
2022-08-01 21:08:17.617782: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:08:18.233760: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-01 21:08:18.299761: This epoch took 120.009839 s

2022-08-01 21:08:18.351761: 
epoch:  80
2022-08-01 21:10:09.097034: train loss : -0.6865
2022-08-01 21:10:18.665037: validation loss: -0.6344
2022-08-01 21:10:18.696402: Average global foreground Dice: [0.7191]
2022-08-01 21:10:18.728773: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:10:19.419519: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-01 21:10:19.453881: This epoch took 121.064113 s

2022-08-01 21:10:19.483031: 
epoch:  81
2022-08-01 21:12:07.306309: train loss : -0.6770
2022-08-01 21:12:15.632141: validation loss: -0.6393
2022-08-01 21:12:15.646275: Average global foreground Dice: [0.7258]
2022-08-01 21:12:15.668770: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:12:16.435039: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-01 21:12:16.465827: This epoch took 116.946013 s

2022-08-01 21:12:16.509752: 
epoch:  82
2022-08-01 21:14:04.406667: train loss : -0.6796
2022-08-01 21:14:11.050952: validation loss: -0.6854
2022-08-01 21:14:11.068538: Average global foreground Dice: [0.7674]
2022-08-01 21:14:11.071317: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:14:11.604431: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-01 21:14:11.607365: saving best epoch checkpoint...
2022-08-01 21:14:11.813548: saving checkpoint...
2022-08-01 21:14:17.092749: done, saving took 5.48 seconds
2022-08-01 21:14:17.105234: This epoch took 120.561454 s

2022-08-01 21:14:17.108157: 
epoch:  83
2022-08-01 21:16:07.033480: train loss : -0.6788
2022-08-01 21:16:15.498623: validation loss: -0.6318
2022-08-01 21:16:15.529571: Average global foreground Dice: [0.7246]
2022-08-01 21:16:15.560799: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:16:16.244750: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-01 21:16:16.268901: saving best epoch checkpoint...
2022-08-01 21:16:16.615304: saving checkpoint...
2022-08-01 21:16:22.605245: done, saving took 6.29 seconds
2022-08-01 21:16:22.619077: This epoch took 125.508629 s

2022-08-01 21:16:22.621383: 
epoch:  84
2022-08-01 21:18:11.388719: train loss : -0.6724
2022-08-01 21:18:20.555573: validation loss: -0.6542
2022-08-01 21:18:20.578079: Average global foreground Dice: [0.7382]
2022-08-01 21:18:20.604806: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:18:21.197709: Suus1 maybe_update_lr lr: 8.5e-05
2022-08-01 21:18:21.225198: saving best epoch checkpoint...
2022-08-01 21:18:21.483703: saving checkpoint...
2022-08-01 21:18:27.443819: done, saving took 6.20 seconds
2022-08-01 21:18:27.459185: This epoch took 124.835151 s

2022-08-01 21:18:27.461671: 
epoch:  85
2022-08-01 21:20:16.452025: train loss : -0.6878
2022-08-01 21:20:24.273827: validation loss: -0.6697
2022-08-01 21:20:24.300427: Average global foreground Dice: [0.7448]
2022-08-01 21:20:24.307813: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:20:24.996970: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-01 21:20:25.008343: saving best epoch checkpoint...
2022-08-01 21:20:25.343498: saving checkpoint...
2022-08-01 21:20:31.140964: done, saving took 6.12 seconds
2022-08-01 21:20:31.161711: This epoch took 123.697478 s

2022-08-01 21:20:31.165079: 
epoch:  86
2022-08-01 21:22:21.928959: train loss : -0.6770
2022-08-01 21:22:29.335097: validation loss: -0.6386
2022-08-01 21:22:29.362644: Average global foreground Dice: [0.723]
2022-08-01 21:22:29.414819: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:22:30.172015: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-01 21:22:30.185319: This epoch took 119.017214 s

2022-08-01 21:22:30.217225: 
epoch:  87
2022-08-01 21:24:20.502234: train loss : -0.6830
2022-08-01 21:24:28.826601: validation loss: -0.6517
2022-08-01 21:24:28.861349: Average global foreground Dice: [0.7402]
2022-08-01 21:24:28.875905: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:24:29.868841: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-01 21:24:29.890634: saving best epoch checkpoint...
2022-08-01 21:24:30.189219: saving checkpoint...
2022-08-01 21:24:36.267439: done, saving took 6.34 seconds
2022-08-01 21:24:36.283228: This epoch took 126.060874 s

2022-08-01 21:24:36.286338: 
epoch:  88
2022-08-01 21:26:25.830525: train loss : -0.6800
2022-08-01 21:26:34.913014: validation loss: -0.6690
2022-08-01 21:26:34.945327: Average global foreground Dice: [0.7456]
2022-08-01 21:26:34.970911: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:26:35.633988: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-01 21:26:35.675831: saving best epoch checkpoint...
2022-08-01 21:26:35.985349: saving checkpoint...
2022-08-01 21:26:41.448563: done, saving took 5.74 seconds
2022-08-01 21:26:41.463312: This epoch took 125.174361 s

2022-08-01 21:26:41.465573: 
epoch:  89
2022-08-01 21:28:31.004673: train loss : -0.6950
2022-08-01 21:28:41.417411: validation loss: -0.6555
2022-08-01 21:28:41.428025: Average global foreground Dice: [0.7424]
2022-08-01 21:28:41.521149: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:28:42.173338: Suus1 maybe_update_lr lr: 8.4e-05
2022-08-01 21:28:42.183943: saving best epoch checkpoint...
2022-08-01 21:28:42.375634: saving checkpoint...
2022-08-01 21:28:48.070228: done, saving took 5.87 seconds
2022-08-01 21:28:48.086354: This epoch took 126.618266 s

2022-08-01 21:28:48.088785: 
epoch:  90
2022-08-01 21:30:36.294985: train loss : -0.6928
2022-08-01 21:30:44.588100: validation loss: -0.6091
2022-08-01 21:30:44.605657: Average global foreground Dice: [0.7031]
2022-08-01 21:30:44.618847: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:30:45.168408: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-01 21:30:45.173714: This epoch took 117.082580 s

2022-08-01 21:30:45.176460: 
epoch:  91
2022-08-01 21:32:33.697270: train loss : -0.6793
2022-08-01 21:32:41.053251: validation loss: -0.6306
2022-08-01 21:32:41.059866: Average global foreground Dice: [0.721]
2022-08-01 21:32:41.067778: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:32:41.603045: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-01 21:32:41.606097: This epoch took 116.427082 s

2022-08-01 21:32:41.608456: 
epoch:  92
2022-08-01 21:34:29.712893: train loss : -0.6848
2022-08-01 21:34:36.420724: validation loss: -0.6578
2022-08-01 21:34:36.424164: Average global foreground Dice: [0.7372]
2022-08-01 21:34:36.426406: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:34:36.872727: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-01 21:34:36.874972: This epoch took 115.259934 s

2022-08-01 21:34:36.877035: 
epoch:  93
2022-08-01 21:36:26.056204: train loss : -0.7059
2022-08-01 21:36:33.144463: validation loss: -0.6277
2022-08-01 21:36:33.176133: Average global foreground Dice: [0.7048]
2022-08-01 21:36:33.208785: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:36:34.005243: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-01 21:36:34.029852: This epoch took 117.150589 s

2022-08-01 21:36:34.079752: 
epoch:  94
2022-08-01 21:38:22.721317: train loss : -0.6878
2022-08-01 21:38:30.945968: validation loss: -0.6787
2022-08-01 21:38:30.974577: Average global foreground Dice: [0.7544]
2022-08-01 21:38:31.002771: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:38:31.548657: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-01 21:38:31.560352: This epoch took 117.451436 s

2022-08-01 21:38:31.580775: 
epoch:  95
2022-08-01 21:40:25.669313: train loss : -0.6915
2022-08-01 21:40:33.135838: validation loss: -0.6651
2022-08-01 21:40:33.139452: Average global foreground Dice: [0.7471]
2022-08-01 21:40:33.141844: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:40:33.617443: Suus1 maybe_update_lr lr: 8.3e-05
2022-08-01 21:40:33.620251: This epoch took 122.028447 s

2022-08-01 21:40:33.622525: 
epoch:  96
2022-08-01 21:42:23.468186: train loss : -0.6842
2022-08-01 21:42:30.737803: validation loss: -0.6694
2022-08-01 21:42:30.767282: Average global foreground Dice: [0.7474]
2022-08-01 21:42:30.787605: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:42:31.689848: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-01 21:42:31.731032: saving best epoch checkpoint...
2022-08-01 21:42:31.983659: saving checkpoint...
2022-08-01 21:42:37.488401: done, saving took 5.72 seconds
2022-08-01 21:42:37.501502: This epoch took 123.876784 s

2022-08-01 21:42:37.504706: 
epoch:  97
2022-08-01 21:44:25.969753: train loss : -0.6926
2022-08-01 21:44:32.791874: validation loss: -0.6765
2022-08-01 21:44:32.795624: Average global foreground Dice: [0.7573]
2022-08-01 21:44:32.798108: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:44:33.262264: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-01 21:44:33.266053: saving best epoch checkpoint...
2022-08-01 21:44:33.474509: saving checkpoint...
2022-08-01 21:44:38.707379: done, saving took 5.44 seconds
2022-08-01 21:44:38.722379: This epoch took 121.215237 s

2022-08-01 21:44:38.724835: 
epoch:  98
2022-08-01 21:46:25.805917: train loss : -0.7024
2022-08-01 21:46:32.820636: validation loss: -0.6213
2022-08-01 21:46:32.825026: Average global foreground Dice: [0.7217]
2022-08-01 21:46:32.827508: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:46:33.314184: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-01 21:46:33.317316: This epoch took 114.590340 s

2022-08-01 21:46:33.321169: 
epoch:  99
2022-08-01 21:48:30.327923: train loss : -0.7080
2022-08-01 21:48:41.589219: validation loss: -0.6638
2022-08-01 21:48:41.618342: Average global foreground Dice: [0.7432]
2022-08-01 21:48:41.650788: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:48:42.391511: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-01 21:48:42.431849: saving scheduled checkpoint file...
2022-08-01 21:48:42.730044: saving checkpoint...
2022-08-01 21:48:48.612188: done, saving took 6.14 seconds
2022-08-01 21:48:48.630973: done
2022-08-01 21:48:48.632981: This epoch took 135.309501 s

2022-08-01 21:48:48.634961: 
epoch:  100
2022-08-01 21:50:36.285131: train loss : -0.7063
2022-08-01 21:50:44.048225: validation loss: -0.6921
2022-08-01 21:50:44.051583: Average global foreground Dice: [0.7678]
2022-08-01 21:50:44.054023: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:50:44.649496: Suus1 maybe_update_lr lr: 8.2e-05
2022-08-01 21:50:44.686966: saving best epoch checkpoint...
2022-08-01 21:50:45.344103: saving checkpoint...
2022-08-01 21:50:51.119256: done, saving took 6.41 seconds
2022-08-01 21:50:51.130436: This epoch took 122.493232 s

2022-08-01 21:50:51.133243: 
epoch:  101
2022-08-01 21:52:39.096704: train loss : -0.7012
2022-08-01 21:52:46.487003: validation loss: -0.6682
2022-08-01 21:52:46.492013: Average global foreground Dice: [0.7506]
2022-08-01 21:52:46.494459: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:52:47.130290: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-01 21:52:47.132599: saving best epoch checkpoint...
2022-08-01 21:52:47.335349: saving checkpoint...
2022-08-01 21:52:52.541468: done, saving took 5.40 seconds
2022-08-01 21:52:52.550288: This epoch took 121.414895 s

2022-08-01 21:52:52.552404: 
epoch:  102
2022-08-01 21:54:40.890376: train loss : -0.7040
2022-08-01 21:54:47.891661: validation loss: -0.6810
2022-08-01 21:54:47.923163: Average global foreground Dice: [0.7588]
2022-08-01 21:54:47.957775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:54:48.547290: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-01 21:54:48.571317: saving best epoch checkpoint...
2022-08-01 21:54:48.808571: saving checkpoint...
2022-08-01 21:54:54.304643: done, saving took 5.72 seconds
2022-08-01 21:54:54.317600: This epoch took 121.763057 s

2022-08-01 21:54:54.320034: 
epoch:  103
2022-08-01 21:56:42.166863: train loss : -0.7052
2022-08-01 21:56:53.330887: validation loss: -0.6801
2022-08-01 21:56:53.336239: Average global foreground Dice: [0.75]
2022-08-01 21:56:53.338774: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:56:53.870076: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-01 21:56:53.902824: saving best epoch checkpoint...
2022-08-01 21:56:54.185086: saving checkpoint...
2022-08-01 21:57:00.006089: done, saving took 6.07 seconds
2022-08-01 21:57:00.019281: This epoch took 125.696754 s

2022-08-01 21:57:00.021875: 
epoch:  104
2022-08-01 21:58:46.464482: train loss : -0.7043
2022-08-01 21:58:53.604807: validation loss: -0.6674
2022-08-01 21:58:53.608116: Average global foreground Dice: [0.7537]
2022-08-01 21:58:53.610341: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 21:58:54.150663: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-01 21:58:54.153428: saving best epoch checkpoint...
2022-08-01 21:58:54.400120: saving checkpoint...
2022-08-01 21:58:59.460221: done, saving took 5.30 seconds
2022-08-01 21:58:59.474616: This epoch took 119.450782 s

2022-08-01 21:58:59.477256: 
epoch:  105
2022-08-01 22:00:49.382224: train loss : -0.6981
2022-08-01 22:00:57.406098: validation loss: -0.6540
2022-08-01 22:00:57.428206: Average global foreground Dice: [0.7405]
2022-08-01 22:00:57.471779: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:00:58.361493: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-01 22:00:58.363872: This epoch took 118.883980 s

2022-08-01 22:00:58.375256: 
epoch:  106
2022-08-01 22:02:45.232615: train loss : -0.7049
2022-08-01 22:02:53.593350: validation loss: -0.6727
2022-08-01 22:02:53.636375: Average global foreground Dice: [0.7505]
2022-08-01 22:02:53.669759: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:02:54.340501: Suus1 maybe_update_lr lr: 8.1e-05
2022-08-01 22:02:54.344769: saving best epoch checkpoint...
2022-08-01 22:02:54.601926: saving checkpoint...
2022-08-01 22:03:00.107067: done, saving took 5.76 seconds
2022-08-01 22:03:00.122108: This epoch took 121.731334 s

2022-08-01 22:03:00.124479: 
epoch:  107
2022-08-01 22:04:50.366310: train loss : -0.7182
2022-08-01 22:04:58.702248: validation loss: -0.6642
2022-08-01 22:04:58.729013: Average global foreground Dice: [0.7448]
2022-08-01 22:04:58.745468: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:04:59.524543: Suus1 maybe_update_lr lr: 8e-05
2022-08-01 22:04:59.527185: saving best epoch checkpoint...
2022-08-01 22:04:59.870122: saving checkpoint...
2022-08-01 22:05:05.048693: done, saving took 5.52 seconds
2022-08-01 22:05:05.063735: This epoch took 124.937338 s

2022-08-01 22:05:05.066502: 
epoch:  108
2022-08-01 22:06:54.800247: train loss : -0.7036
2022-08-01 22:07:02.844748: validation loss: -0.7021
2022-08-01 22:07:02.886188: Average global foreground Dice: [0.7753]
2022-08-01 22:07:02.917836: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:07:03.537584: Suus1 maybe_update_lr lr: 8e-05
2022-08-01 22:07:03.573857: saving best epoch checkpoint...
2022-08-01 22:07:03.950492: saving checkpoint...
2022-08-01 22:07:09.812318: done, saving took 6.22 seconds
2022-08-01 22:07:09.828905: This epoch took 124.760214 s

2022-08-01 22:07:09.831463: 
epoch:  109
2022-08-01 22:09:02.142943: train loss : -0.7127
2022-08-01 22:09:12.412619: validation loss: -0.6693
2022-08-01 22:09:12.454178: Average global foreground Dice: [0.75]
2022-08-01 22:09:12.467773: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:09:13.330810: Suus1 maybe_update_lr lr: 8e-05
2022-08-01 22:09:13.356528: saving best epoch checkpoint...
2022-08-01 22:09:13.596001: saving checkpoint...
2022-08-01 22:09:19.407312: done, saving took 6.02 seconds
2022-08-01 22:09:19.422911: This epoch took 129.589114 s

2022-08-01 22:09:19.425516: 
epoch:  110
2022-08-01 22:11:05.870777: train loss : -0.7153
2022-08-01 22:11:12.861185: validation loss: -0.6750
2022-08-01 22:11:12.865155: Average global foreground Dice: [0.7517]
2022-08-01 22:11:12.870876: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:11:13.393771: Suus1 maybe_update_lr lr: 8e-05
2022-08-01 22:11:13.397073: saving best epoch checkpoint...
2022-08-01 22:11:13.587344: saving checkpoint...
2022-08-01 22:11:18.896912: done, saving took 5.50 seconds
2022-08-01 22:11:18.908020: This epoch took 119.480231 s

2022-08-01 22:11:18.910257: 
epoch:  111
2022-08-01 22:13:07.759910: train loss : -0.7097
2022-08-01 22:13:15.204232: validation loss: -0.6847
2022-08-01 22:13:15.209151: Average global foreground Dice: [0.7598]
2022-08-01 22:13:15.227166: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:13:15.813093: Suus1 maybe_update_lr lr: 8e-05
2022-08-01 22:13:15.840811: saving best epoch checkpoint...
2022-08-01 22:13:16.123871: saving checkpoint...
2022-08-01 22:13:21.445873: done, saving took 5.59 seconds
2022-08-01 22:13:21.456493: This epoch took 122.543991 s

2022-08-01 22:13:21.458492: 
epoch:  112
2022-08-01 22:15:07.754353: train loss : -0.7111
2022-08-01 22:15:14.076057: validation loss: -0.6716
2022-08-01 22:15:14.080380: Average global foreground Dice: [0.7505]
2022-08-01 22:15:14.082936: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:15:14.536979: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-01 22:15:14.539525: saving best epoch checkpoint...
2022-08-01 22:15:14.675143: saving checkpoint...
2022-08-01 22:15:19.768126: done, saving took 5.23 seconds
2022-08-01 22:15:19.781225: This epoch took 118.320576 s

2022-08-01 22:15:19.783596: 
epoch:  113
2022-08-01 22:17:07.710485: train loss : -0.7091
2022-08-01 22:17:14.474370: validation loss: -0.6733
2022-08-01 22:17:14.478261: Average global foreground Dice: [0.7528]
2022-08-01 22:17:14.480771: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:17:14.955376: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-01 22:17:14.958114: saving best epoch checkpoint...
2022-08-01 22:17:15.111168: saving checkpoint...
2022-08-01 22:17:20.166661: done, saving took 5.21 seconds
2022-08-01 22:17:20.179444: This epoch took 120.393457 s

2022-08-01 22:17:20.181934: 
epoch:  114
2022-08-01 22:19:10.058614: train loss : -0.7056
2022-08-01 22:19:17.768041: validation loss: -0.6817
2022-08-01 22:19:17.793558: Average global foreground Dice: [0.7561]
2022-08-01 22:19:17.830798: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:19:18.644261: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-01 22:19:18.668834: saving best epoch checkpoint...
2022-08-01 22:19:18.982457: saving checkpoint...
2022-08-01 22:19:24.359976: done, saving took 5.68 seconds
2022-08-01 22:19:24.373910: This epoch took 124.189613 s

2022-08-01 22:19:24.376322: 
epoch:  115
2022-08-01 22:21:12.764390: train loss : -0.7289
2022-08-01 22:21:22.159228: validation loss: -0.6934
2022-08-01 22:21:22.179384: Average global foreground Dice: [0.7695]
2022-08-01 22:21:22.212049: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:21:22.996595: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-01 22:21:23.023765: saving best epoch checkpoint...
2022-08-01 22:21:23.300613: saving checkpoint...
2022-08-01 22:21:28.325836: done, saving took 5.27 seconds
2022-08-01 22:21:28.340444: This epoch took 123.961706 s

2022-08-01 22:21:28.342884: 
epoch:  116
2022-08-01 22:23:16.283090: train loss : -0.7274
2022-08-01 22:23:26.478045: validation loss: -0.6879
2022-08-01 22:23:26.482058: Average global foreground Dice: [0.7665]
2022-08-01 22:23:26.484722: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:23:27.046840: Suus1 maybe_update_lr lr: 7.9e-05
2022-08-01 22:23:27.083869: saving best epoch checkpoint...
2022-08-01 22:23:27.353419: saving checkpoint...
2022-08-01 22:23:32.895363: done, saving took 5.80 seconds
2022-08-01 22:23:32.910211: This epoch took 124.565232 s

2022-08-01 22:23:32.912306: 
epoch:  117
2022-08-01 22:25:18.251739: train loss : -0.7196
2022-08-01 22:25:26.675770: validation loss: -0.6935
2022-08-01 22:25:26.682518: Average global foreground Dice: [0.7767]
2022-08-01 22:25:26.686230: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:25:27.205287: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-01 22:25:27.208456: saving best epoch checkpoint...
2022-08-01 22:25:27.437163: saving checkpoint...
2022-08-01 22:25:32.443174: done, saving took 5.23 seconds
2022-08-01 22:25:32.458076: This epoch took 119.543674 s

2022-08-01 22:25:32.460050: 
epoch:  118
2022-08-01 22:27:21.837032: train loss : -0.7052
2022-08-01 22:27:29.448891: validation loss: -0.6822
2022-08-01 22:27:29.452963: Average global foreground Dice: [0.7582]
2022-08-01 22:27:29.455651: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:27:29.983384: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-01 22:27:29.994678: saving best epoch checkpoint...
2022-08-01 22:27:30.349779: saving checkpoint...
2022-08-01 22:27:35.479718: done, saving took 5.48 seconds
2022-08-01 22:27:35.494555: This epoch took 123.032449 s

2022-08-01 22:27:35.496617: 
epoch:  119
2022-08-01 22:29:23.501008: train loss : -0.7238
2022-08-01 22:29:33.290174: validation loss: -0.6816
2022-08-01 22:29:33.342390: Average global foreground Dice: [0.7605]
2022-08-01 22:29:33.382185: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:29:34.193905: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-01 22:29:34.211620: saving best epoch checkpoint...
2022-08-01 22:29:34.725717: saving checkpoint...
2022-08-01 22:29:40.842394: done, saving took 6.61 seconds
2022-08-01 22:29:40.860420: This epoch took 125.361776 s

2022-08-01 22:29:40.862705: 
epoch:  120
2022-08-01 22:31:28.981106: train loss : -0.7187
2022-08-01 22:31:38.291347: validation loss: -0.6672
2022-08-01 22:31:38.319307: Average global foreground Dice: [0.7479]
2022-08-01 22:31:38.350229: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:31:39.120030: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-01 22:31:39.137633: This epoch took 118.272700 s

2022-08-01 22:31:39.142318: 
epoch:  121
2022-08-01 22:33:31.770195: train loss : -0.7291
2022-08-01 22:33:43.526253: validation loss: -0.7050
2022-08-01 22:33:43.558070: Average global foreground Dice: [0.7782]
2022-08-01 22:33:43.578154: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:33:44.392736: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-01 22:33:44.396589: saving best epoch checkpoint...
2022-08-01 22:33:44.696292: saving checkpoint...
2022-08-01 22:33:50.531717: done, saving took 6.08 seconds
2022-08-01 22:33:50.544165: This epoch took 131.361372 s

2022-08-01 22:33:50.546361: 
epoch:  122
2022-08-01 22:35:41.445233: train loss : -0.7032
2022-08-01 22:35:50.291870: validation loss: -0.6692
2022-08-01 22:35:50.324507: Average global foreground Dice: [0.7503]
2022-08-01 22:35:50.333712: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:35:51.206627: Suus1 maybe_update_lr lr: 7.8e-05
2022-08-01 22:35:51.236035: This epoch took 120.687451 s

2022-08-01 22:35:51.267820: 
epoch:  123
2022-08-01 22:37:39.706941: train loss : -0.7194
2022-08-01 22:37:49.980091: validation loss: -0.6812
2022-08-01 22:37:50.047385: Average global foreground Dice: [0.7655]
2022-08-01 22:37:50.080784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:37:50.835227: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-01 22:37:50.875799: saving best epoch checkpoint...
2022-08-01 22:37:51.125892: saving checkpoint...
2022-08-01 22:37:57.027093: done, saving took 6.12 seconds
2022-08-01 22:37:57.041139: This epoch took 125.738374 s

2022-08-01 22:37:57.043730: 
epoch:  124
2022-08-01 22:39:43.740610: train loss : -0.7346
2022-08-01 22:39:50.640704: validation loss: -0.6736
2022-08-01 22:39:50.644085: Average global foreground Dice: [0.7451]
2022-08-01 22:39:50.646381: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:39:51.164340: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-01 22:39:51.167938: This epoch took 114.121811 s

2022-08-01 22:39:51.170046: 
epoch:  125
2022-08-01 22:41:37.739350: train loss : -0.7364
2022-08-01 22:41:45.088037: validation loss: -0.6991
2022-08-01 22:41:45.099629: Average global foreground Dice: [0.7723]
2022-08-01 22:41:45.106188: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:41:45.659495: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-01 22:41:45.664289: saving best epoch checkpoint...
2022-08-01 22:41:45.901608: saving checkpoint...
2022-08-01 22:41:50.880802: done, saving took 5.21 seconds
2022-08-01 22:41:50.895714: This epoch took 119.722570 s

2022-08-01 22:41:50.897862: 
epoch:  126
2022-08-01 22:43:41.224466: train loss : -0.7345
2022-08-01 22:43:52.009259: validation loss: -0.7158
2022-08-01 22:43:52.041247: Average global foreground Dice: [0.7866]
2022-08-01 22:43:52.047900: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:43:52.561596: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-01 22:43:52.625380: saving best epoch checkpoint...
2022-08-01 22:43:53.026074: saving checkpoint...
2022-08-01 22:43:58.536151: done, saving took 5.90 seconds
2022-08-01 22:43:58.550656: This epoch took 127.650721 s

2022-08-01 22:43:58.553009: 
epoch:  127
2022-08-01 22:45:46.540855: train loss : -0.7196
2022-08-01 22:45:54.191354: validation loss: -0.7178
2022-08-01 22:45:54.196080: Average global foreground Dice: [0.7897]
2022-08-01 22:45:54.198933: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:45:54.692072: Suus1 maybe_update_lr lr: 7.7e-05
2022-08-01 22:45:54.694790: saving best epoch checkpoint...
2022-08-01 22:45:54.896887: saving checkpoint...
2022-08-01 22:46:00.174454: done, saving took 5.48 seconds
2022-08-01 22:46:00.189705: This epoch took 121.634627 s

2022-08-01 22:46:00.192567: 
epoch:  128
2022-08-01 22:47:51.479034: train loss : -0.7220
2022-08-01 22:48:00.515411: validation loss: -0.6844
2022-08-01 22:48:00.573324: Average global foreground Dice: [0.7613]
2022-08-01 22:48:00.594787: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:48:01.325381: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-01 22:48:01.344991: This epoch took 121.150239 s

2022-08-01 22:48:01.349180: 
epoch:  129
2022-08-01 22:49:51.053681: train loss : -0.7324
2022-08-01 22:50:00.802163: validation loss: -0.6682
2022-08-01 22:50:00.834577: Average global foreground Dice: [0.7452]
2022-08-01 22:50:00.862772: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:50:01.563609: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-01 22:50:01.568000: This epoch took 120.180731 s

2022-08-01 22:50:01.570266: 
epoch:  130
2022-08-01 22:51:49.307726: train loss : -0.7289
2022-08-01 22:51:59.272860: validation loss: -0.6719
2022-08-01 22:51:59.342699: Average global foreground Dice: [0.7458]
2022-08-01 22:51:59.366094: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:52:00.073715: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-01 22:52:00.089951: This epoch took 118.517341 s

2022-08-01 22:52:00.092402: 
epoch:  131
2022-08-01 22:53:50.341373: train loss : -0.7143
2022-08-01 22:53:58.212708: validation loss: -0.6773
2022-08-01 22:53:58.217106: Average global foreground Dice: [0.7517]
2022-08-01 22:53:58.219528: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:53:58.763156: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-01 22:53:58.766157: This epoch took 118.671582 s

2022-08-01 22:53:58.768618: 
epoch:  132
2022-08-01 22:55:50.789341: train loss : -0.7144
2022-08-01 22:56:00.143936: validation loss: -0.6099
2022-08-01 22:56:00.178661: Average global foreground Dice: [0.707]
2022-08-01 22:56:00.213018: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:56:01.192433: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-01 22:56:01.195279: This epoch took 122.423961 s

2022-08-01 22:56:01.197344: 
epoch:  133
2022-08-01 22:57:49.135237: train loss : -0.7368
2022-08-01 22:57:56.923812: validation loss: -0.6973
2022-08-01 22:57:56.927106: Average global foreground Dice: [0.7758]
2022-08-01 22:57:56.929538: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:57:57.448955: Suus1 maybe_update_lr lr: 7.6e-05
2022-08-01 22:57:57.453717: This epoch took 116.254326 s

2022-08-01 22:57:57.455996: 
epoch:  134
2022-08-01 22:59:46.950286: train loss : -0.7326
2022-08-01 22:59:53.839390: validation loss: -0.6879
2022-08-01 22:59:53.842994: Average global foreground Dice: [0.7692]
2022-08-01 22:59:53.845317: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 22:59:54.332810: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-01 22:59:54.336366: This epoch took 116.873729 s

2022-08-01 22:59:54.339436: 
epoch:  135
2022-08-01 23:01:42.879437: train loss : -0.7375
2022-08-01 23:01:50.539320: validation loss: -0.6625
2022-08-01 23:01:50.576783: Average global foreground Dice: [0.7474]
2022-08-01 23:01:50.609796: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:01:51.284532: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-01 23:01:51.309120: This epoch took 116.967289 s

2022-08-01 23:01:51.328588: 
epoch:  136
2022-08-01 23:03:40.798803: train loss : -0.7405
2022-08-01 23:03:48.839108: validation loss: -0.7117
2022-08-01 23:03:48.842585: Average global foreground Dice: [0.7799]
2022-08-01 23:03:48.845119: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:03:49.312241: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-01 23:03:49.314615: This epoch took 117.971832 s

2022-08-01 23:03:49.316785: 
epoch:  137
2022-08-01 23:05:38.473820: train loss : -0.7475
2022-08-01 23:05:45.595017: validation loss: -0.7070
2022-08-01 23:05:45.620884: Average global foreground Dice: [0.7839]
2022-08-01 23:05:45.641284: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:05:46.349645: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-01 23:05:46.352082: This epoch took 117.033247 s

2022-08-01 23:05:46.354246: 
epoch:  138
2022-08-01 23:07:33.994293: train loss : -0.7436
2022-08-01 23:07:41.204715: validation loss: -0.6610
2022-08-01 23:07:41.210724: Average global foreground Dice: [0.7408]
2022-08-01 23:07:41.213742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:07:41.747535: Suus1 maybe_update_lr lr: 7.5e-05
2022-08-01 23:07:41.749972: This epoch took 115.393489 s

2022-08-01 23:07:41.752077: 
epoch:  139
2022-08-01 23:09:29.721996: train loss : -0.7377
2022-08-01 23:09:37.514662: validation loss: -0.7060
2022-08-01 23:09:37.552740: Average global foreground Dice: [0.7828]
2022-08-01 23:09:37.582314: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:09:38.133121: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-01 23:09:38.141859: This epoch took 116.387727 s

2022-08-01 23:09:38.164774: 
epoch:  140
2022-08-01 23:11:27.788421: train loss : -0.7545
2022-08-01 23:11:35.309991: validation loss: -0.7223
2022-08-01 23:11:35.312980: Average global foreground Dice: [0.786]
2022-08-01 23:11:35.315180: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:11:35.817011: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-01 23:11:35.819609: saving best epoch checkpoint...
2022-08-01 23:11:35.982234: saving checkpoint...
2022-08-01 23:11:41.130883: done, saving took 5.31 seconds
2022-08-01 23:11:41.142759: This epoch took 122.974038 s

2022-08-01 23:11:41.144900: 
epoch:  141
2022-08-01 23:13:29.618494: train loss : -0.7410
2022-08-01 23:13:38.662292: validation loss: -0.7041
2022-08-01 23:13:38.671604: Average global foreground Dice: [0.7749]
2022-08-01 23:13:38.674022: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:13:39.316353: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-01 23:13:39.318971: saving best epoch checkpoint...
2022-08-01 23:13:39.486943: saving checkpoint...
2022-08-01 23:13:44.644308: done, saving took 5.32 seconds
2022-08-01 23:13:44.657256: This epoch took 123.510059 s

2022-08-01 23:13:44.659804: 
epoch:  142
2022-08-01 23:15:33.323611: train loss : -0.7466
2022-08-01 23:15:41.039804: validation loss: -0.7045
2022-08-01 23:15:41.076271: Average global foreground Dice: [0.7715]
2022-08-01 23:15:41.109451: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:15:41.801728: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-01 23:15:41.819450: saving best epoch checkpoint...
2022-08-01 23:15:42.062506: saving checkpoint...
2022-08-01 23:15:47.367278: done, saving took 5.51 seconds
2022-08-01 23:15:47.379153: This epoch took 122.717013 s

2022-08-01 23:15:47.381646: 
epoch:  143
2022-08-01 23:17:35.408506: train loss : -0.7411
2022-08-01 23:17:42.692812: validation loss: -0.7017
2022-08-01 23:17:42.696675: Average global foreground Dice: [0.7682]
2022-08-01 23:17:42.699010: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:17:43.167992: Suus1 maybe_update_lr lr: 7.4e-05
2022-08-01 23:17:43.170674: saving best epoch checkpoint...
2022-08-01 23:17:43.340512: saving checkpoint...
2022-08-01 23:17:48.536261: done, saving took 5.36 seconds
2022-08-01 23:17:48.550051: This epoch took 121.165973 s

2022-08-01 23:17:48.552230: 
epoch:  144
2022-08-01 23:19:37.905207: train loss : -0.7366
2022-08-01 23:19:45.713929: validation loss: -0.7025
2022-08-01 23:19:45.759452: Average global foreground Dice: [0.7704]
2022-08-01 23:19:45.798785: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:19:46.464763: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-01 23:19:46.493978: saving best epoch checkpoint...
2022-08-01 23:19:46.734784: saving checkpoint...
2022-08-01 23:19:52.039985: done, saving took 5.52 seconds
2022-08-01 23:19:52.058636: This epoch took 123.504138 s

2022-08-01 23:19:52.060840: 
epoch:  145
2022-08-01 23:21:38.626827: train loss : -0.7424
2022-08-01 23:21:46.131698: validation loss: -0.6729
2022-08-01 23:21:46.140587: Average global foreground Dice: [0.742]
2022-08-01 23:21:46.143270: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:21:46.756536: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-01 23:21:46.759795: This epoch took 114.696999 s

2022-08-01 23:21:46.762986: 
epoch:  146
2022-08-01 23:23:35.265551: train loss : -0.7492
2022-08-01 23:23:44.092400: validation loss: -0.7061
2022-08-01 23:23:44.097015: Average global foreground Dice: [0.777]
2022-08-01 23:23:44.099351: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:23:44.735292: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-01 23:23:44.772834: This epoch took 118.006444 s

2022-08-01 23:23:44.788774: 
epoch:  147
2022-08-01 23:25:35.382862: train loss : -0.7454
2022-08-01 23:25:42.719717: validation loss: -0.7043
2022-08-01 23:25:42.729266: Average global foreground Dice: [0.7731]
2022-08-01 23:25:42.756775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:25:43.326087: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-01 23:25:43.359811: This epoch took 118.566557 s

2022-08-01 23:25:43.381848: 
epoch:  148
2022-08-01 23:27:31.752411: train loss : -0.7479
2022-08-01 23:27:40.424869: validation loss: -0.7032
2022-08-01 23:27:40.470458: Average global foreground Dice: [0.7811]
2022-08-01 23:27:40.501789: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:27:41.075447: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-01 23:27:41.105977: saving best epoch checkpoint...
2022-08-01 23:27:41.420018: saving checkpoint...
2022-08-01 23:27:46.414503: done, saving took 5.27 seconds
2022-08-01 23:27:46.431783: This epoch took 123.045420 s

2022-08-01 23:27:46.434127: 
epoch:  149
2022-08-01 23:29:36.706143: train loss : -0.7490
2022-08-01 23:29:45.626402: validation loss: -0.6777
2022-08-01 23:29:45.663193: Average global foreground Dice: [0.7535]
2022-08-01 23:29:45.702793: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:29:46.559321: Suus1 maybe_update_lr lr: 7.3e-05
2022-08-01 23:29:46.585025: saving scheduled checkpoint file...
2022-08-01 23:29:46.991653: saving checkpoint...
2022-08-01 23:29:52.695176: done, saving took 6.09 seconds
2022-08-01 23:29:52.715826: done
2022-08-01 23:29:52.718618: This epoch took 126.282575 s

2022-08-01 23:29:52.720598: 
epoch:  150
2022-08-01 23:31:44.071045: train loss : -0.7427
2022-08-01 23:31:51.913917: validation loss: -0.7285
2022-08-01 23:31:51.950335: Average global foreground Dice: [0.8002]
2022-08-01 23:31:51.984792: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:31:52.682267: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-01 23:31:52.739816: saving best epoch checkpoint...
2022-08-01 23:31:53.045793: saving checkpoint...
2022-08-01 23:31:58.932893: done, saving took 6.13 seconds
2022-08-01 23:31:58.946798: This epoch took 126.223966 s

2022-08-01 23:31:58.949086: 
epoch:  151
2022-08-01 23:33:44.798409: train loss : -0.7484
2022-08-01 23:33:52.971509: validation loss: -0.7072
2022-08-01 23:33:52.982197: Average global foreground Dice: [0.7788]
2022-08-01 23:33:52.984646: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:33:53.509153: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-01 23:33:53.511846: saving best epoch checkpoint...
2022-08-01 23:33:53.683567: saving checkpoint...
2022-08-01 23:33:58.658337: done, saving took 5.14 seconds
2022-08-01 23:33:58.670728: This epoch took 119.719487 s

2022-08-01 23:33:58.672889: 
epoch:  152
2022-08-01 23:35:48.498673: train loss : -0.7389
2022-08-01 23:35:56.756129: validation loss: -0.7143
2022-08-01 23:35:56.786347: Average global foreground Dice: [0.7886]
2022-08-01 23:35:56.817806: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:35:57.464313: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-01 23:35:57.466716: saving best epoch checkpoint...
2022-08-01 23:35:57.614484: saving checkpoint...
2022-08-01 23:36:02.783648: done, saving took 5.31 seconds
2022-08-01 23:36:02.796136: This epoch took 124.121142 s

2022-08-01 23:36:02.798326: 
epoch:  153
2022-08-01 23:37:52.288106: train loss : -0.7484
2022-08-01 23:37:59.602172: validation loss: -0.6830
2022-08-01 23:37:59.624950: Average global foreground Dice: [0.7565]
2022-08-01 23:37:59.649469: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:38:00.406238: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-01 23:38:00.427542: This epoch took 117.627189 s

2022-08-01 23:38:00.439745: 
epoch:  154
2022-08-01 23:39:51.330368: train loss : -0.7424
2022-08-01 23:39:59.155784: validation loss: -0.7190
2022-08-01 23:39:59.165178: Average global foreground Dice: [0.7892]
2022-08-01 23:39:59.170752: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:39:59.699328: Suus1 maybe_update_lr lr: 7.2e-05
2022-08-01 23:39:59.713955: saving best epoch checkpoint...
2022-08-01 23:40:00.000480: saving checkpoint...
2022-08-01 23:40:05.095644: done, saving took 5.35 seconds
2022-08-01 23:40:05.111026: This epoch took 124.645594 s

2022-08-01 23:40:05.113258: 
epoch:  155
2022-08-01 23:41:52.315883: train loss : -0.7480
2022-08-01 23:42:01.077878: validation loss: -0.6885
2022-08-01 23:42:01.100062: Average global foreground Dice: [0.7691]
2022-08-01 23:42:01.119519: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:42:01.667502: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-01 23:42:01.691839: This epoch took 116.576296 s

2022-08-01 23:42:01.721159: 
epoch:  156
2022-08-01 23:43:52.735861: train loss : -0.7604
2022-08-01 23:44:00.858901: validation loss: -0.7197
2022-08-01 23:44:00.901361: Average global foreground Dice: [0.7866]
2022-08-01 23:44:00.905933: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:44:01.489352: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-01 23:44:01.493719: saving best epoch checkpoint...
2022-08-01 23:44:01.807774: saving checkpoint...
2022-08-01 23:44:07.061700: done, saving took 5.53 seconds
2022-08-01 23:44:07.078228: This epoch took 125.335856 s

2022-08-01 23:44:07.080195: 
epoch:  157
2022-08-01 23:45:59.099132: train loss : -0.7524
2022-08-01 23:46:06.913599: validation loss: -0.6922
2022-08-01 23:46:06.916858: Average global foreground Dice: [0.7707]
2022-08-01 23:46:06.919152: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:46:07.414124: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-01 23:46:07.416956: This epoch took 120.334550 s

2022-08-01 23:46:07.420011: 
epoch:  158
2022-08-01 23:47:56.877970: train loss : -0.7594
2022-08-01 23:48:03.972339: validation loss: -0.6845
2022-08-01 23:48:03.976861: Average global foreground Dice: [0.7632]
2022-08-01 23:48:03.979414: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:48:04.658782: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-01 23:48:04.686044: This epoch took 117.263904 s

2022-08-01 23:48:04.718781: 
epoch:  159
2022-08-01 23:49:51.847478: train loss : -0.7629
2022-08-01 23:49:59.208560: validation loss: -0.7286
2022-08-01 23:49:59.234488: Average global foreground Dice: [0.7975]
2022-08-01 23:49:59.238677: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:49:59.773222: Suus1 maybe_update_lr lr: 7.1e-05
2022-08-01 23:49:59.775789: saving best epoch checkpoint...
2022-08-01 23:50:00.043882: saving checkpoint...
2022-08-01 23:50:05.169118: done, saving took 5.39 seconds
2022-08-01 23:50:05.187125: This epoch took 120.438200 s

2022-08-01 23:50:05.189642: 
epoch:  160
2022-08-01 23:51:54.759657: train loss : -0.7493
2022-08-01 23:52:03.096441: validation loss: -0.7166
2022-08-01 23:52:03.119760: Average global foreground Dice: [0.7818]
2022-08-01 23:52:03.143802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:52:03.747964: Suus1 maybe_update_lr lr: 7e-05
2022-08-01 23:52:03.777959: saving best epoch checkpoint...
2022-08-01 23:52:03.950975: saving checkpoint...
2022-08-01 23:52:09.036333: done, saving took 5.24 seconds
2022-08-01 23:52:09.048355: This epoch took 123.856373 s

2022-08-01 23:52:09.050297: 
epoch:  161
2022-08-01 23:53:58.236147: train loss : -0.7567
2022-08-01 23:54:05.917840: validation loss: -0.7050
2022-08-01 23:54:05.967313: Average global foreground Dice: [0.7761]
2022-08-01 23:54:06.002849: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:54:06.660734: Suus1 maybe_update_lr lr: 7e-05
2022-08-01 23:54:06.686863: saving best epoch checkpoint...
2022-08-01 23:54:06.884202: saving checkpoint...
2022-08-01 23:54:12.554877: done, saving took 5.85 seconds
2022-08-01 23:54:12.565952: This epoch took 123.513762 s

2022-08-01 23:54:12.568119: 
epoch:  162
2022-08-01 23:56:02.158258: train loss : -0.7634
2022-08-01 23:56:09.375095: validation loss: -0.7281
2022-08-01 23:56:09.378680: Average global foreground Dice: [0.7948]
2022-08-01 23:56:09.380932: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:56:09.947496: Suus1 maybe_update_lr lr: 7e-05
2022-08-01 23:56:09.949872: saving best epoch checkpoint...
2022-08-01 23:56:10.173677: saving checkpoint...
2022-08-01 23:56:15.267350: done, saving took 5.32 seconds
2022-08-01 23:56:15.281281: This epoch took 122.710986 s

2022-08-01 23:56:15.283638: 
epoch:  163
2022-08-01 23:58:06.616245: train loss : -0.7434
2022-08-01 23:58:13.935756: validation loss: -0.7220
2022-08-01 23:58:13.969194: Average global foreground Dice: [0.7913]
2022-08-01 23:58:13.996761: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-01 23:58:14.606003: Suus1 maybe_update_lr lr: 7e-05
2022-08-01 23:58:14.650867: saving best epoch checkpoint...
2022-08-01 23:58:14.935591: saving checkpoint...
2022-08-01 23:58:20.392063: done, saving took 5.71 seconds
2022-08-01 23:58:20.404577: This epoch took 125.118837 s

2022-08-01 23:58:20.406760: 
epoch:  164
2022-08-02 00:00:09.228184: train loss : -0.7574
2022-08-02 00:00:17.514558: validation loss: -0.7020
2022-08-02 00:00:17.533767: Average global foreground Dice: [0.7734]
2022-08-02 00:00:17.536066: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:00:18.088351: Suus1 maybe_update_lr lr: 7e-05
2022-08-02 00:00:18.102416: This epoch took 117.693424 s

2022-08-02 00:00:18.104654: 
epoch:  165
2022-08-02 00:02:06.272039: train loss : -0.7517
2022-08-02 00:02:18.082991: validation loss: -0.7279
2022-08-02 00:02:18.154499: Average global foreground Dice: [0.7986]
2022-08-02 00:02:18.184497: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:02:18.997773: Suus1 maybe_update_lr lr: 7e-05
2022-08-02 00:02:19.034987: saving best epoch checkpoint...
2022-08-02 00:02:19.339405: saving checkpoint...
2022-08-02 00:02:24.684999: done, saving took 5.62 seconds
2022-08-02 00:02:24.701429: This epoch took 126.593338 s

2022-08-02 00:02:24.704294: 
epoch:  166
2022-08-02 00:04:12.230987: train loss : -0.7456
2022-08-02 00:04:19.403220: validation loss: -0.7341
2022-08-02 00:04:19.435453: Average global foreground Dice: [0.7984]
2022-08-02 00:04:19.460537: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:04:20.353121: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-02 00:04:20.358843: saving best epoch checkpoint...
2022-08-02 00:04:20.671048: saving checkpoint...
2022-08-02 00:04:25.874800: done, saving took 5.51 seconds
2022-08-02 00:04:25.888412: This epoch took 121.182076 s

2022-08-02 00:04:25.890339: 
epoch:  167
2022-08-02 00:06:15.260094: train loss : -0.7565
2022-08-02 00:06:23.882432: validation loss: -0.6927
2022-08-02 00:06:23.916555: Average global foreground Dice: [0.7644]
2022-08-02 00:06:23.936640: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:06:24.583874: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-02 00:06:24.615845: This epoch took 118.723285 s

2022-08-02 00:06:24.649773: 
epoch:  168
2022-08-02 00:08:14.620432: train loss : -0.7549
2022-08-02 00:08:23.722940: validation loss: -0.7090
2022-08-02 00:08:23.726290: Average global foreground Dice: [0.7786]
2022-08-02 00:08:23.728724: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:08:24.378326: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-02 00:08:24.386250: This epoch took 119.700483 s

2022-08-02 00:08:24.390511: 
epoch:  169
2022-08-02 00:10:12.451160: train loss : -0.7492
2022-08-02 00:10:19.700157: validation loss: -0.7142
2022-08-02 00:10:19.706756: Average global foreground Dice: [0.7887]
2022-08-02 00:10:19.709205: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:10:20.221601: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-02 00:10:20.224484: This epoch took 115.831336 s

2022-08-02 00:10:20.226657: 
epoch:  170
2022-08-02 00:12:11.882719: train loss : -0.7552
2022-08-02 00:12:21.612737: validation loss: -0.7099
2022-08-02 00:12:21.648323: Average global foreground Dice: [0.7838]
2022-08-02 00:12:21.690661: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:12:22.334572: Suus1 maybe_update_lr lr: 6.9e-05
2022-08-02 00:12:22.377813: This epoch took 122.149083 s

2022-08-02 00:12:22.399784: 
epoch:  171
2022-08-02 00:14:11.196282: train loss : -0.7590
2022-08-02 00:14:19.276980: validation loss: -0.7088
2022-08-02 00:14:19.308399: Average global foreground Dice: [0.7732]
2022-08-02 00:14:19.353888: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:14:20.075480: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-02 00:14:20.086627: This epoch took 117.661848 s

2022-08-02 00:14:20.089075: 
epoch:  172
2022-08-02 00:16:14.053535: train loss : -0.7562
2022-08-02 00:16:22.693085: validation loss: -0.7188
2022-08-02 00:16:22.735428: Average global foreground Dice: [0.7838]
2022-08-02 00:16:22.752800: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:16:23.440901: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-02 00:16:23.476194: This epoch took 123.369630 s

2022-08-02 00:16:23.513896: 
epoch:  173
2022-08-02 00:18:11.321190: train loss : -0.7635
2022-08-02 00:18:18.480177: validation loss: -0.7105
2022-08-02 00:18:18.501342: Average global foreground Dice: [0.7818]
2022-08-02 00:18:18.520784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:18:19.183938: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-02 00:18:19.186681: This epoch took 115.609878 s

2022-08-02 00:18:19.188881: 
epoch:  174
2022-08-02 00:20:07.921819: train loss : -0.7511
2022-08-02 00:20:16.423760: validation loss: -0.7174
2022-08-02 00:20:16.459441: Average global foreground Dice: [0.7948]
2022-08-02 00:20:16.483778: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:20:17.161202: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-02 00:20:17.190118: saving best epoch checkpoint...
2022-08-02 00:20:17.532094: saving checkpoint...
2022-08-02 00:20:22.971198: done, saving took 5.75 seconds
2022-08-02 00:20:22.984699: This epoch took 123.793608 s

2022-08-02 00:20:22.987355: 
epoch:  175
2022-08-02 00:22:12.325397: train loss : -0.7653
2022-08-02 00:22:22.787520: validation loss: -0.7242
2022-08-02 00:22:22.818522: Average global foreground Dice: [0.7932]
2022-08-02 00:22:22.842820: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:22:23.512920: Suus1 maybe_update_lr lr: 6.8e-05
2022-08-02 00:22:23.564805: saving best epoch checkpoint...
2022-08-02 00:22:24.009993: saving checkpoint...
2022-08-02 00:22:29.585097: done, saving took 5.99 seconds
2022-08-02 00:22:29.637553: This epoch took 126.648182 s

2022-08-02 00:22:29.639691: 
epoch:  176
2022-08-02 00:24:16.818359: train loss : -0.7631
2022-08-02 00:24:23.517431: validation loss: -0.7167
2022-08-02 00:24:23.520835: Average global foreground Dice: [0.7892]
2022-08-02 00:24:23.523156: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:24:24.002245: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-02 00:24:24.004711: saving best epoch checkpoint...
2022-08-02 00:24:24.199874: saving checkpoint...
2022-08-02 00:24:29.061725: done, saving took 5.05 seconds
2022-08-02 00:24:29.076982: This epoch took 119.435405 s

2022-08-02 00:24:29.079180: 
epoch:  177
2022-08-02 00:26:21.338573: train loss : -0.7553
2022-08-02 00:26:30.801042: validation loss: -0.6888
2022-08-02 00:26:30.848210: Average global foreground Dice: [0.7657]
2022-08-02 00:26:30.872802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:26:31.831706: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-02 00:26:31.844992: This epoch took 122.763760 s

2022-08-02 00:26:31.875015: 
epoch:  178
2022-08-02 00:28:20.497939: train loss : -0.7597
2022-08-02 00:28:27.875327: validation loss: -0.7279
2022-08-02 00:28:27.928538: Average global foreground Dice: [0.7934]
2022-08-02 00:28:27.953880: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:28:28.691006: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-02 00:28:28.711805: This epoch took 116.806234 s

2022-08-02 00:28:28.744763: 
epoch:  179
2022-08-02 00:30:17.507529: train loss : -0.7633
2022-08-02 00:30:26.543953: validation loss: -0.7330
2022-08-02 00:30:26.581398: Average global foreground Dice: [0.7991]
2022-08-02 00:30:26.598855: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:30:27.235708: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-02 00:30:27.257087: saving best epoch checkpoint...
2022-08-02 00:30:27.667051: saving checkpoint...
2022-08-02 00:30:32.665852: done, saving took 5.38 seconds
2022-08-02 00:30:32.681647: This epoch took 123.902739 s

2022-08-02 00:30:32.683618: 
epoch:  180
2022-08-02 00:32:20.188888: train loss : -0.7716
2022-08-02 00:32:29.745107: validation loss: -0.7253
2022-08-02 00:32:29.783483: Average global foreground Dice: [0.7915]
2022-08-02 00:32:29.829830: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:32:30.561948: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-02 00:32:30.582809: saving best epoch checkpoint...
2022-08-02 00:32:31.151206: saving checkpoint...
2022-08-02 00:32:36.403301: done, saving took 5.79 seconds
2022-08-02 00:32:36.414653: This epoch took 123.729004 s

2022-08-02 00:32:36.416765: 
epoch:  181
2022-08-02 00:34:24.815400: train loss : -0.7632
2022-08-02 00:34:31.798203: validation loss: -0.7244
2022-08-02 00:34:31.801341: Average global foreground Dice: [0.7806]
2022-08-02 00:34:31.803763: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:34:32.345694: Suus1 maybe_update_lr lr: 6.7e-05
2022-08-02 00:34:32.348043: This epoch took 115.929337 s

2022-08-02 00:34:32.350108: 
epoch:  182
2022-08-02 00:36:22.125490: train loss : -0.7647
2022-08-02 00:36:32.357123: validation loss: -0.7263
2022-08-02 00:36:32.386676: Average global foreground Dice: [0.7931]
2022-08-02 00:36:32.420757: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:36:33.371310: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-02 00:36:33.397457: saving best epoch checkpoint...
2022-08-02 00:36:33.769610: saving checkpoint...
2022-08-02 00:36:39.606747: done, saving took 6.17 seconds
2022-08-02 00:36:39.621354: This epoch took 127.262241 s

2022-08-02 00:36:39.623810: 
epoch:  183
2022-08-02 00:38:27.697996: train loss : -0.7685
2022-08-02 00:38:34.715671: validation loss: -0.7178
2022-08-02 00:38:34.719824: Average global foreground Dice: [0.7892]
2022-08-02 00:38:34.722319: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:38:35.218359: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-02 00:38:35.220839: saving best epoch checkpoint...
2022-08-02 00:38:35.402906: saving checkpoint...
2022-08-02 00:38:40.364382: done, saving took 5.14 seconds
2022-08-02 00:38:40.377003: This epoch took 120.751041 s

2022-08-02 00:38:40.379451: 
epoch:  184
2022-08-02 00:40:29.326183: train loss : -0.7735
2022-08-02 00:40:36.325731: validation loss: -0.7340
2022-08-02 00:40:36.343127: Average global foreground Dice: [0.799]
2022-08-02 00:40:36.370756: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:40:36.891144: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-02 00:40:36.909821: saving best epoch checkpoint...
2022-08-02 00:40:37.177265: saving checkpoint...
2022-08-02 00:40:43.740398: done, saving took 6.81 seconds
2022-08-02 00:40:43.750053: This epoch took 123.368147 s

2022-08-02 00:40:43.752203: 
epoch:  185
2022-08-02 00:42:32.977184: train loss : -0.7723
2022-08-02 00:42:39.857363: validation loss: -0.7210
2022-08-02 00:42:39.884893: Average global foreground Dice: [0.7873]
2022-08-02 00:42:39.891065: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:42:40.475141: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-02 00:42:40.509949: This epoch took 116.755732 s

2022-08-02 00:42:40.533804: 
epoch:  186
2022-08-02 00:44:30.931738: train loss : -0.7642
2022-08-02 00:44:39.440903: validation loss: -0.7041
2022-08-02 00:44:39.466840: Average global foreground Dice: [0.7719]
2022-08-02 00:44:39.478740: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:44:40.462729: Suus1 maybe_update_lr lr: 6.6e-05
2022-08-02 00:44:40.502843: This epoch took 119.942693 s

2022-08-02 00:44:40.539590: 
epoch:  187
2022-08-02 00:46:29.250860: train loss : -0.7724
2022-08-02 00:46:36.224531: validation loss: -0.7102
2022-08-02 00:46:36.228115: Average global foreground Dice: [0.7772]
2022-08-02 00:46:36.230381: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:46:36.801355: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-02 00:46:36.804743: This epoch took 116.232688 s

2022-08-02 00:46:36.807157: 
epoch:  188
2022-08-02 00:48:28.198037: train loss : -0.7635
2022-08-02 00:48:37.467990: validation loss: -0.7238
2022-08-02 00:48:37.536173: Average global foreground Dice: [0.7936]
2022-08-02 00:48:37.582785: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:48:38.696233: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-02 00:48:38.728860: This epoch took 121.919624 s

2022-08-02 00:48:38.772763: 
epoch:  189
2022-08-02 00:50:30.259249: train loss : -0.7733
2022-08-02 00:50:38.363814: validation loss: -0.6892
2022-08-02 00:50:38.385006: Average global foreground Dice: [0.7576]
2022-08-02 00:50:38.415775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:50:39.626731: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-02 00:50:39.629264: This epoch took 120.813881 s

2022-08-02 00:50:39.631314: 
epoch:  190
2022-08-02 00:52:26.697204: train loss : -0.7683
2022-08-02 00:52:35.736243: validation loss: -0.7213
2022-08-02 00:52:35.750160: Average global foreground Dice: [0.7791]
2022-08-02 00:52:35.793725: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:52:36.353299: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-02 00:52:36.366392: This epoch took 116.732896 s

2022-08-02 00:52:36.382782: 
epoch:  191
2022-08-02 00:54:25.266373: train loss : -0.7714
2022-08-02 00:54:33.442954: validation loss: -0.7270
2022-08-02 00:54:33.490386: Average global foreground Dice: [0.795]
2022-08-02 00:54:33.518893: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:54:34.657156: Suus1 maybe_update_lr lr: 6.5e-05
2022-08-02 00:54:34.701903: This epoch took 118.304120 s

2022-08-02 00:54:34.746805: 
epoch:  192
2022-08-02 00:56:27.917992: train loss : -0.7644
2022-08-02 00:56:39.191583: validation loss: -0.7088
2022-08-02 00:56:39.227825: Average global foreground Dice: [0.7775]
2022-08-02 00:56:39.269206: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:56:40.084242: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-02 00:56:40.103244: This epoch took 125.314475 s

2022-08-02 00:56:40.135759: 
epoch:  193
2022-08-02 00:58:29.199743: train loss : -0.7638
2022-08-02 00:58:38.223546: validation loss: -0.7310
2022-08-02 00:58:38.267444: Average global foreground Dice: [0.7936]
2022-08-02 00:58:38.306803: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 00:58:38.886593: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-02 00:58:38.889003: This epoch took 118.743973 s

2022-08-02 00:58:38.890967: 
epoch:  194
2022-08-02 01:00:27.933884: train loss : -0.7711
2022-08-02 01:00:34.618873: validation loss: -0.7355
2022-08-02 01:00:34.622478: Average global foreground Dice: [0.8013]
2022-08-02 01:00:34.624897: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:00:35.112541: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-02 01:00:35.116173: This epoch took 116.223090 s

2022-08-02 01:00:35.118494: 
epoch:  195
2022-08-02 01:02:23.375062: train loss : -0.7712
2022-08-02 01:02:31.512019: validation loss: -0.7404
2022-08-02 01:02:31.522240: Average global foreground Dice: [0.8067]
2022-08-02 01:02:31.542174: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:02:32.120530: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-02 01:02:32.123029: saving best epoch checkpoint...
2022-08-02 01:02:32.322036: saving checkpoint...
2022-08-02 01:02:37.179739: done, saving took 5.05 seconds
2022-08-02 01:02:37.200173: This epoch took 122.079043 s

2022-08-02 01:02:37.202349: 
epoch:  196
2022-08-02 01:04:25.209908: train loss : -0.7763
2022-08-02 01:04:33.021413: validation loss: -0.7116
2022-08-02 01:04:33.066899: Average global foreground Dice: [0.7713]
2022-08-02 01:04:33.123785: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:04:33.731427: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-02 01:04:33.745317: This epoch took 116.540774 s

2022-08-02 01:04:33.747652: 
epoch:  197
2022-08-02 01:06:20.835391: train loss : -0.7838
2022-08-02 01:06:28.620188: validation loss: -0.7462
2022-08-02 01:06:28.639494: Average global foreground Dice: [0.8045]
2022-08-02 01:06:28.641917: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:06:29.297160: Suus1 maybe_update_lr lr: 6.4e-05
2022-08-02 01:06:29.305575: saving best epoch checkpoint...
2022-08-02 01:06:29.633542: saving checkpoint...
2022-08-02 01:06:34.887150: done, saving took 5.58 seconds
2022-08-02 01:06:34.904238: This epoch took 121.154381 s

2022-08-02 01:06:34.907549: 
epoch:  198
2022-08-02 01:08:24.329125: train loss : -0.7741
2022-08-02 01:08:33.467253: validation loss: -0.7492
2022-08-02 01:08:33.509454: Average global foreground Dice: [0.8145]
2022-08-02 01:08:33.520321: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:08:34.307321: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-02 01:08:34.334586: saving best epoch checkpoint...
2022-08-02 01:08:34.533528: saving checkpoint...
2022-08-02 01:08:40.284536: done, saving took 5.95 seconds
2022-08-02 01:08:40.301599: This epoch took 125.391911 s

2022-08-02 01:08:40.304415: 
epoch:  199
2022-08-02 01:10:29.270501: train loss : -0.7682
2022-08-02 01:10:38.339115: validation loss: -0.7312
2022-08-02 01:10:38.350299: Average global foreground Dice: [0.7957]
2022-08-02 01:10:38.391042: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:10:39.282770: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-02 01:10:39.285404: saving scheduled checkpoint file...
2022-08-02 01:10:39.482320: saving checkpoint...
2022-08-02 01:10:44.475600: done, saving took 5.19 seconds
2022-08-02 01:10:44.490625: done
2022-08-02 01:10:44.492837: saving best epoch checkpoint...
2022-08-02 01:10:44.601774: saving checkpoint...
2022-08-02 01:10:49.711763: done, saving took 5.22 seconds
2022-08-02 01:10:49.723766: This epoch took 129.416844 s

2022-08-02 01:10:49.725915: 
epoch:  200
2022-08-02 01:12:37.527370: train loss : -0.7613
2022-08-02 01:12:46.005936: validation loss: -0.7478
2022-08-02 01:12:46.037628: Average global foreground Dice: [0.8051]
2022-08-02 01:12:46.057999: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:12:46.911905: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-02 01:12:46.935060: saving best epoch checkpoint...
2022-08-02 01:12:47.120020: saving checkpoint...
2022-08-02 01:12:52.413815: done, saving took 5.47 seconds
2022-08-02 01:12:52.425822: This epoch took 122.697656 s

2022-08-02 01:12:52.428044: 
epoch:  201
2022-08-02 01:14:41.131583: train loss : -0.7695
2022-08-02 01:14:49.437652: validation loss: -0.7220
2022-08-02 01:14:49.451543: Average global foreground Dice: [0.7972]
2022-08-02 01:14:49.463926: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:14:50.074212: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-02 01:14:50.094731: saving best epoch checkpoint...
2022-08-02 01:14:50.316938: saving checkpoint...
2022-08-02 01:14:55.433580: done, saving took 5.33 seconds
2022-08-02 01:14:55.455659: This epoch took 123.025511 s

2022-08-02 01:14:55.458127: 
epoch:  202
2022-08-02 01:16:43.919252: train loss : -0.7726
2022-08-02 01:16:50.742214: validation loss: -0.7220
2022-08-02 01:16:50.772292: Average global foreground Dice: [0.7887]
2022-08-02 01:16:50.787678: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:16:51.667961: Suus1 maybe_update_lr lr: 6.3e-05
2022-08-02 01:16:51.705805: This epoch took 116.245488 s

2022-08-02 01:16:51.732754: 
epoch:  203
2022-08-02 01:18:41.496908: train loss : -0.7724
2022-08-02 01:18:49.377047: validation loss: -0.7113
2022-08-02 01:18:49.412274: Average global foreground Dice: [0.783]
2022-08-02 01:18:49.429021: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:18:49.993507: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-02 01:18:50.007291: This epoch took 118.252523 s

2022-08-02 01:18:50.012078: 
epoch:  204
2022-08-02 01:20:38.369604: train loss : -0.7720
2022-08-02 01:20:45.190030: validation loss: -0.7434
2022-08-02 01:20:45.193343: Average global foreground Dice: [0.8056]
2022-08-02 01:20:45.196008: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:20:45.716711: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-02 01:20:45.719739: This epoch took 115.691062 s

2022-08-02 01:20:45.722270: 
epoch:  205
2022-08-02 01:22:30.826344: train loss : -0.7689
2022-08-02 01:22:37.296355: validation loss: -0.7342
2022-08-02 01:22:37.299577: Average global foreground Dice: [0.793]
2022-08-02 01:22:37.301812: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:22:37.742308: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-02 01:22:37.744588: This epoch took 112.019954 s

2022-08-02 01:22:37.746525: 
epoch:  206
2022-08-02 01:24:21.676224: train loss : -0.7805
2022-08-02 01:24:28.021392: validation loss: -0.7267
2022-08-02 01:24:28.024657: Average global foreground Dice: [0.7858]
2022-08-02 01:24:28.027061: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:24:28.496026: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-02 01:24:28.498496: This epoch took 110.749871 s

2022-08-02 01:24:28.500818: 
epoch:  207
2022-08-02 01:26:13.159503: train loss : -0.7776
2022-08-02 01:26:20.762525: validation loss: -0.7433
2022-08-02 01:26:20.766120: Average global foreground Dice: [0.8036]
2022-08-02 01:26:20.768481: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:26:21.276786: Suus1 maybe_update_lr lr: 6.2e-05
2022-08-02 01:26:21.279673: saving best epoch checkpoint...
2022-08-02 01:26:21.523045: saving checkpoint...
2022-08-02 01:26:26.667014: done, saving took 5.38 seconds
2022-08-02 01:26:26.683089: This epoch took 118.180367 s

2022-08-02 01:26:26.685231: 
epoch:  208
2022-08-02 01:28:14.187428: train loss : -0.7796
2022-08-02 01:28:22.911589: validation loss: -0.7434
2022-08-02 01:28:22.948230: Average global foreground Dice: [0.8066]
2022-08-02 01:28:22.971751: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:28:23.712435: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-02 01:28:23.745815: saving best epoch checkpoint...
2022-08-02 01:28:24.244594: saving checkpoint...
2022-08-02 01:28:29.558374: done, saving took 5.78 seconds
2022-08-02 01:28:29.574945: This epoch took 122.887632 s

2022-08-02 01:28:29.577391: 
epoch:  209
2022-08-02 01:30:16.741360: train loss : -0.7757
2022-08-02 01:30:24.611382: validation loss: -0.7254
2022-08-02 01:30:24.650499: Average global foreground Dice: [0.7833]
2022-08-02 01:30:24.687795: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:30:25.889904: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-02 01:30:25.920612: This epoch took 116.341118 s

2022-08-02 01:30:25.961575: 
epoch:  210
2022-08-02 01:32:17.237880: train loss : -0.7847
2022-08-02 01:32:25.099414: validation loss: -0.7347
2022-08-02 01:32:25.125666: Average global foreground Dice: [0.7958]
2022-08-02 01:32:25.188795: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:32:25.914245: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-02 01:32:25.920977: This epoch took 119.933607 s

2022-08-02 01:32:25.930789: 
epoch:  211
2022-08-02 01:34:16.625010: train loss : -0.7884
2022-08-02 01:34:24.670722: validation loss: -0.7339
2022-08-02 01:34:24.692945: Average global foreground Dice: [0.796]
2022-08-02 01:34:24.712723: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:34:25.416025: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-02 01:34:25.433345: This epoch took 119.499990 s

2022-08-02 01:34:25.460820: 
epoch:  212
2022-08-02 01:36:14.853008: train loss : -0.7766
2022-08-02 01:36:23.635305: validation loss: -0.7745
2022-08-02 01:36:23.664578: Average global foreground Dice: [0.8297]
2022-08-02 01:36:23.666903: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:36:24.413603: Suus1 maybe_update_lr lr: 6.1e-05
2022-08-02 01:36:24.448792: saving best epoch checkpoint...
2022-08-02 01:36:24.687363: saving checkpoint...
2022-08-02 01:36:30.000309: done, saving took 5.52 seconds
2022-08-02 01:36:30.014081: This epoch took 124.540333 s

2022-08-02 01:36:30.016429: 
epoch:  213
2022-08-02 01:38:17.043599: train loss : -0.7810
2022-08-02 01:38:23.642741: validation loss: -0.7138
2022-08-02 01:38:23.647022: Average global foreground Dice: [0.7783]
2022-08-02 01:38:23.649292: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:38:24.147223: Suus1 maybe_update_lr lr: 6e-05
2022-08-02 01:38:24.149659: This epoch took 114.130661 s

2022-08-02 01:38:24.151868: 
epoch:  214
2022-08-02 01:40:14.303478: train loss : -0.7805
2022-08-02 01:40:22.404692: validation loss: -0.7062
2022-08-02 01:40:22.432583: Average global foreground Dice: [0.7746]
2022-08-02 01:40:22.477789: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:40:23.348391: Suus1 maybe_update_lr lr: 6e-05
2022-08-02 01:40:23.384065: This epoch took 119.230082 s

2022-08-02 01:40:23.427802: 
epoch:  215
2022-08-02 01:42:17.987103: train loss : -0.7738
2022-08-02 01:42:27.342218: validation loss: -0.7221
2022-08-02 01:42:27.388194: Average global foreground Dice: [0.7872]
2022-08-02 01:42:27.419487: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:42:28.079566: Suus1 maybe_update_lr lr: 6e-05
2022-08-02 01:42:28.117871: This epoch took 124.641136 s

2022-08-02 01:42:28.161791: 
epoch:  216
2022-08-02 01:44:19.676988: train loss : -0.7761
2022-08-02 01:44:27.035459: validation loss: -0.7358
2022-08-02 01:44:27.050133: Average global foreground Dice: [0.7941]
2022-08-02 01:44:27.068962: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:44:27.617894: Suus1 maybe_update_lr lr: 6e-05
2022-08-02 01:44:27.620565: This epoch took 119.424763 s

2022-08-02 01:44:27.633794: 
epoch:  217
2022-08-02 01:46:14.896075: train loss : -0.7807
2022-08-02 01:46:22.788334: validation loss: -0.7393
2022-08-02 01:46:22.791877: Average global foreground Dice: [0.8115]
2022-08-02 01:46:22.794185: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:46:23.280900: Suus1 maybe_update_lr lr: 6e-05
2022-08-02 01:46:23.282542: This epoch took 115.626781 s

2022-08-02 01:46:23.285256: 
epoch:  218
2022-08-02 01:48:12.531918: train loss : -0.7786
2022-08-02 01:48:20.329897: validation loss: -0.7246
2022-08-02 01:48:20.365342: Average global foreground Dice: [0.7917]
2022-08-02 01:48:20.403775: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:48:21.138457: Suus1 maybe_update_lr lr: 6e-05
2022-08-02 01:48:21.169031: This epoch took 117.881846 s

2022-08-02 01:48:21.202780: 
epoch:  219
2022-08-02 01:50:15.295566: train loss : -0.7800
2022-08-02 01:50:24.340331: validation loss: -0.7352
2022-08-02 01:50:24.377412: Average global foreground Dice: [0.7971]
2022-08-02 01:50:24.404812: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:50:25.052479: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-02 01:50:25.062455: This epoch took 123.826689 s

2022-08-02 01:50:25.068522: 
epoch:  220
2022-08-02 01:52:14.882638: train loss : -0.7795
2022-08-02 01:52:22.074334: validation loss: -0.7435
2022-08-02 01:52:22.089208: Average global foreground Dice: [0.8039]
2022-08-02 01:52:22.096924: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:52:22.665771: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-02 01:52:22.668766: This epoch took 117.598023 s

2022-08-02 01:52:22.671287: 
epoch:  221
2022-08-02 01:54:11.230089: train loss : -0.7775
2022-08-02 01:54:19.462992: validation loss: -0.7326
2022-08-02 01:54:19.477795: Average global foreground Dice: [0.8027]
2022-08-02 01:54:19.480138: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:54:19.961162: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-02 01:54:19.963982: This epoch took 117.290092 s

2022-08-02 01:54:19.966510: 
epoch:  222
2022-08-02 01:56:06.579819: train loss : -0.7830
2022-08-02 01:56:14.210151: validation loss: -0.7448
2022-08-02 01:56:14.242726: Average global foreground Dice: [0.8057]
2022-08-02 01:56:14.302805: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:56:15.111301: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-02 01:56:15.138788: This epoch took 115.169896 s

2022-08-02 01:56:15.167194: 
epoch:  223
2022-08-02 01:58:05.998129: train loss : -0.7794
2022-08-02 01:58:15.344846: validation loss: -0.7169
2022-08-02 01:58:15.362393: Average global foreground Dice: [0.7811]
2022-08-02 01:58:15.388697: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 01:58:16.015518: Suus1 maybe_update_lr lr: 5.9e-05
2022-08-02 01:58:16.032665: This epoch took 120.839826 s

2022-08-02 01:58:16.041766: 
epoch:  224
2022-08-02 02:00:06.067755: train loss : -0.7883
2022-08-02 02:00:16.249681: validation loss: -0.7356
2022-08-02 02:00:16.273051: Average global foreground Dice: [0.7931]
2022-08-02 02:00:16.288408: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:00:16.888563: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-02 02:00:16.926919: This epoch took 120.874102 s

2022-08-02 02:00:16.959245: 
epoch:  225
2022-08-02 02:02:12.178257: train loss : -0.7772
2022-08-02 02:02:22.525678: validation loss: -0.7197
2022-08-02 02:02:22.558552: Average global foreground Dice: [0.7823]
2022-08-02 02:02:22.605060: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:02:23.282296: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-02 02:02:23.325829: This epoch took 126.328814 s

2022-08-02 02:02:23.344855: 
epoch:  226
2022-08-02 02:04:12.176643: train loss : -0.7780
2022-08-02 02:04:19.869015: validation loss: -0.7508
2022-08-02 02:04:19.899237: Average global foreground Dice: [0.8132]
2022-08-02 02:04:19.946768: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:04:20.620571: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-02 02:04:20.650907: This epoch took 117.299621 s

2022-08-02 02:04:20.659710: 
epoch:  227
2022-08-02 02:06:11.357016: train loss : -0.7829
2022-08-02 02:06:19.592033: validation loss: -0.7169
2022-08-02 02:06:19.616614: Average global foreground Dice: [0.7866]
2022-08-02 02:06:19.657836: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:06:20.250212: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-02 02:06:20.261867: This epoch took 119.594995 s

2022-08-02 02:06:20.275866: 
epoch:  228
2022-08-02 02:08:06.128834: train loss : -0.7834
2022-08-02 02:08:13.878349: validation loss: -0.7208
2022-08-02 02:08:13.913566: Average global foreground Dice: [0.7814]
2022-08-02 02:08:13.937816: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:08:14.575457: Suus1 maybe_update_lr lr: 5.8e-05
2022-08-02 02:08:14.593844: This epoch took 114.305353 s

2022-08-02 02:08:14.611789: 
epoch:  229
2022-08-02 02:10:03.755008: train loss : -0.7855
2022-08-02 02:10:14.584110: validation loss: -0.7236
2022-08-02 02:10:14.615757: Average global foreground Dice: [0.7853]
2022-08-02 02:10:14.635144: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:10:15.251682: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-02 02:10:15.289904: This epoch took 120.649086 s

2022-08-02 02:10:15.338904: 
epoch:  230
2022-08-02 02:12:06.461211: train loss : -0.7831
2022-08-02 02:12:16.679977: validation loss: -0.7390
2022-08-02 02:12:16.726847: Average global foreground Dice: [0.8012]
2022-08-02 02:12:16.769772: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:12:17.423558: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-02 02:12:17.435808: This epoch took 122.054042 s

2022-08-02 02:12:17.439449: 
epoch:  231
2022-08-02 02:14:04.776763: train loss : -0.7950
2022-08-02 02:14:12.851559: validation loss: -0.7331
2022-08-02 02:14:12.883809: Average global foreground Dice: [0.7916]
2022-08-02 02:14:12.907353: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:14:13.776105: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-02 02:14:13.817928: This epoch took 116.362315 s

2022-08-02 02:14:13.852787: 
epoch:  232
2022-08-02 02:16:03.864784: train loss : -0.7969
2022-08-02 02:16:11.992355: validation loss: -0.7287
2022-08-02 02:16:12.017338: Average global foreground Dice: [0.796]
2022-08-02 02:16:12.043772: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:16:12.666446: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-02 02:16:12.690574: This epoch took 118.780808 s

2022-08-02 02:16:12.707788: 
epoch:  233
2022-08-02 02:18:02.540020: train loss : -0.7815
2022-08-02 02:18:12.939923: validation loss: -0.7136
2022-08-02 02:18:12.985419: Average global foreground Dice: [0.7798]
2022-08-02 02:18:13.015781: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:18:13.741557: Suus1 maybe_update_lr lr: 5.7e-05
2022-08-02 02:18:13.744319: This epoch took 121.002530 s

2022-08-02 02:18:13.746564: 
epoch:  234
2022-08-02 02:20:01.828600: train loss : -0.7892
2022-08-02 02:20:09.628983: validation loss: -0.7348
2022-08-02 02:20:09.632988: Average global foreground Dice: [0.7989]
2022-08-02 02:20:09.635452: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:20:10.145117: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-02 02:20:10.147486: This epoch took 116.398685 s

2022-08-02 02:20:10.149496: 
epoch:  235
2022-08-02 02:21:58.116020: train loss : -0.7784
2022-08-02 02:22:05.184448: validation loss: -0.7479
2022-08-02 02:22:05.188197: Average global foreground Dice: [0.8011]
2022-08-02 02:22:05.196151: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:22:05.750484: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-02 02:22:05.761681: This epoch took 115.610132 s

2022-08-02 02:22:05.766773: 
epoch:  236
2022-08-02 02:23:56.000068: train loss : -0.7838
2022-08-02 02:24:03.812933: validation loss: -0.7281
2022-08-02 02:24:03.861912: Average global foreground Dice: [0.7813]
2022-08-02 02:24:03.888773: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:24:04.368283: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-02 02:24:04.371490: This epoch took 118.595008 s

2022-08-02 02:24:04.373859: 
epoch:  237
2022-08-02 02:25:52.238172: train loss : -0.7731
2022-08-02 02:25:59.884226: validation loss: -0.7499
2022-08-02 02:25:59.924536: Average global foreground Dice: [0.8096]
2022-08-02 02:25:59.940656: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:26:00.693600: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-02 02:26:00.716779: This epoch took 116.340631 s

2022-08-02 02:26:00.724580: 
epoch:  238
2022-08-02 02:27:49.719692: train loss : -0.7893
2022-08-02 02:27:57.936339: validation loss: -0.7252
2022-08-02 02:27:57.964963: Average global foreground Dice: [0.7893]
2022-08-02 02:27:57.994357: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:27:58.835160: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-02 02:27:58.859812: This epoch took 118.129098 s

2022-08-02 02:27:58.888788: 
epoch:  239
2022-08-02 02:29:47.270366: train loss : -0.7943
2022-08-02 02:29:54.894260: validation loss: -0.7272
2022-08-02 02:29:54.915393: Average global foreground Dice: [0.7858]
2022-08-02 02:29:54.938916: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:29:55.579061: Suus1 maybe_update_lr lr: 5.6e-05
2022-08-02 02:29:55.622820: This epoch took 116.690003 s

2022-08-02 02:29:55.668432: 
epoch:  240
2022-08-02 02:31:45.019972: train loss : -0.7909
2022-08-02 02:31:51.945332: validation loss: -0.7329
2022-08-02 02:31:51.948810: Average global foreground Dice: [0.7956]
2022-08-02 02:31:51.951209: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:31:52.480243: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-02 02:31:52.482720: This epoch took 116.801130 s

2022-08-02 02:31:52.484937: 
epoch:  241
2022-08-02 02:33:42.103248: train loss : -0.7925
2022-08-02 02:33:49.326560: validation loss: -0.7428
2022-08-02 02:33:49.342899: Average global foreground Dice: [0.8032]
2022-08-02 02:33:49.360000: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:33:49.923525: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-02 02:33:49.964875: This epoch took 117.477731 s

2022-08-02 02:33:50.000782: 
epoch:  242
2022-08-02 02:35:38.671007: train loss : -0.7936
2022-08-02 02:35:46.167730: validation loss: -0.7522
2022-08-02 02:35:46.171936: Average global foreground Dice: [0.8147]
2022-08-02 02:35:46.174366: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:35:46.670351: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-02 02:35:46.672904: This epoch took 116.655408 s

2022-08-02 02:35:46.686393: 
epoch:  243
2022-08-02 02:37:36.041706: train loss : -0.7972
2022-08-02 02:37:43.868536: validation loss: -0.7283
2022-08-02 02:37:43.901396: Average global foreground Dice: [0.7954]
2022-08-02 02:37:43.926862: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:37:44.640822: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-02 02:37:44.669945: This epoch took 117.970044 s

2022-08-02 02:37:44.700780: 
epoch:  244
2022-08-02 02:39:32.545756: train loss : -0.7915
2022-08-02 02:39:40.199212: validation loss: -0.7385
2022-08-02 02:39:40.226370: Average global foreground Dice: [0.7991]
2022-08-02 02:39:40.251759: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:39:40.839211: Suus1 maybe_update_lr lr: 5.5e-05
2022-08-02 02:39:40.841923: This epoch took 116.124135 s

2022-08-02 02:39:40.843938: 
epoch:  245
2022-08-02 02:41:29.286323: train loss : -0.7857
2022-08-02 02:41:35.866735: validation loss: -0.7608
2022-08-02 02:41:35.870539: Average global foreground Dice: [0.8184]
2022-08-02 02:41:35.873016: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:41:36.370981: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-02 02:41:36.373508: saving best epoch checkpoint...
2022-08-02 02:41:36.527572: saving checkpoint...
2022-08-02 02:41:41.437979: done, saving took 5.06 seconds
2022-08-02 02:41:41.453361: This epoch took 120.607342 s

2022-08-02 02:41:41.455624: 
epoch:  246
2022-08-02 02:43:28.983729: train loss : -0.7998
2022-08-02 02:43:37.624960: validation loss: -0.7416
2022-08-02 02:43:37.658453: Average global foreground Dice: [0.8071]
2022-08-02 02:43:37.692768: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:43:38.518001: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-02 02:43:38.572824: saving best epoch checkpoint...
2022-08-02 02:43:39.294196: saving checkpoint...
2022-08-02 02:43:44.540538: done, saving took 5.94 seconds
2022-08-02 02:43:44.556764: This epoch took 123.098979 s

2022-08-02 02:43:44.559309: 
epoch:  247
2022-08-02 02:45:36.102443: train loss : -0.7774
2022-08-02 02:45:46.451458: validation loss: -0.7463
2022-08-02 02:45:46.477369: Average global foreground Dice: [0.8092]
2022-08-02 02:45:46.534829: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:45:47.473546: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-02 02:45:47.504908: saving best epoch checkpoint...
2022-08-02 02:45:47.877581: saving checkpoint...
2022-08-02 02:45:53.677953: done, saving took 6.14 seconds
2022-08-02 02:45:53.694970: This epoch took 129.133253 s

2022-08-02 02:45:53.697271: 
epoch:  248
2022-08-02 02:47:44.120620: train loss : -0.7813
2022-08-02 02:47:53.630937: validation loss: -0.7279
2022-08-02 02:47:53.659969: Average global foreground Dice: [0.796]
2022-08-02 02:47:53.688963: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:47:54.595552: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-02 02:47:54.618905: This epoch took 120.919546 s

2022-08-02 02:47:54.650761: 
epoch:  249
2022-08-02 02:49:44.414496: train loss : -0.7873
2022-08-02 02:49:53.136203: validation loss: -0.7328
2022-08-02 02:49:53.141125: Average global foreground Dice: [0.7993]
2022-08-02 02:49:53.153315: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:49:53.671530: Suus1 maybe_update_lr lr: 5.4e-05
2022-08-02 02:49:53.683347: saving scheduled checkpoint file...
2022-08-02 02:49:54.056693: saving checkpoint...
2022-08-02 02:49:59.455024: done, saving took 5.77 seconds
2022-08-02 02:49:59.472404: done
2022-08-02 02:49:59.475583: This epoch took 124.791815 s

2022-08-02 02:49:59.477639: 
epoch:  250
2022-08-02 02:51:48.564548: train loss : -0.7917
2022-08-02 02:51:56.611450: validation loss: -0.7489
2022-08-02 02:51:56.614841: Average global foreground Dice: [0.8074]
2022-08-02 02:51:56.617122: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:51:57.126999: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-02 02:51:57.130771: saving best epoch checkpoint...
2022-08-02 02:51:57.393361: saving checkpoint...
2022-08-02 02:52:02.307661: done, saving took 5.17 seconds
2022-08-02 02:52:02.324386: This epoch took 122.844641 s

2022-08-02 02:52:02.326704: 
epoch:  251
2022-08-02 02:53:52.423862: train loss : -0.7911
2022-08-02 02:54:00.734553: validation loss: -0.7352
2022-08-02 02:54:00.757052: Average global foreground Dice: [0.7929]
2022-08-02 02:54:00.787799: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:54:01.683711: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-02 02:54:01.699562: This epoch took 119.370860 s

2022-08-02 02:54:01.707511: 
epoch:  252
2022-08-02 02:55:50.604639: train loss : -0.7981
2022-08-02 02:55:57.844965: validation loss: -0.7391
2022-08-02 02:55:57.854617: Average global foreground Dice: [0.799]
2022-08-02 02:55:57.857163: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:55:58.387150: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-02 02:55:58.389733: This epoch took 116.674619 s

2022-08-02 02:55:58.392336: 
epoch:  253
2022-08-02 02:57:44.649561: train loss : -0.7895
2022-08-02 02:57:52.574537: validation loss: -0.7601
2022-08-02 02:57:52.577909: Average global foreground Dice: [0.8215]
2022-08-02 02:57:52.580615: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:57:53.087645: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-02 02:57:53.090472: saving best epoch checkpoint...
2022-08-02 02:57:53.365746: saving checkpoint...
2022-08-02 02:57:58.212771: done, saving took 5.12 seconds
2022-08-02 02:57:58.225769: This epoch took 119.830998 s

2022-08-02 02:57:58.228927: 
epoch:  254
2022-08-02 02:59:47.023518: train loss : -0.7907
2022-08-02 02:59:56.130236: validation loss: -0.7420
2022-08-02 02:59:56.162265: Average global foreground Dice: [0.8057]
2022-08-02 02:59:56.191274: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 02:59:56.819954: Suus1 maybe_update_lr lr: 5.3e-05
2022-08-02 02:59:56.848794: saving best epoch checkpoint...
2022-08-02 02:59:57.105987: saving checkpoint...
2022-08-02 03:00:03.088814: done, saving took 6.23 seconds
2022-08-02 03:00:03.102989: This epoch took 124.871850 s

2022-08-02 03:00:03.105587: 
epoch:  255
2022-08-02 03:01:51.279648: train loss : -0.7936
2022-08-02 03:02:00.311750: validation loss: -0.7359
2022-08-02 03:02:00.348186: Average global foreground Dice: [0.7993]
2022-08-02 03:02:00.366328: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:02:01.096632: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-02 03:02:01.140998: This epoch took 118.033211 s

2022-08-02 03:02:01.155804: 
epoch:  256
2022-08-02 03:03:51.216752: train loss : -0.7953
2022-08-02 03:03:59.687846: validation loss: -0.7336
2022-08-02 03:03:59.718518: Average global foreground Dice: [0.7961]
2022-08-02 03:03:59.744881: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:04:00.387108: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-02 03:04:00.421424: This epoch took 119.245596 s

2022-08-02 03:04:00.452761: 
epoch:  257
2022-08-02 03:05:49.842585: train loss : -0.7801
2022-08-02 03:05:59.150930: validation loss: -0.7142
2022-08-02 03:05:59.182435: Average global foreground Dice: [0.7784]
2022-08-02 03:05:59.205766: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:05:59.957277: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-02 03:05:59.988623: This epoch took 119.505819 s

2022-08-02 03:06:00.015455: 
epoch:  258
2022-08-02 03:07:49.858273: train loss : -0.7916
2022-08-02 03:07:57.257444: validation loss: -0.7704
2022-08-02 03:07:57.299390: Average global foreground Dice: [0.8282]
2022-08-02 03:07:57.320540: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:07:57.977231: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-02 03:07:57.980243: This epoch took 117.932908 s

2022-08-02 03:07:57.983478: 
epoch:  259
2022-08-02 03:09:45.278757: train loss : -0.8013
2022-08-02 03:09:53.758236: validation loss: -0.7289
2022-08-02 03:09:53.813334: Average global foreground Dice: [0.7891]
2022-08-02 03:09:53.834802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:09:54.423636: Suus1 maybe_update_lr lr: 5.2e-05
2022-08-02 03:09:54.432614: This epoch took 116.444493 s

2022-08-02 03:09:54.438241: 
epoch:  260
2022-08-02 03:11:40.390017: train loss : -0.8032
2022-08-02 03:11:48.584070: validation loss: -0.7462
2022-08-02 03:11:48.588965: Average global foreground Dice: [0.8075]
2022-08-02 03:11:48.591827: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:11:49.124106: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-02 03:11:49.126847: This epoch took 114.686293 s

2022-08-02 03:11:49.128743: 
epoch:  261
2022-08-02 03:13:36.324807: train loss : -0.7976
2022-08-02 03:13:45.748356: validation loss: -0.7258
2022-08-02 03:13:45.776719: Average global foreground Dice: [0.7845]
2022-08-02 03:13:45.805780: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:13:46.584827: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-02 03:13:46.612530: This epoch took 117.481917 s

2022-08-02 03:13:46.640942: 
epoch:  262
2022-08-02 03:15:37.462318: train loss : -0.7853
2022-08-02 03:15:46.227321: validation loss: -0.7426
2022-08-02 03:15:46.238546: Average global foreground Dice: [0.805]
2022-08-02 03:15:46.243634: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:15:46.778592: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-02 03:15:46.806853: This epoch took 120.128062 s

2022-08-02 03:15:46.839762: 
epoch:  263
2022-08-02 03:17:39.651408: train loss : -0.7914
2022-08-02 03:17:50.758661: validation loss: -0.7411
2022-08-02 03:17:50.794225: Average global foreground Dice: [0.8004]
2022-08-02 03:17:50.824761: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:17:51.466819: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-02 03:17:51.500823: This epoch took 124.628003 s

2022-08-02 03:17:51.542766: 
epoch:  264
2022-08-02 03:19:41.790750: train loss : -0.7998
2022-08-02 03:19:49.525111: validation loss: -0.7539
2022-08-02 03:19:49.528395: Average global foreground Dice: [0.8093]
2022-08-02 03:19:49.530630: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:19:50.089185: Suus1 maybe_update_lr lr: 5.1e-05
2022-08-02 03:19:50.121793: This epoch took 118.558648 s

2022-08-02 03:19:50.133077: 
epoch:  265
2022-08-02 03:21:39.211176: train loss : -0.7988
2022-08-02 03:21:49.982808: validation loss: -0.7304
2022-08-02 03:21:50.008064: Average global foreground Dice: [0.7956]
2022-08-02 03:21:50.056017: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:21:50.804260: Suus1 maybe_update_lr lr: 5e-05
2022-08-02 03:21:50.835835: This epoch took 120.684844 s

2022-08-02 03:21:50.867273: 
epoch:  266
2022-08-02 03:23:42.349197: train loss : -0.7978
2022-08-02 03:23:49.452336: validation loss: -0.7335
2022-08-02 03:23:49.478371: Average global foreground Dice: [0.7943]
2022-08-02 03:23:49.513477: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:23:50.305082: Suus1 maybe_update_lr lr: 5e-05
2022-08-02 03:23:50.348864: This epoch took 119.456108 s

2022-08-02 03:23:50.386760: 
epoch:  267
2022-08-02 03:25:37.761393: train loss : -0.8074
2022-08-02 03:25:46.172101: validation loss: -0.7524
2022-08-02 03:25:46.213143: Average global foreground Dice: [0.8157]
2022-08-02 03:25:46.259807: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:25:47.003418: Suus1 maybe_update_lr lr: 5e-05
2022-08-02 03:25:47.019069: This epoch took 116.577849 s

2022-08-02 03:25:47.022409: 
epoch:  268
2022-08-02 03:27:34.890023: train loss : -0.8029
2022-08-02 03:27:41.801139: validation loss: -0.7360
2022-08-02 03:27:41.804492: Average global foreground Dice: [0.7904]
2022-08-02 03:27:41.806793: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:27:42.325913: Suus1 maybe_update_lr lr: 5e-05
2022-08-02 03:27:42.328358: This epoch took 115.304026 s

2022-08-02 03:27:42.331373: 
epoch:  269
2022-08-02 03:29:30.313661: train loss : -0.8069
2022-08-02 03:29:38.120955: validation loss: -0.7377
2022-08-02 03:29:38.162647: Average global foreground Dice: [0.8018]
2022-08-02 03:29:38.187891: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:29:39.087430: Suus1 maybe_update_lr lr: 5e-05
2022-08-02 03:29:39.108820: This epoch took 116.773974 s

2022-08-02 03:29:39.128752: 
epoch:  270
2022-08-02 03:31:28.467192: train loss : -0.7993
2022-08-02 03:31:37.064988: validation loss: -0.7316
2022-08-02 03:31:37.104546: Average global foreground Dice: [0.7923]
2022-08-02 03:31:37.112962: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:31:37.883665: Suus1 maybe_update_lr lr: 5e-05
2022-08-02 03:31:37.916505: This epoch took 118.767582 s

2022-08-02 03:31:37.934772: 
epoch:  271
2022-08-02 03:33:29.747595: train loss : -0.8047
2022-08-02 03:33:39.146134: validation loss: -0.7414
2022-08-02 03:33:39.150913: Average global foreground Dice: [0.7986]
2022-08-02 03:33:39.163293: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:33:39.677438: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-02 03:33:39.681534: This epoch took 121.719596 s

2022-08-02 03:33:39.685137: 
epoch:  272
2022-08-02 03:35:28.252862: train loss : -0.7958
2022-08-02 03:35:37.922034: validation loss: -0.7461
2022-08-02 03:35:37.926766: Average global foreground Dice: [0.8029]
2022-08-02 03:35:37.929228: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:35:38.464952: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-02 03:35:38.469915: This epoch took 118.782307 s

2022-08-02 03:35:38.472257: 
epoch:  273
2022-08-02 03:37:28.090135: train loss : -0.8016
2022-08-02 03:37:38.262717: validation loss: -0.7556
2022-08-02 03:37:38.326424: Average global foreground Dice: [0.8104]
2022-08-02 03:37:38.328643: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:37:39.180439: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-02 03:37:39.213816: This epoch took 120.738527 s

2022-08-02 03:37:39.254786: 
epoch:  274
2022-08-02 03:39:26.472407: train loss : -0.7963
2022-08-02 03:39:35.448024: validation loss: -0.7476
2022-08-02 03:39:35.468544: Average global foreground Dice: [0.8091]
2022-08-02 03:39:35.493766: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:39:36.222166: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-02 03:39:36.249846: This epoch took 116.957925 s

2022-08-02 03:39:36.259159: 
epoch:  275
2022-08-02 03:41:27.064998: train loss : -0.7952
2022-08-02 03:41:35.701939: validation loss: -0.7426
2022-08-02 03:41:35.749655: Average global foreground Dice: [0.8026]
2022-08-02 03:41:35.778884: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:41:36.635816: Suus1 maybe_update_lr lr: 4.9e-05
2022-08-02 03:41:36.670884: This epoch took 120.382843 s

2022-08-02 03:41:36.743787: 
epoch:  276
2022-08-02 03:43:26.966825: train loss : -0.8004
2022-08-02 03:43:35.537451: validation loss: -0.7550
2022-08-02 03:43:35.571458: Average global foreground Dice: [0.812]
2022-08-02 03:43:35.611083: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:43:36.252675: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-02 03:43:36.255550: saving best epoch checkpoint...
2022-08-02 03:43:36.470214: saving checkpoint...
2022-08-02 03:43:41.668707: done, saving took 5.40 seconds
2022-08-02 03:43:41.683564: This epoch took 124.906781 s

2022-08-02 03:43:41.685924: 
epoch:  277
2022-08-02 03:45:28.763363: train loss : -0.8043
2022-08-02 03:45:36.270834: validation loss: -0.7568
2022-08-02 03:45:36.284399: Average global foreground Dice: [0.8181]
2022-08-02 03:45:36.293997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:45:36.830619: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-02 03:45:36.833147: saving best epoch checkpoint...
2022-08-02 03:45:37.144804: saving checkpoint...
2022-08-02 03:45:42.361400: done, saving took 5.53 seconds
2022-08-02 03:45:42.378002: This epoch took 120.689975 s

2022-08-02 03:45:42.380435: 
epoch:  278
2022-08-02 03:47:31.618942: train loss : -0.7916
2022-08-02 03:47:40.021867: validation loss: -0.7400
2022-08-02 03:47:40.057361: Average global foreground Dice: [0.7962]
2022-08-02 03:47:40.098637: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:47:40.861624: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-02 03:47:40.893912: This epoch took 118.511065 s

2022-08-02 03:47:40.908904: 
epoch:  279
2022-08-02 03:49:30.347955: train loss : -0.8027
2022-08-02 03:49:38.200356: validation loss: -0.7481
2022-08-02 03:49:38.228863: Average global foreground Dice: [0.8086]
2022-08-02 03:49:38.248302: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:49:38.781742: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-02 03:49:38.784369: This epoch took 117.841613 s

2022-08-02 03:49:38.786572: 
epoch:  280
2022-08-02 03:51:28.800524: train loss : -0.8011
2022-08-02 03:51:36.485601: validation loss: -0.7319
2022-08-02 03:51:36.502370: Average global foreground Dice: [0.7888]
2022-08-02 03:51:36.505822: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:51:37.103320: Suus1 maybe_update_lr lr: 4.8e-05
2022-08-02 03:51:37.106124: This epoch took 118.317598 s

2022-08-02 03:51:37.108544: 
epoch:  281
2022-08-02 03:53:24.743780: train loss : -0.7999
2022-08-02 03:53:32.200562: validation loss: -0.7568
2022-08-02 03:53:32.209468: Average global foreground Dice: [0.8121]
2022-08-02 03:53:32.220055: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:53:32.816090: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-02 03:53:32.820798: This epoch took 115.709762 s

2022-08-02 03:53:32.826049: 
epoch:  282
2022-08-02 03:55:22.436669: train loss : -0.8099
2022-08-02 03:55:30.400711: validation loss: -0.7421
2022-08-02 03:55:30.436298: Average global foreground Dice: [0.7951]
2022-08-02 03:55:30.506970: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:55:31.266265: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-02 03:55:31.307976: This epoch took 118.476219 s

2022-08-02 03:55:31.319001: 
epoch:  283
2022-08-02 03:57:20.153937: train loss : -0.8061
2022-08-02 03:57:28.403116: validation loss: -0.7436
2022-08-02 03:57:28.450553: Average global foreground Dice: [0.8062]
2022-08-02 03:57:28.478884: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:57:29.333657: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-02 03:57:29.337135: This epoch took 118.015488 s

2022-08-02 03:57:29.357257: 
epoch:  284
2022-08-02 03:59:19.310247: train loss : -0.8046
2022-08-02 03:59:26.399043: validation loss: -0.7447
2022-08-02 03:59:26.402910: Average global foreground Dice: [0.8118]
2022-08-02 03:59:26.406601: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 03:59:26.904872: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-02 03:59:26.907312: This epoch took 117.547315 s

2022-08-02 03:59:26.908900: 
epoch:  285
2022-08-02 04:01:16.167168: train loss : -0.7907
2022-08-02 04:01:27.019443: validation loss: -0.7267
2022-08-02 04:01:27.036926: Average global foreground Dice: [0.7908]
2022-08-02 04:01:27.041112: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:01:27.651903: Suus1 maybe_update_lr lr: 4.7e-05
2022-08-02 04:01:27.669279: This epoch took 120.757491 s

2022-08-02 04:01:27.677594: 
epoch:  286
2022-08-02 04:03:17.537673: train loss : -0.8004
2022-08-02 04:03:27.267952: validation loss: -0.7326
2022-08-02 04:03:27.278483: Average global foreground Dice: [0.8004]
2022-08-02 04:03:27.289834: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:03:27.890788: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-02 04:03:27.933821: This epoch took 120.246679 s

2022-08-02 04:03:27.957533: 
epoch:  287
2022-08-02 04:05:18.707568: train loss : -0.7962
2022-08-02 04:05:27.437048: validation loss: -0.7454
2022-08-02 04:05:27.447453: Average global foreground Dice: [0.801]
2022-08-02 04:05:27.450001: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:05:28.192082: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-02 04:05:28.230926: This epoch took 120.266413 s

2022-08-02 04:05:28.272785: 
epoch:  288
2022-08-02 04:07:18.301069: train loss : -0.8049
2022-08-02 04:07:26.062468: validation loss: -0.7357
2022-08-02 04:07:26.089187: Average global foreground Dice: [0.7991]
2022-08-02 04:07:26.117579: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:07:26.712202: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-02 04:07:26.738841: This epoch took 118.405878 s

2022-08-02 04:07:26.762922: 
epoch:  289
2022-08-02 04:09:16.930160: train loss : -0.8006
2022-08-02 04:09:23.684752: validation loss: -0.7225
2022-08-02 04:09:23.693893: Average global foreground Dice: [0.7943]
2022-08-02 04:09:23.699766: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:09:24.219439: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-02 04:09:24.222168: This epoch took 117.418889 s

2022-08-02 04:09:24.224489: 
epoch:  290
2022-08-02 04:11:13.797933: train loss : -0.7968
2022-08-02 04:11:21.921328: validation loss: -0.7243
2022-08-02 04:11:21.929277: Average global foreground Dice: [0.7832]
2022-08-02 04:11:21.932742: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:11:22.458048: Suus1 maybe_update_lr lr: 4.6e-05
2022-08-02 04:11:22.460788: This epoch took 118.234106 s

2022-08-02 04:11:22.463163: 
epoch:  291
2022-08-02 04:13:09.922782: train loss : -0.8070
2022-08-02 04:13:17.211845: validation loss: -0.7361
2022-08-02 04:13:17.215245: Average global foreground Dice: [0.7965]
2022-08-02 04:13:17.217535: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:13:17.700170: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-02 04:13:17.703038: This epoch took 115.237356 s

2022-08-02 04:13:17.705327: 
epoch:  292
2022-08-02 04:15:05.589264: train loss : -0.7984
2022-08-02 04:15:13.458936: validation loss: -0.7270
2022-08-02 04:15:13.462414: Average global foreground Dice: [0.7882]
2022-08-02 04:15:13.465170: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:15:13.944788: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-02 04:15:13.955645: This epoch took 116.247987 s

2022-08-02 04:15:13.963202: 
epoch:  293
2022-08-02 04:17:04.222736: train loss : -0.8079
2022-08-02 04:17:12.230667: validation loss: -0.7152
2022-08-02 04:17:12.305578: Average global foreground Dice: [0.7823]
2022-08-02 04:17:12.337750: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:17:13.047680: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-02 04:17:13.051208: This epoch took 119.085721 s

2022-08-02 04:17:13.075014: 
epoch:  294
2022-08-02 04:19:02.292195: train loss : -0.8031
2022-08-02 04:19:09.698623: validation loss: -0.7449
2022-08-02 04:19:09.701755: Average global foreground Dice: [0.8053]
2022-08-02 04:19:09.704045: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:19:10.186898: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-02 04:19:10.189345: This epoch took 117.081557 s

2022-08-02 04:19:10.191761: 
epoch:  295
2022-08-02 04:21:00.002608: train loss : -0.8060
2022-08-02 04:21:10.843515: validation loss: -0.7617
2022-08-02 04:21:10.879929: Average global foreground Dice: [0.8176]
2022-08-02 04:21:10.941082: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:21:11.774661: Suus1 maybe_update_lr lr: 4.5e-05
2022-08-02 04:21:11.812190: This epoch took 121.618119 s

2022-08-02 04:21:11.833755: 
epoch:  296
2022-08-02 04:23:00.674999: train loss : -0.8013
2022-08-02 04:23:08.825186: validation loss: -0.7553
2022-08-02 04:23:08.883563: Average global foreground Dice: [0.8096]
2022-08-02 04:23:08.891212: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:23:09.568316: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-02 04:23:09.601829: This epoch took 117.748075 s

2022-08-02 04:23:09.634758: 
epoch:  297
2022-08-02 04:24:59.590230: train loss : -0.8066
2022-08-02 04:25:08.700695: validation loss: -0.7613
2022-08-02 04:25:08.738696: Average global foreground Dice: [0.8171]
2022-08-02 04:25:08.765836: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:25:09.597787: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-02 04:25:09.627009: This epoch took 119.971164 s

2022-08-02 04:25:09.629420: 
epoch:  298
2022-08-02 04:26:59.874747: train loss : -0.7965
2022-08-02 04:27:08.197147: validation loss: -0.7352
2022-08-02 04:27:08.231125: Average global foreground Dice: [0.7961]
2022-08-02 04:27:08.262826: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:27:09.407191: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-02 04:27:09.411646: This epoch took 119.780056 s

2022-08-02 04:27:09.427753: 
epoch:  299
2022-08-02 04:28:59.312183: train loss : -0.7991
2022-08-02 04:29:08.985296: validation loss: -0.7598
2022-08-02 04:29:09.026517: Average global foreground Dice: [0.8159]
2022-08-02 04:29:09.054799: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:29:09.750895: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-02 04:29:09.781955: saving scheduled checkpoint file...
2022-08-02 04:29:10.036100: saving checkpoint...
2022-08-02 04:29:15.179949: done, saving took 5.38 seconds
2022-08-02 04:29:15.193650: done
2022-08-02 04:29:15.196020: This epoch took 125.756130 s

2022-08-02 04:29:15.198177: 
epoch:  300
2022-08-02 04:31:05.979257: train loss : -0.8014
2022-08-02 04:31:15.981502: validation loss: -0.7385
2022-08-02 04:31:16.034893: Average global foreground Dice: [0.795]
2022-08-02 04:31:16.066757: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:31:16.907317: Suus1 maybe_update_lr lr: 4.4e-05
2022-08-02 04:31:16.910285: This epoch took 121.709885 s

2022-08-02 04:31:16.912818: 
epoch:  301
2022-08-02 04:33:07.881046: train loss : -0.8015
2022-08-02 04:33:19.721020: validation loss: -0.7300
2022-08-02 04:33:19.724964: Average global foreground Dice: [0.7925]
2022-08-02 04:33:19.727210: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:33:20.303790: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-02 04:33:20.324795: This epoch took 123.409678 s

2022-08-02 04:33:20.336747: 
epoch:  302
2022-08-02 04:35:09.176240: train loss : -0.8024
2022-08-02 04:35:18.777889: validation loss: -0.7532
2022-08-02 04:35:18.790529: Average global foreground Dice: [0.8077]
2022-08-02 04:35:18.793055: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:35:19.329309: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-02 04:35:19.332096: This epoch took 118.993075 s

2022-08-02 04:35:19.334988: 
epoch:  303
2022-08-02 04:37:08.877385: train loss : -0.7979
2022-08-02 04:37:17.545446: validation loss: -0.7321
2022-08-02 04:37:17.644385: Average global foreground Dice: [0.7927]
2022-08-02 04:37:17.669476: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:37:18.484170: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-02 04:37:18.486808: This epoch took 119.149458 s

2022-08-02 04:37:18.489268: 
epoch:  304
2022-08-02 04:39:05.331485: train loss : -0.8089
2022-08-02 04:39:13.972869: validation loss: -0.7179
2022-08-02 04:39:13.993507: Average global foreground Dice: [0.7838]
2022-08-02 04:39:13.995863: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:39:14.500938: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-02 04:39:14.503595: This epoch took 116.012285 s

2022-08-02 04:39:14.516036: 
epoch:  305
2022-08-02 04:41:02.925001: train loss : -0.8039
2022-08-02 04:41:12.256916: validation loss: -0.7407
2022-08-02 04:41:12.277530: Average global foreground Dice: [0.8031]
2022-08-02 04:41:12.310787: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:41:13.106189: Suus1 maybe_update_lr lr: 4.3e-05
2022-08-02 04:41:13.116131: This epoch took 118.585314 s

2022-08-02 04:41:13.122868: 
epoch:  306
2022-08-02 04:43:03.248401: train loss : -0.8091
2022-08-02 04:43:10.350115: validation loss: -0.7420
2022-08-02 04:43:10.383071: Average global foreground Dice: [0.7987]
2022-08-02 04:43:10.389815: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:43:10.926524: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-02 04:43:10.929322: This epoch took 117.773373 s

2022-08-02 04:43:10.931509: 
epoch:  307
2022-08-02 04:44:59.325444: train loss : -0.8088
2022-08-02 04:45:08.120333: validation loss: -0.7378
2022-08-02 04:45:08.157592: Average global foreground Dice: [0.7973]
2022-08-02 04:45:08.201836: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:45:08.972444: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-02 04:45:08.975278: This epoch took 118.041402 s

2022-08-02 04:45:08.977635: 
epoch:  308
2022-08-02 04:47:00.923842: train loss : -0.8001
2022-08-02 04:47:10.225851: validation loss: -0.7647
2022-08-02 04:47:10.235802: Average global foreground Dice: [0.822]
2022-08-02 04:47:10.257112: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:47:10.963580: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-02 04:47:10.976419: This epoch took 121.996536 s

2022-08-02 04:47:11.012794: 
epoch:  309
2022-08-02 04:49:01.819559: train loss : -0.8076
2022-08-02 04:49:08.617065: validation loss: -0.7546
2022-08-02 04:49:08.620880: Average global foreground Dice: [0.8126]
2022-08-02 04:49:08.623370: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:49:09.111604: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-02 04:49:09.114581: This epoch took 118.064636 s

2022-08-02 04:49:09.117149: 
epoch:  310
2022-08-02 04:50:56.189694: train loss : -0.8082
2022-08-02 04:51:03.439266: validation loss: -0.7576
2022-08-02 04:51:03.443378: Average global foreground Dice: [0.8122]
2022-08-02 04:51:03.445879: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:51:03.955139: Suus1 maybe_update_lr lr: 4.2e-05
2022-08-02 04:51:03.971449: This epoch took 114.851801 s

2022-08-02 04:51:03.975182: 
epoch:  311
2022-08-02 04:52:50.394077: train loss : -0.8091
2022-08-02 04:52:58.822848: validation loss: -0.7496
2022-08-02 04:52:58.833602: Average global foreground Dice: [0.8061]
2022-08-02 04:52:58.846567: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:52:59.378838: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-02 04:52:59.381849: This epoch took 115.403614 s

2022-08-02 04:52:59.384040: 
epoch:  312
2022-08-02 04:54:46.875130: train loss : -0.8096
2022-08-02 04:54:53.951623: validation loss: -0.7365
2022-08-02 04:54:53.957035: Average global foreground Dice: [0.8011]
2022-08-02 04:54:53.964794: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:54:54.548374: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-02 04:54:54.566783: This epoch took 115.180638 s

2022-08-02 04:54:54.576360: 
epoch:  313
2022-08-02 04:56:43.662565: train loss : -0.8137
2022-08-02 04:56:52.643449: validation loss: -0.7527
2022-08-02 04:56:52.669296: Average global foreground Dice: [0.8068]
2022-08-02 04:56:52.697816: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:56:53.321710: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-02 04:56:53.340942: This epoch took 118.744144 s

2022-08-02 04:56:53.353796: 
epoch:  314
2022-08-02 04:58:39.284637: train loss : -0.8136
2022-08-02 04:58:47.408022: validation loss: -0.7652
2022-08-02 04:58:47.411967: Average global foreground Dice: [0.8176]
2022-08-02 04:58:47.414902: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 04:58:48.009890: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-02 04:58:48.017884: saving best epoch checkpoint...
2022-08-02 04:58:48.259300: saving checkpoint...
2022-08-02 04:58:53.349269: done, saving took 5.33 seconds
2022-08-02 04:58:53.371626: This epoch took 120.001849 s

2022-08-02 04:58:53.373980: 
epoch:  315
2022-08-02 05:00:41.841866: train loss : -0.8095
2022-08-02 05:00:50.496660: validation loss: -0.7331
2022-08-02 05:00:50.500025: Average global foreground Dice: [0.7997]
2022-08-02 05:00:50.532832: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:00:51.185228: Suus1 maybe_update_lr lr: 4.1e-05
2022-08-02 05:00:51.216859: This epoch took 117.840596 s

2022-08-02 05:00:51.238780: 
epoch:  316
2022-08-02 05:02:39.443171: train loss : -0.8101
2022-08-02 05:02:48.396481: validation loss: -0.7520
2022-08-02 05:02:48.435771: Average global foreground Dice: [0.8098]
2022-08-02 05:02:48.467839: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:02:49.146506: Suus1 maybe_update_lr lr: 4e-05
2022-08-02 05:02:49.169923: This epoch took 117.911138 s

2022-08-02 05:02:49.202771: 
epoch:  317
2022-08-02 05:04:39.997398: train loss : -0.8107
2022-08-02 05:04:48.677140: validation loss: -0.7434
2022-08-02 05:04:48.680459: Average global foreground Dice: [0.8054]
2022-08-02 05:04:48.682756: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:04:49.327191: Suus1 maybe_update_lr lr: 4e-05
2022-08-02 05:04:49.329955: This epoch took 120.104104 s

2022-08-02 05:04:49.332184: 
epoch:  318
2022-08-02 05:06:40.210607: train loss : -0.8100
2022-08-02 05:06:50.033822: validation loss: -0.7459
2022-08-02 05:06:50.075816: Average global foreground Dice: [0.8052]
2022-08-02 05:06:50.094907: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:06:50.978711: Suus1 maybe_update_lr lr: 4e-05
2022-08-02 05:06:51.004707: This epoch took 121.669835 s

2022-08-02 05:06:51.052929: 
epoch:  319
2022-08-02 05:08:40.540794: train loss : -0.8050
2022-08-02 05:08:48.024982: validation loss: -0.7635
2022-08-02 05:08:48.052612: Average global foreground Dice: [0.8189]
2022-08-02 05:08:48.087774: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:08:48.764075: Suus1 maybe_update_lr lr: 4e-05
2022-08-02 05:08:48.797419: saving best epoch checkpoint...
2022-08-02 05:08:49.119127: saving checkpoint...
2022-08-02 05:08:54.704530: done, saving took 5.88 seconds
2022-08-02 05:08:54.720085: This epoch took 123.614320 s

2022-08-02 05:08:54.722329: 
epoch:  320
2022-08-02 05:10:44.207474: train loss : -0.8114
2022-08-02 05:10:52.865224: validation loss: -0.7522
2022-08-02 05:10:52.911416: Average global foreground Dice: [0.8113]
2022-08-02 05:10:52.930997: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:10:53.727771: Suus1 maybe_update_lr lr: 4e-05
2022-08-02 05:10:53.758808: saving best epoch checkpoint...
2022-08-02 05:10:53.992921: saving checkpoint...
2022-08-02 05:10:59.708159: done, saving took 5.92 seconds
2022-08-02 05:10:59.718731: This epoch took 124.994205 s

2022-08-02 05:10:59.720893: 
epoch:  321
2022-08-02 05:12:46.401219: train loss : -0.8076
2022-08-02 05:12:53.785881: validation loss: -0.7661
2022-08-02 05:12:53.792324: Average global foreground Dice: [0.8162]
2022-08-02 05:12:53.814587: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:12:54.494591: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-02 05:12:54.522158: saving best epoch checkpoint...
2022-08-02 05:12:54.744132: saving checkpoint...
2022-08-02 05:12:59.989007: done, saving took 5.45 seconds
2022-08-02 05:13:00.000557: This epoch took 120.277407 s

2022-08-02 05:13:00.002719: 
epoch:  322
2022-08-02 05:14:49.083159: train loss : -0.8126
2022-08-02 05:14:55.849849: validation loss: -0.7394
2022-08-02 05:14:55.853325: Average global foreground Dice: [0.8032]
2022-08-02 05:14:55.855702: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:14:56.370613: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-02 05:14:56.373407: This epoch took 116.368234 s

2022-08-02 05:14:56.375439: 
epoch:  323
2022-08-02 05:16:46.816885: train loss : -0.8035
2022-08-02 05:16:56.895858: validation loss: -0.7363
2022-08-02 05:16:56.939149: Average global foreground Dice: [0.797]
2022-08-02 05:16:56.981166: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:16:57.681130: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-02 05:16:57.731882: This epoch took 121.354371 s

2022-08-02 05:16:57.773790: 
epoch:  324
2022-08-02 05:18:47.056120: train loss : -0.8097
2022-08-02 05:18:57.791763: validation loss: -0.7233
2022-08-02 05:18:57.825223: Average global foreground Dice: [0.7854]
2022-08-02 05:18:57.857745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:18:58.611922: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-02 05:18:58.649197: This epoch took 120.842414 s

2022-08-02 05:18:58.686766: 
epoch:  325
2022-08-02 05:20:48.569469: train loss : -0.8114
2022-08-02 05:20:56.335240: validation loss: -0.7648
2022-08-02 05:20:56.339036: Average global foreground Dice: [0.8212]
2022-08-02 05:20:56.341611: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:20:56.887825: Suus1 maybe_update_lr lr: 3.9e-05
2022-08-02 05:20:56.891032: This epoch took 118.166255 s

2022-08-02 05:20:56.893198: 
epoch:  326
2022-08-02 05:22:43.515165: train loss : -0.8179
2022-08-02 05:22:50.775539: validation loss: -0.7482
2022-08-02 05:22:50.785994: Average global foreground Dice: [0.8051]
2022-08-02 05:22:50.788644: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:22:51.363532: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-02 05:22:51.365973: This epoch took 114.470729 s

2022-08-02 05:22:51.368071: 
epoch:  327
2022-08-02 05:24:37.513889: train loss : -0.8133
2022-08-02 05:24:45.746284: validation loss: -0.7508
2022-08-02 05:24:45.769309: Average global foreground Dice: [0.8116]
2022-08-02 05:24:45.772420: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:24:46.269742: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-02 05:24:46.272241: This epoch took 114.902086 s

2022-08-02 05:24:46.274335: 
epoch:  328
2022-08-02 05:26:36.542309: train loss : -0.8049
2022-08-02 05:26:44.964456: validation loss: -0.7418
2022-08-02 05:26:45.000351: Average global foreground Dice: [0.7989]
2022-08-02 05:26:45.024457: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:26:45.658031: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-02 05:26:45.663198: This epoch took 119.386469 s

2022-08-02 05:26:45.672517: 
epoch:  329
2022-08-02 05:28:33.940035: train loss : -0.8117
2022-08-02 05:28:41.708348: validation loss: -0.7504
2022-08-02 05:28:41.746372: Average global foreground Dice: [0.8068]
2022-08-02 05:28:41.772784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:28:42.480534: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-02 05:28:42.497847: This epoch took 116.816741 s

2022-08-02 05:28:42.515512: 
epoch:  330
2022-08-02 05:30:30.980688: train loss : -0.8174
2022-08-02 05:30:38.545551: validation loss: -0.7573
2022-08-02 05:30:38.548905: Average global foreground Dice: [0.815]
2022-08-02 05:30:38.568268: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:30:39.101055: Suus1 maybe_update_lr lr: 3.8e-05
2022-08-02 05:30:39.103679: This epoch took 116.566928 s

2022-08-02 05:30:39.105783: 
epoch:  331
2022-08-02 05:32:25.861074: train loss : -0.8087
2022-08-02 05:32:33.572122: validation loss: -0.7458
2022-08-02 05:32:33.578013: Average global foreground Dice: [0.8022]
2022-08-02 05:32:33.580410: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:32:34.079109: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-02 05:32:34.083421: This epoch took 114.975333 s

2022-08-02 05:32:34.087044: 
epoch:  332
2022-08-02 05:34:23.500754: train loss : -0.8112
2022-08-02 05:34:31.881977: validation loss: -0.7360
2022-08-02 05:34:31.914679: Average global foreground Dice: [0.7952]
2022-08-02 05:34:31.934271: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:34:33.036900: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-02 05:34:33.085839: This epoch took 118.996514 s

2022-08-02 05:34:33.118793: 
epoch:  333
2022-08-02 05:36:21.023595: train loss : -0.8122
2022-08-02 05:36:28.640755: validation loss: -0.7473
2022-08-02 05:36:28.667049: Average global foreground Dice: [0.8037]
2022-08-02 05:36:28.669672: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:36:29.268512: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-02 05:36:29.297837: This epoch took 116.156980 s

2022-08-02 05:36:29.327102: 
epoch:  334
2022-08-02 05:38:19.242867: train loss : -0.8128
2022-08-02 05:38:27.810931: validation loss: -0.7584
2022-08-02 05:38:27.818129: Average global foreground Dice: [0.813]
2022-08-02 05:38:27.837744: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:38:28.630583: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-02 05:38:28.665787: This epoch took 119.310285 s

2022-08-02 05:38:28.700781: 
epoch:  335
2022-08-02 05:40:16.968773: train loss : -0.8158
2022-08-02 05:40:24.115928: validation loss: -0.7393
2022-08-02 05:40:24.119100: Average global foreground Dice: [0.796]
2022-08-02 05:40:24.121468: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:40:24.600564: Suus1 maybe_update_lr lr: 3.7e-05
2022-08-02 05:40:24.603075: This epoch took 115.886467 s

2022-08-02 05:40:24.605232: 
epoch:  336
2022-08-02 05:42:15.575923: train loss : -0.8042
2022-08-02 05:42:25.196946: validation loss: -0.7373
2022-08-02 05:42:25.216602: Average global foreground Dice: [0.7944]
2022-08-02 05:42:25.239389: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:42:26.148112: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-02 05:42:26.183822: This epoch took 121.576374 s

2022-08-02 05:42:26.222825: 
epoch:  337
2022-08-02 05:44:15.254912: train loss : -0.8112
2022-08-02 05:44:24.963159: validation loss: -0.7664
2022-08-02 05:44:24.995399: Average global foreground Dice: [0.8198]
2022-08-02 05:44:25.015810: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:44:26.029768: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-02 05:44:26.077872: This epoch took 119.832579 s

2022-08-02 05:44:26.124870: 
epoch:  338
2022-08-02 05:46:18.161842: train loss : -0.8138
2022-08-02 05:46:25.376435: validation loss: -0.7596
2022-08-02 05:46:25.379915: Average global foreground Dice: [0.816]
2022-08-02 05:46:25.382331: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:46:25.872051: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-02 05:46:25.874472: This epoch took 119.730869 s

2022-08-02 05:46:25.876679: 
epoch:  339
2022-08-02 05:48:23.634394: train loss : -0.8113
2022-08-02 05:48:32.596292: validation loss: -0.7413
2022-08-02 05:48:32.608312: Average global foreground Dice: [0.7971]
2022-08-02 05:48:32.641972: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:48:33.417191: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-02 05:48:33.449822: This epoch took 127.570917 s

2022-08-02 05:48:33.481766: 
epoch:  340
2022-08-02 05:50:20.479730: train loss : -0.8196
2022-08-02 05:50:28.648024: validation loss: -0.7372
2022-08-02 05:50:28.671315: Average global foreground Dice: [0.7959]
2022-08-02 05:50:28.708428: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:50:29.295067: Suus1 maybe_update_lr lr: 3.6e-05
2022-08-02 05:50:29.297534: This epoch took 115.783761 s

2022-08-02 05:50:29.301964: 
epoch:  341
2022-08-02 05:52:14.950742: train loss : -0.8165
2022-08-02 05:52:21.601341: validation loss: -0.7582
2022-08-02 05:52:21.605280: Average global foreground Dice: [0.8131]
2022-08-02 05:52:21.607825: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:52:22.116429: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-02 05:52:22.118951: This epoch took 112.814372 s

2022-08-02 05:52:22.121439: 
epoch:  342
2022-08-02 05:54:13.503726: train loss : -0.8050
2022-08-02 05:54:21.841804: validation loss: -0.7565
2022-08-02 05:54:21.883240: Average global foreground Dice: [0.8107]
2022-08-02 05:54:21.920765: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:54:22.808871: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-02 05:54:22.827388: This epoch took 120.703353 s

2022-08-02 05:54:22.868224: 
epoch:  343
2022-08-02 05:56:11.400205: train loss : -0.8159
2022-08-02 05:56:19.104247: validation loss: -0.7427
2022-08-02 05:56:19.112830: Average global foreground Dice: [0.8019]
2022-08-02 05:56:19.122787: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:56:20.834210: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-02 05:56:20.864824: This epoch took 117.958033 s

2022-08-02 05:56:20.886747: 
epoch:  344
2022-08-02 05:58:08.299459: train loss : -0.8199
2022-08-02 05:58:16.298540: validation loss: -0.7741
2022-08-02 05:58:16.301921: Average global foreground Dice: [0.8317]
2022-08-02 05:58:16.304122: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 05:58:16.811605: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-02 05:58:16.814152: saving best epoch checkpoint...
2022-08-02 05:58:17.057272: saving checkpoint...
2022-08-02 05:58:21.948652: done, saving took 5.13 seconds
2022-08-02 05:58:21.962575: This epoch took 121.053830 s

2022-08-02 05:58:21.964874: 
epoch:  345
2022-08-02 06:00:09.913077: train loss : -0.8156
2022-08-02 06:00:17.614139: validation loss: -0.7646
2022-08-02 06:00:17.631445: Average global foreground Dice: [0.8211]
2022-08-02 06:00:17.634533: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:00:18.232702: Suus1 maybe_update_lr lr: 3.5e-05
2022-08-02 06:00:18.246186: saving best epoch checkpoint...
2022-08-02 06:00:18.452884: saving checkpoint...
2022-08-02 06:00:23.478861: done, saving took 5.22 seconds
2022-08-02 06:00:23.495498: This epoch took 121.528233 s

2022-08-02 06:00:23.498297: 
epoch:  346
2022-08-02 06:02:11.842747: train loss : -0.8150
2022-08-02 06:02:20.145189: validation loss: -0.7678
2022-08-02 06:02:20.149198: Average global foreground Dice: [0.8224]
2022-08-02 06:02:20.151518: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:02:20.776533: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-02 06:02:20.798268: saving best epoch checkpoint...
2022-08-02 06:02:21.227064: saving checkpoint...
2022-08-02 06:02:26.678109: done, saving took 5.85 seconds
2022-08-02 06:02:26.695834: This epoch took 123.195596 s

2022-08-02 06:02:26.698090: 
epoch:  347
2022-08-02 06:04:13.326413: train loss : -0.8157
2022-08-02 06:04:21.493338: validation loss: -0.7391
2022-08-02 06:04:21.525591: Average global foreground Dice: [0.7964]
2022-08-02 06:04:21.552818: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:04:22.292150: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-02 06:04:22.314909: This epoch took 115.614581 s

2022-08-02 06:04:22.347758: 
epoch:  348
2022-08-02 06:06:10.020697: train loss : -0.8134
2022-08-02 06:06:17.768349: validation loss: -0.7689
2022-08-02 06:06:17.799046: Average global foreground Dice: [0.8258]
2022-08-02 06:06:17.809920: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:06:18.519765: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-02 06:06:18.522292: saving best epoch checkpoint...
2022-08-02 06:06:18.779249: saving checkpoint...
2022-08-02 06:06:23.986123: done, saving took 5.46 seconds
2022-08-02 06:06:24.002810: This epoch took 121.630987 s

2022-08-02 06:06:24.005308: 
epoch:  349
2022-08-02 06:08:10.177119: train loss : -0.8192
2022-08-02 06:08:17.978142: validation loss: -0.7666
2022-08-02 06:08:17.993130: Average global foreground Dice: [0.8169]
2022-08-02 06:08:18.000176: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:08:18.856573: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-02 06:08:18.859300: saving scheduled checkpoint file...
2022-08-02 06:08:18.995924: saving checkpoint...
2022-08-02 06:08:23.750007: done, saving took 4.89 seconds
2022-08-02 06:08:23.763287: done
2022-08-02 06:08:23.765480: saving best epoch checkpoint...
2022-08-02 06:08:23.870652: saving checkpoint...
2022-08-02 06:08:28.689106: done, saving took 4.92 seconds
2022-08-02 06:08:28.701975: This epoch took 124.694411 s

2022-08-02 06:08:28.703991: 
epoch:  350
2022-08-02 06:10:18.606130: train loss : -0.8140
2022-08-02 06:10:25.568396: validation loss: -0.7428
2022-08-02 06:10:25.585807: Average global foreground Dice: [0.8061]
2022-08-02 06:10:25.588228: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:10:26.147851: Suus1 maybe_update_lr lr: 3.4e-05
2022-08-02 06:10:26.150771: This epoch took 117.444628 s

2022-08-02 06:10:26.153217: 
epoch:  351
2022-08-02 06:12:14.987752: train loss : -0.8179
2022-08-02 06:12:23.025429: validation loss: -0.7533
2022-08-02 06:12:23.060682: Average global foreground Dice: [0.8116]
2022-08-02 06:12:23.096757: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:12:23.696408: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-02 06:12:23.709539: This epoch took 117.554220 s

2022-08-02 06:12:23.725065: 
epoch:  352
2022-08-02 06:14:13.175628: train loss : -0.8189
2022-08-02 06:14:20.782107: validation loss: -0.7450
2022-08-02 06:14:20.785761: Average global foreground Dice: [0.8053]
2022-08-02 06:14:20.788324: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:14:21.346601: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-02 06:14:21.348977: This epoch took 117.615290 s

2022-08-02 06:14:21.351051: 
epoch:  353
2022-08-02 06:16:07.203170: train loss : -0.8133
2022-08-02 06:16:14.419490: validation loss: -0.7470
2022-08-02 06:16:14.438518: Average global foreground Dice: [0.8057]
2022-08-02 06:16:14.441161: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:16:14.954805: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-02 06:16:14.957937: This epoch took 113.604674 s

2022-08-02 06:16:14.960514: 
epoch:  354
2022-08-02 06:18:01.150142: train loss : -0.8213
2022-08-02 06:18:08.111355: validation loss: -0.7603
2022-08-02 06:18:08.114356: Average global foreground Dice: [0.8083]
2022-08-02 06:18:08.116541: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:18:08.575639: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-02 06:18:08.578741: This epoch took 113.615828 s

2022-08-02 06:18:08.580950: 
epoch:  355
2022-08-02 06:19:53.278087: train loss : -0.8183
2022-08-02 06:19:59.355503: validation loss: -0.7518
2022-08-02 06:19:59.358965: Average global foreground Dice: [0.8106]
2022-08-02 06:19:59.361356: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:19:59.813101: Suus1 maybe_update_lr lr: 3.3e-05
2022-08-02 06:19:59.816525: This epoch took 111.233472 s

2022-08-02 06:19:59.819001: 
epoch:  356
2022-08-02 06:21:43.186295: train loss : -0.8170
2022-08-02 06:21:49.665526: validation loss: -0.7658
2022-08-02 06:21:49.668709: Average global foreground Dice: [0.8193]
2022-08-02 06:21:49.671144: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:21:50.132363: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-02 06:21:50.135130: This epoch took 110.313644 s

2022-08-02 06:21:50.137373: 
epoch:  357
2022-08-02 06:23:36.789740: train loss : -0.8205
2022-08-02 06:23:44.470013: validation loss: -0.7421
2022-08-02 06:23:44.494709: Average global foreground Dice: [0.8008]
2022-08-02 06:23:44.535788: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:23:45.321824: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-02 06:23:45.324365: This epoch took 115.184918 s

2022-08-02 06:23:45.326406: 
epoch:  358
2022-08-02 06:25:30.482590: train loss : -0.8211
2022-08-02 06:25:37.027342: validation loss: -0.7748
2022-08-02 06:25:37.030478: Average global foreground Dice: [0.8255]
2022-08-02 06:25:37.032649: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:25:37.489122: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-02 06:25:37.491481: This epoch took 112.162858 s

2022-08-02 06:25:37.493651: 
epoch:  359
2022-08-02 06:27:29.638357: train loss : -0.8197
2022-08-02 06:27:37.752498: validation loss: -0.7621
2022-08-02 06:27:37.801563: Average global foreground Dice: [0.8172]
2022-08-02 06:27:37.841910: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:27:38.835363: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-02 06:27:38.864798: saving best epoch checkpoint...
2022-08-02 06:27:39.187573: saving checkpoint...
2022-08-02 06:27:44.511838: done, saving took 5.63 seconds
2022-08-02 06:27:44.523323: This epoch took 127.027581 s

2022-08-02 06:27:44.525528: 
epoch:  360
2022-08-02 06:29:31.434856: train loss : -0.8197
2022-08-02 06:29:38.043985: validation loss: -0.7551
2022-08-02 06:29:38.047463: Average global foreground Dice: [0.8085]
2022-08-02 06:29:38.049592: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:29:38.515976: Suus1 maybe_update_lr lr: 3.2e-05
2022-08-02 06:29:38.518610: This epoch took 113.990756 s

2022-08-02 06:29:38.520988: 
epoch:  361
2022-08-02 06:31:27.750235: train loss : -0.8187
2022-08-02 06:31:35.024332: validation loss: -0.7244
2022-08-02 06:31:35.027778: Average global foreground Dice: [0.7781]
2022-08-02 06:31:35.030087: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:31:35.485439: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-02 06:31:35.488098: This epoch took 116.965020 s

2022-08-02 06:31:35.490537: 
epoch:  362
2022-08-02 06:33:23.869667: train loss : -0.8143
2022-08-02 06:33:31.847538: validation loss: -0.7379
2022-08-02 06:33:31.886334: Average global foreground Dice: [0.7902]
2022-08-02 06:33:31.916777: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:33:32.797461: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-02 06:33:32.802851: This epoch took 117.310146 s

2022-08-02 06:33:32.805302: 
epoch:  363
2022-08-02 06:35:20.055327: train loss : -0.8166
2022-08-02 06:35:27.514501: validation loss: -0.7416
2022-08-02 06:35:27.535718: Average global foreground Dice: [0.8012]
2022-08-02 06:35:27.552205: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:35:28.139903: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-02 06:35:28.173662: This epoch took 115.361536 s

2022-08-02 06:35:28.213033: 
epoch:  364
2022-08-02 06:37:19.036753: train loss : -0.8206
2022-08-02 06:37:27.022848: validation loss: -0.7419
2022-08-02 06:37:27.044042: Average global foreground Dice: [0.7978]
2022-08-02 06:37:27.069323: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:37:27.962218: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-02 06:37:27.994809: This epoch took 119.751901 s

2022-08-02 06:37:28.027748: 
epoch:  365
2022-08-02 06:39:17.494608: train loss : -0.8108
2022-08-02 06:39:26.626172: validation loss: -0.7512
2022-08-02 06:39:26.630087: Average global foreground Dice: [0.8117]
2022-08-02 06:39:26.633496: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:39:27.284431: Suus1 maybe_update_lr lr: 3.1e-05
2022-08-02 06:39:27.286914: This epoch took 119.237068 s

2022-08-02 06:39:27.291180: 
epoch:  366
2022-08-02 06:41:11.926421: train loss : -0.8220
2022-08-02 06:41:18.899079: validation loss: -0.7512
2022-08-02 06:41:18.906376: Average global foreground Dice: [0.8099]
2022-08-02 06:41:18.908656: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:41:19.588815: Suus1 maybe_update_lr lr: 3e-05
2022-08-02 06:41:19.615233: This epoch took 112.321861 s

2022-08-02 06:41:19.636631: 
epoch:  367
2022-08-02 06:43:04.531254: train loss : -0.8269
2022-08-02 06:43:10.785969: validation loss: -0.7342
2022-08-02 06:43:10.808815: Average global foreground Dice: [0.7949]
2022-08-02 06:43:10.838753: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:43:11.346323: Suus1 maybe_update_lr lr: 3e-05
2022-08-02 06:43:11.368400: This epoch took 111.711406 s

2022-08-02 06:43:11.387968: 
epoch:  368
2022-08-02 06:44:59.285178: train loss : -0.8213
2022-08-02 06:45:06.969004: validation loss: -0.7753
2022-08-02 06:45:06.993040: Average global foreground Dice: [0.8244]
2022-08-02 06:45:07.036807: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:45:07.627887: Suus1 maybe_update_lr lr: 3e-05
2022-08-02 06:45:07.652634: This epoch took 116.243396 s

2022-08-02 06:45:07.685757: 
epoch:  369
2022-08-02 06:47:00.953577: train loss : -0.8219
2022-08-02 06:47:10.708179: validation loss: -0.7409
2022-08-02 06:47:10.751473: Average global foreground Dice: [0.7965]
2022-08-02 06:47:10.781012: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:47:11.476720: Suus1 maybe_update_lr lr: 3e-05
2022-08-02 06:47:11.497816: This epoch took 123.778052 s

2022-08-02 06:47:11.539923: 
epoch:  370
2022-08-02 06:49:00.740104: train loss : -0.8220
2022-08-02 06:49:09.082876: validation loss: -0.7746
2022-08-02 06:49:09.100536: Average global foreground Dice: [0.8281]
2022-08-02 06:49:09.130678: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:49:09.859855: Suus1 maybe_update_lr lr: 3e-05
2022-08-02 06:49:09.862929: This epoch took 118.299141 s

2022-08-02 06:49:09.865098: 
epoch:  371
2022-08-02 06:50:58.126226: train loss : -0.8164
2022-08-02 06:51:06.824260: validation loss: -0.7586
2022-08-02 06:51:06.853412: Average global foreground Dice: [0.813]
2022-08-02 06:51:06.876292: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:51:07.562725: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-02 06:51:07.565560: This epoch took 117.698441 s

2022-08-02 06:51:07.568099: 
epoch:  372
2022-08-02 06:52:53.446743: train loss : -0.8202
2022-08-02 06:53:00.905131: validation loss: -0.7385
2022-08-02 06:53:00.908642: Average global foreground Dice: [0.7947]
2022-08-02 06:53:00.919885: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:53:01.408878: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-02 06:53:01.411714: This epoch took 113.841306 s

2022-08-02 06:53:01.414052: 
epoch:  373
2022-08-02 06:54:48.278170: train loss : -0.8238
2022-08-02 06:54:55.161348: validation loss: -0.7723
2022-08-02 06:54:55.164758: Average global foreground Dice: [0.8269]
2022-08-02 06:54:55.167009: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:54:55.678948: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-02 06:54:55.681643: This epoch took 114.265333 s

2022-08-02 06:54:55.683943: 
epoch:  374
2022-08-02 06:56:45.703629: train loss : -0.8208
2022-08-02 06:56:53.342301: validation loss: -0.7781
2022-08-02 06:56:53.367193: Average global foreground Dice: [0.826]
2022-08-02 06:56:53.386049: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:56:54.002290: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-02 06:56:54.022230: This epoch took 118.335440 s

2022-08-02 06:56:54.049401: 
epoch:  375
2022-08-02 06:58:43.295199: train loss : -0.8221
2022-08-02 06:58:51.700457: validation loss: -0.7846
2022-08-02 06:58:51.706935: Average global foreground Dice: [0.8355]
2022-08-02 06:58:51.712351: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 06:58:52.216575: Suus1 maybe_update_lr lr: 2.9e-05
2022-08-02 06:58:52.219168: saving best epoch checkpoint...
2022-08-02 06:58:52.461357: saving checkpoint...
2022-08-02 06:58:57.558191: done, saving took 5.34 seconds
2022-08-02 06:58:57.573220: This epoch took 123.513581 s

2022-08-02 06:58:57.575656: 
epoch:  376
2022-08-02 07:00:44.090327: train loss : -0.8221
2022-08-02 07:00:51.339220: validation loss: -0.7692
2022-08-02 07:00:51.342487: Average global foreground Dice: [0.8237]
2022-08-02 07:00:51.344931: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:00:51.839606: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-02 07:00:51.842215: saving best epoch checkpoint...
2022-08-02 07:00:52.056224: saving checkpoint...
2022-08-02 07:00:56.995440: done, saving took 5.15 seconds
2022-08-02 07:00:57.014293: This epoch took 119.436588 s

2022-08-02 07:00:57.016594: 
epoch:  377
2022-08-02 07:02:46.576066: train loss : -0.8173
2022-08-02 07:02:56.761594: validation loss: -0.7414
2022-08-02 07:02:56.781388: Average global foreground Dice: [0.8002]
2022-08-02 07:02:56.816921: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:02:57.671231: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-02 07:02:57.673863: This epoch took 120.654942 s

2022-08-02 07:02:57.675874: 
epoch:  378
2022-08-02 07:04:45.332018: train loss : -0.8222
2022-08-02 07:04:53.144492: validation loss: -0.7573
2022-08-02 07:04:53.180330: Average global foreground Dice: [0.8114]
2022-08-02 07:04:53.202752: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:04:53.806066: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-02 07:04:53.814764: This epoch took 116.136602 s

2022-08-02 07:04:53.842917: 
epoch:  379
2022-08-02 07:06:42.720779: train loss : -0.8218
2022-08-02 07:06:51.169699: validation loss: -0.7465
2022-08-02 07:06:51.192892: Average global foreground Dice: [0.807]
2022-08-02 07:06:51.223153: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:06:52.307643: Suus1 maybe_update_lr lr: 2.8e-05
2022-08-02 07:06:52.339850: This epoch took 118.469027 s

2022-08-02 07:06:52.371778: 
epoch:  380
2022-08-02 07:08:41.852161: train loss : -0.8254
2022-08-02 07:08:51.514831: validation loss: -0.7589
2022-08-02 07:08:51.543857: Average global foreground Dice: [0.8159]
2022-08-02 07:08:51.569795: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:08:52.754230: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-02 07:08:52.775809: This epoch took 120.375030 s

2022-08-02 07:08:52.801842: 
epoch:  381
2022-08-02 07:10:43.281371: train loss : -0.8245
2022-08-02 07:10:51.189779: validation loss: -0.7435
2022-08-02 07:10:51.219500: Average global foreground Dice: [0.8044]
2022-08-02 07:10:51.246914: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:10:51.814884: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-02 07:10:51.817241: This epoch took 118.999003 s

2022-08-02 07:10:51.819422: 
epoch:  382
2022-08-02 07:12:38.670118: train loss : -0.8270
2022-08-02 07:12:46.368626: validation loss: -0.7617
2022-08-02 07:12:46.386392: Average global foreground Dice: [0.8186]
2022-08-02 07:12:46.415184: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:12:47.013890: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-02 07:12:47.021957: This epoch took 115.200393 s

2022-08-02 07:12:47.040887: 
epoch:  383
2022-08-02 07:14:34.294461: train loss : -0.8265
2022-08-02 07:14:41.140341: validation loss: -0.7379
2022-08-02 07:14:41.143482: Average global foreground Dice: [0.793]
2022-08-02 07:14:41.145624: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:14:41.617217: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-02 07:14:41.619513: This epoch took 114.569547 s

2022-08-02 07:14:41.622002: 
epoch:  384
2022-08-02 07:16:32.740411: train loss : -0.8189
2022-08-02 07:16:43.574927: validation loss: -0.7248
2022-08-02 07:16:43.592593: Average global foreground Dice: [0.7861]
2022-08-02 07:16:43.637771: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:16:44.258354: Suus1 maybe_update_lr lr: 2.7e-05
2022-08-02 07:16:44.298697: This epoch took 122.674306 s

2022-08-02 07:16:44.322208: 
epoch:  385
2022-08-02 07:18:34.724015: train loss : -0.8253
2022-08-02 07:18:43.008732: validation loss: -0.7718
2022-08-02 07:18:43.049479: Average global foreground Dice: [0.8221]
2022-08-02 07:18:43.069116: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:18:43.770217: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-02 07:18:43.800934: This epoch took 119.475496 s

2022-08-02 07:18:43.833748: 
epoch:  386
2022-08-02 07:20:39.406809: train loss : -0.8151
2022-08-02 07:20:48.775696: validation loss: -0.7486
2022-08-02 07:20:48.811430: Average global foreground Dice: [0.8068]
2022-08-02 07:20:48.861793: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:20:49.435244: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-02 07:20:49.469959: This epoch took 125.601201 s

2022-08-02 07:20:49.498107: 
epoch:  387
2022-08-02 07:22:39.552215: train loss : -0.8229
2022-08-02 07:22:50.267895: validation loss: -0.7691
2022-08-02 07:22:50.297285: Average global foreground Dice: [0.819]
2022-08-02 07:22:50.372757: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:22:51.555100: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-02 07:22:51.587883: This epoch took 122.057117 s

2022-08-02 07:22:51.614448: 
epoch:  388
2022-08-02 07:24:41.143215: train loss : -0.8192
2022-08-02 07:24:50.843136: validation loss: -0.7218
2022-08-02 07:24:50.854833: Average global foreground Dice: [0.7819]
2022-08-02 07:24:50.882596: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:24:51.471267: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-02 07:24:51.497964: This epoch took 119.850058 s

2022-08-02 07:24:51.505072: 
epoch:  389
2022-08-02 07:26:42.705423: train loss : -0.8223
2022-08-02 07:26:49.347851: validation loss: -0.7711
2022-08-02 07:26:49.351385: Average global foreground Dice: [0.8261]
2022-08-02 07:26:49.353563: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:26:49.940031: Suus1 maybe_update_lr lr: 2.6e-05
2022-08-02 07:26:49.954592: This epoch took 118.446117 s

2022-08-02 07:26:49.961529: 
epoch:  390
2022-08-02 07:28:37.679192: train loss : -0.8224
2022-08-02 07:28:44.636731: validation loss: -0.7406
2022-08-02 07:28:44.661057: Average global foreground Dice: [0.8013]
2022-08-02 07:28:44.680226: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:28:45.366664: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-02 07:28:45.369151: This epoch took 115.403063 s

2022-08-02 07:28:45.372886: 
epoch:  391
2022-08-02 07:30:32.718696: train loss : -0.8281
2022-08-02 07:30:40.808052: validation loss: -0.7547
2022-08-02 07:30:40.813690: Average global foreground Dice: [0.8071]
2022-08-02 07:30:40.816034: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:30:41.326427: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-02 07:30:41.328828: This epoch took 115.952123 s

2022-08-02 07:30:41.331206: 
epoch:  392
2022-08-02 07:32:31.259715: train loss : -0.8197
2022-08-02 07:32:38.201275: validation loss: -0.7451
2022-08-02 07:32:38.204903: Average global foreground Dice: [0.8008]
2022-08-02 07:32:38.207441: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:32:38.684955: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-02 07:32:38.687482: This epoch took 117.353959 s

2022-08-02 07:32:38.689608: 
epoch:  393
2022-08-02 07:34:26.082186: train loss : -0.8292
2022-08-02 07:34:33.515345: validation loss: -0.7587
2022-08-02 07:34:33.570307: Average global foreground Dice: [0.8133]
2022-08-02 07:34:33.596092: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:34:34.302320: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-02 07:34:34.314223: This epoch took 115.622518 s

2022-08-02 07:34:34.334854: 
epoch:  394
2022-08-02 07:36:23.504788: train loss : -0.8227
2022-08-02 07:36:32.628830: validation loss: -0.7573
2022-08-02 07:36:32.656410: Average global foreground Dice: [0.8105]
2022-08-02 07:36:32.677776: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:36:33.193976: Suus1 maybe_update_lr lr: 2.5e-05
2022-08-02 07:36:33.212811: This epoch took 118.863409 s

2022-08-02 07:36:33.238748: 
epoch:  395
2022-08-02 07:38:21.716077: train loss : -0.8264
2022-08-02 07:38:28.713188: validation loss: -0.7640
2022-08-02 07:38:28.716959: Average global foreground Dice: [0.8145]
2022-08-02 07:38:28.723329: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:38:29.269505: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-02 07:38:29.272574: This epoch took 116.016791 s

2022-08-02 07:38:29.275315: 
epoch:  396
2022-08-02 07:40:17.056262: train loss : -0.8206
2022-08-02 07:40:23.509721: validation loss: -0.7795
2022-08-02 07:40:23.513398: Average global foreground Dice: [0.8321]
2022-08-02 07:40:23.515470: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:40:24.004884: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-02 07:40:24.025762: This epoch took 114.747985 s

2022-08-02 07:40:24.045902: 
epoch:  397
2022-08-02 07:42:13.746506: train loss : -0.8245
2022-08-02 07:42:22.684959: validation loss: -0.7673
2022-08-02 07:42:22.729762: Average global foreground Dice: [0.8214]
2022-08-02 07:42:22.776789: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:42:23.365353: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-02 07:42:23.368150: This epoch took 119.301038 s

2022-08-02 07:42:23.370515: 
epoch:  398
2022-08-02 07:44:12.263473: train loss : -0.8252
2022-08-02 07:44:19.944721: validation loss: -0.7709
2022-08-02 07:44:19.968352: Average global foreground Dice: [0.8247]
2022-08-02 07:44:20.005802: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:44:20.728259: Suus1 maybe_update_lr lr: 2.4e-05
2022-08-02 07:44:20.760804: This epoch took 117.388160 s

2022-08-02 07:44:20.793755: 
epoch:  399
2022-08-02 07:46:11.135341: train loss : -0.8251
2022-08-02 07:46:18.439309: validation loss: -0.7528
2022-08-02 07:46:18.442465: Average global foreground Dice: [0.8052]
2022-08-02 07:46:18.444686: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:46:18.919420: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-02 07:46:18.922457: saving scheduled checkpoint file...
2022-08-02 07:46:19.068491: saving checkpoint...
2022-08-02 07:46:23.978264: done, saving took 5.05 seconds
2022-08-02 07:46:23.992147: done
2022-08-02 07:46:23.994348: This epoch took 123.166578 s

2022-08-02 07:46:23.996669: 
epoch:  400
2022-08-02 07:48:11.916353: train loss : -0.8267
2022-08-02 07:48:19.438018: validation loss: -0.7916
2022-08-02 07:48:19.470381: Average global foreground Dice: [0.8417]
2022-08-02 07:48:19.501143: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:48:20.213721: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-02 07:48:20.249085: saving best epoch checkpoint...
2022-08-02 07:48:20.511415: saving checkpoint...
2022-08-02 07:48:25.703233: done, saving took 5.44 seconds
2022-08-02 07:48:25.717293: This epoch took 121.718482 s

2022-08-02 07:48:25.719812: 
epoch:  401
2022-08-02 07:50:12.467497: train loss : -0.8294
2022-08-02 07:50:20.288268: validation loss: -0.7753
2022-08-02 07:50:20.292719: Average global foreground Dice: [0.8298]
2022-08-02 07:50:20.295178: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:50:20.853260: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-02 07:50:20.881169: saving best epoch checkpoint...
2022-08-02 07:50:21.133993: saving checkpoint...
2022-08-02 07:50:26.542596: done, saving took 5.65 seconds
2022-08-02 07:50:26.557395: This epoch took 120.835394 s

2022-08-02 07:50:26.559972: 
epoch:  402
2022-08-02 07:52:12.906048: train loss : -0.8288
2022-08-02 07:52:21.650331: validation loss: -0.7545
2022-08-02 07:52:21.668968: Average global foreground Dice: [0.8102]
2022-08-02 07:52:21.672184: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:52:22.213023: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-02 07:52:22.215420: This epoch took 115.652876 s

2022-08-02 07:52:22.217514: 
epoch:  403
2022-08-02 07:54:11.200019: train loss : -0.8296
2022-08-02 07:54:20.296046: validation loss: -0.7758
2022-08-02 07:54:20.353399: Average global foreground Dice: [0.8279]
2022-08-02 07:54:20.386782: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:54:20.987400: Suus1 maybe_update_lr lr: 2.3e-05
2022-08-02 07:54:21.014785: saving best epoch checkpoint...
2022-08-02 07:54:21.295149: saving checkpoint...
2022-08-02 07:54:26.442983: done, saving took 5.41 seconds
2022-08-02 07:54:26.459146: This epoch took 124.237397 s

2022-08-02 07:54:26.461344: 
epoch:  404
2022-08-02 07:56:14.475008: train loss : -0.8286
2022-08-02 07:56:22.048579: validation loss: -0.7184
2022-08-02 07:56:22.091531: Average global foreground Dice: [0.7785]
2022-08-02 07:56:22.145918: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:56:22.751651: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-02 07:56:22.754013: This epoch took 116.290557 s

2022-08-02 07:56:22.756003: 
epoch:  405
2022-08-02 07:58:09.329826: train loss : -0.8316
2022-08-02 07:58:17.022876: validation loss: -0.7500
2022-08-02 07:58:17.047436: Average global foreground Dice: [0.8083]
2022-08-02 07:58:17.077561: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 07:58:17.715741: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-02 07:58:17.725516: This epoch took 114.967404 s

2022-08-02 07:58:17.728046: 
epoch:  406
2022-08-02 08:00:07.365655: train loss : -0.8244
2022-08-02 08:00:14.842963: validation loss: -0.7657
2022-08-02 08:00:14.855768: Average global foreground Dice: [0.8176]
2022-08-02 08:00:14.862333: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:00:15.432065: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-02 08:00:15.434607: This epoch took 117.704179 s

2022-08-02 08:00:15.436733: 
epoch:  407
2022-08-02 08:02:04.555503: train loss : -0.8222
2022-08-02 08:02:11.784611: validation loss: -0.7658
2022-08-02 08:02:11.803381: Average global foreground Dice: [0.8145]
2022-08-02 08:02:11.824077: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:02:12.374926: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-02 08:02:12.385168: This epoch took 116.946183 s

2022-08-02 08:02:12.387758: 
epoch:  408
2022-08-02 08:03:59.001437: train loss : -0.8267
2022-08-02 08:04:05.960949: validation loss: -0.7632
2022-08-02 08:04:05.968280: Average global foreground Dice: [0.8187]
2022-08-02 08:04:05.970686: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:04:06.480941: Suus1 maybe_update_lr lr: 2.2e-05
2022-08-02 08:04:06.483631: This epoch took 114.093756 s

2022-08-02 08:04:06.485743: 
epoch:  409
2022-08-02 08:05:54.035012: train loss : -0.8326
2022-08-02 08:06:01.962389: validation loss: -0.7572
2022-08-02 08:06:01.994402: Average global foreground Dice: [0.8093]
2022-08-02 08:06:02.028792: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:06:02.697285: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-02 08:06:02.700066: This epoch took 116.212263 s

2022-08-02 08:06:02.702386: 
epoch:  410
2022-08-02 08:07:50.490884: train loss : -0.8261
2022-08-02 08:07:57.519745: validation loss: -0.7453
2022-08-02 08:07:57.523349: Average global foreground Dice: [0.8041]
2022-08-02 08:07:57.525692: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:07:58.023353: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-02 08:07:58.026271: This epoch took 115.321648 s

2022-08-02 08:07:58.028375: 
epoch:  411
2022-08-02 08:09:45.386452: train loss : -0.8257
2022-08-02 08:09:53.077018: validation loss: -0.7602
2022-08-02 08:09:53.117468: Average global foreground Dice: [0.8131]
2022-08-02 08:09:53.166018: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:09:54.035407: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-02 08:09:54.037870: This epoch took 116.007449 s

2022-08-02 08:09:54.039830: 
epoch:  412
2022-08-02 08:11:40.567773: train loss : -0.8325
2022-08-02 08:11:47.699732: validation loss: -0.7546
2022-08-02 08:11:47.706016: Average global foreground Dice: [0.8068]
2022-08-02 08:11:47.718779: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:11:48.251195: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-02 08:11:48.253707: This epoch took 114.211882 s

2022-08-02 08:11:48.256174: 
epoch:  413
2022-08-02 08:13:37.376048: train loss : -0.8322
2022-08-02 08:13:45.271235: validation loss: -0.7456
2022-08-02 08:13:45.288365: Average global foreground Dice: [0.7994]
2022-08-02 08:13:45.300157: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:13:45.990127: Suus1 maybe_update_lr lr: 2.1e-05
2022-08-02 08:13:45.993921: This epoch took 117.735674 s

2022-08-02 08:13:46.013754: 
epoch:  414
2022-08-02 08:15:33.833358: train loss : -0.8343
2022-08-02 08:15:42.695416: validation loss: -0.7529
2022-08-02 08:15:42.749526: Average global foreground Dice: [0.8095]
2022-08-02 08:15:42.790984: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:15:43.517311: Suus1 maybe_update_lr lr: 2e-05
2022-08-02 08:15:43.537746: This epoch took 117.505337 s

2022-08-02 08:15:43.553755: 
epoch:  415
2022-08-02 08:17:32.948269: train loss : -0.8269
2022-08-02 08:17:40.185155: validation loss: -0.7471
2022-08-02 08:17:40.188889: Average global foreground Dice: [0.8071]
2022-08-02 08:17:40.191399: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:17:40.749345: Suus1 maybe_update_lr lr: 2e-05
2022-08-02 08:17:40.752175: This epoch took 117.151462 s

2022-08-02 08:17:40.754504: 
epoch:  416
2022-08-02 08:19:28.238029: train loss : -0.8273
2022-08-02 08:19:36.154019: validation loss: -0.7333
2022-08-02 08:19:36.179304: Average global foreground Dice: [0.793]
2022-08-02 08:19:36.224287: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:19:36.957233: Suus1 maybe_update_lr lr: 2e-05
2022-08-02 08:19:36.960205: This epoch took 116.203401 s

2022-08-02 08:19:36.962213: 
epoch:  417
2022-08-02 08:21:23.623324: train loss : -0.8312
2022-08-02 08:21:30.932578: validation loss: -0.7535
2022-08-02 08:21:30.947047: Average global foreground Dice: [0.8174]
2022-08-02 08:21:30.956110: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:21:31.510292: Suus1 maybe_update_lr lr: 2e-05
2022-08-02 08:21:31.513980: This epoch took 114.549703 s

2022-08-02 08:21:31.518051: 
epoch:  418
2022-08-02 08:23:20.402420: train loss : -0.8249
2022-08-02 08:23:28.146621: validation loss: -0.7550
2022-08-02 08:23:28.151180: Average global foreground Dice: [0.8099]
2022-08-02 08:23:28.159627: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:23:28.728990: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-02 08:23:28.731502: This epoch took 117.209637 s

2022-08-02 08:23:28.733747: 
epoch:  419
2022-08-02 08:25:17.257520: train loss : -0.8318
2022-08-02 08:25:26.654691: validation loss: -0.7777
2022-08-02 08:25:26.660880: Average global foreground Dice: [0.8276]
2022-08-02 08:25:26.663361: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:25:27.160029: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-02 08:25:27.167034: This epoch took 118.431201 s

2022-08-02 08:25:27.170067: 
epoch:  420
2022-08-02 08:27:14.170881: train loss : -0.8307
2022-08-02 08:27:21.736521: validation loss: -0.7502
2022-08-02 08:27:21.763377: Average global foreground Dice: [0.8045]
2022-08-02 08:27:21.799352: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:27:22.608250: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-02 08:27:22.637817: This epoch took 115.464283 s

2022-08-02 08:27:22.670775: 
epoch:  421
2022-08-02 08:29:10.095679: train loss : -0.8314
2022-08-02 08:29:17.202000: validation loss: -0.7611
2022-08-02 08:29:17.212982: Average global foreground Dice: [0.8143]
2022-08-02 08:29:17.222626: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:29:17.704246: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-02 08:29:17.706564: This epoch took 115.002791 s

2022-08-02 08:29:17.708947: 
epoch:  422
2022-08-02 08:31:04.779629: train loss : -0.8326
2022-08-02 08:31:11.618654: validation loss: -0.7757
2022-08-02 08:31:11.621986: Average global foreground Dice: [0.8228]
2022-08-02 08:31:11.624381: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:31:12.127495: Suus1 maybe_update_lr lr: 1.9e-05
2022-08-02 08:31:12.130106: This epoch took 114.418212 s

2022-08-02 08:31:12.132482: 
epoch:  423
2022-08-02 08:32:59.801980: train loss : -0.8286
2022-08-02 08:33:08.577650: validation loss: -0.7716
2022-08-02 08:33:08.592503: Average global foreground Dice: [0.823]
2022-08-02 08:33:08.608137: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:33:09.213628: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-02 08:33:09.245811: This epoch took 117.111298 s

2022-08-02 08:33:09.259069: 
epoch:  424
2022-08-02 08:34:58.430821: train loss : -0.8307
2022-08-02 08:35:05.421997: validation loss: -0.7791
2022-08-02 08:35:05.425607: Average global foreground Dice: [0.8341]
2022-08-02 08:35:05.428011: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:35:05.950968: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-02 08:35:05.953485: This epoch took 116.660723 s

2022-08-02 08:35:05.955679: 
epoch:  425
2022-08-02 08:36:53.907824: train loss : -0.8275
2022-08-02 08:37:01.184496: validation loss: -0.7821
2022-08-02 08:37:01.202424: Average global foreground Dice: [0.8295]
2022-08-02 08:37:01.218768: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:37:01.733883: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-02 08:37:01.736498: This epoch took 115.778648 s

2022-08-02 08:37:01.738830: 
epoch:  426
2022-08-02 08:38:47.635664: train loss : -0.8387
2022-08-02 08:38:54.943574: validation loss: -0.7498
2022-08-02 08:38:54.946906: Average global foreground Dice: [0.8022]
2022-08-02 08:38:54.949562: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:38:55.419435: Suus1 maybe_update_lr lr: 1.8e-05
2022-08-02 08:38:55.422126: This epoch took 113.681040 s

2022-08-02 08:38:55.424080: 
epoch:  427
2022-08-02 08:40:44.597375: train loss : -0.8316
2022-08-02 08:40:51.921825: validation loss: -0.7745
2022-08-02 08:40:51.956242: Average global foreground Dice: [0.8302]
2022-08-02 08:40:51.988810: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:40:52.831891: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-02 08:40:52.864861: This epoch took 117.438666 s

2022-08-02 08:40:52.898456: 
epoch:  428
2022-08-02 08:42:40.881768: train loss : -0.8297
2022-08-02 08:42:48.167271: validation loss: -0.7822
2022-08-02 08:42:48.206498: Average global foreground Dice: [0.8299]
2022-08-02 08:42:48.229934: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:42:48.972057: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-02 08:42:48.974470: saving best epoch checkpoint...
2022-08-02 08:42:49.117974: saving checkpoint...
2022-08-02 08:42:54.190766: done, saving took 5.21 seconds
2022-08-02 08:42:54.203460: This epoch took 121.277829 s

2022-08-02 08:42:54.205754: 
epoch:  429
2022-08-02 08:44:42.367981: train loss : -0.8319
2022-08-02 08:44:50.004392: validation loss: -0.7620
2022-08-02 08:44:50.019063: Average global foreground Dice: [0.8136]
2022-08-02 08:44:50.021642: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:44:50.511748: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-02 08:44:50.529569: This epoch took 116.321736 s

2022-08-02 08:44:50.541655: 
epoch:  430
2022-08-02 08:46:37.557873: train loss : -0.8303
2022-08-02 08:46:45.703065: validation loss: -0.7582
2022-08-02 08:46:45.707003: Average global foreground Dice: [0.8169]
2022-08-02 08:46:45.709200: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:46:46.225530: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-02 08:46:46.244937: This epoch took 115.699503 s

2022-08-02 08:46:46.268493: 
epoch:  431
2022-08-02 08:48:35.514531: train loss : -0.8332
2022-08-02 08:48:42.090188: validation loss: -0.7695
2022-08-02 08:48:42.093794: Average global foreground Dice: [0.8206]
2022-08-02 08:48:42.096183: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:48:42.588084: Suus1 maybe_update_lr lr: 1.7e-05
2022-08-02 08:48:42.591095: This epoch took 116.303206 s

2022-08-02 08:48:42.593673: 
epoch:  432
2022-08-02 08:50:31.505003: train loss : -0.8284
2022-08-02 08:50:41.073636: validation loss: -0.7571
2022-08-02 08:50:41.085410: Average global foreground Dice: [0.8172]
2022-08-02 08:50:41.097327: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:50:41.663928: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-02 08:50:41.677096: This epoch took 119.080963 s

2022-08-02 08:50:41.679802: 
epoch:  433
2022-08-02 08:52:32.923114: train loss : -0.8299
2022-08-02 08:52:42.064561: validation loss: -0.7681
2022-08-02 08:52:42.068123: Average global foreground Dice: [0.821]
2022-08-02 08:52:42.070760: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:52:42.654461: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-02 08:52:42.706001: saving best epoch checkpoint...
2022-08-02 08:52:43.034722: saving checkpoint...
2022-08-02 08:52:48.153300: done, saving took 5.42 seconds
2022-08-02 08:52:48.168909: This epoch took 126.486775 s

2022-08-02 08:52:48.171906: 
epoch:  434
2022-08-02 08:54:36.343116: train loss : -0.8353
2022-08-02 08:54:43.931161: validation loss: -0.7739
2022-08-02 08:54:43.935106: Average global foreground Dice: [0.8247]
2022-08-02 08:54:43.937514: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:54:44.435305: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-02 08:54:44.437871: saving best epoch checkpoint...
2022-08-02 08:54:44.732709: saving checkpoint...
2022-08-02 08:54:49.920602: done, saving took 5.48 seconds
2022-08-02 08:54:49.936804: This epoch took 121.762486 s

2022-08-02 08:54:49.939240: 
epoch:  435
2022-08-02 08:56:37.369107: train loss : -0.8318
2022-08-02 08:56:44.419098: validation loss: -0.7701
2022-08-02 08:56:44.448401: Average global foreground Dice: [0.8236]
2022-08-02 08:56:44.450965: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:56:45.054782: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-02 08:56:45.057557: saving best epoch checkpoint...
2022-08-02 08:56:45.323059: saving checkpoint...
2022-08-02 08:56:50.730464: done, saving took 5.67 seconds
2022-08-02 08:56:50.746241: This epoch took 120.804804 s

2022-08-02 08:56:50.748613: 
epoch:  436
2022-08-02 08:58:38.586335: train loss : -0.8316
2022-08-02 08:58:47.252074: validation loss: -0.7749
2022-08-02 08:58:47.287469: Average global foreground Dice: [0.8257]
2022-08-02 08:58:47.339961: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 08:58:48.207435: Suus1 maybe_update_lr lr: 1.6e-05
2022-08-02 08:58:48.224761: saving best epoch checkpoint...
2022-08-02 08:58:48.455032: saving checkpoint...
2022-08-02 08:58:53.544163: done, saving took 5.29 seconds
2022-08-02 08:58:53.555815: This epoch took 122.805065 s

2022-08-02 08:58:53.558177: 
epoch:  437
2022-08-02 09:00:42.036761: train loss : -0.8300
2022-08-02 09:00:51.241977: validation loss: -0.7648
2022-08-02 09:00:51.261777: Average global foreground Dice: [0.8182]
2022-08-02 09:00:51.279886: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:00:51.895557: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-02 09:00:51.928024: This epoch took 118.367543 s

2022-08-02 09:00:51.956544: 
epoch:  438
2022-08-02 09:02:40.569401: train loss : -0.8319
2022-08-02 09:02:47.422359: validation loss: -0.7543
2022-08-02 09:02:47.435758: Average global foreground Dice: [0.8093]
2022-08-02 09:02:47.466641: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:02:48.087958: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-02 09:02:48.199491: This epoch took 116.216658 s

2022-08-02 09:02:48.219340: 
epoch:  439
2022-08-02 09:04:36.229989: train loss : -0.8285
2022-08-02 09:04:43.125805: validation loss: -0.7679
2022-08-02 09:04:43.129270: Average global foreground Dice: [0.8221]
2022-08-02 09:04:43.131622: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:04:43.630726: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-02 09:04:43.633653: This epoch took 115.411780 s

2022-08-02 09:04:43.635859: 
epoch:  440
2022-08-02 09:06:30.528306: train loss : -0.8337
2022-08-02 09:06:37.439513: validation loss: -0.7692
2022-08-02 09:06:37.475490: Average global foreground Dice: [0.8233]
2022-08-02 09:06:37.498501: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:06:38.044919: Suus1 maybe_update_lr lr: 1.5e-05
2022-08-02 09:06:38.047791: This epoch took 114.409599 s

2022-08-02 09:06:38.050051: 
epoch:  441
2022-08-02 09:08:25.700406: train loss : -0.8341
2022-08-02 09:08:32.831549: validation loss: -0.7629
2022-08-02 09:08:32.835050: Average global foreground Dice: [0.8198]
2022-08-02 09:08:32.837533: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:08:33.334257: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-02 09:08:33.336787: This epoch took 115.284598 s

2022-08-02 09:08:33.339031: 
epoch:  442
2022-08-02 09:10:19.885816: train loss : -0.8338
2022-08-02 09:10:27.558419: validation loss: -0.7581
2022-08-02 09:10:27.576857: Average global foreground Dice: [0.8163]
2022-08-02 09:10:27.614029: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:10:28.331314: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-02 09:10:28.367903: This epoch took 115.026702 s

2022-08-02 09:10:28.399484: 
epoch:  443
2022-08-02 09:12:13.776627: train loss : -0.8331
2022-08-02 09:12:20.966061: validation loss: -0.7625
2022-08-02 09:12:20.997367: Average global foreground Dice: [0.8178]
2022-08-02 09:12:21.029782: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:12:21.676913: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-02 09:12:21.689394: This epoch took 113.255245 s

2022-08-02 09:12:21.709632: 
epoch:  444
2022-08-02 09:14:10.143459: train loss : -0.8343
2022-08-02 09:14:17.917067: validation loss: -0.7738
2022-08-02 09:14:17.936853: Average global foreground Dice: [0.8203]
2022-08-02 09:14:17.964133: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:14:18.547648: Suus1 maybe_update_lr lr: 1.4e-05
2022-08-02 09:14:18.550630: This epoch took 116.831159 s

2022-08-02 09:14:18.553245: 
epoch:  445
2022-08-02 09:16:05.416895: train loss : -0.8252
2022-08-02 09:16:12.151075: validation loss: -0.7508
2022-08-02 09:16:12.184433: Average global foreground Dice: [0.8059]
2022-08-02 09:16:12.224752: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:16:12.807884: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-02 09:16:12.810231: This epoch took 114.254512 s

2022-08-02 09:16:12.812609: 
epoch:  446
2022-08-02 09:18:00.259135: train loss : -0.8335
2022-08-02 09:18:07.810981: validation loss: -0.7848
2022-08-02 09:18:07.816010: Average global foreground Dice: [0.8334]
2022-08-02 09:18:07.818498: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:18:08.319386: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-02 09:18:08.321890: This epoch took 115.507045 s

2022-08-02 09:18:08.323993: 
epoch:  447
2022-08-02 09:19:56.671835: train loss : -0.8319
2022-08-02 09:20:04.077507: validation loss: -0.7632
2022-08-02 09:20:04.081288: Average global foreground Dice: [0.8116]
2022-08-02 09:20:04.083784: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:20:04.656894: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-02 09:20:04.659869: This epoch took 116.333596 s

2022-08-02 09:20:04.662579: 
epoch:  448
2022-08-02 09:21:50.131817: train loss : -0.8352
2022-08-02 09:21:57.667534: validation loss: -0.7712
2022-08-02 09:21:57.670989: Average global foreground Dice: [0.8235]
2022-08-02 09:21:57.673360: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:21:58.213745: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-02 09:21:58.217290: This epoch took 113.552614 s

2022-08-02 09:21:58.229367: 
epoch:  449
2022-08-02 09:23:44.400662: train loss : -0.8353
2022-08-02 09:23:51.471645: validation loss: -0.7683
2022-08-02 09:23:51.516268: Average global foreground Dice: [0.8216]
2022-08-02 09:23:51.534786: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:23:52.081551: Suus1 maybe_update_lr lr: 1.3e-05
2022-08-02 09:23:52.084724: saving scheduled checkpoint file...
2022-08-02 09:23:52.280421: saving checkpoint...
2022-08-02 09:23:57.516549: done, saving took 5.43 seconds
2022-08-02 09:23:57.531778: done
2022-08-02 09:23:57.534530: This epoch took 119.302537 s

2022-08-02 09:23:57.536519: 
epoch:  450
2022-08-02 09:25:45.175909: train loss : -0.8317
2022-08-02 09:25:53.342842: validation loss: -0.7638
2022-08-02 09:25:53.379543: Average global foreground Dice: [0.8166]
2022-08-02 09:25:53.424769: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:25:54.074177: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-02 09:25:54.077126: This epoch took 116.538409 s

2022-08-02 09:25:54.079331: 
epoch:  451
2022-08-02 09:27:42.677421: train loss : -0.8354
2022-08-02 09:27:49.621583: validation loss: -0.7455
2022-08-02 09:27:49.625311: Average global foreground Dice: [0.8011]
2022-08-02 09:27:49.628115: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:27:50.113531: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-02 09:27:50.116652: This epoch took 116.035205 s

2022-08-02 09:27:50.119597: 
epoch:  452
2022-08-02 09:29:36.707024: train loss : -0.8321
2022-08-02 09:29:44.047025: validation loss: -0.7558
2022-08-02 09:29:44.126750: Average global foreground Dice: [0.8153]
2022-08-02 09:29:44.146024: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:29:44.726880: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-02 09:29:44.729822: This epoch took 114.608039 s

2022-08-02 09:29:44.732144: 
epoch:  453
2022-08-02 09:31:33.834943: train loss : -0.8329
2022-08-02 09:31:41.011419: validation loss: -0.7638
2022-08-02 09:31:41.016792: Average global foreground Dice: [0.8208]
2022-08-02 09:31:41.034185: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:31:41.607966: Suus1 maybe_update_lr lr: 1.2e-05
2022-08-02 09:31:41.611035: This epoch took 116.876445 s

2022-08-02 09:31:41.613394: 
epoch:  454
2022-08-02 09:33:33.122124: train loss : -0.8376
2022-08-02 09:33:43.921296: validation loss: -0.7675
2022-08-02 09:33:43.932706: Average global foreground Dice: [0.8187]
2022-08-02 09:33:43.949132: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:33:44.597183: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-02 09:33:44.605814: This epoch took 122.990048 s

2022-08-02 09:33:44.634746: 
epoch:  455
2022-08-02 09:35:33.565288: train loss : -0.8332
2022-08-02 09:35:42.373715: validation loss: -0.7730
2022-08-02 09:35:42.381776: Average global foreground Dice: [0.8242]
2022-08-02 09:35:42.384191: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:35:42.897072: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-02 09:35:42.899471: This epoch took 118.232680 s

2022-08-02 09:35:42.901535: 
epoch:  456
2022-08-02 09:37:29.716604: train loss : -0.8390
2022-08-02 09:37:36.505144: validation loss: -0.7729
2022-08-02 09:37:36.512123: Average global foreground Dice: [0.8246]
2022-08-02 09:37:36.514689: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:37:37.004291: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-02 09:37:37.007138: This epoch took 114.103494 s

2022-08-02 09:37:37.009760: 
epoch:  457
2022-08-02 09:39:28.021602: train loss : -0.8361
2022-08-02 09:39:35.795163: validation loss: -0.7536
2022-08-02 09:39:35.831414: Average global foreground Dice: [0.8025]
2022-08-02 09:39:35.858812: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:39:36.501737: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-02 09:39:36.511757: This epoch took 119.499409 s

2022-08-02 09:39:36.526041: 
epoch:  458
2022-08-02 09:41:23.832222: train loss : -0.8336
2022-08-02 09:41:32.084516: validation loss: -0.7700
2022-08-02 09:41:32.088621: Average global foreground Dice: [0.8222]
2022-08-02 09:41:32.091207: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:41:32.586792: Suus1 maybe_update_lr lr: 1.1e-05
2022-08-02 09:41:32.589895: This epoch took 116.053145 s

2022-08-02 09:41:32.592534: 
epoch:  459
2022-08-02 09:43:21.854571: train loss : -0.8294
2022-08-02 09:43:29.545795: validation loss: -0.7577
2022-08-02 09:43:29.549503: Average global foreground Dice: [0.8112]
2022-08-02 09:43:29.552413: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:43:30.063385: Suus1 maybe_update_lr lr: 1e-05
2022-08-02 09:43:30.066314: This epoch took 117.471347 s

2022-08-02 09:43:30.068832: 
epoch:  460
2022-08-02 09:45:20.880150: train loss : -0.8339
2022-08-02 09:45:29.427335: validation loss: -0.7757
2022-08-02 09:45:29.462400: Average global foreground Dice: [0.8301]
2022-08-02 09:45:29.496604: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:45:30.269950: Suus1 maybe_update_lr lr: 1e-05
2022-08-02 09:45:30.319121: This epoch took 120.248036 s

2022-08-02 09:45:30.351763: 
epoch:  461
2022-08-02 09:47:16.188495: train loss : -0.8384
2022-08-02 09:47:24.871454: validation loss: -0.7724
2022-08-02 09:47:24.881191: Average global foreground Dice: [0.8209]
2022-08-02 09:47:24.892912: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:47:25.519940: Suus1 maybe_update_lr lr: 1e-05
2022-08-02 09:47:25.553179: This epoch took 115.179401 s

2022-08-02 09:47:25.590878: 
epoch:  462
2022-08-02 09:49:13.223808: train loss : -0.8319
2022-08-02 09:49:20.459476: validation loss: -0.7263
2022-08-02 09:49:20.463630: Average global foreground Dice: [0.7867]
2022-08-02 09:49:20.466836: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:49:21.027950: Suus1 maybe_update_lr lr: 1e-05
2022-08-02 09:49:21.034686: This epoch took 115.417656 s

2022-08-02 09:49:21.037609: 
epoch:  463
2022-08-02 09:51:10.051539: train loss : -0.8332
2022-08-02 09:51:16.658600: validation loss: -0.7872
2022-08-02 09:51:16.663001: Average global foreground Dice: [0.8355]
2022-08-02 09:51:16.665839: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:51:17.138963: Suus1 maybe_update_lr lr: 9e-06
2022-08-02 09:51:17.142204: This epoch took 116.102204 s

2022-08-02 09:51:17.145244: 
epoch:  464
2022-08-02 09:53:05.642438: train loss : -0.8346
2022-08-02 09:53:12.716863: validation loss: -0.7485
2022-08-02 09:53:12.721377: Average global foreground Dice: [0.8083]
2022-08-02 09:53:12.730324: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:53:13.288514: Suus1 maybe_update_lr lr: 9e-06
2022-08-02 09:53:13.311814: This epoch took 116.162771 s

2022-08-02 09:53:13.338769: 
epoch:  465
2022-08-02 09:54:59.746806: train loss : -0.8361
2022-08-02 09:55:06.997093: validation loss: -0.7617
2022-08-02 09:55:07.000990: Average global foreground Dice: [0.8151]
2022-08-02 09:55:07.006465: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:55:07.528971: Suus1 maybe_update_lr lr: 9e-06
2022-08-02 09:55:07.531840: This epoch took 114.162059 s

2022-08-02 09:55:07.534268: 
epoch:  466
2022-08-02 09:56:56.953714: train loss : -0.8325
2022-08-02 09:57:04.627205: validation loss: -0.7639
2022-08-02 09:57:04.630618: Average global foreground Dice: [0.8128]
2022-08-02 09:57:04.633332: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:57:05.112434: Suus1 maybe_update_lr lr: 9e-06
2022-08-02 09:57:05.115644: This epoch took 117.579308 s

2022-08-02 09:57:05.117753: 
epoch:  467
2022-08-02 09:58:53.255887: train loss : -0.8306
2022-08-02 09:59:00.275002: validation loss: -0.7617
2022-08-02 09:59:00.278273: Average global foreground Dice: [0.8138]
2022-08-02 09:59:00.280388: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 09:59:00.793358: Suus1 maybe_update_lr lr: 8e-06
2022-08-02 09:59:00.795957: This epoch took 115.675816 s

2022-08-02 09:59:00.798103: 
epoch:  468
2022-08-02 10:00:51.265721: train loss : -0.8356
2022-08-02 10:01:00.192877: validation loss: -0.7548
2022-08-02 10:01:00.241423: Average global foreground Dice: [0.8029]
2022-08-02 10:01:00.256745: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:01:00.878520: Suus1 maybe_update_lr lr: 8e-06
2022-08-02 10:01:00.910866: This epoch took 120.110719 s

2022-08-02 10:01:00.952797: 
epoch:  469
2022-08-02 10:02:50.088781: train loss : -0.8361
2022-08-02 10:02:57.618233: validation loss: -0.7605
2022-08-02 10:02:57.621838: Average global foreground Dice: [0.8085]
2022-08-02 10:02:57.624675: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:02:58.168205: Suus1 maybe_update_lr lr: 8e-06
2022-08-02 10:02:58.197236: This epoch took 117.225483 s

2022-08-02 10:02:58.219913: 
epoch:  470
2022-08-02 10:04:47.775835: train loss : -0.8333
2022-08-02 10:04:56.438554: validation loss: -0.7650
2022-08-02 10:04:56.455704: Average global foreground Dice: [0.8194]
2022-08-02 10:04:56.480740: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:04:57.076346: Suus1 maybe_update_lr lr: 8e-06
2022-08-02 10:04:57.102226: This epoch took 118.859412 s

2022-08-02 10:04:57.133769: 
epoch:  471
2022-08-02 10:06:47.025152: train loss : -0.8332
2022-08-02 10:06:54.693011: validation loss: -0.7766
2022-08-02 10:06:54.700831: Average global foreground Dice: [0.8275]
2022-08-02 10:06:54.706438: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:06:55.248734: Suus1 maybe_update_lr lr: 7e-06
2022-08-02 10:06:55.251643: This epoch took 118.087268 s

2022-08-02 10:06:55.254020: 
epoch:  472
2022-08-02 10:08:43.191778: train loss : -0.8364
2022-08-02 10:08:49.936629: validation loss: -0.7608
2022-08-02 10:08:49.940222: Average global foreground Dice: [0.8132]
2022-08-02 10:08:49.942623: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:08:50.412817: Suus1 maybe_update_lr lr: 7e-06
2022-08-02 10:08:50.415860: This epoch took 115.159417 s

2022-08-02 10:08:50.418462: 
epoch:  473
2022-08-02 10:10:40.436547: train loss : -0.8351
2022-08-02 10:10:49.771056: validation loss: -0.7786
2022-08-02 10:10:49.796756: Average global foreground Dice: [0.8271]
2022-08-02 10:10:49.834561: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:10:50.468271: Suus1 maybe_update_lr lr: 7e-06
2022-08-02 10:10:50.484004: This epoch took 120.062881 s

2022-08-02 10:10:50.503996: 
epoch:  474
2022-08-02 10:12:38.863892: train loss : -0.8388
2022-08-02 10:12:47.571908: validation loss: -0.7638
2022-08-02 10:12:47.585664: Average global foreground Dice: [0.8156]
2022-08-02 10:12:47.596691: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:12:48.266342: Suus1 maybe_update_lr lr: 7e-06
2022-08-02 10:12:48.269228: This epoch took 117.748460 s

2022-08-02 10:12:48.271388: 
epoch:  475
2022-08-02 10:14:36.622550: train loss : -0.8373
2022-08-02 10:14:44.063294: validation loss: -0.7637
2022-08-02 10:14:44.088639: Average global foreground Dice: [0.8153]
2022-08-02 10:14:44.108261: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:14:45.169674: Suus1 maybe_update_lr lr: 7e-06
2022-08-02 10:14:45.184067: This epoch took 116.910137 s

2022-08-02 10:14:45.229060: 
epoch:  476
2022-08-02 10:16:38.724347: train loss : -0.8349
2022-08-02 10:16:47.041785: validation loss: -0.7562
2022-08-02 10:16:47.045047: Average global foreground Dice: [0.8091]
2022-08-02 10:16:47.047235: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:16:47.687602: Suus1 maybe_update_lr lr: 6e-06
2022-08-02 10:16:47.712810: This epoch took 122.450016 s

2022-08-02 10:16:47.741334: 
epoch:  477
2022-08-02 10:18:35.953740: train loss : -0.8363
2022-08-02 10:18:45.232583: validation loss: -0.7705
2022-08-02 10:18:45.249416: Average global foreground Dice: [0.823]
2022-08-02 10:18:45.264084: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:18:45.766316: Suus1 maybe_update_lr lr: 6e-06
2022-08-02 10:18:45.806855: This epoch took 118.037782 s

2022-08-02 10:18:45.813309: 
epoch:  478
2022-08-02 10:20:32.389208: train loss : -0.8365
2022-08-02 10:20:40.121920: validation loss: -0.7503
2022-08-02 10:20:40.124463: Average global foreground Dice: [0.8123]
2022-08-02 10:20:40.137184: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:20:40.736233: Suus1 maybe_update_lr lr: 6e-06
2022-08-02 10:20:40.738929: This epoch took 114.898082 s

2022-08-02 10:20:40.741562: 
epoch:  479
2022-08-02 10:22:29.762935: train loss : -0.8332
2022-08-02 10:22:39.765697: validation loss: -0.7680
2022-08-02 10:22:39.774453: Average global foreground Dice: [0.8245]
2022-08-02 10:22:39.802940: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:22:41.004617: Suus1 maybe_update_lr lr: 6e-06
2022-08-02 10:22:41.017900: This epoch took 120.272447 s

2022-08-02 10:22:41.023009: 
epoch:  480
2022-08-02 10:24:32.058446: train loss : -0.8339
2022-08-02 10:24:40.167199: validation loss: -0.7387
2022-08-02 10:24:40.171582: Average global foreground Dice: [0.7894]
2022-08-02 10:24:40.174205: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:24:40.882652: Suus1 maybe_update_lr lr: 5e-06
2022-08-02 10:24:40.885646: This epoch took 119.844195 s

2022-08-02 10:24:40.888404: 
epoch:  481
2022-08-02 10:26:28.524306: train loss : -0.8371
2022-08-02 10:26:35.217851: validation loss: -0.7576
2022-08-02 10:26:35.221319: Average global foreground Dice: [0.8182]
2022-08-02 10:26:35.223678: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:26:35.715249: Suus1 maybe_update_lr lr: 5e-06
2022-08-02 10:26:35.718487: This epoch took 114.827495 s

2022-08-02 10:26:35.720824: 
epoch:  482
2022-08-02 10:28:25.621461: train loss : -0.8355
2022-08-02 10:28:34.761160: validation loss: -0.7709
2022-08-02 10:28:34.787299: Average global foreground Dice: [0.818]
2022-08-02 10:28:34.830762: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:28:35.950848: Suus1 maybe_update_lr lr: 5e-06
2022-08-02 10:28:35.973914: This epoch took 120.250510 s

2022-08-02 10:28:35.986761: 
epoch:  483
2022-08-02 10:30:27.494398: train loss : -0.8391
2022-08-02 10:30:38.814756: validation loss: -0.7625
2022-08-02 10:30:38.845751: Average global foreground Dice: [0.8125]
2022-08-02 10:30:38.848222: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:30:39.666967: Suus1 maybe_update_lr lr: 5e-06
2022-08-02 10:30:39.710175: This epoch took 123.714443 s

2022-08-02 10:30:39.755751: 
epoch:  484
2022-08-02 10:32:27.143150: train loss : -0.8361
2022-08-02 10:32:34.687471: validation loss: -0.7730
2022-08-02 10:32:34.691553: Average global foreground Dice: [0.8271]
2022-08-02 10:32:34.694242: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:32:35.224194: Suus1 maybe_update_lr lr: 4e-06
2022-08-02 10:32:35.227267: This epoch took 115.417518 s

2022-08-02 10:32:35.229542: 
epoch:  485
2022-08-02 10:34:25.736193: train loss : -0.8346
2022-08-02 10:34:34.179983: validation loss: -0.7734
2022-08-02 10:34:34.227294: Average global foreground Dice: [0.8262]
2022-08-02 10:34:34.252399: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:34:35.222220: Suus1 maybe_update_lr lr: 4e-06
2022-08-02 10:34:35.245937: This epoch took 120.014102 s

2022-08-02 10:34:35.269870: 
epoch:  486
2022-08-02 10:36:22.861611: train loss : -0.8404
2022-08-02 10:36:29.899592: validation loss: -0.7777
2022-08-02 10:36:29.904921: Average global foreground Dice: [0.8265]
2022-08-02 10:36:29.910164: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:36:30.401027: Suus1 maybe_update_lr lr: 4e-06
2022-08-02 10:36:30.403911: This epoch took 115.113136 s

2022-08-02 10:36:30.406373: 
epoch:  487
2022-08-02 10:38:17.650175: train loss : -0.8413
2022-08-02 10:38:25.086615: validation loss: -0.7672
2022-08-02 10:38:25.090248: Average global foreground Dice: [0.8152]
2022-08-02 10:38:25.092606: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:38:25.668186: Suus1 maybe_update_lr lr: 3e-06
2022-08-02 10:38:25.676556: This epoch took 115.267632 s

2022-08-02 10:38:25.679667: 
epoch:  488
2022-08-02 10:40:12.329138: train loss : -0.8405
2022-08-02 10:40:19.764458: validation loss: -0.7569
2022-08-02 10:40:19.767876: Average global foreground Dice: [0.8117]
2022-08-02 10:40:19.770088: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:40:20.273057: Suus1 maybe_update_lr lr: 3e-06
2022-08-02 10:40:20.275508: This epoch took 114.593335 s

2022-08-02 10:40:20.277997: 
epoch:  489
2022-08-02 10:42:07.764173: train loss : -0.8393
2022-08-02 10:42:14.861185: validation loss: -0.7656
2022-08-02 10:42:14.872569: Average global foreground Dice: [0.8225]
2022-08-02 10:42:14.877373: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:42:15.397574: Suus1 maybe_update_lr lr: 3e-06
2022-08-02 10:42:15.400182: This epoch took 115.119762 s

2022-08-02 10:42:15.402523: 
epoch:  490
2022-08-02 10:44:04.467269: train loss : -0.8399
2022-08-02 10:44:12.063483: validation loss: -0.7760
2022-08-02 10:44:12.098810: Average global foreground Dice: [0.8269]
2022-08-02 10:44:12.143794: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:44:12.679633: Suus1 maybe_update_lr lr: 3e-06
2022-08-02 10:44:12.682246: This epoch took 117.277210 s

2022-08-02 10:44:12.684671: 
epoch:  491
2022-08-02 10:46:00.770272: train loss : -0.8343
2022-08-02 10:46:07.823468: validation loss: -0.7471
2022-08-02 10:46:07.853626: Average global foreground Dice: [0.8022]
2022-08-02 10:46:07.883524: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:46:08.528248: Suus1 maybe_update_lr lr: 2e-06
2022-08-02 10:46:08.531126: This epoch took 115.844172 s

2022-08-02 10:46:08.533189: 
epoch:  492
2022-08-02 10:47:55.761358: train loss : -0.8378
2022-08-02 10:48:02.929714: validation loss: -0.7759
2022-08-02 10:48:02.947815: Average global foreground Dice: [0.8256]
2022-08-02 10:48:02.959955: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:48:03.468807: Suus1 maybe_update_lr lr: 2e-06
2022-08-02 10:48:03.471204: This epoch took 114.935861 s

2022-08-02 10:48:03.473491: 
epoch:  493
2022-08-02 10:49:52.153220: train loss : -0.8370
2022-08-02 10:49:59.638923: validation loss: -0.7704
2022-08-02 10:49:59.642308: Average global foreground Dice: [0.8203]
2022-08-02 10:49:59.644720: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:50:00.125216: Suus1 maybe_update_lr lr: 2e-06
2022-08-02 10:50:00.127609: This epoch took 116.651939 s

2022-08-02 10:50:00.129737: 
epoch:  494
2022-08-02 10:51:50.826699: train loss : -0.8384
2022-08-02 10:51:58.083380: validation loss: -0.7706
2022-08-02 10:51:58.133837: Average global foreground Dice: [0.8251]
2022-08-02 10:51:58.177805: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:51:59.164755: Suus1 maybe_update_lr lr: 2e-06
2022-08-02 10:51:59.196816: This epoch took 119.064840 s

2022-08-02 10:51:59.226297: 
epoch:  495
2022-08-02 10:53:51.757008: train loss : -0.8392
2022-08-02 10:54:01.853959: validation loss: -0.7560
2022-08-02 10:54:01.881746: Average global foreground Dice: [0.802]
2022-08-02 10:54:01.913792: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:54:02.653938: Suus1 maybe_update_lr lr: 1e-06
2022-08-02 10:54:02.721603: This epoch took 123.472564 s

2022-08-02 10:54:02.756583: 
epoch:  496
2022-08-02 10:55:50.612789: train loss : -0.8404
2022-08-02 10:55:57.365838: validation loss: -0.7565
2022-08-02 10:55:57.369900: Average global foreground Dice: [0.8105]
2022-08-02 10:55:57.372437: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:55:57.857179: Suus1 maybe_update_lr lr: 1e-06
2022-08-02 10:55:57.859746: This epoch took 115.070965 s

2022-08-02 10:55:57.862115: 
epoch:  497
2022-08-02 10:57:45.122261: train loss : -0.8395
2022-08-02 10:57:53.809855: validation loss: -0.7882
2022-08-02 10:57:53.852216: Average global foreground Dice: [0.8367]
2022-08-02 10:57:53.894357: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:57:54.735830: Suus1 maybe_update_lr lr: 1e-06
2022-08-02 10:57:54.753848: This epoch took 116.889590 s

2022-08-02 10:57:54.799759: 
epoch:  498
2022-08-02 10:59:43.831185: train loss : -0.8366
2022-08-02 10:59:50.685431: validation loss: -0.7618
2022-08-02 10:59:50.698710: Average global foreground Dice: [0.8117]
2022-08-02 10:59:50.701173: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 10:59:51.206918: Suus1 maybe_update_lr lr: 0.0
2022-08-02 10:59:51.209694: This epoch took 116.392269 s

2022-08-02 10:59:51.214627: 
epoch:  499
2022-08-02 11:01:35.853775: train loss : -0.8433
2022-08-02 11:01:43.724041: validation loss: -0.7805
2022-08-02 11:01:43.754249: Average global foreground Dice: [0.8311]
2022-08-02 11:01:43.788801: (interpret this as an estimate for the Dice of the different classes. This is not exact.)
2022-08-02 11:01:44.504512: Suus1 maybe_update_lr lr: 0.0
2022-08-02 11:01:44.506789: saving scheduled checkpoint file...
2022-08-02 11:01:44.704717: saving checkpoint...
2022-08-02 11:01:50.147808: done, saving took 5.64 seconds
2022-08-02 11:01:50.199062: done
2022-08-02 11:01:50.201669: This epoch took 118.983652 s

2022-08-02 11:01:50.311399: saving checkpoint...
2022-08-02 11:01:55.279932: done, saving took 5.08 seconds
panc_0003 (2, 216, 396, 396)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 80
computing Gaussian
done
prediction done
suus panc_0003 transposed
suus panc_0003 not saving softmax
suus panc_0003 voeg toe aan pred_gt tuples voor later
panc_0018 (2, 223, 582, 582)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
suus panc_0018 transposed
suus panc_0018 not saving softmax
suus panc_0018 voeg toe aan pred_gt tuples voor later
panc_0036 (2, 223, 489, 489)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0036 transposed
suus panc_0036 not saving softmax
suus panc_0036 voeg toe aan pred_gt tuples voor later
panc_0042 (2, 310, 407, 407)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 140
using precomputed Gaussian
prediction done
suus panc_0042 transposed
suus panc_0042 not saving softmax
suus panc_0042 voeg toe aan pred_gt tuples voor later
panc_0044 (2, 233, 564, 564)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 564, 564)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 93, 186, 279, 372], [0, 67, 135, 202, 269, 337, 404]]
number of tiles: 175
using precomputed Gaussian
prediction done
suus panc_0044 transposed
suus panc_0044 not saving softmax
suus panc_0044 voeg toe aan pred_gt tuples voor later
panc_0045 (2, 216, 559, 559)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0045 transposed
suus panc_0045 not saving softmax
suus panc_0045 voeg toe aan pred_gt tuples voor later
panc_0050 (2, 310, 419, 419)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
suus panc_0050 transposed
suus panc_0050 not saving softmax
suus panc_0050 voeg toe aan pred_gt tuples voor later
panc_0059 (2, 203, 560, 560)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0059 transposed
suus panc_0059 not saving softmax
suus panc_0059 voeg toe aan pred_gt tuples voor later
panc_0064 (2, 213, 535, 535)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 213, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 100, 133], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0064 transposed
suus panc_0064 not saving softmax
suus panc_0064 voeg toe aan pred_gt tuples voor later
panc_0075 (2, 224, 535, 535)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 108, 144], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0075 transposed
suus panc_0075 not saving softmax
suus panc_0075 voeg toe aan pred_gt tuples voor later
panc_0076 (2, 210, 535, 535)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
suus panc_0076 transposed
suus panc_0076 not saving softmax
suus panc_0076 voeg toe aan pred_gt tuples voor later
panc_0079 (2, 251, 582, 582)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 252
using precomputed Gaussian
prediction done
suus panc_0079 transposed
suus panc_0079 not saving softmax
suus panc_0079 voeg toe aan pred_gt tuples voor later
panc_0080 (2, 181, 582, 582)
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 181, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 101], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 168
using precomputed Gaussian
prediction done
suus panc_0080 transposed
suus panc_0080 not saving softmax
suus panc_0080 voeg toe aan pred_gt tuples voor later
2022-08-02 11:19:56.168695: finished prediction
2022-08-02 11:19:56.176668: evaluation of raw predictions
2022-08-02 11:20:09.258582: determining postprocessing
Foreground vs background
before: 0.8191455079416341
after:  0.8260145099843006
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0045.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0050.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0059.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0018.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0075.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0036.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0076.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0003.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0064.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0042.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0079.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0044.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/validation_raw/panc_0080.nii.gz
done
Done training all the folds! Now start the same command but with continue option, to generate log files


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='501', fold='0', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-02 11:20:42.438702: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-02 11:20:42.452893: The split file contains 5 splits.
2022-08-02 11:20:42.455408: Desired fold for training: 0
2022-08-02 11:20:42.457613: This split has 54 training and 14 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-02 11:20:44.930547: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-02 11:20:45.954711: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-02 11:20:57.893342: Unable to plot network architecture:
2022-08-02 11:20:57.897223: local variable 'g' referenced before assignment
2022-08-02 11:20:57.899426: 
printing the network instead:

2022-08-02 11:20:57.902167: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-02 11:20:57.922540: 

2022-08-02 11:20:58.522480: saving checkpoint...
2022-08-02 11:21:05.150781: done, saving took 7.22 seconds
suus panc_0006 voeg toe aan pred_gt tuples voor later
suus panc_0008 voeg toe aan pred_gt tuples voor later
suus panc_0028 voeg toe aan pred_gt tuples voor later
suus panc_0039 voeg toe aan pred_gt tuples voor later
suus panc_0043 voeg toe aan pred_gt tuples voor later
suus panc_0051 voeg toe aan pred_gt tuples voor later
suus panc_0052 voeg toe aan pred_gt tuples voor later
suus panc_0056 voeg toe aan pred_gt tuples voor later
suus panc_0058 voeg toe aan pred_gt tuples voor later
suus panc_0061 voeg toe aan pred_gt tuples voor later
suus panc_0062 voeg toe aan pred_gt tuples voor later
suus panc_0069 voeg toe aan pred_gt tuples voor later
suus panc_0072 voeg toe aan pred_gt tuples voor later
suus panc_0074 voeg toe aan pred_gt tuples voor later
2022-08-02 11:21:05.828570: finished prediction
2022-08-02 11:21:05.831706: evaluation of raw predictions
2022-08-02 11:21:13.274752: determining postprocessing
Foreground vs background
before: 0.8559273342480184
after:  0.853315300660113
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='501', fold='1', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-02 11:21:35.592679: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-02 11:21:35.606329: The split file contains 5 splits.
2022-08-02 11:21:35.609014: Desired fold for training: 1
2022-08-02 11:21:35.611288: This split has 54 training and 14 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-02 11:21:37.822644: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-02 11:22:50.832607: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-02 11:23:04.966489: Unable to plot network architecture:
2022-08-02 11:23:04.969139: local variable 'g' referenced before assignment
2022-08-02 11:23:04.971575: 
printing the network instead:

2022-08-02 11:23:04.981214: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-02 11:23:05.000464: 

2022-08-02 11:23:05.809223: saving checkpoint...
2022-08-02 11:23:12.466055: done, saving took 7.42 seconds
suus panc_0004 voeg toe aan pred_gt tuples voor later
suus panc_0011 voeg toe aan pred_gt tuples voor later
suus panc_0015 voeg toe aan pred_gt tuples voor later
suus panc_0024 voeg toe aan pred_gt tuples voor later
suus panc_0027 voeg toe aan pred_gt tuples voor later
suus panc_0030 voeg toe aan pred_gt tuples voor later
suus panc_0031 voeg toe aan pred_gt tuples voor later
suus panc_0032 voeg toe aan pred_gt tuples voor later
suus panc_0035 voeg toe aan pred_gt tuples voor later
suus panc_0037 voeg toe aan pred_gt tuples voor later
suus panc_0048 voeg toe aan pred_gt tuples voor later
suus panc_0053 voeg toe aan pred_gt tuples voor later
suus panc_0054 voeg toe aan pred_gt tuples voor later
suus panc_0055 voeg toe aan pred_gt tuples voor later
2022-08-02 11:23:13.113352: finished prediction
2022-08-02 11:23:13.115947: evaluation of raw predictions
2022-08-02 11:23:20.125225: determining postprocessing
Foreground vs background
before: 0.8103730580123047
after:  0.8077987918941627
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='501', fold='2', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-02 11:23:44.375645: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-02 11:23:44.387184: The split file contains 5 splits.
2022-08-02 11:23:44.389969: Desired fold for training: 2
2022-08-02 11:23:44.392581: This split has 54 training and 14 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-02 11:23:46.581460: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-02 11:23:47.595377: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-02 11:24:00.456562: Unable to plot network architecture:
2022-08-02 11:24:00.478733: local variable 'g' referenced before assignment
2022-08-02 11:24:00.512755: 
printing the network instead:

2022-08-02 11:24:00.534771: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-02 11:24:00.562435: 

2022-08-02 11:24:00.937315: saving checkpoint...
2022-08-02 11:24:09.194129: done, saving took 8.60 seconds
suus panc_0002 voeg toe aan pred_gt tuples voor later
suus panc_0005 voeg toe aan pred_gt tuples voor later
suus panc_0007 voeg toe aan pred_gt tuples voor later
suus panc_0019 voeg toe aan pred_gt tuples voor later
suus panc_0023 voeg toe aan pred_gt tuples voor later
suus panc_0029 voeg toe aan pred_gt tuples voor later
suus panc_0038 voeg toe aan pred_gt tuples voor later
suus panc_0040 voeg toe aan pred_gt tuples voor later
suus panc_0063 voeg toe aan pred_gt tuples voor later
suus panc_0066 voeg toe aan pred_gt tuples voor later
suus panc_0067 voeg toe aan pred_gt tuples voor later
suus panc_0068 voeg toe aan pred_gt tuples voor later
suus panc_0071 voeg toe aan pred_gt tuples voor later
suus panc_0078 voeg toe aan pred_gt tuples voor later
2022-08-02 11:24:09.817527: finished prediction
2022-08-02 11:24:09.820077: evaluation of raw predictions
2022-08-02 11:24:15.283245: determining postprocessing
Foreground vs background
before: 0.806589430704717
after:  0.7949016206805453
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='501', fold='3', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-02 11:24:40.138229: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-02 11:24:40.205701: The split file contains 5 splits.
2022-08-02 11:24:40.208834: Desired fold for training: 3
2022-08-02 11:24:40.212802: This split has 55 training and 13 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-02 11:24:42.359792: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-02 11:25:44.327041: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-02 11:25:58.318516: Unable to plot network architecture:
2022-08-02 11:25:58.350760: local variable 'g' referenced before assignment
2022-08-02 11:25:58.370780: 
printing the network instead:

2022-08-02 11:25:58.403781: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-02 11:25:58.428644: 

2022-08-02 11:25:58.993709: saving checkpoint...
2022-08-02 11:26:06.208951: done, saving took 7.76 seconds
suus panc_0010 voeg toe aan pred_gt tuples voor later
suus panc_0012 voeg toe aan pred_gt tuples voor later
suus panc_0013 voeg toe aan pred_gt tuples voor later
suus panc_0014 voeg toe aan pred_gt tuples voor later
suus panc_0016 voeg toe aan pred_gt tuples voor later
suus panc_0020 voeg toe aan pred_gt tuples voor later
suus panc_0021 voeg toe aan pred_gt tuples voor later
suus panc_0022 voeg toe aan pred_gt tuples voor later
suus panc_0034 voeg toe aan pred_gt tuples voor later
suus panc_0046 voeg toe aan pred_gt tuples voor later
suus panc_0047 voeg toe aan pred_gt tuples voor later
suus panc_0060 voeg toe aan pred_gt tuples voor later
suus panc_0077 voeg toe aan pred_gt tuples voor later
2022-08-02 11:26:06.743907: finished prediction
2022-08-02 11:26:06.746380: evaluation of raw predictions
2022-08-02 11:26:13.448806: determining postprocessing
Foreground vs background
before: 0.839060037651967
after:  0.8405487532950637
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
done


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Suus0 - run_training. Args:
Namespace(network='3d_fullres', network_trainer='nnUNetTrainerV2_Hybrid2', task='501', fold='4', validation_only=False, continue_training=True, p='nnUNetPlansv2.1', use_compressed_data=False, deterministic=False, npz=False, find_lr=False, valbest=False, fp32=False, val_folder='validation_raw', disable_saving=False, disable_postprocessing_on_folds=False, val_disable_overwrite=False, disable_next_stage_pred=False, pretrained_weights=None)
###############################################
I am running the following nnUNet: 3d_fullres
My trainer class is:  <class 'nnunet.training.network_training.nnUNetTrainerV2_Hybrid2.nnUNetTrainerV2_Hybrid2'>
For that I will be using the following configuration:
num_classes:  1
modalities:  {0: 'CT'}
use_mask_for_norm OrderedDict([(0, False)])
keep_only_largest_region None
min_region_size_per_class None
min_size_per_class None
normalization_schemes OrderedDict([(0, 'CT')])
stages...

stage:  0
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([120, 285, 285]), 'current_spacing': array([1.7987096 , 1.54576606, 1.54576606]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

stage:  1
{'batch_size': 2, 'num_pool_per_axis': [4, 5, 5], 'patch_size': array([ 80, 192, 160]), 'median_patient_size_in_voxels': array([216, 512, 512]), 'current_spacing': array([1.      , 0.859375, 0.859375]), 'original_spacing': array([1.      , 0.859375, 0.859375]), 'do_dummy_2D_data_aug': False, 'pool_op_kernel_sizes': [[2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]}

I am using stage 1 from these plans
I am using batch dice + CE loss

I am using data from this folder:  /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/nnUNetData_plans_v2.1
###############################################
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
loading dataset
loading all case properties
2022-08-02 11:26:38.431746: Using splits from existing split file: /exports/lkeb-hpc/smaijer/data/nnUNet_preprocessed/Task501/splits_final.pkl
2022-08-02 11:26:38.441653: The split file contains 5 splits.
2022-08-02 11:26:38.444175: Desired fold for training: 4
2022-08-02 11:26:38.446347: This split has 55 training and 13 validation cases.
unpacking dataset
done
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
SuusA - Load checkpoint (final, latest, best)
2022-08-02 11:26:40.669528: loading checkpoint /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model train= True
SuusB run_training - zet learning rate als  
2022-08-02 11:27:59.768959: Suus1 maybe_update_lr lr: 0.0
SuusC - run_training!
using pin_memory on device 0
using pin_memory on device 0
Suus for now disable cause it breaks the logs
2022-08-02 11:28:16.462688: Unable to plot network architecture:
2022-08-02 11:28:16.465853: local variable 'g' referenced before assignment
2022-08-02 11:28:16.468202: 
printing the network instead:

2022-08-02 11:28:16.470491: Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
2022-08-02 11:28:16.484256: 

2022-08-02 11:28:17.163272: saving checkpoint...
2022-08-02 11:28:23.139533: done, saving took 6.62 seconds
suus panc_0003 voeg toe aan pred_gt tuples voor later
suus panc_0018 voeg toe aan pred_gt tuples voor later
suus panc_0036 voeg toe aan pred_gt tuples voor later
suus panc_0042 voeg toe aan pred_gt tuples voor later
suus panc_0044 voeg toe aan pred_gt tuples voor later
suus panc_0045 voeg toe aan pred_gt tuples voor later
suus panc_0050 voeg toe aan pred_gt tuples voor later
suus panc_0059 voeg toe aan pred_gt tuples voor later
suus panc_0064 voeg toe aan pred_gt tuples voor later
suus panc_0075 voeg toe aan pred_gt tuples voor later
suus panc_0076 voeg toe aan pred_gt tuples voor later
suus panc_0079 voeg toe aan pred_gt tuples voor later
suus panc_0080 voeg toe aan pred_gt tuples voor later
2022-08-02 11:28:23.484075: finished prediction
2022-08-02 11:28:23.487411: evaluation of raw predictions
2022-08-02 11:28:29.725731: determining postprocessing
Foreground vs background
before: 0.8191455079416341
after:  0.8260145099843006
Removing all but the largest foreground region improved results!
for_which_classes [1]
min_valid_object_sizes None
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[[1]]
min_object_sizes
None
done
Start postprocessing..


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Foreground vs background
before: 0.8261342590327556
after:  0.8242579765870475
Only one class present, no need to do each class separately as this is covered in fg vs bg
done
for which classes:
[]
min_object_sizes
None
done
Done postprocessing! Now start inferencing its own train and test files.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

using model stored in  /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1
This model expects 1 input modalities for each image
Found 68 unique case ids, here are some examples: ['panc_0028' 'panc_0054' 'panc_0042' 'panc_0043' 'panc_0004' 'panc_0051'
 'panc_0050' 'panc_0045' 'panc_0067' 'panc_0038']
If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc
number of cases: 68
number of cases that still need to be predicted: 68
emptying cuda cache
loading parameters for folds, None
folds is None so we will automatically look for output folders (not using 'all'!)
found the following folds:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4']
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus5 - zet de plans properties
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
using the following model files:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']
starting preprocessing generator
starting prediction...
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0003.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 80
computing Gaussian
done
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0005.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0002.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 195, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 120
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0007.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0004.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0006.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0010.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 553, 553)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 90, 180, 271, 361], [0, 79, 157, 236, 314, 393]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 553, 553)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 90, 180, 271, 361], [0, 79, 157, 236, 314, 393]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 553, 553)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 90, 180, 271, 361], [0, 79, 157, 236, 314, 393]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 553, 553)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 90, 180, 271, 361], [0, 79, 157, 236, 314, 393]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 553, 553)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 90, 180, 271, 361], [0, 79, 157, 236, 314, 393]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0012.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 214, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 100, 134], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 214, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 100, 134], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 214, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 100, 134], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 214, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 100, 134], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 214, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 100, 134], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0008.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0014.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 231, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 113, 151], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 231, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 113, 151], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 231, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 113, 151], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 231, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 113, 151], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 231, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 113, 151], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0011.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 538, 538)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 173, 260, 346], [0, 76, 151, 227, 302, 378]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 538, 538)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 173, 260, 346], [0, 76, 151, 227, 302, 378]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 538, 538)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 173, 260, 346], [0, 76, 151, 227, 302, 378]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 538, 538)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 173, 260, 346], [0, 76, 151, 227, 302, 378]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 538, 538)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 86, 173, 260, 346], [0, 76, 151, 227, 302, 378]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0013.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 186, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 71, 106], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 186, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 71, 106], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 186, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 71, 106], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 186, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 71, 106], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 186, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 71, 106], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0016.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0019.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0015.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 247, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 67, 100, 134, 167], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 247, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 67, 100, 134, 167], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 247, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 67, 100, 134, 167], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 247, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 67, 100, 134, 167], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 247, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 67, 100, 134, 167], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0021.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 570, 570)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 94, 189, 284, 378], [0, 68, 137, 205, 273, 342, 410]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 570, 570)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 94, 189, 284, 378], [0, 68, 137, 205, 273, 342, 410]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 570, 570)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 94, 189, 284, 378], [0, 68, 137, 205, 273, 342, 410]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 570, 570)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 94, 189, 284, 378], [0, 68, 137, 205, 273, 342, 410]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 570, 570)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 94, 189, 284, 378], [0, 68, 137, 205, 273, 342, 410]]
number of tiles: 175
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0018.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0020.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0023.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0027.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0022.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 112
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 112
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 112
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 112
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 396, 396)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 68, 136, 204], [0, 79, 157, 236]]
number of tiles: 112
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0029.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 199, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 79, 119], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 199, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 79, 119], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 199, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 79, 119], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 199, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 79, 119], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 199, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 79, 119], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0024.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0028.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0031.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0034.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0030.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 547, 547)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 89, 178, 266, 355], [0, 77, 155, 232, 310, 387]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 547, 547)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 89, 178, 266, 355], [0, 77, 155, 232, 310, 387]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 547, 547)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 89, 178, 266, 355], [0, 77, 155, 232, 310, 387]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 547, 547)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 89, 178, 266, 355], [0, 77, 155, 232, 310, 387]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 547, 547)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 89, 178, 266, 355], [0, 77, 155, 232, 310, 387]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0036.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0032.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0035.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 227, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 110, 147], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 227, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 110, 147], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 227, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 110, 147], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 227, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 110, 147], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 227, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 110, 147], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0038.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 205, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 94, 125], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0040.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 120
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0037.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0043.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0039.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0042.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0045.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0047.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 489, 489)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 74, 148, 223, 297], [0, 66, 132, 197, 263, 329]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0044.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 564, 564)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 93, 186, 279, 372], [0, 67, 135, 202, 269, 337, 404]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 564, 564)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 93, 186, 279, 372], [0, 67, 135, 202, 269, 337, 404]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 564, 564)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 93, 186, 279, 372], [0, 67, 135, 202, 269, 337, 404]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 564, 564)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 93, 186, 279, 372], [0, 67, 135, 202, 269, 337, 404]]
number of tiles: 175
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 564, 564)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 93, 186, 279, 372], [0, 67, 135, 202, 269, 337, 404]]
number of tiles: 175
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0050.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0046.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 198, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 79, 118], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0048.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0005.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 210, 512, 512) after crop: (1, 210, 512, 512) spacing: [1.         0.85799998 0.85799998] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.85799998, 0.85799998]), 'spacing_transposed': array([1.        , 0.85799998, 0.85799998]), 'data.shape (data is transposed)': (1, 210, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 210, 511, 511)} 

(1, 210, 511, 511)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0012.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 214, 512, 512) after crop: (1, 214, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 214, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 214, 582, 582)} 

(1, 214, 582, 582)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0019.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 210, 512, 512) after crop: (1, 210, 512, 512) spacing: [1.        0.8984375 0.8984375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([1.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 210, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 210, 535, 535)} 

(1, 210, 535, 535)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0027.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 205, 512, 512) after crop: (1, 205, 512, 512) spacing: [1.        0.8203125 0.8203125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8203125, 0.8203125]), 'spacing_transposed': array([1.       , 0.8203125, 0.8203125]), 'data.shape (data is transposed)': (1, 205, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 205, 489, 489)} 

(1, 205, 489, 489)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0034.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 205, 512, 512) after crop: (1, 205, 512, 512) spacing: [1.         0.78200001 0.78200001] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.78200001, 0.78200001]), 'spacing_transposed': array([1.        , 0.78200001, 0.78200001]), 'data.shape (data is transposed)': (1, 205, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 205, 466, 466)} 

(1, 205, 466, 466)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0040.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 251, 512, 512) after crop: (1, 251, 512, 512) spacing: [1.      0.78125 0.78125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.     , 0.78125, 0.78125]), 'spacing_transposed': array([1.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 251, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 251, 465, 465)} 

(1, 251, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0047.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 201, 512, 512) after crop: (1, 201, 512, 512) spacing: [1.        0.8203125 0.8203125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8203125, 0.8203125]), 'spacing_transposed': array([1.       , 0.8203125, 0.8203125]), 'data.shape (data is transposed)': (1, 201, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 201, 489, 489)} 

(1, 201, 489, 489)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0054.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 218, 512, 512) after crop: (1, 218, 512, 512) spacing: [1.       0.859375 0.859375] 

no resampling necessary
no resampling necessary
before: {'spacing': array([1.      , 0.859375, 0.859375]), 'spacing_transposed': array([1.      , 0.859375, 0.859375]), 'data.shape (data is transposed)': (1, 218, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 218, 512, 512)} 

(1, 218, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0061.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 223, 512, 512) after crop: (1, 223, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 223, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 223, 559, 559)} 

(1, 223, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0068.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 206, 512, 512) after crop: (1, 206, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 206, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 206, 582, 582)} 

(1, 206, 582, 582)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0076.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 210, 512, 512) after crop: (1, 210, 512, 512) spacing: [1.        0.8984375 0.8984375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([1.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 210, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 210, 535, 535)} 

(1, 210, 535, 535)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0007.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 201, 512, 512) after crop: (1, 201, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 201, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 201, 559, 559)} 

(1, 201, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0014.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 231, 512, 512) after crop: (1, 231, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 231, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 231, 559, 559)} 

(1, 231, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0021.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 223, 512, 512) after crop: (1, 223, 512, 512) spacing: [1.         0.95703125 0.95703125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.95703125, 0.95703125]), 'spacing_transposed': array([1.        , 0.95703125, 0.95703125]), 'data.shape (data is transposed)': (1, 223, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 223, 570, 570)} 

(1, 223, 570, 570)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0029.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 199, 512, 512) after crop: (1, 199, 512, 512) spacing: [1.      0.78125 0.78125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.     , 0.78125, 0.78125]), 'spacing_transposed': array([1.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 199, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 199, 465, 465)} 

(1, 199, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0036.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 223, 512, 512) after crop: (1, 223, 512, 512) spacing: [1.        0.8203125 0.8203125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8203125, 0.8203125]), 'spacing_transposed': array([1.       , 0.8203125, 0.8203125]), 'data.shape (data is transposed)': (1, 223, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 223, 489, 489)} 

(1, 223, 489, 489)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0043.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 203, 512, 512) after crop: (1, 203, 512, 512) spacing: [1.         0.78200001 0.78200001] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.78200001, 0.78200001]), 'spacing_transposed': array([1.        , 0.78200001, 0.78200001]), 'data.shape (data is transposed)': (1, 203, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 203, 466, 466)} 

(1, 203, 466, 466)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0050.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.       0.703125 0.703125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.      , 0.703125, 0.703125]), 'spacing_transposed': array([1.      , 0.703125, 0.703125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 419, 419)} 

(1, 310, 419, 419)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0056.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 215, 512, 512) after crop: (1, 215, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 215, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 215, 582, 582)} 

(1, 215, 582, 582)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0063.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.         0.74218798 0.74218798] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.74218798, 0.74218798]), 'spacing_transposed': array([1.        , 0.74218798, 0.74218798]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 442, 442)} 

(1, 310, 442, 442)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0071.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 194, 512, 512) after crop: (1, 194, 512, 512) spacing: [1.       0.890625 0.890625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.      , 0.890625, 0.890625]), 'spacing_transposed': array([1.      , 0.890625, 0.890625]), 'data.shape (data is transposed)': (1, 194, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 194, 531, 531)} 

(1, 194, 531, 531)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0078.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.      0.78125 0.78125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.     , 0.78125, 0.78125]), 'spacing_transposed': array([1.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 465, 465)} 

(1, 310, 465, 465)
This worker has ended successfully, no errors to report
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0052.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0054.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 218, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 69, 104, 138], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0051.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 185, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 105], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 185, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 105], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 185, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 105], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 185, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 105], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 185, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 105], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0056.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 101, 135], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 101, 135], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 101, 135], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 101, 135], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 215, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 101, 135], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0053.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0055.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 193, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 75, 113], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 193, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 75, 113], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 193, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 75, 113], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 193, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 75, 113], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 80
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 193, 407, 407)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 75, 113], [0, 72, 143, 215], [0, 62, 124, 185, 247]]
number of tiles: 80
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0059.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 203, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 92, 123], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0061.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 223, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 107, 143], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0058.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 217, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 217, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 217, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 217, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 217, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0063.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0060.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0062.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 211, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 98, 131], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 211, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 98, 131], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 211, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 98, 131], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 211, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 98, 131], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 211, 560, 560)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 98, 131], [0, 92, 184, 276, 368], [0, 80, 160, 240, 320, 400]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0066.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 100
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0068.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 206, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 63, 94, 126], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0064.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 213, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 100, 133], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 213, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 100, 133], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 213, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 100, 133], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 213, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 100, 133], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 213, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 33, 66, 100, 133], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0071.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 194, 531, 531)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 114], [0, 85, 170, 254, 339], [0, 74, 148, 223, 297, 371]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 194, 531, 531)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 114], [0, 85, 170, 254, 339], [0, 74, 148, 223, 297, 371]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 194, 531, 531)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 114], [0, 85, 170, 254, 339], [0, 74, 148, 223, 297, 371]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 194, 531, 531)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 114], [0, 85, 170, 254, 339], [0, 74, 148, 223, 297, 371]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 194, 531, 531)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 114], [0, 85, 170, 254, 339], [0, 74, 148, 223, 297, 371]]
number of tiles: 120
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0067.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 221, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 35, 70, 106, 141], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0069.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0074.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 201, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 30, 60, 91, 121], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0076.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 210, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 65, 98, 130], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0072.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 204, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 93, 124], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 204, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 93, 124], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0004.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 221, 512, 512) after crop: (1, 221, 512, 512) spacing: [1.        0.8984375 0.8984375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([1.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 221, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 221, 535, 535)} 

(1, 221, 535, 535)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0011.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 221, 512, 512) after crop: (1, 221, 512, 512) spacing: [1.         0.90234375 0.90234375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.90234375, 0.90234375]), 'spacing_transposed': array([1.        , 0.90234375, 0.90234375]), 'data.shape (data is transposed)': (1, 221, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 221, 538, 538)} 

(1, 221, 538, 538)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0018.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 223, 512, 512) after crop: (1, 223, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 223, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 223, 582, 582)} 

(1, 223, 582, 582)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0024.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.       0.703125 0.703125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.      , 0.703125, 0.703125]), 'spacing_transposed': array([1.      , 0.703125, 0.703125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 419, 419)} 

(1, 310, 419, 419)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0032.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 233, 512, 512) after crop: (1, 233, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 233, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 233, 582, 582)} 

(1, 233, 582, 582)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0039.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 201, 512, 512) after crop: (1, 201, 512, 512) spacing: [1.        0.8984375 0.8984375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([1.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 201, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 201, 535, 535)} 

(1, 201, 535, 535)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0046.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 198, 512, 512) after crop: (1, 198, 512, 512) spacing: [1.         0.78200001 0.78200001] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.78200001, 0.78200001]), 'spacing_transposed': array([1.        , 0.78200001, 0.78200001]), 'data.shape (data is transposed)': (1, 198, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 198, 466, 466)} 

(1, 198, 466, 466)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0053.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.        0.7421875 0.7421875] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.7421875, 0.7421875]), 'spacing_transposed': array([1.       , 0.7421875, 0.7421875]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 442, 442)} 

(1, 310, 442, 442)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0060.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.      0.78125 0.78125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.     , 0.78125, 0.78125]), 'spacing_transposed': array([1.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 465, 465)} 

(1, 310, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0067.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 221, 512, 512) after crop: (1, 221, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 221, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 221, 582, 582)} 

(1, 221, 582, 582)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0075.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 224, 512, 512) after crop: (1, 224, 512, 512) spacing: [1.        0.8984375 0.8984375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([1.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 224, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 224, 535, 535)} 

(1, 224, 535, 535)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0006.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 223, 512, 512) after crop: (1, 223, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 223, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 223, 559, 559)} 

(1, 223, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0013.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 186, 512, 512) after crop: (1, 186, 512, 512) spacing: [1.        0.7421875 0.7421875] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.7421875, 0.7421875]), 'spacing_transposed': array([1.       , 0.7421875, 0.7421875]), 'data.shape (data is transposed)': (1, 186, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 186, 442, 442)} 

(1, 186, 442, 442)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0020.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 218, 512, 512) after crop: (1, 218, 512, 512) spacing: [1.         0.85799998 0.85799998] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.85799998, 0.85799998]), 'spacing_transposed': array([1.        , 0.85799998, 0.85799998]), 'data.shape (data is transposed)': (1, 218, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 218, 511, 511)} 

(1, 218, 511, 511)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0028.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 206, 512, 512) after crop: (1, 206, 512, 512) spacing: [1.        0.8984375 0.8984375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([1.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 206, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 206, 535, 535)} 

(1, 206, 535, 535)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0035.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 227, 512, 512) after crop: (1, 227, 512, 512) spacing: [1.       0.859375 0.859375] 

no resampling necessary
no resampling necessary
before: {'spacing': array([1.      , 0.859375, 0.859375]), 'spacing_transposed': array([1.      , 0.859375, 0.859375]), 'data.shape (data is transposed)': (1, 227, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 227, 512, 512)} 

(1, 227, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0042.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.         0.68359375 0.68359375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.68359375, 0.68359375]), 'spacing_transposed': array([1.        , 0.68359375, 0.68359375]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 407, 407)} 

(1, 310, 407, 407)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0048.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 206, 512, 512) after crop: (1, 206, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 206, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 206, 559, 559)} 

(1, 206, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0055.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 193, 512, 512) after crop: (1, 193, 512, 512) spacing: [1.         0.68359375 0.68359375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.68359375, 0.68359375]), 'spacing_transposed': array([1.        , 0.68359375, 0.68359375]), 'data.shape (data is transposed)': (1, 193, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 193, 407, 407)} 

(1, 193, 407, 407)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0062.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 211, 512, 512) after crop: (1, 211, 512, 512) spacing: [1.         0.93945312 0.93945312] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.93945312, 0.93945312]), 'spacing_transposed': array([1.        , 0.93945312, 0.93945312]), 'data.shape (data is transposed)': (1, 211, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 211, 560, 560)} 

(1, 211, 560, 560)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0069.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 209, 512, 512) after crop: (1, 209, 512, 512) spacing: [1.       0.859375 0.859375] 

no resampling necessary
no resampling necessary
before: {'spacing': array([1.      , 0.859375, 0.859375]), 'spacing_transposed': array([1.      , 0.859375, 0.859375]), 'data.shape (data is transposed)': (1, 209, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 209, 512, 512)} 

(1, 209, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0077.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 233, 512, 512) after crop: (1, 233, 512, 512) spacing: [1.       0.859375 0.859375] 

no resampling necessary
no resampling necessary
before: {'spacing': array([1.      , 0.859375, 0.859375]), 'spacing_transposed': array([1.      , 0.859375, 0.859375]), 'data.shape (data is transposed)': (1, 233, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 233, 512, 512)} 

(1, 233, 512, 512)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0003.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 216, 512, 512) after crop: (1, 216, 512, 512) spacing: [1.        0.6640625 0.6640625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.6640625, 0.6640625]), 'spacing_transposed': array([1.       , 0.6640625, 0.6640625]), 'data.shape (data is transposed)': (1, 216, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 216, 396, 396)} 

(1, 216, 396, 396)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0010.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 206, 512, 512) after crop: (1, 206, 512, 512) spacing: [1.         0.92773438 0.92773438] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.92773438, 0.92773438]), 'spacing_transposed': array([1.        , 0.92773438, 0.92773438]), 'data.shape (data is transposed)': (1, 206, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 206, 553, 553)} 

(1, 206, 553, 553)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0016.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.      0.78125 0.78125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.     , 0.78125, 0.78125]), 'spacing_transposed': array([1.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 465, 465)} 

(1, 310, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0023.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.       0.703125 0.703125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.      , 0.703125, 0.703125]), 'spacing_transposed': array([1.      , 0.703125, 0.703125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 419, 419)} 

(1, 310, 419, 419)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0031.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.      0.78125 0.78125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.     , 0.78125, 0.78125]), 'spacing_transposed': array([1.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 465, 465)} 

(1, 310, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0038.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 205, 512, 512) after crop: (1, 205, 512, 512) spacing: [1.         0.78200001 0.78200001] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.78200001, 0.78200001]), 'spacing_transposed': array([1.        , 0.78200001, 0.78200001]), 'data.shape (data is transposed)': (1, 205, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 205, 466, 466)} 

(1, 205, 466, 466)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0045.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 216, 512, 512) after crop: (1, 216, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 216, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 216, 559, 559)} 

(1, 216, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0052.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 209, 512, 512) after crop: (1, 209, 512, 512) spacing: [1.      0.78125 0.78125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.     , 0.78125, 0.78125]), 'spacing_transposed': array([1.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 209, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 209, 465, 465)} 

(1, 209, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0059.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 203, 512, 512) after crop: (1, 203, 512, 512) spacing: [1.         0.93945312 0.93945312] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.93945312, 0.93945312]), 'spacing_transposed': array([1.        , 0.93945312, 0.93945312]), 'data.shape (data is transposed)': (1, 203, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 203, 560, 560)} 

(1, 203, 560, 560)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0066.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 201, 512, 512) after crop: (1, 201, 512, 512) spacing: [1.      0.78125 0.78125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.     , 0.78125, 0.78125]), 'spacing_transposed': array([1.     , 0.78125, 0.78125]), 'data.shape (data is transposed)': (1, 201, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 201, 465, 465)} 

(1, 201, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0074.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 201, 512, 512) after crop: (1, 201, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 201, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 201, 559, 559)} 

(1, 201, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0080.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 181, 512, 512) after crop: (1, 181, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 181, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 181, 582, 582)} 

(1, 181, 582, 582)
This worker has ended successfully, no errors to report
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0003.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0002.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0004.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0010.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0008.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0011.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0016.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0015.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0018.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0023.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0022.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0024.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0031.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0030.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0032.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0038.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0037.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0039.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0045.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0044.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0046.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0052.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0051.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0053.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0059.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0058.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0060.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0066.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0064.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0067.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0074.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0072.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0075.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0080.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0005.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0007.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0006.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0012.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0014.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0013.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0019.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0021.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0020.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0027.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0029.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0028.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0034.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0036.nii.gz
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0035.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0040.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0043.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0042.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0047.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0050.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0048.nii.gz
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0054.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0056.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0055.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0061.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0063.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0062.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0068.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0071.nii.gz
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0069.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0076.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0078.nii.gz
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0077.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0079.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 204, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 93, 124], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 204, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 93, 124], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 204, 466, 466)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 31, 62, 93, 124], [0, 91, 183, 274], [0, 76, 153, 230, 306]]
number of tiles: 100
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0078.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0075.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 108, 144], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 108, 144], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 108, 144], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 108, 144], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 224, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 72, 108, 144], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0077.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 233, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 76, 115, 153], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0080.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 181, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 101], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 168
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 181, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 101], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 168
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 181, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 101], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 168
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 181, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 101], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 168
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 181, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 67, 101], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 168
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTr/panc_0079.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 252
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 252
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 252
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 252
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 251, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 103, 137, 171], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 252
using precomputed Gaussian
prediction done
inference done. Now waiting for the segmentation export to finish...
postprocessing...


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

using model stored in  /exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1
This model expects 1 input modalities for each image
Found 12 unique case ids, here are some examples: ['panc_0065' 'panc_0033' 'panc_0026' 'panc_0082' 'panc_0026' 'panc_0073'
 'panc_0001' 'panc_0026' 'panc_0033' 'panc_0009']
If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc
number of cases: 12
number of cases that still need to be predicted: 12
emptying cuda cache
loading parameters for folds, None
folds is None so we will automatically look for output folders (not using 'all'!)
found the following folds:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4']
Suus1 - Initialise de NetworkTrainer
Suus2 - Initialise de nnUNetTrainer
Suus3 - Initialise de nnUNetTrainerV2
Suus5 - zet de plans properties
Suus4 - Initialise de trainer echt
Suus5 - zet de plans properties
Suus6 - Zet de data augmentation params
Suus7 - zet deep supervision weights die de meerdere outputs prioriteit geven
Img size: [ 80 192 160]
Patch size: (16, 16, 16)
Feature size: (5, 12, 10)
Suus10 - StackedConvLayers, input: 512 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 256, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 256 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 128, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 128 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 64, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 64 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Suus10 - StackedConvLayers, input: 32 en output: 32, first_stride: None, num_convs: 1, conv_kwargs: {'stride': 1, 'dilation': 1, 'bias': True, 'kernel_size': [3, 3, 3], 'padding': [1, 1, 1]}
Hybrid(
  (encoder): UNETREncoder(
    (vit): ViT(
      (patch_embedding): PatchEmbeddingBlock(
        (patch_embeddings): Sequential(
          (0): Rearrange('b c (h p1) (w p2) (d p3) -> b (h w d) (p1 p2 p3 c)', p1=16, p2=16, p3=16)
          (1): Linear(in_features=4096, out_features=768, bias=True)
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (blocks): ModuleList(
        (0): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerBlock(
          (mlp): MLPBlock(
            (linear1): Linear(in_features=768, out_features=3072, bias=True)
            (linear2): Linear(in_features=3072, out_features=768, bias=True)
            (fn): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): SABlock(
            (out_proj): Linear(in_features=768, out_features=768, bias=True)
            (qkv): Linear(in_features=768, out_features=2304, bias=False)
            (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=12)
            (out_rearrange): Rearrange('b h l d -> b l (h d)')
            (drop_output): Dropout(p=0.0, inplace=False)
            (drop_weights): Dropout(p=0.0, inplace=False)
          )
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (encoder1): UnetrBasicBlock(
      (layer): UnetResBlock(
        (conv1): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (conv2): Convolution(
          (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)
        )
        (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        (norm1): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (norm2): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (conv3): Convolution(
          (conv): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
        )
        (norm3): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (encoder2): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
        (1): Convolution(
          (conv): ConvTranspose3d(64, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder3): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList(
        (0): Convolution(
          (conv): ConvTranspose3d(128, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
        )
      )
    )
    (encoder4): UnetrPrUpBlock(
      (transp_conv_init): Convolution(
        (conv): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
      )
      (blocks): ModuleList()
    )
  )
  (decoder): Generic_UNETDecoder()
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(512, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(256, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(128, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(64, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
            (instnorm): InstanceNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (tu): ModuleList(
    (0): ConvTranspose3d(768, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
    (3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv3d(256, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (1): Conv3d(128, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (2): Conv3d(64, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
    (3): Conv3d(32, 2, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)
  )
)
using the following model files:  ['/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_0/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_1/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_3/model_final_checkpoint.model', '/exports/lkeb-hpc/smaijer/results/nnUNet/3d_fullres/Task501/nnUNetTrainerV2_Hybrid2__nnUNetPlansv2.1/fold_4/model_final_checkpoint.model']
starting preprocessing generator
starting prediction...
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0001.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 240, 512, 512) after crop: (1, 240, 512, 512) spacing: [1.       0.859375 0.859375] 

no resampling necessary
no resampling necessary
before: {'spacing': array([1.      , 0.859375, 0.859375]), 'spacing_transposed': array([1.      , 0.859375, 0.859375]), 'data.shape (data is transposed)': (1, 240, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 240, 512, 512)} 

(1, 240, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0049.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 222, 512, 512) after crop: (1, 222, 512, 512) spacing: [1.        0.8984375 0.8984375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.8984375, 0.8984375]), 'spacing_transposed': array([1.       , 0.8984375, 0.8984375]), 'data.shape (data is transposed)': (1, 222, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 222, 535, 535)} 

(1, 222, 535, 535)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0026.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.       0.859375 0.859375] 

no resampling necessary
no resampling necessary
before: {'spacing': array([1.      , 0.859375, 0.859375]), 'spacing_transposed': array([1.      , 0.859375, 0.859375]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 512, 512)} 

(1, 310, 512, 512)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0073.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.       0.703125 0.703125] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.      , 0.703125, 0.703125]), 'spacing_transposed': array([1.      , 0.703125, 0.703125]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 419, 419)} 

(1, 310, 419, 419)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0009.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 196, 512, 512) after crop: (1, 196, 512, 512) spacing: [1.         0.85799998 0.85799998] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.        , 0.85799998, 0.85799998]), 'spacing_transposed': array([1.        , 0.85799998, 0.85799998]), 'data.shape (data is transposed)': (1, 196, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 196, 511, 511)} 

(1, 196, 511, 511)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0057.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 228, 512, 512) after crop: (1, 228, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 228, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 228, 582, 582)} 

(1, 228, 582, 582)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0033.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.00000012 0.7421875  0.7421875 ] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.00000012, 0.7421875 , 0.7421875 ]), 'spacing_transposed': array([1.00000012, 0.7421875 , 0.7421875 ]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 442, 442)} 

(1, 310, 442, 442)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0081.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 209, 512, 512) after crop: (1, 209, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 209, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 209, 582, 582)} 

(1, 209, 582, 582)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0041.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 237, 512, 512) after crop: (1, 237, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 237, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 237, 559, 559)} 

(1, 237, 559, 559)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0082.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 226, 512, 512) after crop: (1, 226, 512, 512) spacing: [1.     0.9375 0.9375] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.    , 0.9375, 0.9375]), 'spacing_transposed': array([1.    , 0.9375, 0.9375]), 'data.shape (data is transposed)': (1, 226, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 226, 559, 559)} 

(1, 226, 559, 559)
This worker has ended successfully, no errors to report
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0017.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 310, 512, 512) after crop: (1, 310, 512, 512) spacing: [1.00000012 0.78125    0.78125   ] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.00000012, 0.78125   , 0.78125   ]), 'spacing_transposed': array([1.00000012, 0.78125   , 0.78125   ]), 'data.shape (data is transposed)': (1, 310, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 310, 465, 465)} 

(1, 310, 465, 465)
preprocessing /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0065.nii.gz
using preprocessor GenericPreprocessor
before crop: (1, 216, 512, 512) after crop: (1, 216, 512, 512) spacing: [1.        0.9765625 0.9765625] 

no separate z, order 3
no separate z, order 1
before: {'spacing': array([1.       , 0.9765625, 0.9765625]), 'spacing_transposed': array([1.       , 0.9765625, 0.9765625]), 'data.shape (data is transposed)': (1, 216, 512, 512)} 
after:  {'spacing': array([1.      , 0.859375, 0.859375]), 'data.shape (data is resampled)': (1, 216, 582, 582)} 

(1, 216, 582, 582)
This worker has ended successfully, no errors to report
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0001.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0009.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0041.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0049.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0057.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0082.nii.gz
force_separate_z: None interpolation order: 1
no resampling necessary
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0026.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0033.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0017.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0073.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0081.nii.gz
force_separate_z: None interpolation order: 1
separate z: False lowres axis None
no separate z, order 1
suus we gaan schrijven naar /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0065.nii.gz
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0001.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
computing Gaussian
done
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 240, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 40, 80, 120, 160], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0026.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 512, 512)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 80, 160, 240, 320], [0, 70, 141, 211, 282, 352]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0009.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 196, 511, 511)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 77, 116], [0, 80, 160, 239, 319], [0, 70, 140, 211, 281, 351]]
number of tiles: 120
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0033.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 442, 442)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 83, 167, 250], [0, 70, 141, 212, 282]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0041.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 237, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 39, 78, 118, 157], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0017.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 465, 465)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 91, 182, 273], [0, 76, 152, 229, 305]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0049.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 222, 535, 535)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 71, 106, 142], [0, 86, 172, 257, 343], [0, 75, 150, 225, 300, 375]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0073.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 310, 419, 419)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 38, 77, 115, 153, 192, 230], [0, 76, 151, 227], [0, 65, 130, 194, 259]]
number of tiles: 140
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0057.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 228, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 37, 74, 111, 148], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0081.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 209, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 32, 64, 97, 129], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0082.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 226, 559, 559)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 36, 73, 110, 146], [0, 92, 184, 275, 367], [0, 80, 160, 239, 319, 399]]
number of tiles: 150
using precomputed Gaussian
prediction done
predicting /exports/lkeb-hpc/smaijer/output/501/3d_fullres/nnUNetTrainerV2_Hybrid2/501/imagesTs/panc_0065.nii.gz
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
debug: mirroring True mirror_axes (0, 1, 2)
step_size: 0.5
do mirror: True
data shape: (1, 216, 582, 582)
patch size: [ 80 192 160]
steps (x, y, and z): [[0, 34, 68, 102, 136], [0, 78, 156, 234, 312, 390], [0, 70, 141, 211, 281, 352, 422]]
number of tiles: 210
using precomputed Gaussian
prediction done
inference done. Now waiting for the segmentation export to finish...
postprocessing...
Done inferencing! Now start the evaluation.


Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet



Please cite the following paper when using nnUNet:

Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation." Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z


If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet

Program finished with exit code 0 at: Thu Jul 28 18:36:53 CEST 2022
